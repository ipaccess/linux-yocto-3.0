From aca2f234088fb667f25f2aa34febd352472ec9f9 Mon Sep 17 00:00:00 2001
From: Michael van der Westhuizen <michael@smart-africa.com>
Date: Mon, 12 Jan 2015 19:19:27 +0000
Subject: [PATCH] Intel t2k BSP patch

---
 arch/arm/Kconfig                                   |   25 +-
 arch/arm/Makefile                                  |    1 +
 arch/arm/boot/compressed/head.S                    |    9 +-
 arch/arm/common/dmabounce.c                        |   18 +
 arch/arm/common/gic.c                              |   20 +-
 arch/arm/configs/t2200_defconfig                   | 1957 ++++++
 arch/arm/configs/t2200_initramfs_defconfig         | 1984 ++++++
 arch/arm/configs/t4000_x2_defconfig                | 1505 +++++
 arch/arm/configs/t4000_x4_defconfig                | 1505 +++++
 arch/arm/include/asm/dma-mapping.h                 |   46 +
 arch/arm/include/asm/entry-macro-multi.S           |   13 +
 arch/arm/include/asm/fixmap.h                      |   20 +
 arch/arm/include/asm/hardirq.h                     |    6 +-
 arch/arm/include/asm/mach/map.h                    |    2 +
 arch/arm/include/asm/processor.h                   |    3 +
 arch/arm/include/asm/serial.h                      |   10 +-
 arch/arm/include/asm/smp.h                         |    4 +
 arch/arm/include/asm/smp_twd.h                     |    1 +
 arch/arm/include/asm/thread_info.h                 |   57 +
 arch/arm/include/asm/uaccess.h                     |   36 +
 arch/arm/kernel/asm-offsets.c                      |    3 +
 arch/arm/kernel/entry-header.S                     |    3 +
 arch/arm/kernel/irq.c                              |    3 +
 arch/arm/kernel/perf_event.c                       |   16 +-
 arch/arm/kernel/pmu.c                              |    2 +
 arch/arm/kernel/process.c                          |   93 +-
 arch/arm/kernel/setup.c                            |   14 +
 arch/arm/kernel/smp.c                              |   94 +-
 arch/arm/kernel/smp_twd.c                          |    6 +-
 arch/arm/kernel/sys_arm.c                          |   16 +
 arch/arm/kernel/vmlinux.lds.S                      |   14 +
 arch/arm/mach-transcede/Kconfig                    |  244 +
 arch/arm/mach-transcede/Makefile                   |   17 +
 arch/arm/mach-transcede/Makefile.boot              |    3 +
 arch/arm/mach-transcede/clkrst.c                   |  411 ++
 arch/arm/mach-transcede/core.h                     |   30 +
 arch/arm/mach-transcede/ddr_protection.c           |  645 ++
 arch/arm/mach-transcede/headamp.S                  |  113 +
 arch/arm/mach-transcede/headsmp.S                  |   58 +
 arch/arm/mach-transcede/iccom.c                    |  512 ++
 arch/arm/mach-transcede/include/mach/clk-rst.h     |  112 +
 arch/arm/mach-transcede/include/mach/clkrst.h      |  119 +
 .../mach-transcede/include/mach/ddr_protection.h   |   47 +
 arch/arm/mach-transcede/include/mach/debug-macro.S |   54 +
 arch/arm/mach-transcede/include/mach/drv_if.h      |   51 +
 arch/arm/mach-transcede/include/mach/entry-macro.S |  183 +
 arch/arm/mach-transcede/include/mach/exp-bus.h     |   89 +
 arch/arm/mach-transcede/include/mach/gpio-2200.h   |  178 +
 arch/arm/mach-transcede/include/mach/gpio-4000.h   |  138 +
 arch/arm/mach-transcede/include/mach/gpio.h        |   34 +
 arch/arm/mach-transcede/include/mach/hardware.h    |   40 +
 arch/arm/mach-transcede/include/mach/i2c.h         |   60 +
 arch/arm/mach-transcede/include/mach/io.h          |   84 +
 arch/arm/mach-transcede/include/mach/irqs-2200.h   |  151 +
 arch/arm/mach-transcede/include/mach/irqs-4000.h   |  119 +
 arch/arm/mach-transcede/include/mach/irqs.h        |   50 +
 arch/arm/mach-transcede/include/mach/memory.h      |   36 +
 .../arm/mach-transcede/include/mach/mips_monitor.h |   81 +
 arch/arm/mach-transcede/include/mach/mlog.h        |  490 ++
 arch/arm/mach-transcede/include/mach/mmu_protect.h |   82 +
 .../mach-transcede/include/mach/mmu_protect_v7.h   |  145 +
 arch/arm/mach-transcede/include/mach/pcie-t2200.h  |   81 +
 arch/arm/mach-transcede/include/mach/pcie.h        |  180 +
 .../mach-transcede/include/mach/periodic_task.h    |   44 +
 arch/arm/mach-transcede/include/mach/revision.h    |   36 +
 .../arm/mach-transcede/include/mach/serdes-t2200.h |   27 +
 arch/arm/mach-transcede/include/mach/serdes.h      |   51 +
 arch/arm/mach-transcede/include/mach/spa.h         |   67 +
 arch/arm/mach-transcede/include/mach/spi.h         |   70 +
 arch/arm/mach-transcede/include/mach/syscfg.h      |  551 ++
 arch/arm/mach-transcede/include/mach/sysheap.h     |   77 +
 arch/arm/mach-transcede/include/mach/syslib.h      |   92 +
 arch/arm/mach-transcede/include/mach/system.h      |   68 +
 arch/arm/mach-transcede/include/mach/tcb.h         |  589 ++
 arch/arm/mach-transcede/include/mach/timex.h       |   34 +
 .../mach-transcede/include/mach/transcede-2200.h   |  949 +++
 .../mach-transcede/include/mach/transcede-4000.h   |  317 +
 arch/arm/mach-transcede/include/mach/uncompress.h  |   68 +
 arch/arm/mach-transcede/include/mach/usb_mmap.h    | 1018 +++
 arch/arm/mach-transcede/include/mach/vmalloc.h     |   35 +
 arch/arm/mach-transcede/l-arm.c                    |  293 +
 arch/arm/mach-transcede/mips_monitor.c             |  366 +
 arch/arm/mach-transcede/mlog.c                     | 2584 +++++++
 arch/arm/mach-transcede/mmu_protect.c              |  749 +++
 arch/arm/mach-transcede/pcie-t2200.c               | 1427 ++++
 arch/arm/mach-transcede/periodic_task.c            |  545 ++
 arch/arm/mach-transcede/platsmp-t2200.c            |  208 +
 arch/arm/mach-transcede/platsmp-t4000.c            |  187 +
 arch/arm/mach-transcede/platsmp.c                  |   32 +
 arch/arm/mach-transcede/revision.c                 |   76 +
 arch/arm/mach-transcede/serdes-t2200.c             |  292 +
 arch/arm/mach-transcede/sysheap.c                  |  689 ++
 arch/arm/mach-transcede/syslib.c                   |  269 +
 arch/arm/mach-transcede/transcede-2200.c           | 1831 +++++
 arch/arm/mach-transcede/transcede-4000.c           | 1292 ++++
 arch/arm/mach-transcede/u-arm.c                    |  328 +
 arch/arm/mach-transcede/u-arm.h                    |   37 +
 arch/arm/mm/Kconfig                                |    4 +-
 arch/arm/mm/dma-mapping.c                          |    9 +
 arch/arm/mm/fault.c                                |   25 +-
 arch/arm/mm/init.c                                 |    7 +
 arch/arm/mm/ioremap.c                              |    2 +
 arch/arm/mm/mmu.c                                  |   10 +
 arch/arm/mm/proc-v7.S                              |    7 +-
 drivers/char/random.c                              |   67 +
 drivers/crypto/Kconfig                             |   10 +
 drivers/crypto/Makefile                            |    2 +
 drivers/crypto/comcerto_crypto.c                   | 1112 +++
 drivers/crypto/comcerto_crypto.h                   |   92 +
 drivers/crypto/comcerto_espah.c                    |  894 +++
 drivers/crypto/comcerto_espah.h                    |  479 ++
 drivers/crypto/comcerto_types.h                    |   46 +
 drivers/gpio/Makefile                              |    1 +
 drivers/gpio/gpio-t2200.c                          |  211 +
 drivers/i2c/busses/Kconfig                         |   10 +
 drivers/i2c/busses/Makefile                        |    1 +
 drivers/i2c/busses/i2c-transcede.c                 |  666 ++
 drivers/misc/Kconfig                               |    7 +
 drivers/misc/Makefile                              |    1 +
 drivers/misc/transcede_usim.c                      | 1019 +++
 drivers/misc/transcede_usim_regs.h                 |  699 ++
 drivers/misc/transcede_usim_t0_handler.c           |  171 +
 drivers/misc/transcede_usim_t1_handler.c           |  336 +
 drivers/net/Kconfig                                |    2 +
 drivers/net/Makefile                               |    3 +
 drivers/net/transcede/Kconfig                      |   33 +
 drivers/net/transcede/Makefile                     |    7 +
 drivers/net/transcede/api.h                        |   50 +
 drivers/net/transcede/c4000_cpri.c                 |  331 +
 drivers/net/transcede/c4000_cpri.h                 |   48 +
 drivers/net/transcede/c4000_eth.c                  | 2433 +++++++
 drivers/net/transcede/c4000_eth.h                  |  651 ++
 drivers/net/transcede/c4000_gemac.c                |  223 +
 drivers/net/transcede/m84xxx_adm.h                 |  606 ++
 drivers/net/transcede/m84xxx_common.h              |   53 +
 drivers/net/transcede/m84xxx_sch.h                 |  326 +
 drivers/net/transcede/qos_fops.h                   |  390 ++
 drivers/net/transcede/qoscom.c                     | 2498 +++++++
 drivers/net/transcede/qoscom.h                     |   95 +
 drivers/net/transcede/t2200_eth.c                  | 4780 +++++++++++++
 drivers/net/transcede/t2200_eth.h                  | 1782 +++++
 drivers/net/transcede/t3300_reth.c                 |  633 ++
 drivers/net/transcede/t3300_reth.h                 |   96 +
 drivers/net/transcede/t3300_smi.c                  |  501 ++
 drivers/net/transcede/t3300_smi.h                  |  115 +
 drivers/net/transcede/t3300_ved.c                  |  474 ++
 drivers/net/transcede/t3300_ved.h                  |   48 +
 drivers/net/transcede/transcede_ethtool.c          |  430 ++
 drivers/net/transcede/transcede_gem_AL.c           | 1929 ++++++
 drivers/net/transcede/transcede_gemac.h            | 1046 +++
 drivers/net/transcede/transcede_mii.c              |  655 ++
 drivers/net/transcede/transcede_mii.h              |   40 +
 drivers/net/transcede/userio.c                     |  124 +
 drivers/net/transcede/userio.h                     |   75 +
 drivers/net/usb/asix.c                             |  476 +-
 drivers/net/usb/usbnet.c                           |   10 +
 drivers/spi/Kconfig                                |   26 +
 drivers/spi/Makefile                               |    2 +
 drivers/spi/cdce62005.c                            |  501 ++
 drivers/spi/spi_transcede.c                        |  937 +++
 drivers/spi/spidev.c                               |   66 +-
 drivers/tty/serial/8250.c                          |    4 +
 drivers/tty/serial/8250_early.c                    |    2 +
 drivers/usb/Kconfig                                |    2 +
 drivers/usb/Makefile                               |    2 +
 drivers/usb/dwc_otg/Kconfig                        |   37 +
 drivers/usb/dwc_otg/Makefile                       |   30 +
 drivers/usb/dwc_otg/dwc_cc.c                       |  532 ++
 drivers/usb/dwc_otg/dwc_cc.h                       |  225 +
 drivers/usb/dwc_otg/dwc_cfi_common.h               |  142 +
 drivers/usb/dwc_otg/dwc_common_fbsd.c              | 1308 ++++
 drivers/usb/dwc_otg/dwc_common_linux.c             | 1442 ++++
 drivers/usb/dwc_otg/dwc_common_nbsd.c              | 1275 ++++
 drivers/usb/dwc_otg/dwc_crypto.c                   |  308 +
 drivers/usb/dwc_otg/dwc_crypto.h                   |  111 +
 drivers/usb/dwc_otg/dwc_dh.c                       |  291 +
 drivers/usb/dwc_otg/dwc_dh.h                       |  106 +
 drivers/usb/dwc_otg/dwc_list.h                     |  594 ++
 drivers/usb/dwc_otg/dwc_mem.c                      |  245 +
 drivers/usb/dwc_otg/dwc_modpow.c                   |  633 ++
 drivers/usb/dwc_otg/dwc_modpow.h                   |   34 +
 drivers/usb/dwc_otg/dwc_notifier.c                 |  319 +
 drivers/usb/dwc_otg/dwc_notifier.h                 |  122 +
 drivers/usb/dwc_otg/dwc_os.h                       | 1237 ++++
 drivers/usb/dwc_otg/dwc_otg_adp.c                  |  854 +++
 drivers/usb/dwc_otg/dwc_otg_adp.h                  |   80 +
 drivers/usb/dwc_otg/dwc_otg_attr.c                 | 1441 ++++
 drivers/usb/dwc_otg/dwc_otg_attr.h                 |   91 +
 drivers/usb/dwc_otg/dwc_otg_cfi.c                  | 1877 ++++++
 drivers/usb/dwc_otg/dwc_otg_cfi.h                  |  320 +
 drivers/usb/dwc_otg/dwc_otg_cil.c                  | 7087 ++++++++++++++++++++
 drivers/usb/dwc_otg/dwc_otg_cil.h                  | 1456 ++++
 drivers/usb/dwc_otg/dwc_otg_cil_intr.c             | 1432 ++++
 drivers/usb/dwc_otg/dwc_otg_core_if.h              |  707 ++
 drivers/usb/dwc_otg/dwc_otg_dbg.h                  |  113 +
 drivers/usb/dwc_otg/dwc_otg_driver.c               | 1877 ++++++
 drivers/usb/dwc_otg/dwc_otg_driver.h               |   88 +
 drivers/usb/dwc_otg/dwc_otg_hcd.c                  | 3348 +++++++++
 drivers/usb/dwc_otg/dwc_otg_hcd.h                  |  803 +++
 drivers/usb/dwc_otg/dwc_otg_hcd_ddma.c             | 1122 ++++
 drivers/usb/dwc_otg/dwc_otg_hcd_if.h               |  412 ++
 drivers/usb/dwc_otg/dwc_otg_hcd_intr.c             | 2095 ++++++
 drivers/usb/dwc_otg/dwc_otg_hcd_linux.c            |  885 +++
 drivers/usb/dwc_otg/dwc_otg_hcd_queue.c            |  726 ++
 drivers/usb/dwc_otg/dwc_otg_os_dep.h               |  112 +
 drivers/usb/dwc_otg/dwc_otg_pcd.c                  | 2641 ++++++++
 drivers/usb/dwc_otg/dwc_otg_pcd.h                  |  262 +
 drivers/usb/dwc_otg/dwc_otg_pcd_if.h               |  357 +
 drivers/usb/dwc_otg/dwc_otg_pcd_intr.c             | 4821 +++++++++++++
 drivers/usb/dwc_otg/dwc_otg_pcd_linux.c            | 1320 ++++
 drivers/usb/dwc_otg/dwc_otg_regs.h                 | 2545 +++++++
 drivers/usb/dwc_otg/linux/dwc_otg_plat.h           |  263 +
 drivers/usb/dwc_otg/usb.h                          |  946 +++
 drivers/watchdog/mpcore_wdt.c                      |  118 +-
 fs/fcntl.c                                         |    1 +
 fs/ioctl.c                                         |   23 +
 fs/select.c                                        |   38 +
 include/asm-generic/sections.h                     |    3 +
 include/crypto/aead.h                              |    7 +
 include/linux/crypto.h                             |   14 +-
 include/linux/gfp.h                                |    2 +-
 include/linux/init_task.h                          |   72 +-
 include/linux/kthread.h                            |   10 +
 include/linux/mindspeed/ifdhandler.h               |  138 +
 include/linux/mindspeed/pcscdefines.h              |   36 +
 include/linux/mindspeed/transcede_usim.h           |  147 +
 include/linux/miscdevice.h                         |    1 +
 include/linux/net.h                                |    4 +
 include/linux/random.h                             |    4 +
 include/linux/sched.h                              |   25 +-
 include/linux/skbuff.h                             |   88 +-
 include/linux/socket.h                             |   17 +-
 include/linux/spi/cdce62005.h                      |  137 +
 include/net/inet_sock.h                            |   56 +
 include/net/ip.h                                   |   22 +-
 include/net/sock.h                                 |   49 +-
 kernel/fork.c                                      |  114 +-
 kernel/irq/handle.c                                |   31 +
 kernel/kthread.c                                   |   63 +
 kernel/pid.c                                       |    1 +
 kernel/rtmutex.c                                   |  156 +
 kernel/sched.c                                     |   50 +-
 kernel/smp.c                                       |    4 +
 kernel/wait.c                                      |    4 +
 net/core/filter.c                                  |    4 +
 net/core/iovec.c                                   |   54 +
 net/core/neighbour.c                               |   12 +-
 net/core/skbuff.c                                  |  693 +-
 net/core/sock.c                                    |   34 +-
 net/ipv4/esp4.c                                    |  551 ++
 net/ipv4/ip_output.c                               |  123 +-
 net/ipv4/ip_sockglue.c                             |   50 +
 net/ipv4/udp.c                                     |  145 +-
 net/ipv4/xfrm4_output.c                            |    1 +
 net/socket.c                                       |   75 +-
 255 files changed, 107697 insertions(+), 366 deletions(-)
 create mode 100644 arch/arm/configs/t2200_defconfig
 create mode 100644 arch/arm/configs/t2200_initramfs_defconfig
 create mode 100644 arch/arm/configs/t4000_x2_defconfig
 create mode 100644 arch/arm/configs/t4000_x4_defconfig
 create mode 100644 arch/arm/mach-transcede/Kconfig
 create mode 100644 arch/arm/mach-transcede/Makefile
 create mode 100644 arch/arm/mach-transcede/Makefile.boot
 create mode 100644 arch/arm/mach-transcede/clkrst.c
 create mode 100644 arch/arm/mach-transcede/core.h
 create mode 100644 arch/arm/mach-transcede/ddr_protection.c
 create mode 100644 arch/arm/mach-transcede/headamp.S
 create mode 100644 arch/arm/mach-transcede/headsmp.S
 create mode 100644 arch/arm/mach-transcede/iccom.c
 create mode 100644 arch/arm/mach-transcede/include/mach/clk-rst.h
 create mode 100644 arch/arm/mach-transcede/include/mach/clkrst.h
 create mode 100644 arch/arm/mach-transcede/include/mach/ddr_protection.h
 create mode 100644 arch/arm/mach-transcede/include/mach/debug-macro.S
 create mode 100644 arch/arm/mach-transcede/include/mach/drv_if.h
 create mode 100644 arch/arm/mach-transcede/include/mach/entry-macro.S
 create mode 100644 arch/arm/mach-transcede/include/mach/exp-bus.h
 create mode 100644 arch/arm/mach-transcede/include/mach/gpio-2200.h
 create mode 100644 arch/arm/mach-transcede/include/mach/gpio-4000.h
 create mode 100644 arch/arm/mach-transcede/include/mach/gpio.h
 create mode 100644 arch/arm/mach-transcede/include/mach/hardware.h
 create mode 100644 arch/arm/mach-transcede/include/mach/i2c.h
 create mode 100644 arch/arm/mach-transcede/include/mach/io.h
 create mode 100644 arch/arm/mach-transcede/include/mach/irqs-2200.h
 create mode 100644 arch/arm/mach-transcede/include/mach/irqs-4000.h
 create mode 100644 arch/arm/mach-transcede/include/mach/irqs.h
 create mode 100644 arch/arm/mach-transcede/include/mach/memory.h
 create mode 100644 arch/arm/mach-transcede/include/mach/mips_monitor.h
 create mode 100644 arch/arm/mach-transcede/include/mach/mlog.h
 create mode 100644 arch/arm/mach-transcede/include/mach/mmu_protect.h
 create mode 100644 arch/arm/mach-transcede/include/mach/mmu_protect_v7.h
 create mode 100644 arch/arm/mach-transcede/include/mach/pcie-t2200.h
 create mode 100644 arch/arm/mach-transcede/include/mach/pcie.h
 create mode 100644 arch/arm/mach-transcede/include/mach/periodic_task.h
 create mode 100644 arch/arm/mach-transcede/include/mach/revision.h
 create mode 100644 arch/arm/mach-transcede/include/mach/serdes-t2200.h
 create mode 100644 arch/arm/mach-transcede/include/mach/serdes.h
 create mode 100644 arch/arm/mach-transcede/include/mach/spa.h
 create mode 100644 arch/arm/mach-transcede/include/mach/spi.h
 create mode 100644 arch/arm/mach-transcede/include/mach/syscfg.h
 create mode 100644 arch/arm/mach-transcede/include/mach/sysheap.h
 create mode 100644 arch/arm/mach-transcede/include/mach/syslib.h
 create mode 100644 arch/arm/mach-transcede/include/mach/system.h
 create mode 100644 arch/arm/mach-transcede/include/mach/tcb.h
 create mode 100644 arch/arm/mach-transcede/include/mach/timex.h
 create mode 100644 arch/arm/mach-transcede/include/mach/transcede-2200.h
 create mode 100644 arch/arm/mach-transcede/include/mach/transcede-4000.h
 create mode 100644 arch/arm/mach-transcede/include/mach/uncompress.h
 create mode 100644 arch/arm/mach-transcede/include/mach/usb_mmap.h
 create mode 100644 arch/arm/mach-transcede/include/mach/vmalloc.h
 create mode 100644 arch/arm/mach-transcede/l-arm.c
 create mode 100644 arch/arm/mach-transcede/mips_monitor.c
 create mode 100644 arch/arm/mach-transcede/mlog.c
 create mode 100644 arch/arm/mach-transcede/mmu_protect.c
 create mode 100644 arch/arm/mach-transcede/pcie-t2200.c
 create mode 100644 arch/arm/mach-transcede/periodic_task.c
 create mode 100644 arch/arm/mach-transcede/platsmp-t2200.c
 create mode 100644 arch/arm/mach-transcede/platsmp-t4000.c
 create mode 100644 arch/arm/mach-transcede/platsmp.c
 create mode 100644 arch/arm/mach-transcede/revision.c
 create mode 100644 arch/arm/mach-transcede/serdes-t2200.c
 create mode 100644 arch/arm/mach-transcede/sysheap.c
 create mode 100644 arch/arm/mach-transcede/syslib.c
 create mode 100644 arch/arm/mach-transcede/transcede-2200.c
 create mode 100644 arch/arm/mach-transcede/transcede-4000.c
 create mode 100644 arch/arm/mach-transcede/u-arm.c
 create mode 100644 arch/arm/mach-transcede/u-arm.h
 create mode 100644 drivers/crypto/comcerto_crypto.c
 create mode 100644 drivers/crypto/comcerto_crypto.h
 create mode 100644 drivers/crypto/comcerto_espah.c
 create mode 100644 drivers/crypto/comcerto_espah.h
 create mode 100644 drivers/crypto/comcerto_types.h
 create mode 100644 drivers/gpio/gpio-t2200.c
 create mode 100644 drivers/i2c/busses/i2c-transcede.c
 create mode 100644 drivers/misc/transcede_usim.c
 create mode 100644 drivers/misc/transcede_usim_regs.h
 create mode 100644 drivers/misc/transcede_usim_t0_handler.c
 create mode 100644 drivers/misc/transcede_usim_t1_handler.c
 create mode 100644 drivers/net/transcede/Kconfig
 create mode 100644 drivers/net/transcede/Makefile
 create mode 100644 drivers/net/transcede/api.h
 create mode 100644 drivers/net/transcede/c4000_cpri.c
 create mode 100644 drivers/net/transcede/c4000_cpri.h
 create mode 100644 drivers/net/transcede/c4000_eth.c
 create mode 100644 drivers/net/transcede/c4000_eth.h
 create mode 100644 drivers/net/transcede/c4000_gemac.c
 create mode 100644 drivers/net/transcede/m84xxx_adm.h
 create mode 100644 drivers/net/transcede/m84xxx_common.h
 create mode 100644 drivers/net/transcede/m84xxx_sch.h
 create mode 100644 drivers/net/transcede/qos_fops.h
 create mode 100644 drivers/net/transcede/qoscom.c
 create mode 100644 drivers/net/transcede/qoscom.h
 create mode 100644 drivers/net/transcede/t2200_eth.c
 create mode 100644 drivers/net/transcede/t2200_eth.h
 create mode 100644 drivers/net/transcede/t3300_reth.c
 create mode 100644 drivers/net/transcede/t3300_reth.h
 create mode 100644 drivers/net/transcede/t3300_smi.c
 create mode 100644 drivers/net/transcede/t3300_smi.h
 create mode 100644 drivers/net/transcede/t3300_ved.c
 create mode 100644 drivers/net/transcede/t3300_ved.h
 create mode 100644 drivers/net/transcede/transcede_ethtool.c
 create mode 100644 drivers/net/transcede/transcede_gem_AL.c
 create mode 100644 drivers/net/transcede/transcede_gemac.h
 create mode 100644 drivers/net/transcede/transcede_mii.c
 create mode 100644 drivers/net/transcede/transcede_mii.h
 create mode 100644 drivers/net/transcede/userio.c
 create mode 100644 drivers/net/transcede/userio.h
 create mode 100644 drivers/spi/cdce62005.c
 create mode 100644 drivers/spi/spi_transcede.c
 create mode 100644 drivers/usb/dwc_otg/Kconfig
 create mode 100644 drivers/usb/dwc_otg/Makefile
 create mode 100644 drivers/usb/dwc_otg/dwc_cc.c
 create mode 100644 drivers/usb/dwc_otg/dwc_cc.h
 create mode 100644 drivers/usb/dwc_otg/dwc_cfi_common.h
 create mode 100644 drivers/usb/dwc_otg/dwc_common_fbsd.c
 create mode 100644 drivers/usb/dwc_otg/dwc_common_linux.c
 create mode 100644 drivers/usb/dwc_otg/dwc_common_nbsd.c
 create mode 100644 drivers/usb/dwc_otg/dwc_crypto.c
 create mode 100644 drivers/usb/dwc_otg/dwc_crypto.h
 create mode 100644 drivers/usb/dwc_otg/dwc_dh.c
 create mode 100644 drivers/usb/dwc_otg/dwc_dh.h
 create mode 100644 drivers/usb/dwc_otg/dwc_list.h
 create mode 100644 drivers/usb/dwc_otg/dwc_mem.c
 create mode 100644 drivers/usb/dwc_otg/dwc_modpow.c
 create mode 100644 drivers/usb/dwc_otg/dwc_modpow.h
 create mode 100644 drivers/usb/dwc_otg/dwc_notifier.c
 create mode 100644 drivers/usb/dwc_otg/dwc_notifier.h
 create mode 100644 drivers/usb/dwc_otg/dwc_os.h
 create mode 100644 drivers/usb/dwc_otg/dwc_otg_adp.c
 create mode 100644 drivers/usb/dwc_otg/dwc_otg_adp.h
 create mode 100644 drivers/usb/dwc_otg/dwc_otg_attr.c
 create mode 100644 drivers/usb/dwc_otg/dwc_otg_attr.h
 create mode 100644 drivers/usb/dwc_otg/dwc_otg_cfi.c
 create mode 100644 drivers/usb/dwc_otg/dwc_otg_cfi.h
 create mode 100644 drivers/usb/dwc_otg/dwc_otg_cil.c
 create mode 100644 drivers/usb/dwc_otg/dwc_otg_cil.h
 create mode 100644 drivers/usb/dwc_otg/dwc_otg_cil_intr.c
 create mode 100644 drivers/usb/dwc_otg/dwc_otg_core_if.h
 create mode 100644 drivers/usb/dwc_otg/dwc_otg_dbg.h
 create mode 100644 drivers/usb/dwc_otg/dwc_otg_driver.c
 create mode 100644 drivers/usb/dwc_otg/dwc_otg_driver.h
 create mode 100644 drivers/usb/dwc_otg/dwc_otg_hcd.c
 create mode 100644 drivers/usb/dwc_otg/dwc_otg_hcd.h
 create mode 100644 drivers/usb/dwc_otg/dwc_otg_hcd_ddma.c
 create mode 100644 drivers/usb/dwc_otg/dwc_otg_hcd_if.h
 create mode 100644 drivers/usb/dwc_otg/dwc_otg_hcd_intr.c
 create mode 100644 drivers/usb/dwc_otg/dwc_otg_hcd_linux.c
 create mode 100644 drivers/usb/dwc_otg/dwc_otg_hcd_queue.c
 create mode 100644 drivers/usb/dwc_otg/dwc_otg_os_dep.h
 create mode 100644 drivers/usb/dwc_otg/dwc_otg_pcd.c
 create mode 100644 drivers/usb/dwc_otg/dwc_otg_pcd.h
 create mode 100644 drivers/usb/dwc_otg/dwc_otg_pcd_if.h
 create mode 100644 drivers/usb/dwc_otg/dwc_otg_pcd_intr.c
 create mode 100644 drivers/usb/dwc_otg/dwc_otg_pcd_linux.c
 create mode 100644 drivers/usb/dwc_otg/dwc_otg_regs.h
 create mode 100644 drivers/usb/dwc_otg/linux/dwc_otg_plat.h
 create mode 100644 drivers/usb/dwc_otg/usb.h
 create mode 100644 include/linux/mindspeed/ifdhandler.h
 create mode 100644 include/linux/mindspeed/pcscdefines.h
 create mode 100644 include/linux/mindspeed/transcede_usim.h
 create mode 100644 include/linux/spi/cdce62005.h

diff --git a/arch/arm/Kconfig b/arch/arm/Kconfig
index 5b35cab..5a50899 100644
--- a/arch/arm/Kconfig
+++ b/arch/arm/Kconfig
@@ -798,6 +798,16 @@ config ARCH_TCC_926
 	help
 	  Support for Telechips TCC ARM926-based systems.
 
+config ARCH_TRANSCEDE
+	bool "Mindspeed Transcede"
+	select GENERIC_TIME
+	select GENERIC_CLOCKEVENTS
+	select HAVE_SCHED_CLOCK
+	help
+	  This enables support for Mindspeed's Transcede development boards.
+	  If you would like to build your kernel to run on one of these boards
+	  then you must say 'Y' here. Otherwise say 'N'
+
 config ARCH_U300
 	bool "ST-Ericsson U300 Series"
 	depends on MMU
@@ -994,6 +1004,8 @@ source "arch/arm/mach-shmobile/Kconfig"
 
 source "arch/arm/mach-tegra/Kconfig"
 
+source "arch/arm/mach-transcede/Kconfig"
+
 source "arch/arm/mach-u300/Kconfig"
 
 source "arch/arm/mach-ux500/Kconfig"
@@ -1348,7 +1360,7 @@ config SMP
 	depends on REALVIEW_EB_ARM11MP || REALVIEW_EB_A9MP || \
 		 MACH_REALVIEW_PB11MP || MACH_REALVIEW_PBX || ARCH_OMAP4 || \
 		 ARCH_EXYNOS4 || ARCH_TEGRA || ARCH_U8500 || ARCH_VEXPRESS_CA9X4 || \
-		 ARCH_MSM_SCORPIONMP || ARCH_SHMOBILE
+		 ARCH_MSM_SCORPIONMP || ARCH_SHMOBILE || ARCH_TRANSCEDE
 	select USE_GENERIC_SMP_HELPERS
 	select HAVE_ARM_SCU if !ARCH_MSM_SCORPIONMP
 	help
@@ -1425,7 +1437,7 @@ config NR_CPUS
 
 config HOTPLUG_CPU
 	bool "Support for hot-pluggable CPUs (EXPERIMENTAL)"
-	depends on SMP && HOTPLUG && EXPERIMENTAL
+	depends on SMP && HOTPLUG && EXPERIMENTAL && !ARCH_TRANSCEDE
 	help
 	  Say Y here to experiment with turning CPUs off and on.  CPUs
 	  can be controlled through /sys/devices/system/cpu.
@@ -1441,6 +1453,13 @@ config LOCAL_TIMERS
 	  accounting to be spread across the timer interval, preventing a
 	  "thundering herd" at every timer tick.
 
+config GLOBAL_POLLING
+	bool "Use polling with global timer interrupts"
+	depends on SMP
+	default n
+	help
+	  Enable support for global timers on SMP platforms and polling over it.
+
 source kernel/Kconfig.preempt
 
 config HZ
@@ -1450,6 +1469,8 @@ config HZ
 	default OMAP_32K_TIMER_HZ if ARCH_OMAP && OMAP_32K_TIMER
 	default AT91_TIMER_HZ if ARCH_AT91
 	default SHMOBILE_TIMER_HZ if ARCH_SHMOBILE
+	default 1000 if MACH_M84XXX
+	default 100 if MACH_M822XX
 	default 100
 
 config THUMB2_KERNEL
diff --git a/arch/arm/Makefile b/arch/arm/Makefile
index f5b2b39..6e69c60 100644
--- a/arch/arm/Makefile
+++ b/arch/arm/Makefile
@@ -184,6 +184,7 @@ machine-$(CONFIG_ARCH_SHARK)		:= shark
 machine-$(CONFIG_ARCH_SHMOBILE) 	:= shmobile
 machine-$(CONFIG_ARCH_TCC8K)		:= tcc8k
 machine-$(CONFIG_ARCH_TEGRA)		:= tegra
+machine-$(CONFIG_ARCH_TRANSCEDE)	:= transcede
 machine-$(CONFIG_ARCH_U300)		:= u300
 machine-$(CONFIG_ARCH_U8500)		:= ux500
 machine-$(CONFIG_ARCH_VERSATILE)	:= versatile
diff --git a/arch/arm/boot/compressed/head.S b/arch/arm/boot/compressed/head.S
index 4d1f07d..dc483f0 100644
--- a/arch/arm/boot/compressed/head.S
+++ b/arch/arm/boot/compressed/head.S
@@ -9,6 +9,7 @@
  * published by the Free Software Foundation.
  */
 #include <linux/linkage.h>
+#include <asm/mach-types.h>
 
 /*
  * Debugging stuff
@@ -132,7 +133,13 @@ start:
 		.word	start			@ absolute load/run zImage address
 		.word	_edata			@ zImage end address
  THUMB(		.thumb			)
-1:		mov	r7, r1			@ save architecture ID
+1:
+#ifdef CONFIG_ARCH_TRANSCEDE
+		mov	r7, #4000
+		cmp	r1, r7
+		ldreq	r1, =MACH_TYPE_TRANSCEDE
+#endif
+		mov	r7, r1			@ save architecture ID
 		mov	r8, r2			@ save atags pointer
 
 #ifndef __ARM_ARCH_2__
diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 841df7d..ebab027 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -322,6 +322,9 @@ static inline void unmap_single(struct device *dev, dma_addr_t dma_addr,
 
 /* ************************************************** */
 
+extern unsigned long (*__dma_ext_map_mem)(const void * kaddr, size_t size, uint dir);
+extern unsigned long (*__dma_ext_unmap_mem)(const void * kaddr, size_t size, uint dir);
+
 /*
  * see if a buffer address is in an 'unsafe' range.  if it is
  * allocate a 'safe' buffer and copy the unsafe buffer into it.
@@ -336,6 +339,13 @@ dma_addr_t __dma_map_single(struct device *dev, void *ptr, size_t size,
 
 	BUG_ON(!valid_dma_direction(dir));
 
+	if (__dma_ext_map_mem!=NULL)
+	{
+	    dma_addr_t addr = (dma_addr_t)__dma_ext_map_mem((const void*)ptr, size, dir);
+	    if (addr != 0)
+		return addr;
+	}
+
 	return map_single(dev, ptr, size, dir);
 }
 EXPORT_SYMBOL(__dma_map_single);
@@ -352,6 +362,14 @@ void __dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 	dev_dbg(dev, "%s(ptr=%p,size=%d,dir=%x)\n",
 		__func__, (void *) dma_addr, size, dir);
 
+	if (__dma_ext_unmap_mem!=NULL)
+	{
+	    dma_addr_t addr = (dma_addr_t)__dma_ext_unmap_mem((const void*)ptr, size, dir);
+	    if (addr != 0)
+		return;
+	}
+
+
 	unmap_single(dev, dma_addr, size, dir);
 }
 EXPORT_SYMBOL(__dma_unmap_single);
diff --git a/arch/arm/common/gic.c b/arch/arm/common/gic.c
index 5c1dd07..05e29d5 100644
--- a/arch/arm/common/gic.c
+++ b/arch/arm/common/gic.c
@@ -329,8 +329,10 @@ static void __init gic_dist_init(struct gic_chip_data *gic,
 
 static void __cpuinit gic_cpu_init(struct gic_chip_data *gic)
 {
-	void __iomem *dist_base = gic->dist_base;
 	void __iomem *base = gic->cpu_base;
+
+#if defined(CONFIG_ASMP_CORE_STARTUP) || defined(CONFIG_GLOBAL_POLLING)
+	void __iomem *dist_base = gic->dist_base;
 	int i;
 
 	/*
@@ -346,6 +348,7 @@ static void __cpuinit gic_cpu_init(struct gic_chip_data *gic)
 	for (i = 0; i < 32; i += 4)
 		writel_relaxed(0xa0a0a0a0, dist_base + GIC_DIST_PRI + i * 4 / 4);
 
+#endif
 	writel_relaxed(0xf0, base + GIC_CPU_PRIMASK);
 	writel_relaxed(1, base + GIC_CPU_CTRL);
 }
@@ -400,4 +403,19 @@ void gic_raise_softirq(const struct cpumask *mask, unsigned int irq)
 	/* this always happens on GIC0 */
 	writel_relaxed(map << 16 | irq, gic_data[0].dist_base + GIC_DIST_SOFTINT);
 }
+
+void gic_clear_cpu(struct irq_data *d, cpumask_t mask_val)
+{
+       void __iomem *reg = gic_dist_base(d) + GIC_DIST_TARGET + (gic_irq(d) & ~3);
+       unsigned int shift = (gic_irq(d) % 4) * 8;
+       unsigned int cpu = first_cpu(mask_val);
+       u32 val;
+
+       raw_spin_lock(&irq_controller_lock);
+       d->node = cpu;
+       val = readl(reg) & ~(0xff << shift);
+       val &= ~(1 << (cpu + shift));
+       writel(val, reg);
+       raw_spin_unlock(&irq_controller_lock);
+}
 #endif
diff --git a/arch/arm/configs/t2200_defconfig b/arch/arm/configs/t2200_defconfig
new file mode 100644
index 0000000..51540d8
--- /dev/null
+++ b/arch/arm/configs/t2200_defconfig
@@ -0,0 +1,1957 @@
+#
+# Automatically generated make config: don't edit
+# Linux/arm 3.0.51 Kernel Configuration
+#
+CONFIG_ARM=y
+CONFIG_MIGHT_HAVE_PCI=y
+CONFIG_SYS_SUPPORTS_APM_EMULATION=y
+CONFIG_HAVE_SCHED_CLOCK=y
+# CONFIG_ARCH_USES_GETTIMEOFFSET is not set
+CONFIG_GENERIC_CLOCKEVENTS=y
+CONFIG_GENERIC_CLOCKEVENTS_BROADCAST=y
+CONFIG_KTIME_SCALAR=y
+CONFIG_HAVE_PROC_CPU=y
+CONFIG_STACKTRACE_SUPPORT=y
+CONFIG_LOCKDEP_SUPPORT=y
+CONFIG_TRACE_IRQFLAGS_SUPPORT=y
+CONFIG_HARDIRQS_SW_RESEND=y
+CONFIG_GENERIC_IRQ_PROBE=y
+CONFIG_GENERIC_LOCKBREAK=y
+CONFIG_RWSEM_GENERIC_SPINLOCK=y
+CONFIG_ARCH_HAS_CPU_IDLE_WAIT=y
+CONFIG_GENERIC_HWEIGHT=y
+CONFIG_GENERIC_CALIBRATE_DELAY=y
+CONFIG_NEED_DMA_MAP_STATE=y
+CONFIG_VECTORS_BASE=0xffff0000
+# CONFIG_ARM_PATCH_PHYS_VIRT is not set
+CONFIG_DEFCONFIG_LIST="/lib/modules/$UNAME_RELEASE/.config"
+CONFIG_HAVE_IRQ_WORK=y
+CONFIG_IRQ_WORK=y
+
+#
+# General setup
+#
+CONFIG_EXPERIMENTAL=y
+CONFIG_INIT_ENV_ARG_LIMIT=32
+CONFIG_CROSS_COMPILE=""
+CONFIG_LOCALVERSION=""
+# CONFIG_LOCALVERSION_AUTO is not set
+CONFIG_HAVE_KERNEL_GZIP=y
+CONFIG_HAVE_KERNEL_LZMA=y
+CONFIG_HAVE_KERNEL_LZO=y
+CONFIG_KERNEL_GZIP=y
+# CONFIG_KERNEL_LZMA is not set
+# CONFIG_KERNEL_LZO is not set
+CONFIG_DEFAULT_HOSTNAME="(none)"
+# CONFIG_SWAP is not set
+CONFIG_SYSVIPC=y
+CONFIG_SYSVIPC_SYSCTL=y
+CONFIG_POSIX_MQUEUE=y
+CONFIG_POSIX_MQUEUE_SYSCTL=y
+# CONFIG_BSD_PROCESS_ACCT is not set
+# CONFIG_FHANDLE is not set
+# CONFIG_TASKSTATS is not set
+# CONFIG_AUDIT is not set
+CONFIG_HAVE_GENERIC_HARDIRQS=y
+
+#
+# IRQ subsystem
+#
+CONFIG_GENERIC_HARDIRQS=y
+CONFIG_HAVE_SPARSE_IRQ=y
+CONFIG_GENERIC_IRQ_SHOW=y
+CONFIG_IRQ_FORCED_THREADING=y
+# CONFIG_SPARSE_IRQ is not set
+
+#
+# RCU Subsystem
+#
+CONFIG_TREE_PREEMPT_RCU=y
+CONFIG_PREEMPT_RCU=y
+# CONFIG_RCU_TRACE is not set
+CONFIG_RCU_FANOUT=32
+# CONFIG_RCU_FANOUT_EXACT is not set
+# CONFIG_TREE_RCU_TRACE is not set
+# CONFIG_RCU_BOOST is not set
+CONFIG_IKCONFIG=y
+CONFIG_IKCONFIG_PROC=y
+CONFIG_LOG_BUF_SHIFT=16
+CONFIG_CGROUPS=y
+# CONFIG_CGROUP_DEBUG is not set
+# CONFIG_CGROUP_FREEZER is not set
+# CONFIG_CGROUP_DEVICE is not set
+# CONFIG_CPUSETS is not set
+# CONFIG_CGROUP_CPUACCT is not set
+# CONFIG_RESOURCE_COUNTERS is not set
+# CONFIG_CGROUP_PERF is not set
+# CONFIG_CGROUP_SCHED is not set
+# CONFIG_BLK_CGROUP is not set
+CONFIG_NAMESPACES=y
+# CONFIG_UTS_NS is not set
+# CONFIG_IPC_NS is not set
+# CONFIG_USER_NS is not set
+# CONFIG_PID_NS is not set
+# CONFIG_NET_NS is not set
+# CONFIG_SCHED_AUTOGROUP is not set
+# CONFIG_SYSFS_DEPRECATED is not set
+# CONFIG_RELAY is not set
+# CONFIG_BLK_DEV_INITRD is not set
+CONFIG_CC_OPTIMIZE_FOR_SIZE=y
+CONFIG_SYSCTL=y
+CONFIG_ANON_INODES=y
+# CONFIG_EXPERT is not set
+CONFIG_UID16=y
+CONFIG_SYSCTL_SYSCALL=y
+CONFIG_KALLSYMS=y
+# CONFIG_KALLSYMS_ALL is not set
+CONFIG_HOTPLUG=y
+CONFIG_PRINTK=y
+CONFIG_BUG=y
+CONFIG_ELF_CORE=y
+CONFIG_BASE_FULL=y
+CONFIG_FUTEX=y
+CONFIG_EPOLL=y
+CONFIG_SIGNALFD=y
+CONFIG_TIMERFD=y
+CONFIG_EVENTFD=y
+CONFIG_SHMEM=y
+CONFIG_AIO=y
+# CONFIG_EMBEDDED is not set
+CONFIG_HAVE_PERF_EVENTS=y
+CONFIG_PERF_USE_VMALLOC=y
+
+#
+# Kernel Performance Events And Counters
+#
+CONFIG_PERF_EVENTS=y
+CONFIG_PERF_COUNTERS=y
+# CONFIG_DEBUG_PERF_USE_VMALLOC is not set
+CONFIG_VM_EVENT_COUNTERS=y
+CONFIG_PCI_QUIRKS=y
+CONFIG_COMPAT_BRK=y
+CONFIG_SLAB=y
+# CONFIG_PROFILING is not set
+CONFIG_HAVE_OPROFILE=y
+# CONFIG_KPROBES is not set
+CONFIG_HAVE_KPROBES=y
+CONFIG_HAVE_KRETPROBES=y
+CONFIG_USE_GENERIC_SMP_HELPERS=y
+CONFIG_HAVE_REGS_AND_STACK_ACCESS_API=y
+CONFIG_HAVE_DMA_API_DEBUG=y
+CONFIG_HAVE_HW_BREAKPOINT=y
+
+#
+# GCOV-based kernel profiling
+#
+# CONFIG_GCOV_KERNEL is not set
+CONFIG_HAVE_GENERIC_DMA_COHERENT=y
+CONFIG_SLABINFO=y
+CONFIG_RT_MUTEXES=y
+CONFIG_BASE_SMALL=0
+CONFIG_MODULES=y
+# CONFIG_MODULE_FORCE_LOAD is not set
+CONFIG_MODULE_UNLOAD=y
+# CONFIG_MODULE_FORCE_UNLOAD is not set
+# CONFIG_MODVERSIONS is not set
+# CONFIG_MODULE_SRCVERSION_ALL is not set
+CONFIG_STOP_MACHINE=y
+CONFIG_BLOCK=y
+CONFIG_LBDAF=y
+# CONFIG_BLK_DEV_BSG is not set
+# CONFIG_BLK_DEV_INTEGRITY is not set
+
+#
+# IO Schedulers
+#
+CONFIG_IOSCHED_NOOP=y
+CONFIG_IOSCHED_DEADLINE=y
+# CONFIG_IOSCHED_CFQ is not set
+CONFIG_DEFAULT_DEADLINE=y
+# CONFIG_DEFAULT_NOOP is not set
+CONFIG_DEFAULT_IOSCHED="deadline"
+# CONFIG_INLINE_SPIN_TRYLOCK is not set
+# CONFIG_INLINE_SPIN_TRYLOCK_BH is not set
+# CONFIG_INLINE_SPIN_LOCK is not set
+# CONFIG_INLINE_SPIN_LOCK_BH is not set
+# CONFIG_INLINE_SPIN_LOCK_IRQ is not set
+# CONFIG_INLINE_SPIN_LOCK_IRQSAVE is not set
+# CONFIG_INLINE_SPIN_UNLOCK is not set
+# CONFIG_INLINE_SPIN_UNLOCK_BH is not set
+# CONFIG_INLINE_SPIN_UNLOCK_IRQ is not set
+# CONFIG_INLINE_SPIN_UNLOCK_IRQRESTORE is not set
+# CONFIG_INLINE_READ_TRYLOCK is not set
+# CONFIG_INLINE_READ_LOCK is not set
+# CONFIG_INLINE_READ_LOCK_BH is not set
+# CONFIG_INLINE_READ_LOCK_IRQ is not set
+# CONFIG_INLINE_READ_LOCK_IRQSAVE is not set
+# CONFIG_INLINE_READ_UNLOCK is not set
+# CONFIG_INLINE_READ_UNLOCK_BH is not set
+# CONFIG_INLINE_READ_UNLOCK_IRQ is not set
+# CONFIG_INLINE_READ_UNLOCK_IRQRESTORE is not set
+# CONFIG_INLINE_WRITE_TRYLOCK is not set
+# CONFIG_INLINE_WRITE_LOCK is not set
+# CONFIG_INLINE_WRITE_LOCK_BH is not set
+# CONFIG_INLINE_WRITE_LOCK_IRQ is not set
+# CONFIG_INLINE_WRITE_LOCK_IRQSAVE is not set
+# CONFIG_INLINE_WRITE_UNLOCK is not set
+# CONFIG_INLINE_WRITE_UNLOCK_BH is not set
+# CONFIG_INLINE_WRITE_UNLOCK_IRQ is not set
+# CONFIG_INLINE_WRITE_UNLOCK_IRQRESTORE is not set
+# CONFIG_MUTEX_SPIN_ON_OWNER is not set
+# CONFIG_FREEZER is not set
+
+#
+# System Type
+#
+CONFIG_MMU=y
+# CONFIG_ARCH_INTEGRATOR is not set
+# CONFIG_ARCH_REALVIEW is not set
+# CONFIG_ARCH_VERSATILE is not set
+# CONFIG_ARCH_VEXPRESS is not set
+# CONFIG_ARCH_AT91 is not set
+# CONFIG_ARCH_BCMRING is not set
+# CONFIG_ARCH_CLPS711X is not set
+# CONFIG_ARCH_CNS3XXX is not set
+# CONFIG_ARCH_GEMINI is not set
+# CONFIG_ARCH_EBSA110 is not set
+# CONFIG_ARCH_EP93XX is not set
+# CONFIG_ARCH_FOOTBRIDGE is not set
+# CONFIG_ARCH_MXC is not set
+# CONFIG_ARCH_MXS is not set
+# CONFIG_ARCH_NETX is not set
+# CONFIG_ARCH_H720X is not set
+# CONFIG_ARCH_IOP13XX is not set
+# CONFIG_ARCH_IOP32X is not set
+# CONFIG_ARCH_IOP33X is not set
+# CONFIG_ARCH_IXP23XX is not set
+# CONFIG_ARCH_IXP2000 is not set
+# CONFIG_ARCH_IXP4XX is not set
+# CONFIG_ARCH_DOVE is not set
+# CONFIG_ARCH_KIRKWOOD is not set
+# CONFIG_ARCH_LOKI is not set
+# CONFIG_ARCH_LPC32XX is not set
+# CONFIG_ARCH_MV78XX0 is not set
+# CONFIG_ARCH_ORION5X is not set
+# CONFIG_ARCH_MMP is not set
+# CONFIG_ARCH_KS8695 is not set
+# CONFIG_ARCH_W90X900 is not set
+# CONFIG_ARCH_NUC93X is not set
+# CONFIG_ARCH_TEGRA is not set
+# CONFIG_ARCH_PNX4008 is not set
+# CONFIG_ARCH_PXA is not set
+# CONFIG_ARCH_MSM is not set
+# CONFIG_ARCH_SHMOBILE is not set
+# CONFIG_ARCH_RPC is not set
+# CONFIG_ARCH_SA1100 is not set
+# CONFIG_ARCH_S3C2410 is not set
+# CONFIG_ARCH_S3C64XX is not set
+# CONFIG_ARCH_S5P64X0 is not set
+# CONFIG_ARCH_S5PC100 is not set
+# CONFIG_ARCH_S5PV210 is not set
+# CONFIG_ARCH_EXYNOS4 is not set
+# CONFIG_ARCH_SHARK is not set
+# CONFIG_ARCH_TCC_926 is not set
+CONFIG_ARCH_TRANSCEDE=y
+# CONFIG_ARCH_U300 is not set
+# CONFIG_ARCH_U8500 is not set
+# CONFIG_ARCH_NOMADIK is not set
+# CONFIG_ARCH_DAVINCI is not set
+# CONFIG_ARCH_OMAP is not set
+# CONFIG_PLAT_SPEAR is not set
+# CONFIG_ARCH_VT8500 is not set
+
+#
+# System MMU
+#
+
+#
+# Transcede Implementation Options
+#
+# CONFIG_MACH_M84XXX is not set
+CONFIG_MACH_M822XX=y
+# CONFIG_TRANSCEDE_PCI_USE_APBB is not set
+# CONFIG_TRANSCEDE_PCI_TX_DMA_ARAM is not set
+# CONFIG_TRANSCEDE_PCI_DEBUG is not set
+# CONFIG_TRANSCEDE_TDM_CLOCK is not set
+# CONFIG_TRANSCEDE_DUALCORE is not set
+CONFIG_TRANSCEDE_UART0_SUPPORT=y
+# CONFIG_TRANSCEDE_UART1_SUPPORT is not set
+# CONFIG_TRANSCEDE_UART2_SUPPORT is not set
+CONFIG_TRANSCEDE_IPSEC_HW_SUPPORT=y
+CONFIG_TRANSCEDE_ELP_SPACC=y
+CONFIG_TRANSCEDE_ELP_CLP30=y
+CONFIG_IPSEC_DMA_MAP_HACK_TX=y
+CONFIG_IPSEC_DMA_MAP_HACK_RX=y
+CONFIG_CLP30_SW_FIFO=y
+CONFIG_TRANSCEDE_ELP_PDU=y
+CONFIG_TRANSCEDE_GEMAC_0=y
+CONFIG_TRANSCEDE_GEMAC_1=y
+CONFIG_TRANSCEDE_GEM_PHY=y
+CONFIG_MTD_NAND_TRANSCEDE=y
+# CONFIG_MTD_TRANSCEDE_NOR_8 is not set
+CONFIG_MTD_TRANSCEDE_NOR_16=y
+# CONFIG_ASMP_CORE_STARTUP is not set
+# CONFIG_RTSM_ONLY is not set
+# CONFIG_TRANSCEDE_MLOG is not set
+CONFIG_TRANSCEDE_ELP_TRNG=y
+CONFIG_TRANSCEDE_MIPS_MONITOR=y
+CONFIG_TRANSCEDE_MTU_CTRL_ENABLED=y
+#
+# Processor Type
+#
+CONFIG_CPU_V7=y
+CONFIG_CPU_32v6K=y
+CONFIG_CPU_32v7=y
+CONFIG_CPU_ABRT_EV7=y
+CONFIG_CPU_PABRT_V7=y
+CONFIG_CPU_CACHE_V7=y
+CONFIG_CPU_CACHE_VIPT=y
+CONFIG_CPU_COPY_V6=y
+CONFIG_CPU_TLB_V7=y
+CONFIG_CPU_HAS_ASID=y
+CONFIG_CPU_CP15=y
+CONFIG_CPU_CP15_MMU=y
+
+#
+# Processor Features
+#
+CONFIG_ARM_THUMB=y
+# CONFIG_ARM_THUMBEE is not set
+# CONFIG_SWP_EMULATE is not set
+# CONFIG_CPU_ICACHE_DISABLE is not set
+# CONFIG_CPU_DCACHE_DISABLE is not set
+# CONFIG_CPU_BPREDICT_DISABLE is not set
+CONFIG_OUTER_CACHE=y
+CONFIG_OUTER_CACHE_SYNC=y
+CONFIG_CACHE_L2X0=y
+CONFIG_CACHE_PL310=y
+CONFIG_ARM_L1_CACHE_SHIFT=5
+CONFIG_ARM_DMA_MEM_BUFFERABLE=y
+CONFIG_CPU_HAS_PMU=y
+# CONFIG_ARM_ERRATA_430973 is not set
+# CONFIG_ARM_ERRATA_458693 is not set
+# CONFIG_ARM_ERRATA_460075 is not set
+# CONFIG_ARM_ERRATA_742230 is not set
+# CONFIG_ARM_ERRATA_742231 is not set
+CONFIG_PL310_ERRATA_588369=y
+# CONFIG_ARM_ERRATA_720789 is not set
+CONFIG_PL310_ERRATA_727915=y
+# CONFIG_ARM_ERRATA_743622 is not set
+# CONFIG_ARM_ERRATA_751472 is not set
+CONFIG_ARM_ERRATA_753970=y
+# CONFIG_ARM_ERRATA_754322 is not set
+# CONFIG_ARM_ERRATA_754327 is not set
+# CONFIG_ARM_ERRATA_764369 is not set
+# CONFIG_PL310_ERRATA_769419 is not set
+# CONFIG_ARM_ERRATA_775420 is not set
+CONFIG_ARM_GIC=y
+
+#
+# Bus support
+#
+CONFIG_PCI=y
+CONFIG_PCI_SYSCALL=y
+CONFIG_ARCH_SUPPORTS_MSI=y
+CONFIG_PCI_MSI=y
+# CONFIG_PCI_DEBUG is not set
+# CONFIG_PCI_STUB is not set
+# CONFIG_PCI_IOV is not set
+# CONFIG_PCCARD is not set
+
+#
+# Kernel Features
+#
+CONFIG_TICK_ONESHOT=y
+# CONFIG_NO_HZ is not set
+CONFIG_HIGH_RES_TIMERS=y
+CONFIG_GENERIC_CLOCKEVENTS_BUILD=y
+CONFIG_SMP=y
+CONFIG_SMP_ON_UP=y
+CONFIG_HAVE_ARM_SCU=y
+CONFIG_HAVE_ARM_TWD=y
+# CONFIG_VMSPLIT_3G is not set
+CONFIG_VMSPLIT_2G=y
+# CONFIG_VMSPLIT_1G is not set
+CONFIG_PAGE_OFFSET=0x80000000
+CONFIG_NR_CPUS=2
+CONFIG_LOCAL_TIMERS=y
+CONFIG_GLOBAL_POLLING=y
+CONFIG_PREEMPT=y
+CONFIG_PREEMPT_RT_BASE=y
+# CONFIG_PREEMPT_NONE is not set
+# CONFIG_PREEMPT_VOLUNTARY is not set
+# CONFIG_PREEMPT_LL is not set
+# CONFIG_PREEMPT_RTB is not set
+CONFIG_PREEMPT_RT_FULL=y
+CONFIG_HZ=100
+# CONFIG_THUMB2_KERNEL is not set
+CONFIG_AEABI=y
+CONFIG_OABI_COMPAT=y
+# CONFIG_ARCH_SPARSEMEM_DEFAULT is not set
+# CONFIG_ARCH_SELECT_MEMORY_MODEL is not set
+CONFIG_HAVE_ARCH_PFN_VALID=y
+CONFIG_HW_PERF_EVENTS=y
+CONFIG_SELECT_MEMORY_MODEL=y
+CONFIG_FLATMEM_MANUAL=y
+CONFIG_FLATMEM=y
+CONFIG_FLAT_NODE_MEM_MAP=y
+CONFIG_HAVE_MEMBLOCK=y
+CONFIG_PAGEFLAGS_EXTENDED=y
+CONFIG_SPLIT_PTLOCK_CPUS=4
+# CONFIG_COMPACTION is not set
+# CONFIG_PHYS_ADDR_T_64BIT is not set
+CONFIG_ZONE_DMA_FLAG=0
+CONFIG_VIRT_TO_BUS=y
+# CONFIG_KSM is not set
+CONFIG_DEFAULT_MMAP_MIN_ADDR=4096
+# CONFIG_CLEANCACHE is not set
+CONFIG_FORCE_MAX_ZONEORDER=11
+CONFIG_ALIGNMENT_TRAP=y
+# CONFIG_UACCESS_WITH_MEMCPY is not set
+# CONFIG_SECCOMP is not set
+# CONFIG_CC_STACKPROTECTOR is not set
+# CONFIG_DEPRECATED_PARAM_STRUCT is not set
+
+#
+# Boot options
+#
+# CONFIG_USE_OF is not set
+CONFIG_ZBOOT_ROM_TEXT=0x0
+CONFIG_ZBOOT_ROM_BSS=0x0
+CONFIG_CMDLINE="root=/dev/nfs nfsroot=10.1.69.3:/work/nfsroot ip=dhcp console=ttyAMA0 mem=128M"
+CONFIG_CMDLINE_FROM_BOOTLOADER=y
+# CONFIG_CMDLINE_EXTEND is not set
+# CONFIG_CMDLINE_FORCE is not set
+# CONFIG_XIP_KERNEL is not set
+# CONFIG_KEXEC is not set
+# CONFIG_CRASH_DUMP is not set
+# CONFIG_AUTO_ZRELADDR is not set
+
+#
+# CPU Power Management
+#
+# CONFIG_CPU_IDLE is not set
+
+#
+# Floating point emulation
+#
+
+#
+# At least one emulation must be selected
+#
+# CONFIG_FPE_NWFPE is not set
+# CONFIG_FPE_FASTFPE is not set
+# CONFIG_VFP is not set
+
+#
+# Userspace binary formats
+#
+CONFIG_BINFMT_ELF=y
+# CONFIG_CORE_DUMP_DEFAULT_ELF_HEADERS is not set
+CONFIG_HAVE_AOUT=y
+# CONFIG_BINFMT_AOUT is not set
+# CONFIG_BINFMT_MISC is not set
+
+#
+# Power management options
+#
+# CONFIG_SUSPEND is not set
+# CONFIG_PM_RUNTIME is not set
+CONFIG_ARCH_SUSPEND_POSSIBLE=y
+CONFIG_NET=y
+
+#
+# Networking options
+#
+CONFIG_PACKET=y
+CONFIG_UNIX=y
+CONFIG_XFRM=y
+CONFIG_XFRM_USER=m
+# CONFIG_XFRM_SUB_POLICY is not set
+# CONFIG_XFRM_MIGRATE is not set
+# CONFIG_XFRM_STATISTICS is not set
+CONFIG_XFRM_IPCOMP=m
+CONFIG_NET_KEY=m
+# CONFIG_NET_KEY_MIGRATE is not set
+CONFIG_INET=y
+CONFIG_IP_MULTICAST=y
+# CONFIG_IP_ADVANCED_ROUTER is not set
+CONFIG_IP_ROUTE_CLASSID=y
+CONFIG_IP_PNP=y
+CONFIG_IP_PNP_DHCP=y
+CONFIG_IP_PNP_BOOTP=y
+# CONFIG_IP_PNP_RARP is not set
+CONFIG_NET_IPIP=y
+# CONFIG_NET_IPGRE_DEMUX is not set
+# CONFIG_IP_MROUTE is not set
+# CONFIG_ARPD is not set
+# CONFIG_SYN_COOKIES is not set
+CONFIG_INET_AH=m
+CONFIG_INET_ESP=m
+CONFIG_INET_IPCOMP=m
+CONFIG_INET_XFRM_TUNNEL=m
+CONFIG_INET_TUNNEL=y
+CONFIG_INET_XFRM_MODE_TRANSPORT=m
+CONFIG_INET_XFRM_MODE_TUNNEL=m
+CONFIG_INET_XFRM_MODE_BEET=m
+# CONFIG_INET_LRO is not set
+CONFIG_INET_DIAG=y
+CONFIG_INET_TCP_DIAG=y
+# CONFIG_TCP_CONG_ADVANCED is not set
+CONFIG_TCP_CONG_CUBIC=y
+CONFIG_DEFAULT_TCP_CONG="cubic"
+# CONFIG_TCP_MD5SIG is not set
+CONFIG_IPV6=m
+CONFIG_IPV6_PRIVACY=y
+# CONFIG_IPV6_ROUTER_PREF is not set
+# CONFIG_IPV6_OPTIMISTIC_DAD is not set
+CONFIG_INET6_AH=m
+CONFIG_INET6_ESP=m
+CONFIG_INET6_IPCOMP=m
+# CONFIG_IPV6_MIP6 is not set
+CONFIG_INET6_XFRM_TUNNEL=m
+CONFIG_INET6_TUNNEL=m
+CONFIG_INET6_XFRM_MODE_TRANSPORT=m
+CONFIG_INET6_XFRM_MODE_TUNNEL=m
+CONFIG_INET6_XFRM_MODE_BEET=m
+# CONFIG_INET6_XFRM_MODE_ROUTEOPTIMIZATION is not set
+# CONFIG_IPV6_SIT is not set
+CONFIG_IPV6_TUNNEL=m
+CONFIG_IPV6_MULTIPLE_TABLES=y
+CONFIG_IPV6_SUBTREES=y
+CONFIG_IPV6_MROUTE=y
+# CONFIG_IPV6_MROUTE_MULTIPLE_TABLES is not set
+# CONFIG_IPV6_PIMSM_V2 is not set
+# CONFIG_NETWORK_SECMARK is not set
+# CONFIG_NETWORK_PHY_TIMESTAMPING is not set
+CONFIG_NETFILTER=y
+# CONFIG_NETFILTER_DEBUG is not set
+CONFIG_NETFILTER_ADVANCED=y
+# CONFIG_BRIDGE_NETFILTER is not set
+
+#
+# Core Netfilter Configuration
+#
+# CONFIG_NETFILTER_NETLINK_QUEUE is not set
+# CONFIG_NETFILTER_NETLINK_LOG is not set
+# CONFIG_NF_CONNTRACK is not set
+# CONFIG_NETFILTER_XTABLES is not set
+# CONFIG_IP_VS is not set
+
+#
+# IP: Netfilter Configuration
+#
+# CONFIG_NF_DEFRAG_IPV4 is not set
+# CONFIG_IP_NF_QUEUE is not set
+# CONFIG_IP_NF_IPTABLES is not set
+# CONFIG_IP_NF_ARPTABLES is not set
+
+#
+# IPv6: Netfilter Configuration
+#
+# CONFIG_NF_DEFRAG_IPV6 is not set
+# CONFIG_IP6_NF_QUEUE is not set
+# CONFIG_IP6_NF_IPTABLES is not set
+# CONFIG_IP_DCCP is not set
+CONFIG_IP_SCTP=m
+# CONFIG_SCTP_DBG_MSG is not set
+# CONFIG_SCTP_DBG_OBJCNT is not set
+# CONFIG_SCTP_HMAC_NONE is not set
+# CONFIG_SCTP_HMAC_SHA1 is not set
+CONFIG_SCTP_HMAC_MD5=y
+# CONFIG_RDS is not set
+# CONFIG_TIPC is not set
+# CONFIG_ATM is not set
+# CONFIG_L2TP is not set
+CONFIG_STP=m
+CONFIG_BRIDGE=m
+# CONFIG_BRIDGE_IGMP_SNOOPING is not set
+# CONFIG_NET_DSA is not set
+# CONFIG_VLAN_8021Q is not set
+# CONFIG_DECNET is not set
+CONFIG_LLC=m
+# CONFIG_LLC2 is not set
+# CONFIG_IPX is not set
+# CONFIG_ATALK is not set
+# CONFIG_X25 is not set
+# CONFIG_LAPB is not set
+# CONFIG_ECONET is not set
+# CONFIG_WAN_ROUTER is not set
+# CONFIG_PHONET is not set
+# CONFIG_IEEE802154 is not set
+CONFIG_NET_SCHED=y
+
+#
+# Queueing/Scheduling
+#
+# CONFIG_NET_SCH_CBQ is not set
+CONFIG_NET_SCH_HTB=m
+CONFIG_NET_SCH_HFSC=m
+CONFIG_NET_SCH_PRIO=m
+# CONFIG_NET_SCH_MULTIQ is not set
+CONFIG_NET_SCH_RED=m
+# CONFIG_NET_SCH_SFB is not set
+CONFIG_NET_SCH_SFQ=m
+CONFIG_NET_SCH_TEQL=m
+CONFIG_NET_SCH_TBF=m
+CONFIG_NET_SCH_GRED=m
+CONFIG_NET_SCH_DSMARK=m
+# CONFIG_NET_SCH_NETEM is not set
+# CONFIG_NET_SCH_DRR is not set
+# CONFIG_NET_SCH_MQPRIO is not set
+# CONFIG_NET_SCH_CHOKE is not set
+# CONFIG_NET_SCH_QFQ is not set
+CONFIG_NET_SCH_INGRESS=m
+
+#
+# Classification
+#
+CONFIG_NET_CLS=y
+CONFIG_NET_CLS_BASIC=m
+CONFIG_NET_CLS_TCINDEX=m
+CONFIG_NET_CLS_ROUTE4=m
+CONFIG_NET_CLS_FW=m
+CONFIG_NET_CLS_U32=m
+# CONFIG_CLS_U32_PERF is not set
+# CONFIG_CLS_U32_MARK is not set
+# CONFIG_NET_CLS_RSVP is not set
+# CONFIG_NET_CLS_RSVP6 is not set
+CONFIG_NET_CLS_FLOW=m
+# CONFIG_NET_CLS_CGROUP is not set
+CONFIG_NET_EMATCH=y
+CONFIG_NET_EMATCH_STACK=32
+CONFIG_NET_EMATCH_CMP=m
+CONFIG_NET_EMATCH_NBYTE=m
+CONFIG_NET_EMATCH_U32=m
+CONFIG_NET_EMATCH_META=m
+CONFIG_NET_EMATCH_TEXT=m
+CONFIG_NET_CLS_ACT=y
+CONFIG_NET_ACT_POLICE=m
+# CONFIG_NET_ACT_GACT is not set
+CONFIG_NET_ACT_MIRRED=m
+# CONFIG_NET_ACT_NAT is not set
+# CONFIG_NET_ACT_PEDIT is not set
+# CONFIG_NET_ACT_SIMP is not set
+CONFIG_NET_ACT_SKBEDIT=m
+# CONFIG_NET_ACT_CSUM is not set
+# CONFIG_NET_CLS_IND is not set
+CONFIG_NET_SCH_FIFO=y
+# CONFIG_DCB is not set
+CONFIG_DNS_RESOLVER=y
+# CONFIG_BATMAN_ADV is not set
+CONFIG_RPS=y
+CONFIG_RFS_ACCEL=y
+CONFIG_XPS=y
+
+#
+# Network testing
+#
+CONFIG_NET_PKTGEN=m
+# CONFIG_HAMRADIO is not set
+# CONFIG_CAN is not set
+# CONFIG_IRDA is not set
+# CONFIG_BT is not set
+# CONFIG_AF_RXRPC is not set
+CONFIG_FIB_RULES=y
+CONFIG_WIRELESS=y
+# CONFIG_CFG80211 is not set
+# CONFIG_LIB80211 is not set
+
+#
+# CFG80211 needs to be enabled for MAC80211
+#
+# CONFIG_WIMAX is not set
+# CONFIG_RFKILL is not set
+# CONFIG_NET_9P is not set
+# CONFIG_CAIF is not set
+# CONFIG_CEPH_LIB is not set
+
+#
+# Device Drivers
+#
+
+#
+# Generic Driver Options
+#
+CONFIG_UEVENT_HELPER_PATH="/sbin/hotplug"
+# CONFIG_DEVTMPFS is not set
+CONFIG_STANDALONE=y
+CONFIG_PREVENT_FIRMWARE_BUILD=y
+CONFIG_FW_LOADER=y
+CONFIG_FIRMWARE_IN_KERNEL=y
+CONFIG_EXTRA_FIRMWARE=""
+# CONFIG_DEBUG_DRIVER is not set
+# CONFIG_DEBUG_DEVRES is not set
+# CONFIG_SYS_HYPERVISOR is not set
+# CONFIG_CONNECTOR is not set
+CONFIG_MTD=y
+# CONFIG_MTD_DEBUG is not set
+# CONFIG_MTD_TESTS is not set
+# CONFIG_MTD_REDBOOT_PARTS is not set
+CONFIG_MTD_CMDLINE_PARTS=y
+# CONFIG_MTD_AFS_PARTS is not set
+# CONFIG_MTD_AR7_PARTS is not set
+
+#
+# User Modules And Translation Layers
+#
+CONFIG_MTD_CHAR=y
+CONFIG_MTD_BLKDEVS=y
+CONFIG_MTD_BLOCK=y
+# CONFIG_FTL is not set
+CONFIG_NFTL=y
+# CONFIG_NFTL_RW is not set
+# CONFIG_INFTL is not set
+# CONFIG_RFD_FTL is not set
+# CONFIG_SSFDC is not set
+# CONFIG_SM_FTL is not set
+# CONFIG_MTD_OOPS is not set
+
+#
+# RAM/ROM/Flash chip drivers
+#
+CONFIG_MTD_CFI=y
+CONFIG_MTD_JEDECPROBE=y
+CONFIG_MTD_GEN_PROBE=y
+CONFIG_MTD_CFI_ADV_OPTIONS=y
+CONFIG_MTD_CFI_NOSWAP=y
+# CONFIG_MTD_CFI_BE_BYTE_SWAP is not set
+# CONFIG_MTD_CFI_LE_BYTE_SWAP is not set
+CONFIG_MTD_CFI_GEOMETRY=y
+CONFIG_MTD_MAP_BANK_WIDTH_1=y
+CONFIG_MTD_MAP_BANK_WIDTH_2=y
+CONFIG_MTD_MAP_BANK_WIDTH_4=y
+# CONFIG_MTD_MAP_BANK_WIDTH_8 is not set
+# CONFIG_MTD_MAP_BANK_WIDTH_16 is not set
+# CONFIG_MTD_MAP_BANK_WIDTH_32 is not set
+CONFIG_MTD_CFI_I1=y
+CONFIG_MTD_CFI_I2=y
+# CONFIG_MTD_CFI_I4 is not set
+# CONFIG_MTD_CFI_I8 is not set
+# CONFIG_MTD_OTP is not set
+CONFIG_MTD_CFI_INTELEXT=y
+CONFIG_MTD_CFI_AMDSTD=y
+# CONFIG_MTD_CFI_STAA is not set
+CONFIG_MTD_CFI_UTIL=y
+CONFIG_MTD_RAM=y
+CONFIG_MTD_ROM=y
+# CONFIG_MTD_ABSENT is not set
+
+#
+# Mapping drivers for chip access
+#
+# CONFIG_MTD_COMPLEX_MAPPINGS is not set
+CONFIG_MTD_PHYSMAP=y
+# CONFIG_MTD_PHYSMAP_COMPAT is not set
+# CONFIG_MTD_ARM_INTEGRATOR is not set
+# CONFIG_MTD_IMPA7 is not set
+# CONFIG_MTD_INTEL_VR_NOR is not set
+CONFIG_MTD_PLATRAM=y
+
+#
+# Self-contained MTD device drivers
+#
+# CONFIG_MTD_PMC551 is not set
+# CONFIG_MTD_DATAFLASH is not set
+# CONFIG_MTD_M25P80 is not set
+# CONFIG_MTD_SST25L is not set
+# CONFIG_MTD_SLRAM is not set
+# CONFIG_MTD_PHRAM is not set
+# CONFIG_MTD_MTDRAM is not set
+# CONFIG_MTD_BLOCK2MTD is not set
+
+#
+# Disk-On-Chip Device Drivers
+#
+# CONFIG_MTD_DOC2000 is not set
+# CONFIG_MTD_DOC2001 is not set
+# CONFIG_MTD_DOC2001PLUS is not set
+CONFIG_MTD_NAND_ECC=y
+# CONFIG_MTD_NAND_ECC_SMC is not set
+CONFIG_MTD_NAND=y
+# CONFIG_MTD_NAND_VERIFY_WRITE is not set
+# CONFIG_MTD_NAND_ECC_BCH is not set
+# CONFIG_MTD_SM_COMMON is not set
+CONFIG_MTD_NAND_MUSEUM_IDS=y
+# CONFIG_MTD_NAND_DENALI is not set
+CONFIG_MTD_NAND_IDS=y
+# CONFIG_MTD_NAND_RICOH is not set
+# CONFIG_MTD_NAND_DISKONCHIP is not set
+# CONFIG_MTD_NAND_CAFE is not set
+# CONFIG_MTD_NAND_NANDSIM is not set
+CONFIG_MTD_NAND_PLATFORM=y
+# CONFIG_MTD_ALAUDA is not set
+# CONFIG_MTD_ONENAND is not set
+
+#
+# LPDDR flash memory drivers
+#
+# CONFIG_MTD_LPDDR is not set
+# CONFIG_MTD_UBI is not set
+# CONFIG_PARPORT is not set
+CONFIG_BLK_DEV=y
+# CONFIG_BLK_CPQ_DA is not set
+# CONFIG_BLK_CPQ_CISS_DA is not set
+# CONFIG_BLK_DEV_DAC960 is not set
+# CONFIG_BLK_DEV_UMEM is not set
+# CONFIG_BLK_DEV_COW_COMMON is not set
+# CONFIG_BLK_DEV_LOOP is not set
+
+#
+# DRBD disabled because PROC_FS, INET or CONNECTOR not selected
+#
+# CONFIG_BLK_DEV_NBD is not set
+# CONFIG_BLK_DEV_SX8 is not set
+# CONFIG_BLK_DEV_UB is not set
+# CONFIG_BLK_DEV_RAM is not set
+# CONFIG_CDROM_PKTCDVD is not set
+# CONFIG_ATA_OVER_ETH is not set
+# CONFIG_BLK_DEV_RBD is not set
+# CONFIG_SENSORS_LIS3LV02D is not set
+CONFIG_MISC_DEVICES=y
+# CONFIG_AD525X_DPOT is not set
+# CONFIG_PHANTOM is not set
+# CONFIG_INTEL_MID_PTI is not set
+# CONFIG_SGI_IOC4 is not set
+# CONFIG_TIFM_CORE is not set
+# CONFIG_ICS932S401 is not set
+# CONFIG_ENCLOSURE_SERVICES is not set
+# CONFIG_HP_ILO is not set
+# CONFIG_APDS9802ALS is not set
+# CONFIG_ISL29003 is not set
+# CONFIG_ISL29020 is not set
+# CONFIG_SENSORS_TSL2550 is not set
+# CONFIG_SENSORS_BH1780 is not set
+# CONFIG_SENSORS_BH1770 is not set
+# CONFIG_SENSORS_APDS990X is not set
+# CONFIG_HMC6352 is not set
+# CONFIG_DS1682 is not set
+# CONFIG_TI_DAC7512 is not set
+# CONFIG_BMP085 is not set
+# CONFIG_PCH_PHUB is not set
+CONFIG_TRANSCEDE_USIM_SUPPORT=y
+# CONFIG_C2PORT is not set
+
+#
+# EEPROM support
+#
+CONFIG_EEPROM_AT24=y
+# CONFIG_EEPROM_AT25 is not set
+# CONFIG_EEPROM_LEGACY is not set
+# CONFIG_EEPROM_MAX6875 is not set
+# CONFIG_EEPROM_93CX6 is not set
+# CONFIG_CB710_CORE is not set
+
+#
+# Texas Instruments shared transport line discipline
+#
+# CONFIG_SENSORS_LIS3_SPI is not set
+# CONFIG_SENSORS_LIS3_I2C is not set
+CONFIG_HAVE_IDE=y
+# CONFIG_IDE is not set
+
+#
+# SCSI device support
+#
+CONFIG_SCSI_MOD=m
+# CONFIG_RAID_ATTRS is not set
+CONFIG_SCSI=m
+CONFIG_SCSI_DMA=y
+# CONFIG_SCSI_TGT is not set
+# CONFIG_SCSI_NETLINK is not set
+CONFIG_SCSI_PROC_FS=y
+
+#
+# SCSI support type (disk, tape, CD-ROM)
+#
+CONFIG_BLK_DEV_SD=m
+# CONFIG_CHR_DEV_ST is not set
+# CONFIG_CHR_DEV_OSST is not set
+# CONFIG_BLK_DEV_SR is not set
+# CONFIG_CHR_DEV_SG is not set
+# CONFIG_CHR_DEV_SCH is not set
+# CONFIG_SCSI_MULTI_LUN is not set
+# CONFIG_SCSI_CONSTANTS is not set
+# CONFIG_SCSI_LOGGING is not set
+# CONFIG_SCSI_SCAN_ASYNC is not set
+CONFIG_SCSI_WAIT_SCAN=m
+
+#
+# SCSI Transports
+#
+# CONFIG_SCSI_SPI_ATTRS is not set
+# CONFIG_SCSI_FC_ATTRS is not set
+# CONFIG_SCSI_ISCSI_ATTRS is not set
+# CONFIG_SCSI_SAS_ATTRS is not set
+# CONFIG_SCSI_SAS_LIBSAS is not set
+# CONFIG_SCSI_SRP_ATTRS is not set
+CONFIG_SCSI_LOWLEVEL=y
+# CONFIG_ISCSI_TCP is not set
+# CONFIG_ISCSI_BOOT_SYSFS is not set
+# CONFIG_SCSI_CXGB3_ISCSI is not set
+# CONFIG_SCSI_CXGB4_ISCSI is not set
+# CONFIG_SCSI_BNX2_ISCSI is not set
+# CONFIG_SCSI_BNX2X_FCOE is not set
+# CONFIG_BE2ISCSI is not set
+# CONFIG_BLK_DEV_3W_XXXX_RAID is not set
+# CONFIG_SCSI_HPSA is not set
+# CONFIG_SCSI_3W_9XXX is not set
+# CONFIG_SCSI_3W_SAS is not set
+# CONFIG_SCSI_ACARD is not set
+# CONFIG_SCSI_AACRAID is not set
+# CONFIG_SCSI_AIC7XXX is not set
+# CONFIG_SCSI_AIC7XXX_OLD is not set
+# CONFIG_SCSI_AIC79XX is not set
+# CONFIG_SCSI_AIC94XX is not set
+# CONFIG_SCSI_MVSAS is not set
+# CONFIG_SCSI_DPT_I2O is not set
+# CONFIG_SCSI_ADVANSYS is not set
+# CONFIG_SCSI_ARCMSR is not set
+# CONFIG_MEGARAID_NEWGEN is not set
+# CONFIG_MEGARAID_LEGACY is not set
+# CONFIG_MEGARAID_SAS is not set
+# CONFIG_SCSI_MPT2SAS is not set
+# CONFIG_SCSI_HPTIOP is not set
+# CONFIG_LIBFC is not set
+# CONFIG_LIBFCOE is not set
+# CONFIG_FCOE is not set
+# CONFIG_SCSI_DMX3191D is not set
+# CONFIG_SCSI_FUTURE_DOMAIN is not set
+# CONFIG_SCSI_IPS is not set
+# CONFIG_SCSI_INITIO is not set
+# CONFIG_SCSI_INIA100 is not set
+# CONFIG_SCSI_STEX is not set
+# CONFIG_SCSI_SYM53C8XX_2 is not set
+# CONFIG_SCSI_QLOGIC_1280 is not set
+# CONFIG_SCSI_QLA_FC is not set
+# CONFIG_SCSI_QLA_ISCSI is not set
+# CONFIG_SCSI_LPFC is not set
+# CONFIG_SCSI_DC395x is not set
+# CONFIG_SCSI_DC390T is not set
+# CONFIG_SCSI_NSP32 is not set
+# CONFIG_SCSI_DEBUG is not set
+# CONFIG_SCSI_PMCRAID is not set
+# CONFIG_SCSI_PM8001 is not set
+# CONFIG_SCSI_SRP is not set
+# CONFIG_SCSI_BFA_FC is not set
+# CONFIG_SCSI_DH is not set
+# CONFIG_SCSI_OSD_INITIATOR is not set
+# CONFIG_ATA is not set
+# CONFIG_MD is not set
+# CONFIG_TARGET_CORE is not set
+# CONFIG_FUSION is not set
+
+#
+# IEEE 1394 (FireWire) support
+#
+# CONFIG_FIREWIRE is not set
+# CONFIG_FIREWIRE_NOSY is not set
+# CONFIG_I2O is not set
+CONFIG_NETDEVICES=y
+# CONFIG_IFB is not set
+# CONFIG_DUMMY is not set
+# CONFIG_BONDING is not set
+# CONFIG_MACVLAN is not set
+# CONFIG_EQUALIZER is not set
+# CONFIG_TUN is not set
+# CONFIG_VETH is not set
+# CONFIG_ARCNET is not set
+CONFIG_MII=y
+CONFIG_PHYLIB=y
+
+#
+# MII PHY device drivers
+#
+# CONFIG_MARVELL_PHY is not set
+# CONFIG_DAVICOM_PHY is not set
+# CONFIG_QSEMI_PHY is not set
+# CONFIG_LXT_PHY is not set
+# CONFIG_CICADA_PHY is not set
+# CONFIG_VITESSE_PHY is not set
+# CONFIG_SMSC_PHY is not set
+# CONFIG_BROADCOM_PHY is not set
+# CONFIG_ICPLUS_PHY is not set
+# CONFIG_REALTEK_PHY is not set
+# CONFIG_NATIONAL_PHY is not set
+# CONFIG_STE10XP is not set
+# CONFIG_LSI_ET1011C_PHY is not set
+# CONFIG_MICREL_PHY is not set
+# CONFIG_FIXED_PHY is not set
+CONFIG_MDIO_BITBANG=y
+CONFIG_NET_ETHERNET=y
+CONFIG_TRANSCEDE_2200_ETH=y
+CONFIG_TRANSCEDE_VED_3300=y
+# CONFIG_AX88796 is not set
+# CONFIG_HAPPYMEAL is not set
+# CONFIG_SUNGEM is not set
+# CONFIG_CASSINI is not set
+# CONFIG_NET_VENDOR_3COM is not set
+# CONFIG_SMC91X is not set
+# CONFIG_DM9000 is not set
+# CONFIG_ENC28J60 is not set
+# CONFIG_ETHOC is not set
+# CONFIG_SMC911X is not set
+# CONFIG_SMSC911X is not set
+# CONFIG_DNET is not set
+# CONFIG_NET_TULIP is not set
+# CONFIG_HP100 is not set
+# CONFIG_IBM_NEW_EMAC_ZMII is not set
+# CONFIG_IBM_NEW_EMAC_RGMII is not set
+# CONFIG_IBM_NEW_EMAC_TAH is not set
+# CONFIG_IBM_NEW_EMAC_EMAC4 is not set
+# CONFIG_IBM_NEW_EMAC_NO_FLOW_CTRL is not set
+# CONFIG_IBM_NEW_EMAC_MAL_CLR_ICINTSTAT is not set
+# CONFIG_IBM_NEW_EMAC_MAL_COMMON_ERR is not set
+# CONFIG_NET_PCI is not set
+# CONFIG_B44 is not set
+# CONFIG_KS8851 is not set
+# CONFIG_KS8851_MLL is not set
+# CONFIG_ATL2 is not set
+# CONFIG_FTMAC100 is not set
+CONFIG_NETDEV_1000=y
+# CONFIG_ACENIC is not set
+# CONFIG_DL2K is not set
+# CONFIG_E1000 is not set
+CONFIG_E1000E=m
+# CONFIG_IP1000 is not set
+# CONFIG_IGB is not set
+# CONFIG_IGBVF is not set
+# CONFIG_NS83820 is not set
+# CONFIG_HAMACHI is not set
+# CONFIG_YELLOWFIN is not set
+# CONFIG_R8169 is not set
+# CONFIG_SIS190 is not set
+# CONFIG_SKGE is not set
+# CONFIG_SKY2 is not set
+# CONFIG_VIA_VELOCITY is not set
+# CONFIG_TIGON3 is not set
+# CONFIG_BNX2 is not set
+# CONFIG_CNIC is not set
+# CONFIG_QLA3XXX is not set
+# CONFIG_ATL1 is not set
+# CONFIG_ATL1E is not set
+# CONFIG_ATL1C is not set
+# CONFIG_JME is not set
+# CONFIG_STMMAC_ETH is not set
+# CONFIG_PCH_GBE is not set
+CONFIG_NETDEV_10000=y
+# CONFIG_CHELSIO_T1 is not set
+# CONFIG_CHELSIO_T3 is not set
+# CONFIG_CHELSIO_T4 is not set
+# CONFIG_CHELSIO_T4VF is not set
+# CONFIG_ENIC is not set
+# CONFIG_IXGBE is not set
+# CONFIG_IXGBEVF is not set
+# CONFIG_IXGB is not set
+# CONFIG_S2IO is not set
+# CONFIG_VXGE is not set
+# CONFIG_MYRI10GE is not set
+# CONFIG_NETXEN_NIC is not set
+# CONFIG_NIU is not set
+# CONFIG_MLX4_EN is not set
+# CONFIG_MLX4_CORE is not set
+# CONFIG_TEHUTI is not set
+# CONFIG_BNX2X is not set
+# CONFIG_QLCNIC is not set
+# CONFIG_QLGE is not set
+# CONFIG_BNA is not set
+# CONFIG_SFC is not set
+# CONFIG_BE2NET is not set
+# CONFIG_TR is not set
+CONFIG_WLAN=y
+# CONFIG_ATMEL is not set
+# CONFIG_PRISM54 is not set
+# CONFIG_USB_ZD1201 is not set
+# CONFIG_HOSTAP is not set
+
+#
+# Enable WiMAX (Networking options) to see the WiMAX drivers
+#
+
+#
+# USB Network Adapters
+#
+# CONFIG_USB_CATC is not set
+# CONFIG_USB_KAWETH is not set
+# CONFIG_USB_PEGASUS is not set
+# CONFIG_USB_RTL8150 is not set
+CONFIG_USB_USBNET=m
+CONFIG_USB_NET_AX8817X=m
+CONFIG_USB_NET_CDCETHER=m
+# CONFIG_USB_NET_CDC_EEM is not set
+CONFIG_USB_NET_CDC_NCM=m
+# CONFIG_USB_NET_DM9601 is not set
+# CONFIG_USB_NET_SMSC75XX is not set
+# CONFIG_USB_NET_SMSC95XX is not set
+# CONFIG_USB_NET_GL620A is not set
+CONFIG_USB_NET_NET1080=m
+# CONFIG_USB_NET_PLUSB is not set
+# CONFIG_USB_NET_MCS7830 is not set
+# CONFIG_USB_NET_RNDIS_HOST is not set
+CONFIG_USB_NET_CDC_SUBSET=m
+# CONFIG_USB_ALI_M5632 is not set
+# CONFIG_USB_AN2720 is not set
+CONFIG_USB_BELKIN=y
+CONFIG_USB_ARMLINUX=y
+# CONFIG_USB_EPSON2888 is not set
+# CONFIG_USB_KC2190 is not set
+CONFIG_USB_NET_ZAURUS=m
+# CONFIG_USB_NET_CX82310_ETH is not set
+# CONFIG_USB_NET_KALMIA is not set
+# CONFIG_USB_NET_INT51X1 is not set
+# CONFIG_USB_IPHETH is not set
+# CONFIG_USB_SIERRA_NET is not set
+# CONFIG_USB_VL600 is not set
+# CONFIG_WAN is not set
+
+#
+# CAIF transport drivers
+#
+# CONFIG_FDDI is not set
+# CONFIG_HIPPI is not set
+# CONFIG_PPP is not set
+# CONFIG_SLIP is not set
+# CONFIG_NET_FC is not set
+# CONFIG_NETPOLL is not set
+# CONFIG_NET_POLL_CONTROLLER is not set
+# CONFIG_VMXNET3 is not set
+# CONFIG_ISDN is not set
+# CONFIG_PHONE is not set
+
+#
+# Input device support
+#
+CONFIG_INPUT=y
+# CONFIG_INPUT_FF_MEMLESS is not set
+# CONFIG_INPUT_POLLDEV is not set
+# CONFIG_INPUT_SPARSEKMAP is not set
+
+#
+# Userland interfaces
+#
+CONFIG_INPUT_MOUSEDEV=y
+# CONFIG_INPUT_MOUSEDEV_PSAUX is not set
+CONFIG_INPUT_MOUSEDEV_SCREEN_X=1024
+CONFIG_INPUT_MOUSEDEV_SCREEN_Y=768
+# CONFIG_INPUT_JOYDEV is not set
+# CONFIG_INPUT_EVDEV is not set
+# CONFIG_INPUT_EVBUG is not set
+
+#
+# Input Device Drivers
+#
+# CONFIG_INPUT_KEYBOARD is not set
+# CONFIG_INPUT_MOUSE is not set
+# CONFIG_INPUT_JOYSTICK is not set
+# CONFIG_INPUT_TABLET is not set
+# CONFIG_INPUT_TOUCHSCREEN is not set
+# CONFIG_INPUT_MISC is not set
+
+#
+# Hardware I/O ports
+#
+CONFIG_SERIO=y
+CONFIG_SERIO_SERPORT=y
+# CONFIG_SERIO_PCIPS2 is not set
+# CONFIG_SERIO_RAW is not set
+# CONFIG_SERIO_ALTERA_PS2 is not set
+# CONFIG_SERIO_PS2MULT is not set
+# CONFIG_GAMEPORT is not set
+
+#
+# Character devices
+#
+CONFIG_VT=y
+CONFIG_CONSOLE_TRANSLATIONS=y
+CONFIG_VT_CONSOLE=y
+CONFIG_HW_CONSOLE=y
+# CONFIG_VT_HW_CONSOLE_BINDING is not set
+CONFIG_UNIX98_PTYS=y
+# CONFIG_DEVPTS_MULTIPLE_INSTANCES is not set
+CONFIG_LEGACY_PTYS=y
+CONFIG_LEGACY_PTY_COUNT=16
+# CONFIG_SERIAL_NONSTANDARD is not set
+# CONFIG_NOZOMI is not set
+# CONFIG_N_GSM is not set
+# CONFIG_TRACE_SINK is not set
+CONFIG_DEVKMEM=y
+
+#
+# Serial drivers
+#
+CONFIG_SERIAL_8250=y
+CONFIG_SERIAL_8250_CONSOLE=y
+CONFIG_SERIAL_8250_PCI=y
+CONFIG_SERIAL_8250_NR_UARTS=1
+CONFIG_SERIAL_8250_RUNTIME_UARTS=1
+CONFIG_SERIAL_8250_EXTENDED=y
+# CONFIG_SERIAL_8250_MANY_PORTS is not set
+# CONFIG_SERIAL_8250_SHARE_IRQ is not set
+# CONFIG_SERIAL_8250_DETECT_IRQ is not set
+# CONFIG_SERIAL_8250_RSA is not set
+
+#
+# Non-8250 serial port support
+#
+# CONFIG_SERIAL_MAX3100 is not set
+# CONFIG_SERIAL_MAX3107 is not set
+# CONFIG_SERIAL_MFD_HSU is not set
+CONFIG_SERIAL_CORE=y
+CONFIG_SERIAL_CORE_CONSOLE=y
+# CONFIG_SERIAL_JSM is not set
+# CONFIG_SERIAL_TIMBERDALE is not set
+# CONFIG_SERIAL_ALTERA_JTAGUART is not set
+# CONFIG_SERIAL_ALTERA_UART is not set
+# CONFIG_SERIAL_PCH_UART is not set
+# CONFIG_SERIAL_XILINX_PS_UART is not set
+# CONFIG_HVC_DCC is not set
+# CONFIG_IPMI_HANDLER is not set
+CONFIG_HW_RANDOM=y
+# CONFIG_HW_RANDOM_TIMERIOMEM is not set
+# CONFIG_R3964 is not set
+# CONFIG_APPLICOM is not set
+# CONFIG_RAW_DRIVER is not set
+# CONFIG_TCG_TPM is not set
+CONFIG_DEVPORT=y
+# CONFIG_RAMOOPS is not set
+CONFIG_I2C=y
+CONFIG_I2C_BOARDINFO=y
+CONFIG_I2C_COMPAT=y
+CONFIG_I2C_CHARDEV=y
+# CONFIG_I2C_MUX is not set
+CONFIG_I2C_HELPER_AUTO=y
+
+#
+# I2C Hardware Bus support
+#
+
+#
+# PC SMBus host controller drivers
+#
+# CONFIG_I2C_ALI1535 is not set
+# CONFIG_I2C_ALI1563 is not set
+# CONFIG_I2C_ALI15X3 is not set
+# CONFIG_I2C_AMD756 is not set
+# CONFIG_I2C_AMD8111 is not set
+# CONFIG_I2C_I801 is not set
+# CONFIG_I2C_ISCH is not set
+# CONFIG_I2C_PIIX4 is not set
+# CONFIG_I2C_NFORCE2 is not set
+# CONFIG_I2C_SIS5595 is not set
+# CONFIG_I2C_SIS630 is not set
+# CONFIG_I2C_SIS96X is not set
+# CONFIG_I2C_VIA is not set
+# CONFIG_I2C_VIAPRO is not set
+
+#
+# I2C system bus drivers (mostly embedded / system-on-chip)
+#
+# CONFIG_I2C_INTEL_MID is not set
+# CONFIG_I2C_OCORES is not set
+# CONFIG_I2C_PCA_PLATFORM is not set
+# CONFIG_I2C_PXA_PCI is not set
+# CONFIG_I2C_SIMTEC is not set
+CONFIG_I2C_TRANSCEDE=y
+# CONFIG_I2C_XILINX is not set
+# CONFIG_I2C_EG20T is not set
+
+#
+# External I2C/SMBus adapter drivers
+#
+# CONFIG_I2C_DIOLAN_U2C is not set
+# CONFIG_I2C_PARPORT_LIGHT is not set
+# CONFIG_I2C_TAOS_EVM is not set
+# CONFIG_I2C_TINY_USB is not set
+
+#
+# Other I2C/SMBus bus drivers
+#
+# CONFIG_I2C_STUB is not set
+# CONFIG_I2C_DEBUG_CORE is not set
+# CONFIG_I2C_DEBUG_ALGO is not set
+# CONFIG_I2C_DEBUG_BUS is not set
+CONFIG_SPI=y
+# CONFIG_SPI_DEBUG is not set
+CONFIG_SPI_MASTER=y
+
+#
+# SPI Master Controller Drivers
+#
+# CONFIG_SPI_ALTERA is not set
+# CONFIG_SPI_BITBANG is not set
+# CONFIG_SPI_PXA2XX_PCI is not set
+# CONFIG_SPI_TOPCLIFF_PCH is not set
+CONFIG_SPI_TRANSCEDE=y
+# CONFIG_SPI_TRANSCEDE_CS_GPIO is not set
+# CONFIG_SPI_XILINX is not set
+# CONFIG_SPI_DESIGNWARE is not set
+
+#
+# SPI Protocol Masters
+#
+CONFIG_SPI_SPIDEV=y
+# CONFIG_SPI_TLE62X0 is not set
+# CONFIG_SPI_CDCE62005 is not set
+
+#
+# PPS support
+#
+# CONFIG_PPS is not set
+
+#
+# PPS generators support
+#
+
+#
+# PTP clock support
+#
+
+#
+# Enable Device Drivers -> PPS to see the PTP clock options.
+#
+# CONFIG_W1 is not set
+# CONFIG_POWER_SUPPLY is not set
+# CONFIG_HWMON is not set
+# CONFIG_THERMAL is not set
+CONFIG_WATCHDOG=y
+# CONFIG_WATCHDOG_NOWAYOUT is not set
+
+#
+# Watchdog Device Drivers
+#
+# CONFIG_SOFT_WATCHDOG is not set
+CONFIG_MPCORE_WATCHDOG=y
+# CONFIG_MAX63XX_WATCHDOG is not set
+# CONFIG_ALIM7101_WDT is not set
+
+#
+# PCI-based Watchdog Cards
+#
+# CONFIG_PCIPCWATCHDOG is not set
+# CONFIG_WDTPCI is not set
+
+#
+# USB-based Watchdog Cards
+#
+# CONFIG_USBPCWATCHDOG is not set
+CONFIG_SSB_POSSIBLE=y
+
+#
+# Sonics Silicon Backplane
+#
+# CONFIG_SSB is not set
+CONFIG_BCMA_POSSIBLE=y
+
+#
+# Broadcom specific AMBA
+#
+# CONFIG_BCMA is not set
+CONFIG_MFD_SUPPORT=y
+# CONFIG_MFD_CORE is not set
+# CONFIG_MFD_88PM860X is not set
+# CONFIG_MFD_SM501 is not set
+# CONFIG_HTC_PASIC3 is not set
+# CONFIG_TPS6105X is not set
+# CONFIG_TPS6507X is not set
+# CONFIG_TWL4030_CORE is not set
+# CONFIG_MFD_STMPE is not set
+# CONFIG_MFD_TC3589X is not set
+# CONFIG_MFD_TMIO is not set
+# CONFIG_PMIC_DA903X is not set
+# CONFIG_PMIC_ADP5520 is not set
+# CONFIG_MFD_MAX8925 is not set
+# CONFIG_MFD_MAX8997 is not set
+# CONFIG_MFD_MAX8998 is not set
+# CONFIG_MFD_WM8400 is not set
+# CONFIG_MFD_WM831X_I2C is not set
+# CONFIG_MFD_WM831X_SPI is not set
+# CONFIG_MFD_WM8350_I2C is not set
+# CONFIG_MFD_WM8994 is not set
+# CONFIG_MFD_PCF50633 is not set
+# CONFIG_MFD_MC13XXX is not set
+# CONFIG_ABX500_CORE is not set
+# CONFIG_EZX_PCAP is not set
+# CONFIG_LPC_SCH is not set
+# CONFIG_MFD_RDC321X is not set
+# CONFIG_MFD_JANZ_CMODIO is not set
+# CONFIG_MFD_VX855 is not set
+# CONFIG_MFD_WL1273_CORE is not set
+# CONFIG_REGULATOR is not set
+# CONFIG_MEDIA_SUPPORT is not set
+
+#
+# Graphics support
+#
+CONFIG_VGA_ARB=y
+CONFIG_VGA_ARB_MAX_GPUS=16
+# CONFIG_DRM is not set
+# CONFIG_STUB_POULSBO is not set
+# CONFIG_VGASTATE is not set
+# CONFIG_VIDEO_OUTPUT_CONTROL is not set
+# CONFIG_FB is not set
+# CONFIG_BACKLIGHT_LCD_SUPPORT is not set
+
+#
+# Display device support
+#
+# CONFIG_DISPLAY_SUPPORT is not set
+
+#
+# Console display driver support
+#
+CONFIG_DUMMY_CONSOLE=y
+# CONFIG_SOUND is not set
+CONFIG_HID_SUPPORT=y
+CONFIG_HID=y
+# CONFIG_HIDRAW is not set
+
+#
+# USB Input Devices
+#
+# CONFIG_USB_HID is not set
+# CONFIG_HID_PID is not set
+
+#
+# Special HID drivers
+#
+CONFIG_USB_SUPPORT=y
+CONFIG_USB_ARCH_HAS_HCD=y
+CONFIG_USB_ARCH_HAS_OHCI=y
+CONFIG_USB_ARCH_HAS_EHCI=y
+CONFIG_USB=m
+# CONFIG_USB_DEBUG is not set
+# CONFIG_USB_ANNOUNCE_NEW_DEVICES is not set
+
+#
+# Miscellaneous USB options
+#
+# CONFIG_USB_DEVICEFS is not set
+CONFIG_USB_DEVICE_CLASS=y
+# CONFIG_USB_DYNAMIC_MINORS is not set
+# CONFIG_USB_MON is not set
+# CONFIG_USB_WUSB is not set
+# CONFIG_USB_WUSB_CBAF is not set
+
+#
+# USB Host Controller Drivers
+#
+# CONFIG_USB_C67X00_HCD is not set
+# CONFIG_USB_XHCI_HCD is not set
+CONFIG_USB_EHCI_HCD=m
+# CONFIG_USB_EHCI_ROOT_HUB_TT is not set
+CONFIG_USB_EHCI_TT_NEWSCHED=y
+# CONFIG_USB_OXU210HP_HCD is not set
+# CONFIG_USB_ISP116X_HCD is not set
+# CONFIG_USB_ISP1760_HCD is not set
+# CONFIG_USB_ISP1362_HCD is not set
+# CONFIG_USB_OHCI_HCD is not set
+# CONFIG_USB_UHCI_HCD is not set
+# CONFIG_USB_SL811_HCD is not set
+# CONFIG_USB_R8A66597_HCD is not set
+# CONFIG_USB_WHCI_HCD is not set
+# CONFIG_USB_HWA_HCD is not set
+
+#
+# Enable Host or Gadget support to see Inventra options
+#
+# CONFIG_USB_MUSB_HDRC is not set
+
+#
+# USB Device Class drivers
+#
+# CONFIG_USB_ACM is not set
+# CONFIG_USB_PRINTER is not set
+# CONFIG_USB_WDM is not set
+# CONFIG_USB_TMC is not set
+
+#
+# NOTE: USB_STORAGE depends on SCSI but BLK_DEV_SD may
+#
+
+#
+# also be needed; see USB_STORAGE Help for more info
+#
+CONFIG_USB_STORAGE=m
+# CONFIG_USB_STORAGE_DEBUG is not set
+# CONFIG_USB_STORAGE_REALTEK is not set
+# CONFIG_USB_STORAGE_DATAFAB is not set
+# CONFIG_USB_STORAGE_FREECOM is not set
+# CONFIG_USB_STORAGE_ISD200 is not set
+# CONFIG_USB_STORAGE_USBAT is not set
+# CONFIG_USB_STORAGE_SDDR09 is not set
+# CONFIG_USB_STORAGE_SDDR55 is not set
+# CONFIG_USB_STORAGE_JUMPSHOT is not set
+# CONFIG_USB_STORAGE_ALAUDA is not set
+# CONFIG_USB_STORAGE_ONETOUCH is not set
+# CONFIG_USB_STORAGE_KARMA is not set
+# CONFIG_USB_STORAGE_CYPRESS_ATACB is not set
+# CONFIG_USB_STORAGE_ENE_UB6250 is not set
+# CONFIG_USB_UAS is not set
+# CONFIG_USB_LIBUSUAL is not set
+
+#
+# USB Imaging devices
+#
+# CONFIG_USB_MDC800 is not set
+# CONFIG_USB_MICROTEK is not set
+
+#
+# USB port drivers
+#
+# CONFIG_USB_SERIAL is not set
+
+#
+# USB Miscellaneous drivers
+#
+# CONFIG_USB_EMI62 is not set
+# CONFIG_USB_EMI26 is not set
+# CONFIG_USB_ADUTUX is not set
+# CONFIG_USB_SEVSEG is not set
+# CONFIG_USB_RIO500 is not set
+# CONFIG_USB_LEGOTOWER is not set
+# CONFIG_USB_LCD is not set
+# CONFIG_USB_LED is not set
+# CONFIG_USB_CYPRESS_CY7C63 is not set
+# CONFIG_USB_CYTHERM is not set
+# CONFIG_USB_IDMOUSE is not set
+# CONFIG_USB_FTDI_ELAN is not set
+# CONFIG_USB_APPLEDISPLAY is not set
+# CONFIG_USB_SISUSBVGA is not set
+# CONFIG_USB_LD is not set
+# CONFIG_USB_TRANCEVIBRATOR is not set
+# CONFIG_USB_IOWARRIOR is not set
+# CONFIG_USB_TEST is not set
+# CONFIG_USB_ISIGHTFW is not set
+# CONFIG_USB_YUREX is not set
+# CONFIG_USB_GADGET is not set
+
+#
+# OTG and related infrastructure
+#
+# CONFIG_USB_ULPI is not set
+# CONFIG_NOP_USB_XCEIV is not set
+CONFIG_DWC_OTG=m
+CONFIG_DWC_OTG_HOST_ONLY=y
+# CONFIG_DWC_OTG_DEVICE_ONLY is not set
+CONFIG_DWC_OTG_TRANSCEDE=y
+# CONFIG_DWC_OTG_DEBUG is not set
+# CONFIG_UWB is not set
+# CONFIG_MMC is not set
+# CONFIG_MEMSTICK is not set
+# CONFIG_NEW_LEDS is not set
+# CONFIG_NFC_DEVICES is not set
+# CONFIG_ACCESSIBILITY is not set
+# CONFIG_INFINIBAND is not set
+CONFIG_RTC_LIB=y
+# CONFIG_RTC_CLASS is not set
+# CONFIG_DMADEVICES is not set
+# CONFIG_AUXDISPLAY is not set
+CONFIG_UIO=y
+# CONFIG_UIO_CIF is not set
+CONFIG_UIO_PDRV=y
+# CONFIG_UIO_PDRV_GENIRQ is not set
+# CONFIG_UIO_AEC is not set
+# CONFIG_UIO_SERCOS3 is not set
+# CONFIG_UIO_PCI_GENERIC is not set
+# CONFIG_UIO_NETX is not set
+# CONFIG_STAGING is not set
+
+#
+# File systems
+#
+# CONFIG_EXT2_FS is not set
+# CONFIG_EXT3_FS is not set
+# CONFIG_EXT4_FS is not set
+# CONFIG_REISERFS_FS is not set
+# CONFIG_JFS_FS is not set
+# CONFIG_XFS_FS is not set
+# CONFIG_GFS2_FS is not set
+# CONFIG_BTRFS_FS is not set
+# CONFIG_NILFS2_FS is not set
+CONFIG_FS_POSIX_ACL=y
+CONFIG_FILE_LOCKING=y
+CONFIG_FSNOTIFY=y
+CONFIG_DNOTIFY=y
+CONFIG_INOTIFY_USER=y
+# CONFIG_FANOTIFY is not set
+# CONFIG_QUOTA is not set
+# CONFIG_QUOTACTL is not set
+CONFIG_AUTOFS4_FS=y
+# CONFIG_FUSE_FS is not set
+
+#
+# Caches
+#
+# CONFIG_FSCACHE is not set
+
+#
+# CD-ROM/DVD Filesystems
+#
+# CONFIG_ISO9660_FS is not set
+# CONFIG_UDF_FS is not set
+
+#
+# DOS/FAT/NT Filesystems
+#
+CONFIG_FAT_FS=y
+CONFIG_MSDOS_FS=y
+CONFIG_VFAT_FS=y
+CONFIG_FAT_DEFAULT_CODEPAGE=437
+CONFIG_FAT_DEFAULT_IOCHARSET="iso8859-1"
+# CONFIG_NTFS_FS is not set
+
+#
+# Pseudo filesystems
+#
+CONFIG_PROC_FS=y
+CONFIG_PROC_SYSCTL=y
+CONFIG_PROC_PAGE_MONITOR=y
+CONFIG_SYSFS=y
+CONFIG_TMPFS=y
+# CONFIG_TMPFS_POSIX_ACL is not set
+# CONFIG_TMPFS_XATTR is not set
+# CONFIG_HUGETLB_PAGE is not set
+# CONFIG_CONFIGFS_FS is not set
+CONFIG_MISC_FILESYSTEMS=y
+# CONFIG_ADFS_FS is not set
+# CONFIG_AFFS_FS is not set
+# CONFIG_ECRYPT_FS is not set
+# CONFIG_HFS_FS is not set
+# CONFIG_HFSPLUS_FS is not set
+# CONFIG_BEFS_FS is not set
+# CONFIG_BFS_FS is not set
+# CONFIG_EFS_FS is not set
+CONFIG_JFFS2_FS=y
+CONFIG_JFFS2_FS_DEBUG=0
+CONFIG_JFFS2_FS_WRITEBUFFER=y
+# CONFIG_JFFS2_FS_WBUF_VERIFY is not set
+# CONFIG_JFFS2_SUMMARY is not set
+# CONFIG_JFFS2_FS_XATTR is not set
+# CONFIG_JFFS2_COMPRESSION_OPTIONS is not set
+CONFIG_JFFS2_ZLIB=y
+# CONFIG_JFFS2_LZO is not set
+CONFIG_JFFS2_RTIME=y
+# CONFIG_JFFS2_RUBIN is not set
+# CONFIG_LOGFS is not set
+# CONFIG_CRAMFS is not set
+# CONFIG_SQUASHFS is not set
+# CONFIG_VXFS_FS is not set
+# CONFIG_MINIX_FS is not set
+# CONFIG_OMFS_FS is not set
+# CONFIG_HPFS_FS is not set
+# CONFIG_QNX4FS_FS is not set
+# CONFIG_ROMFS_FS is not set
+# CONFIG_PSTORE is not set
+# CONFIG_SYSV_FS is not set
+# CONFIG_UFS_FS is not set
+CONFIG_NETWORK_FILESYSTEMS=y
+CONFIG_NFS_FS=y
+CONFIG_NFS_V3=y
+CONFIG_NFS_V3_ACL=y
+CONFIG_NFS_V4=y
+# CONFIG_NFS_V4_1 is not set
+CONFIG_ROOT_NFS=y
+# CONFIG_NFS_USE_LEGACY_DNS is not set
+CONFIG_NFS_USE_KERNEL_DNS=y
+# CONFIG_NFS_USE_NEW_IDMAPPER is not set
+# CONFIG_NFSD is not set
+CONFIG_LOCKD=y
+CONFIG_LOCKD_V4=y
+CONFIG_NFS_ACL_SUPPORT=y
+CONFIG_NFS_COMMON=y
+CONFIG_SUNRPC=y
+CONFIG_SUNRPC_GSS=y
+# CONFIG_CEPH_FS is not set
+# CONFIG_CIFS is not set
+# CONFIG_NCP_FS is not set
+# CONFIG_CODA_FS is not set
+# CONFIG_AFS_FS is not set
+
+#
+# Partition Types
+#
+# CONFIG_PARTITION_ADVANCED is not set
+CONFIG_MSDOS_PARTITION=y
+CONFIG_NLS=y
+CONFIG_NLS_DEFAULT="iso8859-1"
+CONFIG_NLS_CODEPAGE_437=y
+# CONFIG_NLS_CODEPAGE_737 is not set
+# CONFIG_NLS_CODEPAGE_775 is not set
+# CONFIG_NLS_CODEPAGE_850 is not set
+# CONFIG_NLS_CODEPAGE_852 is not set
+# CONFIG_NLS_CODEPAGE_855 is not set
+# CONFIG_NLS_CODEPAGE_857 is not set
+# CONFIG_NLS_CODEPAGE_860 is not set
+# CONFIG_NLS_CODEPAGE_861 is not set
+# CONFIG_NLS_CODEPAGE_862 is not set
+# CONFIG_NLS_CODEPAGE_863 is not set
+# CONFIG_NLS_CODEPAGE_864 is not set
+# CONFIG_NLS_CODEPAGE_865 is not set
+# CONFIG_NLS_CODEPAGE_866 is not set
+# CONFIG_NLS_CODEPAGE_869 is not set
+# CONFIG_NLS_CODEPAGE_936 is not set
+# CONFIG_NLS_CODEPAGE_950 is not set
+# CONFIG_NLS_CODEPAGE_932 is not set
+# CONFIG_NLS_CODEPAGE_949 is not set
+# CONFIG_NLS_CODEPAGE_874 is not set
+# CONFIG_NLS_ISO8859_8 is not set
+# CONFIG_NLS_CODEPAGE_1250 is not set
+# CONFIG_NLS_CODEPAGE_1251 is not set
+# CONFIG_NLS_ASCII is not set
+CONFIG_NLS_ISO8859_1=y
+# CONFIG_NLS_ISO8859_2 is not set
+# CONFIG_NLS_ISO8859_3 is not set
+# CONFIG_NLS_ISO8859_4 is not set
+# CONFIG_NLS_ISO8859_5 is not set
+# CONFIG_NLS_ISO8859_6 is not set
+# CONFIG_NLS_ISO8859_7 is not set
+# CONFIG_NLS_ISO8859_9 is not set
+# CONFIG_NLS_ISO8859_13 is not set
+# CONFIG_NLS_ISO8859_14 is not set
+# CONFIG_NLS_ISO8859_15 is not set
+# CONFIG_NLS_KOI8_R is not set
+# CONFIG_NLS_KOI8_U is not set
+# CONFIG_NLS_UTF8 is not set
+
+#
+# Kernel hacking
+#
+# CONFIG_PRINTK_TIME is not set
+CONFIG_DEFAULT_MESSAGE_LOGLEVEL=4
+CONFIG_ENABLE_WARN_DEPRECATED=y
+CONFIG_ENABLE_MUST_CHECK=y
+CONFIG_FRAME_WARN=1024
+# CONFIG_MAGIC_SYSRQ is not set
+# CONFIG_STRIP_ASM_SYMS is not set
+# CONFIG_UNUSED_SYMBOLS is not set
+CONFIG_DEBUG_FS=y
+# CONFIG_HEADERS_CHECK is not set
+# CONFIG_DEBUG_SECTION_MISMATCH is not set
+CONFIG_DEBUG_KERNEL=y
+# CONFIG_LOCKUP_DETECTOR is not set
+# CONFIG_HARDLOCKUP_DETECTOR is not set
+CONFIG_DETECT_HUNG_TASK=y
+CONFIG_DEFAULT_HUNG_TASK_TIMEOUT=120
+# CONFIG_BOOTPARAM_HUNG_TASK_PANIC is not set
+CONFIG_BOOTPARAM_HUNG_TASK_PANIC_VALUE=0
+# CONFIG_SCHED_DEBUG is not set
+# CONFIG_SCHEDSTATS is not set
+# CONFIG_TIMER_STATS is not set
+# CONFIG_DEBUG_OBJECTS is not set
+# CONFIG_DEBUG_SLAB is not set
+# CONFIG_DEBUG_KMEMLEAK is not set
+# CONFIG_DEBUG_PREEMPT is not set
+# CONFIG_DEBUG_RT_MUTEXES is not set
+# CONFIG_RT_MUTEX_TESTER is not set
+# CONFIG_DEBUG_SPINLOCK is not set
+# CONFIG_DEBUG_MUTEXES is not set
+# CONFIG_DEBUG_LOCK_ALLOC is not set
+# CONFIG_PROVE_LOCKING is not set
+# CONFIG_SPARSE_RCU_POINTER is not set
+# CONFIG_LOCK_STAT is not set
+# CONFIG_DEBUG_SPINLOCK_SLEEP is not set
+# CONFIG_DEBUG_LOCKING_API_SELFTESTS is not set
+# CONFIG_DEBUG_STACK_USAGE is not set
+# CONFIG_DEBUG_KOBJECT is not set
+CONFIG_DEBUG_BUGVERBOSE=y
+# CONFIG_DEBUG_INFO is not set
+# CONFIG_DEBUG_VM is not set
+# CONFIG_DEBUG_WRITECOUNT is not set
+CONFIG_DEBUG_MEMORY_INIT=y
+# CONFIG_DEBUG_LIST is not set
+# CONFIG_TEST_LIST_SORT is not set
+# CONFIG_DEBUG_SG is not set
+# CONFIG_DEBUG_NOTIFIERS is not set
+# CONFIG_DEBUG_CREDENTIALS is not set
+# CONFIG_BOOT_PRINTK_DELAY is not set
+# CONFIG_RCU_TORTURE_TEST is not set
+CONFIG_RCU_CPU_STALL_TIMEOUT=60
+CONFIG_RCU_CPU_STALL_VERBOSE=y
+# CONFIG_BACKTRACE_SELF_TEST is not set
+# CONFIG_DEBUG_BLOCK_EXT_DEVT is not set
+# CONFIG_DEBUG_FORCE_WEAK_PER_CPU is not set
+# CONFIG_DEBUG_PER_CPU_MAPS is not set
+# CONFIG_LKDTM is not set
+# CONFIG_FAULT_INJECTION is not set
+# CONFIG_SYSCTL_SYSCALL_CHECK is not set
+# CONFIG_DEBUG_PAGEALLOC is not set
+CONFIG_HAVE_FUNCTION_TRACER=y
+CONFIG_HAVE_FUNCTION_GRAPH_TRACER=y
+CONFIG_HAVE_DYNAMIC_FTRACE=y
+CONFIG_HAVE_FTRACE_MCOUNT_RECORD=y
+CONFIG_HAVE_C_RECORDMCOUNT=y
+CONFIG_TRACING_SUPPORT=y
+CONFIG_FTRACE=y
+# CONFIG_FUNCTION_TRACER is not set
+# CONFIG_IRQSOFF_TRACER is not set
+# CONFIG_PREEMPT_TRACER is not set
+# CONFIG_SCHED_TRACER is not set
+# CONFIG_MISSED_TIMER_OFFSETS_HIST is not set
+# CONFIG_ENABLE_DEFAULT_TRACERS is not set
+CONFIG_BRANCH_PROFILE_NONE=y
+# CONFIG_PROFILE_ANNOTATED_BRANCHES is not set
+# CONFIG_PROFILE_ALL_BRANCHES is not set
+# CONFIG_STACK_TRACER is not set
+# CONFIG_BLK_DEV_IO_TRACE is not set
+# CONFIG_DYNAMIC_DEBUG is not set
+# CONFIG_DMA_API_DEBUG is not set
+# CONFIG_ATOMIC64_SELFTEST is not set
+# CONFIG_SAMPLES is not set
+CONFIG_HAVE_ARCH_KGDB=y
+# CONFIG_KGDB is not set
+# CONFIG_TEST_KSTRTOX is not set
+# CONFIG_STRICT_DEVMEM is not set
+CONFIG_ARM_UNWIND=y
+CONFIG_DEBUG_USER=y
+CONFIG_DEBUG_LL=y
+CONFIG_EARLY_PRINTK=y
+# CONFIG_DEBUG_ICEDCC is not set
+# CONFIG_OC_ETM is not set
+
+#
+# Security options
+#
+CONFIG_KEYS=y
+# CONFIG_KEYS_DEBUG_PROC_KEYS is not set
+# CONFIG_SECURITY_DMESG_RESTRICT is not set
+# CONFIG_SECURITY is not set
+# CONFIG_SECURITYFS is not set
+CONFIG_DEFAULT_SECURITY_DAC=y
+CONFIG_DEFAULT_SECURITY=""
+CONFIG_CRYPTO=y
+
+#
+# Crypto core or helper
+#
+CONFIG_CRYPTO_ALGAPI=y
+CONFIG_CRYPTO_ALGAPI2=y
+CONFIG_CRYPTO_AEAD=y
+CONFIG_CRYPTO_AEAD2=y
+CONFIG_CRYPTO_BLKCIPHER=y
+CONFIG_CRYPTO_BLKCIPHER2=y
+CONFIG_CRYPTO_HASH=y
+CONFIG_CRYPTO_HASH2=y
+CONFIG_CRYPTO_RNG2=y
+CONFIG_CRYPTO_PCOMP2=y
+CONFIG_CRYPTO_MANAGER=y
+CONFIG_CRYPTO_MANAGER2=y
+CONFIG_CRYPTO_MANAGER_DISABLE_TESTS=y
+# CONFIG_CRYPTO_GF128MUL is not set
+CONFIG_CRYPTO_NULL=y
+# CONFIG_CRYPTO_PCRYPT is not set
+CONFIG_CRYPTO_WORKQUEUE=y
+# CONFIG_CRYPTO_CRYPTD is not set
+CONFIG_CRYPTO_AUTHENC=y
+# CONFIG_CRYPTO_TEST is not set
+
+#
+# Authenticated Encryption with Associated Data
+#
+# CONFIG_CRYPTO_CCM is not set
+# CONFIG_CRYPTO_GCM is not set
+# CONFIG_CRYPTO_SEQIV is not set
+
+#
+# Block modes
+#
+CONFIG_CRYPTO_CBC=y
+# CONFIG_CRYPTO_CTR is not set
+# CONFIG_CRYPTO_CTS is not set
+# CONFIG_CRYPTO_ECB is not set
+# CONFIG_CRYPTO_LRW is not set
+# CONFIG_CRYPTO_PCBC is not set
+# CONFIG_CRYPTO_XTS is not set
+
+#
+# Hash modes
+#
+CONFIG_CRYPTO_HMAC=y
+# CONFIG_CRYPTO_XCBC is not set
+# CONFIG_CRYPTO_VMAC is not set
+
+#
+# Digest
+#
+CONFIG_CRYPTO_CRC32C=y
+# CONFIG_CRYPTO_GHASH is not set
+CONFIG_CRYPTO_MD4=m
+CONFIG_CRYPTO_MD5=y
+# CONFIG_CRYPTO_MICHAEL_MIC is not set
+# CONFIG_CRYPTO_RMD128 is not set
+# CONFIG_CRYPTO_RMD160 is not set
+# CONFIG_CRYPTO_RMD256 is not set
+# CONFIG_CRYPTO_RMD320 is not set
+CONFIG_CRYPTO_SHA1=y
+CONFIG_CRYPTO_SHA256=y
+CONFIG_CRYPTO_SHA512=y
+CONFIG_CRYPTO_TGR192=m
+CONFIG_CRYPTO_WP512=m
+
+#
+# Ciphers
+#
+CONFIG_CRYPTO_AES=y
+CONFIG_CRYPTO_ANUBIS=m
+# CONFIG_CRYPTO_ARC4 is not set
+CONFIG_CRYPTO_BLOWFISH=m
+CONFIG_CRYPTO_CAMELLIA=m
+CONFIG_CRYPTO_CAST5=m
+CONFIG_CRYPTO_CAST6=m
+CONFIG_CRYPTO_DES=y
+CONFIG_CRYPTO_FCRYPT=m
+CONFIG_CRYPTO_KHAZAD=m
+# CONFIG_CRYPTO_SALSA20 is not set
+# CONFIG_CRYPTO_SEED is not set
+CONFIG_CRYPTO_SERPENT=m
+CONFIG_CRYPTO_TEA=m
+CONFIG_CRYPTO_TWOFISH=m
+CONFIG_CRYPTO_TWOFISH_COMMON=m
+
+#
+# Compression
+#
+CONFIG_CRYPTO_DEFLATE=y
+# CONFIG_CRYPTO_ZLIB is not set
+# CONFIG_CRYPTO_LZO is not set
+
+#
+# Random Number Generation
+#
+# CONFIG_CRYPTO_ANSI_CPRNG is not set
+# CONFIG_CRYPTO_USER_API_HASH is not set
+# CONFIG_CRYPTO_USER_API_SKCIPHER is not set
+CONFIG_CRYPTO_HW=y
+# CONFIG_CRYPTO_DEV_HIFN_795X is not set
+CONFIG_CRYPTO_DEV_TRANSCEDE=m
+
+#
+# OCF Configuration
+#
+# CONFIG_OCF_OCF is not set
+# CONFIG_BINARY_PRINTF is not set
+
+#
+# Library routines
+#
+CONFIG_BITREVERSE=y
+CONFIG_CRC_CCITT=m
+CONFIG_CRC16=y
+# CONFIG_CRC_T10DIF is not set
+# CONFIG_CRC_ITU_T is not set
+CONFIG_CRC32=y
+# CONFIG_CRC7 is not set
+CONFIG_LIBCRC32C=y
+CONFIG_ZLIB_INFLATE=y
+CONFIG_ZLIB_DEFLATE=y
+# CONFIG_XZ_DEC is not set
+# CONFIG_XZ_DEC_BCJ is not set
+CONFIG_TEXTSEARCH=y
+CONFIG_TEXTSEARCH_KMP=m
+CONFIG_TEXTSEARCH_BM=m
+CONFIG_TEXTSEARCH_FSM=m
+CONFIG_HAS_IOMEM=y
+CONFIG_HAS_IOPORT=y
+CONFIG_HAS_DMA=y
+CONFIG_CPU_RMAP=y
+CONFIG_NLATTR=y
+# CONFIG_AVERAGE is not set
diff --git a/arch/arm/configs/t2200_initramfs_defconfig b/arch/arm/configs/t2200_initramfs_defconfig
new file mode 100644
index 0000000..d0be483
--- /dev/null
+++ b/arch/arm/configs/t2200_initramfs_defconfig
@@ -0,0 +1,1984 @@
+#
+# Automatically generated make config: don't edit
+# Linux/arm 3.0.51 Kernel Configuration
+#
+CONFIG_ARM=y
+CONFIG_MIGHT_HAVE_PCI=y
+CONFIG_SYS_SUPPORTS_APM_EMULATION=y
+CONFIG_HAVE_SCHED_CLOCK=y
+# CONFIG_ARCH_USES_GETTIMEOFFSET is not set
+CONFIG_GENERIC_CLOCKEVENTS=y
+CONFIG_GENERIC_CLOCKEVENTS_BROADCAST=y
+CONFIG_KTIME_SCALAR=y
+CONFIG_HAVE_PROC_CPU=y
+CONFIG_STACKTRACE_SUPPORT=y
+CONFIG_LOCKDEP_SUPPORT=y
+CONFIG_TRACE_IRQFLAGS_SUPPORT=y
+CONFIG_HARDIRQS_SW_RESEND=y
+CONFIG_GENERIC_IRQ_PROBE=y
+CONFIG_GENERIC_LOCKBREAK=y
+CONFIG_RWSEM_GENERIC_SPINLOCK=y
+CONFIG_ARCH_HAS_CPU_IDLE_WAIT=y
+CONFIG_GENERIC_HWEIGHT=y
+CONFIG_GENERIC_CALIBRATE_DELAY=y
+CONFIG_NEED_DMA_MAP_STATE=y
+CONFIG_VECTORS_BASE=0xffff0000
+# CONFIG_ARM_PATCH_PHYS_VIRT is not set
+CONFIG_DEFCONFIG_LIST="/lib/modules/$UNAME_RELEASE/.config"
+CONFIG_HAVE_IRQ_WORK=y
+CONFIG_IRQ_WORK=y
+
+#
+# General setup
+#
+CONFIG_EXPERIMENTAL=y
+CONFIG_INIT_ENV_ARG_LIMIT=32
+CONFIG_CROSS_COMPILE=""
+CONFIG_LOCALVERSION=""
+# CONFIG_LOCALVERSION_AUTO is not set
+CONFIG_HAVE_KERNEL_GZIP=y
+CONFIG_HAVE_KERNEL_LZMA=y
+CONFIG_HAVE_KERNEL_LZO=y
+CONFIG_KERNEL_GZIP=y
+# CONFIG_KERNEL_LZMA is not set
+# CONFIG_KERNEL_LZO is not set
+CONFIG_DEFAULT_HOSTNAME="(none)"
+# CONFIG_SWAP is not set
+CONFIG_SYSVIPC=y
+CONFIG_SYSVIPC_SYSCTL=y
+CONFIG_POSIX_MQUEUE=y
+CONFIG_POSIX_MQUEUE_SYSCTL=y
+# CONFIG_BSD_PROCESS_ACCT is not set
+# CONFIG_FHANDLE is not set
+# CONFIG_TASKSTATS is not set
+# CONFIG_AUDIT is not set
+CONFIG_HAVE_GENERIC_HARDIRQS=y
+
+#
+# IRQ subsystem
+#
+CONFIG_GENERIC_HARDIRQS=y
+CONFIG_HAVE_SPARSE_IRQ=y
+CONFIG_GENERIC_IRQ_SHOW=y
+CONFIG_IRQ_FORCED_THREADING=y
+# CONFIG_SPARSE_IRQ is not set
+
+#
+# RCU Subsystem
+#
+CONFIG_TREE_PREEMPT_RCU=y
+CONFIG_PREEMPT_RCU=y
+# CONFIG_RCU_TRACE is not set
+CONFIG_RCU_FANOUT=32
+# CONFIG_RCU_FANOUT_EXACT is not set
+# CONFIG_TREE_RCU_TRACE is not set
+# CONFIG_RCU_BOOST is not set
+CONFIG_IKCONFIG=y
+CONFIG_IKCONFIG_PROC=y
+CONFIG_LOG_BUF_SHIFT=16
+CONFIG_CGROUPS=y
+# CONFIG_CGROUP_DEBUG is not set
+# CONFIG_CGROUP_FREEZER is not set
+# CONFIG_CGROUP_DEVICE is not set
+# CONFIG_CPUSETS is not set
+# CONFIG_CGROUP_CPUACCT is not set
+# CONFIG_RESOURCE_COUNTERS is not set
+# CONFIG_CGROUP_PERF is not set
+# CONFIG_CGROUP_SCHED is not set
+# CONFIG_BLK_CGROUP is not set
+CONFIG_NAMESPACES=y
+# CONFIG_UTS_NS is not set
+# CONFIG_IPC_NS is not set
+# CONFIG_USER_NS is not set
+# CONFIG_PID_NS is not set
+# CONFIG_NET_NS is not set
+# CONFIG_SCHED_AUTOGROUP is not set
+# CONFIG_SYSFS_DEPRECATED is not set
+# CONFIG_RELAY is not set
+CONFIG_BLK_DEV_INITRD=y
+CONFIG_INITRAMFS_SOURCE="./initramfs_data.cpio"
+CONFIG_INITRAMFS_ROOT_UID=0
+CONFIG_INITRAMFS_ROOT_GID=0
+CONFIG_RD_GZIP=y
+CONFIG_RD_BZIP2=y
+CONFIG_RD_LZMA=y
+CONFIG_RD_XZ=y
+CONFIG_RD_LZO=y
+# CONFIG_INITRAMFS_COMPRESSION_NONE is not set
+CONFIG_INITRAMFS_COMPRESSION_GZIP=y
+# CONFIG_INITRAMFS_COMPRESSION_BZIP2 is not set
+# CONFIG_INITRAMFS_COMPRESSION_LZMA is not set
+# CONFIG_INITRAMFS_COMPRESSION_XZ is not set
+# CONFIG_INITRAMFS_COMPRESSION_LZO is not set
+CONFIG_CC_OPTIMIZE_FOR_SIZE=y
+CONFIG_SYSCTL=y
+CONFIG_ANON_INODES=y
+# CONFIG_EXPERT is not set
+CONFIG_UID16=y
+CONFIG_SYSCTL_SYSCALL=y
+CONFIG_KALLSYMS=y
+# CONFIG_KALLSYMS_ALL is not set
+CONFIG_HOTPLUG=y
+CONFIG_PRINTK=y
+CONFIG_BUG=y
+CONFIG_ELF_CORE=y
+CONFIG_BASE_FULL=y
+CONFIG_FUTEX=y
+CONFIG_EPOLL=y
+CONFIG_SIGNALFD=y
+CONFIG_TIMERFD=y
+CONFIG_EVENTFD=y
+CONFIG_SHMEM=y
+CONFIG_AIO=y
+# CONFIG_EMBEDDED is not set
+CONFIG_HAVE_PERF_EVENTS=y
+CONFIG_PERF_USE_VMALLOC=y
+
+#
+# Kernel Performance Events And Counters
+#
+CONFIG_PERF_EVENTS=y
+CONFIG_PERF_COUNTERS=y
+# CONFIG_DEBUG_PERF_USE_VMALLOC is not set
+CONFIG_VM_EVENT_COUNTERS=y
+CONFIG_PCI_QUIRKS=y
+CONFIG_COMPAT_BRK=y
+CONFIG_SLAB=y
+# CONFIG_PROFILING is not set
+CONFIG_HAVE_OPROFILE=y
+# CONFIG_KPROBES is not set
+CONFIG_HAVE_KPROBES=y
+CONFIG_HAVE_KRETPROBES=y
+CONFIG_USE_GENERIC_SMP_HELPERS=y
+CONFIG_HAVE_REGS_AND_STACK_ACCESS_API=y
+CONFIG_HAVE_DMA_API_DEBUG=y
+CONFIG_HAVE_HW_BREAKPOINT=y
+
+#
+# GCOV-based kernel profiling
+#
+# CONFIG_GCOV_KERNEL is not set
+CONFIG_HAVE_GENERIC_DMA_COHERENT=y
+CONFIG_SLABINFO=y
+CONFIG_RT_MUTEXES=y
+CONFIG_BASE_SMALL=0
+CONFIG_MODULES=y
+# CONFIG_MODULE_FORCE_LOAD is not set
+CONFIG_MODULE_UNLOAD=y
+# CONFIG_MODULE_FORCE_UNLOAD is not set
+# CONFIG_MODVERSIONS is not set
+# CONFIG_MODULE_SRCVERSION_ALL is not set
+CONFIG_STOP_MACHINE=y
+CONFIG_BLOCK=y
+CONFIG_LBDAF=y
+# CONFIG_BLK_DEV_BSG is not set
+# CONFIG_BLK_DEV_INTEGRITY is not set
+
+#
+# IO Schedulers
+#
+CONFIG_IOSCHED_NOOP=y
+CONFIG_IOSCHED_DEADLINE=y
+# CONFIG_IOSCHED_CFQ is not set
+CONFIG_DEFAULT_DEADLINE=y
+# CONFIG_DEFAULT_NOOP is not set
+CONFIG_DEFAULT_IOSCHED="deadline"
+# CONFIG_INLINE_SPIN_TRYLOCK is not set
+# CONFIG_INLINE_SPIN_TRYLOCK_BH is not set
+# CONFIG_INLINE_SPIN_LOCK is not set
+# CONFIG_INLINE_SPIN_LOCK_BH is not set
+# CONFIG_INLINE_SPIN_LOCK_IRQ is not set
+# CONFIG_INLINE_SPIN_LOCK_IRQSAVE is not set
+# CONFIG_INLINE_SPIN_UNLOCK is not set
+# CONFIG_INLINE_SPIN_UNLOCK_BH is not set
+# CONFIG_INLINE_SPIN_UNLOCK_IRQ is not set
+# CONFIG_INLINE_SPIN_UNLOCK_IRQRESTORE is not set
+# CONFIG_INLINE_READ_TRYLOCK is not set
+# CONFIG_INLINE_READ_LOCK is not set
+# CONFIG_INLINE_READ_LOCK_BH is not set
+# CONFIG_INLINE_READ_LOCK_IRQ is not set
+# CONFIG_INLINE_READ_LOCK_IRQSAVE is not set
+# CONFIG_INLINE_READ_UNLOCK is not set
+# CONFIG_INLINE_READ_UNLOCK_BH is not set
+# CONFIG_INLINE_READ_UNLOCK_IRQ is not set
+# CONFIG_INLINE_READ_UNLOCK_IRQRESTORE is not set
+# CONFIG_INLINE_WRITE_TRYLOCK is not set
+# CONFIG_INLINE_WRITE_LOCK is not set
+# CONFIG_INLINE_WRITE_LOCK_BH is not set
+# CONFIG_INLINE_WRITE_LOCK_IRQ is not set
+# CONFIG_INLINE_WRITE_LOCK_IRQSAVE is not set
+# CONFIG_INLINE_WRITE_UNLOCK is not set
+# CONFIG_INLINE_WRITE_UNLOCK_BH is not set
+# CONFIG_INLINE_WRITE_UNLOCK_IRQ is not set
+# CONFIG_INLINE_WRITE_UNLOCK_IRQRESTORE is not set
+# CONFIG_MUTEX_SPIN_ON_OWNER is not set
+# CONFIG_FREEZER is not set
+
+#
+# System Type
+#
+CONFIG_MMU=y
+# CONFIG_ARCH_INTEGRATOR is not set
+# CONFIG_ARCH_REALVIEW is not set
+# CONFIG_ARCH_VERSATILE is not set
+# CONFIG_ARCH_VEXPRESS is not set
+# CONFIG_ARCH_AT91 is not set
+# CONFIG_ARCH_BCMRING is not set
+# CONFIG_ARCH_CLPS711X is not set
+# CONFIG_ARCH_CNS3XXX is not set
+# CONFIG_ARCH_GEMINI is not set
+# CONFIG_ARCH_EBSA110 is not set
+# CONFIG_ARCH_EP93XX is not set
+# CONFIG_ARCH_FOOTBRIDGE is not set
+# CONFIG_ARCH_MXC is not set
+# CONFIG_ARCH_MXS is not set
+# CONFIG_ARCH_NETX is not set
+# CONFIG_ARCH_H720X is not set
+# CONFIG_ARCH_IOP13XX is not set
+# CONFIG_ARCH_IOP32X is not set
+# CONFIG_ARCH_IOP33X is not set
+# CONFIG_ARCH_IXP23XX is not set
+# CONFIG_ARCH_IXP2000 is not set
+# CONFIG_ARCH_IXP4XX is not set
+# CONFIG_ARCH_DOVE is not set
+# CONFIG_ARCH_KIRKWOOD is not set
+# CONFIG_ARCH_LOKI is not set
+# CONFIG_ARCH_LPC32XX is not set
+# CONFIG_ARCH_MV78XX0 is not set
+# CONFIG_ARCH_ORION5X is not set
+# CONFIG_ARCH_MMP is not set
+# CONFIG_ARCH_KS8695 is not set
+# CONFIG_ARCH_W90X900 is not set
+# CONFIG_ARCH_NUC93X is not set
+# CONFIG_ARCH_TEGRA is not set
+# CONFIG_ARCH_PNX4008 is not set
+# CONFIG_ARCH_PXA is not set
+# CONFIG_ARCH_MSM is not set
+# CONFIG_ARCH_SHMOBILE is not set
+# CONFIG_ARCH_RPC is not set
+# CONFIG_ARCH_SA1100 is not set
+# CONFIG_ARCH_S3C2410 is not set
+# CONFIG_ARCH_S3C64XX is not set
+# CONFIG_ARCH_S5P64X0 is not set
+# CONFIG_ARCH_S5PC100 is not set
+# CONFIG_ARCH_S5PV210 is not set
+# CONFIG_ARCH_EXYNOS4 is not set
+# CONFIG_ARCH_SHARK is not set
+# CONFIG_ARCH_TCC_926 is not set
+CONFIG_ARCH_TRANSCEDE=y
+# CONFIG_ARCH_U300 is not set
+# CONFIG_ARCH_U8500 is not set
+# CONFIG_ARCH_NOMADIK is not set
+# CONFIG_ARCH_DAVINCI is not set
+# CONFIG_ARCH_OMAP is not set
+# CONFIG_PLAT_SPEAR is not set
+# CONFIG_ARCH_VT8500 is not set
+
+#
+# System MMU
+#
+
+#
+# Transcede Implementation Options
+#
+# CONFIG_MACH_M84XXX is not set
+CONFIG_MACH_M822XX=y
+# CONFIG_TRANSCEDE_PCI_USE_APBB is not set
+# CONFIG_TRANSCEDE_PCI_TX_DMA_ARAM is not set
+# CONFIG_TRANSCEDE_PCI_DEBUG is not set
+# CONFIG_TRANSCEDE_TDM_CLOCK is not set
+# CONFIG_TRANSCEDE_DUALCORE is not set
+CONFIG_TRANSCEDE_UART0_SUPPORT=y
+# CONFIG_TRANSCEDE_UART1_SUPPORT is not set
+# CONFIG_TRANSCEDE_UART2_SUPPORT is not set
+CONFIG_TRANSCEDE_IPSEC_HW_SUPPORT=y
+CONFIG_TRANSCEDE_ELP_SPACC=y
+CONFIG_TRANSCEDE_ELP_CLP30=y
+CONFIG_IPSEC_DMA_MAP_HACK_TX=y
+CONFIG_IPSEC_DMA_MAP_HACK_RX=y
+CONFIG_CLP30_SW_FIFO=y
+CONFIG_TRANSCEDE_ELP_PDU=y
+CONFIG_TRANSCEDE_GEMAC_0=y
+CONFIG_TRANSCEDE_GEMAC_1=y
+CONFIG_TRANSCEDE_GEM_PHY=y
+CONFIG_MTD_NAND_TRANSCEDE=y
+# CONFIG_MTD_TRANSCEDE_NOR_8 is not set
+CONFIG_MTD_TRANSCEDE_NOR_16=y
+# CONFIG_ASMP_CORE_STARTUP is not set
+# CONFIG_RTSM_ONLY is not set
+# CONFIG_TRANSCEDE_MLOG is not set
+CONFIG_TRANSCEDE_ELP_TRNG=y
+CONFIG_TRANSCEDE_MIPS_MONITOR=y
+CONFIG_TRANSCEDE_MTU_CTRL_ENABLED=y
+#
+# Processor Type
+#
+CONFIG_CPU_V7=y
+CONFIG_CPU_32v6K=y
+CONFIG_CPU_32v7=y
+CONFIG_CPU_ABRT_EV7=y
+CONFIG_CPU_PABRT_V7=y
+CONFIG_CPU_CACHE_V7=y
+CONFIG_CPU_CACHE_VIPT=y
+CONFIG_CPU_COPY_V6=y
+CONFIG_CPU_TLB_V7=y
+CONFIG_CPU_HAS_ASID=y
+CONFIG_CPU_CP15=y
+CONFIG_CPU_CP15_MMU=y
+
+#
+# Processor Features
+#
+CONFIG_ARM_THUMB=y
+# CONFIG_ARM_THUMBEE is not set
+# CONFIG_SWP_EMULATE is not set
+# CONFIG_CPU_ICACHE_DISABLE is not set
+# CONFIG_CPU_DCACHE_DISABLE is not set
+# CONFIG_CPU_BPREDICT_DISABLE is not set
+CONFIG_OUTER_CACHE=y
+CONFIG_OUTER_CACHE_SYNC=y
+CONFIG_CACHE_L2X0=y
+CONFIG_CACHE_PL310=y
+CONFIG_ARM_L1_CACHE_SHIFT=5
+CONFIG_ARM_DMA_MEM_BUFFERABLE=y
+CONFIG_CPU_HAS_PMU=y
+# CONFIG_ARM_ERRATA_430973 is not set
+# CONFIG_ARM_ERRATA_458693 is not set
+# CONFIG_ARM_ERRATA_460075 is not set
+# CONFIG_ARM_ERRATA_742230 is not set
+# CONFIG_ARM_ERRATA_742231 is not set
+CONFIG_PL310_ERRATA_588369=y
+# CONFIG_ARM_ERRATA_720789 is not set
+CONFIG_PL310_ERRATA_727915=y
+# CONFIG_ARM_ERRATA_743622 is not set
+# CONFIG_ARM_ERRATA_751472 is not set
+CONFIG_ARM_ERRATA_753970=y
+# CONFIG_ARM_ERRATA_754322 is not set
+# CONFIG_ARM_ERRATA_754327 is not set
+# CONFIG_ARM_ERRATA_764369 is not set
+# CONFIG_PL310_ERRATA_769419 is not set
+# CONFIG_ARM_ERRATA_775420 is not set
+CONFIG_ARM_GIC=y
+
+#
+# Bus support
+#
+CONFIG_PCI=y
+CONFIG_PCI_SYSCALL=y
+CONFIG_ARCH_SUPPORTS_MSI=y
+CONFIG_PCI_MSI=y
+# CONFIG_PCI_DEBUG is not set
+# CONFIG_PCI_STUB is not set
+# CONFIG_PCI_IOV is not set
+# CONFIG_PCCARD is not set
+
+#
+# Kernel Features
+#
+CONFIG_TICK_ONESHOT=y
+# CONFIG_NO_HZ is not set
+CONFIG_HIGH_RES_TIMERS=y
+CONFIG_GENERIC_CLOCKEVENTS_BUILD=y
+CONFIG_SMP=y
+CONFIG_SMP_ON_UP=y
+CONFIG_HAVE_ARM_SCU=y
+CONFIG_HAVE_ARM_TWD=y
+# CONFIG_VMSPLIT_3G is not set
+CONFIG_VMSPLIT_2G=y
+# CONFIG_VMSPLIT_1G is not set
+CONFIG_PAGE_OFFSET=0x80000000
+CONFIG_NR_CPUS=2
+CONFIG_LOCAL_TIMERS=y
+CONFIG_GLOBAL_POLLING=y
+CONFIG_PREEMPT=y
+CONFIG_PREEMPT_RT_BASE=y
+# CONFIG_PREEMPT_NONE is not set
+# CONFIG_PREEMPT_VOLUNTARY is not set
+# CONFIG_PREEMPT_LL is not set
+# CONFIG_PREEMPT_RTB is not set
+CONFIG_PREEMPT_RT_FULL=y
+CONFIG_HZ=100
+# CONFIG_THUMB2_KERNEL is not set
+CONFIG_AEABI=y
+CONFIG_OABI_COMPAT=y
+# CONFIG_ARCH_SPARSEMEM_DEFAULT is not set
+# CONFIG_ARCH_SELECT_MEMORY_MODEL is not set
+CONFIG_HAVE_ARCH_PFN_VALID=y
+CONFIG_HW_PERF_EVENTS=y
+CONFIG_SELECT_MEMORY_MODEL=y
+CONFIG_FLATMEM_MANUAL=y
+CONFIG_FLATMEM=y
+CONFIG_FLAT_NODE_MEM_MAP=y
+CONFIG_HAVE_MEMBLOCK=y
+CONFIG_PAGEFLAGS_EXTENDED=y
+CONFIG_SPLIT_PTLOCK_CPUS=4
+# CONFIG_COMPACTION is not set
+# CONFIG_PHYS_ADDR_T_64BIT is not set
+CONFIG_ZONE_DMA_FLAG=0
+CONFIG_VIRT_TO_BUS=y
+# CONFIG_KSM is not set
+CONFIG_DEFAULT_MMAP_MIN_ADDR=4096
+# CONFIG_CLEANCACHE is not set
+CONFIG_FORCE_MAX_ZONEORDER=11
+CONFIG_ALIGNMENT_TRAP=y
+# CONFIG_UACCESS_WITH_MEMCPY is not set
+# CONFIG_SECCOMP is not set
+# CONFIG_CC_STACKPROTECTOR is not set
+# CONFIG_DEPRECATED_PARAM_STRUCT is not set
+
+#
+# Boot options
+#
+# CONFIG_USE_OF is not set
+CONFIG_ZBOOT_ROM_TEXT=0x0
+CONFIG_ZBOOT_ROM_BSS=0x0
+CONFIG_CMDLINE="root=/dev/nfs nfsroot=10.1.69.3:/work/nfsroot ip=dhcp console=ttyAMA0 mem=128M"
+CONFIG_CMDLINE_FROM_BOOTLOADER=y
+# CONFIG_CMDLINE_EXTEND is not set
+# CONFIG_CMDLINE_FORCE is not set
+# CONFIG_XIP_KERNEL is not set
+# CONFIG_KEXEC is not set
+# CONFIG_CRASH_DUMP is not set
+# CONFIG_AUTO_ZRELADDR is not set
+
+#
+# CPU Power Management
+#
+# CONFIG_CPU_IDLE is not set
+
+#
+# Floating point emulation
+#
+
+#
+# At least one emulation must be selected
+#
+# CONFIG_FPE_NWFPE is not set
+# CONFIG_FPE_FASTFPE is not set
+# CONFIG_VFP is not set
+
+#
+# Userspace binary formats
+#
+CONFIG_BINFMT_ELF=y
+# CONFIG_CORE_DUMP_DEFAULT_ELF_HEADERS is not set
+CONFIG_HAVE_AOUT=y
+# CONFIG_BINFMT_AOUT is not set
+# CONFIG_BINFMT_MISC is not set
+
+#
+# Power management options
+#
+# CONFIG_SUSPEND is not set
+# CONFIG_PM_RUNTIME is not set
+CONFIG_ARCH_SUSPEND_POSSIBLE=y
+CONFIG_NET=y
+
+#
+# Networking options
+#
+CONFIG_PACKET=y
+CONFIG_UNIX=y
+CONFIG_XFRM=y
+CONFIG_XFRM_USER=m
+# CONFIG_XFRM_SUB_POLICY is not set
+# CONFIG_XFRM_MIGRATE is not set
+# CONFIG_XFRM_STATISTICS is not set
+CONFIG_XFRM_IPCOMP=m
+CONFIG_NET_KEY=m
+# CONFIG_NET_KEY_MIGRATE is not set
+CONFIG_INET=y
+CONFIG_IP_MULTICAST=y
+# CONFIG_IP_ADVANCED_ROUTER is not set
+CONFIG_IP_ROUTE_CLASSID=y
+CONFIG_IP_PNP=y
+CONFIG_IP_PNP_DHCP=y
+CONFIG_IP_PNP_BOOTP=y
+# CONFIG_IP_PNP_RARP is not set
+CONFIG_NET_IPIP=y
+# CONFIG_NET_IPGRE_DEMUX is not set
+# CONFIG_IP_MROUTE is not set
+# CONFIG_ARPD is not set
+# CONFIG_SYN_COOKIES is not set
+CONFIG_INET_AH=m
+CONFIG_INET_ESP=m
+CONFIG_INET_IPCOMP=m
+CONFIG_INET_XFRM_TUNNEL=m
+CONFIG_INET_TUNNEL=y
+CONFIG_INET_XFRM_MODE_TRANSPORT=m
+CONFIG_INET_XFRM_MODE_TUNNEL=m
+CONFIG_INET_XFRM_MODE_BEET=m
+# CONFIG_INET_LRO is not set
+CONFIG_INET_DIAG=y
+CONFIG_INET_TCP_DIAG=y
+# CONFIG_TCP_CONG_ADVANCED is not set
+CONFIG_TCP_CONG_CUBIC=y
+CONFIG_DEFAULT_TCP_CONG="cubic"
+# CONFIG_TCP_MD5SIG is not set
+CONFIG_IPV6=m
+CONFIG_IPV6_PRIVACY=y
+# CONFIG_IPV6_ROUTER_PREF is not set
+# CONFIG_IPV6_OPTIMISTIC_DAD is not set
+CONFIG_INET6_AH=m
+CONFIG_INET6_ESP=m
+CONFIG_INET6_IPCOMP=m
+# CONFIG_IPV6_MIP6 is not set
+CONFIG_INET6_XFRM_TUNNEL=m
+CONFIG_INET6_TUNNEL=m
+CONFIG_INET6_XFRM_MODE_TRANSPORT=m
+CONFIG_INET6_XFRM_MODE_TUNNEL=m
+CONFIG_INET6_XFRM_MODE_BEET=m
+# CONFIG_INET6_XFRM_MODE_ROUTEOPTIMIZATION is not set
+# CONFIG_IPV6_SIT is not set
+CONFIG_IPV6_TUNNEL=m
+CONFIG_IPV6_MULTIPLE_TABLES=y
+CONFIG_IPV6_SUBTREES=y
+CONFIG_IPV6_MROUTE=y
+# CONFIG_IPV6_MROUTE_MULTIPLE_TABLES is not set
+# CONFIG_IPV6_PIMSM_V2 is not set
+# CONFIG_NETWORK_SECMARK is not set
+# CONFIG_NETWORK_PHY_TIMESTAMPING is not set
+CONFIG_NETFILTER=y
+# CONFIG_NETFILTER_DEBUG is not set
+CONFIG_NETFILTER_ADVANCED=y
+# CONFIG_BRIDGE_NETFILTER is not set
+
+#
+# Core Netfilter Configuration
+#
+# CONFIG_NETFILTER_NETLINK_QUEUE is not set
+# CONFIG_NETFILTER_NETLINK_LOG is not set
+# CONFIG_NF_CONNTRACK is not set
+# CONFIG_NETFILTER_XTABLES is not set
+# CONFIG_IP_VS is not set
+
+#
+# IP: Netfilter Configuration
+#
+# CONFIG_NF_DEFRAG_IPV4 is not set
+# CONFIG_IP_NF_QUEUE is not set
+# CONFIG_IP_NF_IPTABLES is not set
+# CONFIG_IP_NF_ARPTABLES is not set
+
+#
+# IPv6: Netfilter Configuration
+#
+# CONFIG_NF_DEFRAG_IPV6 is not set
+# CONFIG_IP6_NF_QUEUE is not set
+# CONFIG_IP6_NF_IPTABLES is not set
+# CONFIG_IP_DCCP is not set
+CONFIG_IP_SCTP=m
+# CONFIG_SCTP_DBG_MSG is not set
+# CONFIG_SCTP_DBG_OBJCNT is not set
+# CONFIG_SCTP_HMAC_NONE is not set
+# CONFIG_SCTP_HMAC_SHA1 is not set
+CONFIG_SCTP_HMAC_MD5=y
+# CONFIG_RDS is not set
+# CONFIG_TIPC is not set
+# CONFIG_ATM is not set
+# CONFIG_L2TP is not set
+CONFIG_STP=m
+CONFIG_BRIDGE=m
+# CONFIG_BRIDGE_IGMP_SNOOPING is not set
+# CONFIG_NET_DSA is not set
+# CONFIG_VLAN_8021Q is not set
+# CONFIG_DECNET is not set
+CONFIG_LLC=m
+# CONFIG_LLC2 is not set
+# CONFIG_IPX is not set
+# CONFIG_ATALK is not set
+# CONFIG_X25 is not set
+# CONFIG_LAPB is not set
+# CONFIG_ECONET is not set
+# CONFIG_WAN_ROUTER is not set
+# CONFIG_PHONET is not set
+# CONFIG_IEEE802154 is not set
+CONFIG_NET_SCHED=y
+
+#
+# Queueing/Scheduling
+#
+# CONFIG_NET_SCH_CBQ is not set
+CONFIG_NET_SCH_HTB=m
+CONFIG_NET_SCH_HFSC=m
+CONFIG_NET_SCH_PRIO=m
+# CONFIG_NET_SCH_MULTIQ is not set
+CONFIG_NET_SCH_RED=m
+# CONFIG_NET_SCH_SFB is not set
+CONFIG_NET_SCH_SFQ=m
+CONFIG_NET_SCH_TEQL=m
+CONFIG_NET_SCH_TBF=m
+CONFIG_NET_SCH_GRED=m
+CONFIG_NET_SCH_DSMARK=m
+# CONFIG_NET_SCH_NETEM is not set
+# CONFIG_NET_SCH_DRR is not set
+# CONFIG_NET_SCH_MQPRIO is not set
+# CONFIG_NET_SCH_CHOKE is not set
+# CONFIG_NET_SCH_QFQ is not set
+CONFIG_NET_SCH_INGRESS=m
+
+#
+# Classification
+#
+CONFIG_NET_CLS=y
+CONFIG_NET_CLS_BASIC=m
+CONFIG_NET_CLS_TCINDEX=m
+CONFIG_NET_CLS_ROUTE4=m
+CONFIG_NET_CLS_FW=m
+CONFIG_NET_CLS_U32=m
+# CONFIG_CLS_U32_PERF is not set
+# CONFIG_CLS_U32_MARK is not set
+# CONFIG_NET_CLS_RSVP is not set
+# CONFIG_NET_CLS_RSVP6 is not set
+CONFIG_NET_CLS_FLOW=m
+# CONFIG_NET_CLS_CGROUP is not set
+CONFIG_NET_EMATCH=y
+CONFIG_NET_EMATCH_STACK=32
+CONFIG_NET_EMATCH_CMP=m
+CONFIG_NET_EMATCH_NBYTE=m
+CONFIG_NET_EMATCH_U32=m
+CONFIG_NET_EMATCH_META=m
+CONFIG_NET_EMATCH_TEXT=m
+CONFIG_NET_CLS_ACT=y
+CONFIG_NET_ACT_POLICE=m
+# CONFIG_NET_ACT_GACT is not set
+CONFIG_NET_ACT_MIRRED=m
+# CONFIG_NET_ACT_NAT is not set
+# CONFIG_NET_ACT_PEDIT is not set
+# CONFIG_NET_ACT_SIMP is not set
+CONFIG_NET_ACT_SKBEDIT=m
+# CONFIG_NET_ACT_CSUM is not set
+# CONFIG_NET_CLS_IND is not set
+CONFIG_NET_SCH_FIFO=y
+# CONFIG_DCB is not set
+CONFIG_DNS_RESOLVER=y
+# CONFIG_BATMAN_ADV is not set
+CONFIG_RPS=y
+CONFIG_RFS_ACCEL=y
+CONFIG_XPS=y
+
+#
+# Network testing
+#
+CONFIG_NET_PKTGEN=m
+# CONFIG_HAMRADIO is not set
+# CONFIG_CAN is not set
+# CONFIG_IRDA is not set
+# CONFIG_BT is not set
+# CONFIG_AF_RXRPC is not set
+CONFIG_FIB_RULES=y
+CONFIG_WIRELESS=y
+# CONFIG_CFG80211 is not set
+# CONFIG_LIB80211 is not set
+
+#
+# CFG80211 needs to be enabled for MAC80211
+#
+# CONFIG_WIMAX is not set
+# CONFIG_RFKILL is not set
+# CONFIG_NET_9P is not set
+# CONFIG_CAIF is not set
+# CONFIG_CEPH_LIB is not set
+
+#
+# Device Drivers
+#
+
+#
+# Generic Driver Options
+#
+CONFIG_UEVENT_HELPER_PATH="/sbin/hotplug"
+# CONFIG_DEVTMPFS is not set
+CONFIG_STANDALONE=y
+CONFIG_PREVENT_FIRMWARE_BUILD=y
+CONFIG_FW_LOADER=y
+CONFIG_FIRMWARE_IN_KERNEL=y
+CONFIG_EXTRA_FIRMWARE=""
+# CONFIG_DEBUG_DRIVER is not set
+# CONFIG_DEBUG_DEVRES is not set
+# CONFIG_SYS_HYPERVISOR is not set
+# CONFIG_CONNECTOR is not set
+CONFIG_MTD=y
+# CONFIG_MTD_DEBUG is not set
+# CONFIG_MTD_TESTS is not set
+# CONFIG_MTD_REDBOOT_PARTS is not set
+CONFIG_MTD_CMDLINE_PARTS=y
+# CONFIG_MTD_AFS_PARTS is not set
+# CONFIG_MTD_AR7_PARTS is not set
+
+#
+# User Modules And Translation Layers
+#
+CONFIG_MTD_CHAR=y
+CONFIG_MTD_BLKDEVS=y
+CONFIG_MTD_BLOCK=y
+# CONFIG_FTL is not set
+CONFIG_NFTL=y
+# CONFIG_NFTL_RW is not set
+# CONFIG_INFTL is not set
+# CONFIG_RFD_FTL is not set
+# CONFIG_SSFDC is not set
+# CONFIG_SM_FTL is not set
+# CONFIG_MTD_OOPS is not set
+
+#
+# RAM/ROM/Flash chip drivers
+#
+CONFIG_MTD_CFI=y
+CONFIG_MTD_JEDECPROBE=y
+CONFIG_MTD_GEN_PROBE=y
+CONFIG_MTD_CFI_ADV_OPTIONS=y
+CONFIG_MTD_CFI_NOSWAP=y
+# CONFIG_MTD_CFI_BE_BYTE_SWAP is not set
+# CONFIG_MTD_CFI_LE_BYTE_SWAP is not set
+CONFIG_MTD_CFI_GEOMETRY=y
+CONFIG_MTD_MAP_BANK_WIDTH_1=y
+CONFIG_MTD_MAP_BANK_WIDTH_2=y
+CONFIG_MTD_MAP_BANK_WIDTH_4=y
+# CONFIG_MTD_MAP_BANK_WIDTH_8 is not set
+# CONFIG_MTD_MAP_BANK_WIDTH_16 is not set
+# CONFIG_MTD_MAP_BANK_WIDTH_32 is not set
+CONFIG_MTD_CFI_I1=y
+CONFIG_MTD_CFI_I2=y
+# CONFIG_MTD_CFI_I4 is not set
+# CONFIG_MTD_CFI_I8 is not set
+# CONFIG_MTD_OTP is not set
+CONFIG_MTD_CFI_INTELEXT=y
+CONFIG_MTD_CFI_AMDSTD=y
+# CONFIG_MTD_CFI_STAA is not set
+CONFIG_MTD_CFI_UTIL=y
+CONFIG_MTD_RAM=y
+CONFIG_MTD_ROM=y
+# CONFIG_MTD_ABSENT is not set
+
+#
+# Mapping drivers for chip access
+#
+# CONFIG_MTD_COMPLEX_MAPPINGS is not set
+CONFIG_MTD_PHYSMAP=y
+# CONFIG_MTD_PHYSMAP_COMPAT is not set
+# CONFIG_MTD_ARM_INTEGRATOR is not set
+# CONFIG_MTD_IMPA7 is not set
+# CONFIG_MTD_INTEL_VR_NOR is not set
+CONFIG_MTD_PLATRAM=y
+
+#
+# Self-contained MTD device drivers
+#
+# CONFIG_MTD_PMC551 is not set
+# CONFIG_MTD_DATAFLASH is not set
+# CONFIG_MTD_M25P80 is not set
+# CONFIG_MTD_SST25L is not set
+# CONFIG_MTD_SLRAM is not set
+# CONFIG_MTD_PHRAM is not set
+# CONFIG_MTD_MTDRAM is not set
+# CONFIG_MTD_BLOCK2MTD is not set
+
+#
+# Disk-On-Chip Device Drivers
+#
+# CONFIG_MTD_DOC2000 is not set
+# CONFIG_MTD_DOC2001 is not set
+# CONFIG_MTD_DOC2001PLUS is not set
+CONFIG_MTD_NAND_ECC=y
+# CONFIG_MTD_NAND_ECC_SMC is not set
+CONFIG_MTD_NAND=y
+# CONFIG_MTD_NAND_VERIFY_WRITE is not set
+# CONFIG_MTD_NAND_ECC_BCH is not set
+# CONFIG_MTD_SM_COMMON is not set
+CONFIG_MTD_NAND_MUSEUM_IDS=y
+# CONFIG_MTD_NAND_DENALI is not set
+CONFIG_MTD_NAND_IDS=y
+# CONFIG_MTD_NAND_RICOH is not set
+# CONFIG_MTD_NAND_DISKONCHIP is not set
+# CONFIG_MTD_NAND_CAFE is not set
+# CONFIG_MTD_NAND_NANDSIM is not set
+CONFIG_MTD_NAND_PLATFORM=y
+# CONFIG_MTD_ALAUDA is not set
+# CONFIG_MTD_ONENAND is not set
+
+#
+# LPDDR flash memory drivers
+#
+# CONFIG_MTD_LPDDR is not set
+# CONFIG_MTD_UBI is not set
+# CONFIG_PARPORT is not set
+CONFIG_BLK_DEV=y
+# CONFIG_BLK_CPQ_DA is not set
+# CONFIG_BLK_CPQ_CISS_DA is not set
+# CONFIG_BLK_DEV_DAC960 is not set
+# CONFIG_BLK_DEV_UMEM is not set
+# CONFIG_BLK_DEV_COW_COMMON is not set
+# CONFIG_BLK_DEV_LOOP is not set
+
+#
+# DRBD disabled because PROC_FS, INET or CONNECTOR not selected
+#
+# CONFIG_BLK_DEV_NBD is not set
+# CONFIG_BLK_DEV_SX8 is not set
+# CONFIG_BLK_DEV_UB is not set
+# CONFIG_BLK_DEV_RAM is not set
+# CONFIG_CDROM_PKTCDVD is not set
+# CONFIG_ATA_OVER_ETH is not set
+# CONFIG_BLK_DEV_RBD is not set
+# CONFIG_SENSORS_LIS3LV02D is not set
+CONFIG_MISC_DEVICES=y
+# CONFIG_AD525X_DPOT is not set
+# CONFIG_PHANTOM is not set
+# CONFIG_INTEL_MID_PTI is not set
+# CONFIG_SGI_IOC4 is not set
+# CONFIG_TIFM_CORE is not set
+# CONFIG_ICS932S401 is not set
+# CONFIG_ENCLOSURE_SERVICES is not set
+# CONFIG_HP_ILO is not set
+# CONFIG_APDS9802ALS is not set
+# CONFIG_ISL29003 is not set
+# CONFIG_ISL29020 is not set
+# CONFIG_SENSORS_TSL2550 is not set
+# CONFIG_SENSORS_BH1780 is not set
+# CONFIG_SENSORS_BH1770 is not set
+# CONFIG_SENSORS_APDS990X is not set
+# CONFIG_HMC6352 is not set
+# CONFIG_DS1682 is not set
+# CONFIG_TI_DAC7512 is not set
+# CONFIG_BMP085 is not set
+# CONFIG_PCH_PHUB is not set
+CONFIG_TRANSCEDE_USIM_SUPPORT=y
+# CONFIG_C2PORT is not set
+
+#
+# EEPROM support
+#
+CONFIG_EEPROM_AT24=y
+# CONFIG_EEPROM_AT25 is not set
+# CONFIG_EEPROM_LEGACY is not set
+# CONFIG_EEPROM_MAX6875 is not set
+# CONFIG_EEPROM_93CX6 is not set
+# CONFIG_CB710_CORE is not set
+
+#
+# Texas Instruments shared transport line discipline
+#
+# CONFIG_SENSORS_LIS3_SPI is not set
+# CONFIG_SENSORS_LIS3_I2C is not set
+CONFIG_HAVE_IDE=y
+# CONFIG_IDE is not set
+
+#
+# SCSI device support
+#
+CONFIG_SCSI_MOD=m
+# CONFIG_RAID_ATTRS is not set
+CONFIG_SCSI=m
+CONFIG_SCSI_DMA=y
+# CONFIG_SCSI_TGT is not set
+# CONFIG_SCSI_NETLINK is not set
+CONFIG_SCSI_PROC_FS=y
+
+#
+# SCSI support type (disk, tape, CD-ROM)
+#
+CONFIG_BLK_DEV_SD=m
+# CONFIG_CHR_DEV_ST is not set
+# CONFIG_CHR_DEV_OSST is not set
+# CONFIG_BLK_DEV_SR is not set
+# CONFIG_CHR_DEV_SG is not set
+# CONFIG_CHR_DEV_SCH is not set
+# CONFIG_SCSI_MULTI_LUN is not set
+# CONFIG_SCSI_CONSTANTS is not set
+# CONFIG_SCSI_LOGGING is not set
+# CONFIG_SCSI_SCAN_ASYNC is not set
+CONFIG_SCSI_WAIT_SCAN=m
+
+#
+# SCSI Transports
+#
+# CONFIG_SCSI_SPI_ATTRS is not set
+# CONFIG_SCSI_FC_ATTRS is not set
+# CONFIG_SCSI_ISCSI_ATTRS is not set
+# CONFIG_SCSI_SAS_ATTRS is not set
+# CONFIG_SCSI_SAS_LIBSAS is not set
+# CONFIG_SCSI_SRP_ATTRS is not set
+CONFIG_SCSI_LOWLEVEL=y
+# CONFIG_ISCSI_TCP is not set
+# CONFIG_ISCSI_BOOT_SYSFS is not set
+# CONFIG_SCSI_CXGB3_ISCSI is not set
+# CONFIG_SCSI_CXGB4_ISCSI is not set
+# CONFIG_SCSI_BNX2_ISCSI is not set
+# CONFIG_SCSI_BNX2X_FCOE is not set
+# CONFIG_BE2ISCSI is not set
+# CONFIG_BLK_DEV_3W_XXXX_RAID is not set
+# CONFIG_SCSI_HPSA is not set
+# CONFIG_SCSI_3W_9XXX is not set
+# CONFIG_SCSI_3W_SAS is not set
+# CONFIG_SCSI_ACARD is not set
+# CONFIG_SCSI_AACRAID is not set
+# CONFIG_SCSI_AIC7XXX is not set
+# CONFIG_SCSI_AIC7XXX_OLD is not set
+# CONFIG_SCSI_AIC79XX is not set
+# CONFIG_SCSI_AIC94XX is not set
+# CONFIG_SCSI_MVSAS is not set
+# CONFIG_SCSI_DPT_I2O is not set
+# CONFIG_SCSI_ADVANSYS is not set
+# CONFIG_SCSI_ARCMSR is not set
+# CONFIG_MEGARAID_NEWGEN is not set
+# CONFIG_MEGARAID_LEGACY is not set
+# CONFIG_MEGARAID_SAS is not set
+# CONFIG_SCSI_MPT2SAS is not set
+# CONFIG_SCSI_HPTIOP is not set
+# CONFIG_LIBFC is not set
+# CONFIG_LIBFCOE is not set
+# CONFIG_FCOE is not set
+# CONFIG_SCSI_DMX3191D is not set
+# CONFIG_SCSI_FUTURE_DOMAIN is not set
+# CONFIG_SCSI_IPS is not set
+# CONFIG_SCSI_INITIO is not set
+# CONFIG_SCSI_INIA100 is not set
+# CONFIG_SCSI_STEX is not set
+# CONFIG_SCSI_SYM53C8XX_2 is not set
+# CONFIG_SCSI_QLOGIC_1280 is not set
+# CONFIG_SCSI_QLA_FC is not set
+# CONFIG_SCSI_QLA_ISCSI is not set
+# CONFIG_SCSI_LPFC is not set
+# CONFIG_SCSI_DC395x is not set
+# CONFIG_SCSI_DC390T is not set
+# CONFIG_SCSI_NSP32 is not set
+# CONFIG_SCSI_DEBUG is not set
+# CONFIG_SCSI_PMCRAID is not set
+# CONFIG_SCSI_PM8001 is not set
+# CONFIG_SCSI_SRP is not set
+# CONFIG_SCSI_BFA_FC is not set
+# CONFIG_SCSI_DH is not set
+# CONFIG_SCSI_OSD_INITIATOR is not set
+# CONFIG_ATA is not set
+# CONFIG_MD is not set
+# CONFIG_TARGET_CORE is not set
+# CONFIG_FUSION is not set
+
+#
+# IEEE 1394 (FireWire) support
+#
+# CONFIG_FIREWIRE is not set
+# CONFIG_FIREWIRE_NOSY is not set
+# CONFIG_I2O is not set
+CONFIG_NETDEVICES=y
+# CONFIG_IFB is not set
+# CONFIG_DUMMY is not set
+# CONFIG_BONDING is not set
+# CONFIG_MACVLAN is not set
+# CONFIG_EQUALIZER is not set
+# CONFIG_TUN is not set
+# CONFIG_VETH is not set
+# CONFIG_ARCNET is not set
+CONFIG_MII=y
+CONFIG_PHYLIB=y
+
+#
+# MII PHY device drivers
+#
+# CONFIG_MARVELL_PHY is not set
+# CONFIG_DAVICOM_PHY is not set
+# CONFIG_QSEMI_PHY is not set
+# CONFIG_LXT_PHY is not set
+# CONFIG_CICADA_PHY is not set
+# CONFIG_VITESSE_PHY is not set
+# CONFIG_SMSC_PHY is not set
+# CONFIG_BROADCOM_PHY is not set
+# CONFIG_ICPLUS_PHY is not set
+# CONFIG_REALTEK_PHY is not set
+# CONFIG_NATIONAL_PHY is not set
+# CONFIG_STE10XP is not set
+# CONFIG_LSI_ET1011C_PHY is not set
+# CONFIG_MICREL_PHY is not set
+# CONFIG_FIXED_PHY is not set
+CONFIG_MDIO_BITBANG=y
+CONFIG_NET_ETHERNET=y
+CONFIG_TRANSCEDE_2200_ETH=y
+CONFIG_TRANSCEDE_VED_3300=y
+# CONFIG_AX88796 is not set
+# CONFIG_HAPPYMEAL is not set
+# CONFIG_SUNGEM is not set
+# CONFIG_CASSINI is not set
+# CONFIG_NET_VENDOR_3COM is not set
+# CONFIG_SMC91X is not set
+# CONFIG_DM9000 is not set
+# CONFIG_ENC28J60 is not set
+# CONFIG_ETHOC is not set
+# CONFIG_SMC911X is not set
+# CONFIG_SMSC911X is not set
+# CONFIG_DNET is not set
+# CONFIG_NET_TULIP is not set
+# CONFIG_HP100 is not set
+# CONFIG_IBM_NEW_EMAC_ZMII is not set
+# CONFIG_IBM_NEW_EMAC_RGMII is not set
+# CONFIG_IBM_NEW_EMAC_TAH is not set
+# CONFIG_IBM_NEW_EMAC_EMAC4 is not set
+# CONFIG_IBM_NEW_EMAC_NO_FLOW_CTRL is not set
+# CONFIG_IBM_NEW_EMAC_MAL_CLR_ICINTSTAT is not set
+# CONFIG_IBM_NEW_EMAC_MAL_COMMON_ERR is not set
+# CONFIG_NET_PCI is not set
+# CONFIG_B44 is not set
+# CONFIG_KS8851 is not set
+# CONFIG_KS8851_MLL is not set
+# CONFIG_ATL2 is not set
+# CONFIG_FTMAC100 is not set
+CONFIG_NETDEV_1000=y
+# CONFIG_ACENIC is not set
+# CONFIG_DL2K is not set
+# CONFIG_E1000 is not set
+CONFIG_E1000E=m
+# CONFIG_IP1000 is not set
+# CONFIG_IGB is not set
+# CONFIG_IGBVF is not set
+# CONFIG_NS83820 is not set
+# CONFIG_HAMACHI is not set
+# CONFIG_YELLOWFIN is not set
+# CONFIG_R8169 is not set
+# CONFIG_SIS190 is not set
+# CONFIG_SKGE is not set
+# CONFIG_SKY2 is not set
+# CONFIG_VIA_VELOCITY is not set
+# CONFIG_TIGON3 is not set
+# CONFIG_BNX2 is not set
+# CONFIG_CNIC is not set
+# CONFIG_QLA3XXX is not set
+# CONFIG_ATL1 is not set
+# CONFIG_ATL1E is not set
+# CONFIG_ATL1C is not set
+# CONFIG_JME is not set
+# CONFIG_STMMAC_ETH is not set
+# CONFIG_PCH_GBE is not set
+CONFIG_NETDEV_10000=y
+# CONFIG_CHELSIO_T1 is not set
+# CONFIG_CHELSIO_T3 is not set
+# CONFIG_CHELSIO_T4 is not set
+# CONFIG_CHELSIO_T4VF is not set
+# CONFIG_ENIC is not set
+# CONFIG_IXGBE is not set
+# CONFIG_IXGBEVF is not set
+# CONFIG_IXGB is not set
+# CONFIG_S2IO is not set
+# CONFIG_VXGE is not set
+# CONFIG_MYRI10GE is not set
+# CONFIG_NETXEN_NIC is not set
+# CONFIG_NIU is not set
+# CONFIG_MLX4_EN is not set
+# CONFIG_MLX4_CORE is not set
+# CONFIG_TEHUTI is not set
+# CONFIG_BNX2X is not set
+# CONFIG_QLCNIC is not set
+# CONFIG_QLGE is not set
+# CONFIG_BNA is not set
+# CONFIG_SFC is not set
+# CONFIG_BE2NET is not set
+# CONFIG_TR is not set
+CONFIG_WLAN=y
+# CONFIG_ATMEL is not set
+# CONFIG_PRISM54 is not set
+# CONFIG_USB_ZD1201 is not set
+# CONFIG_HOSTAP is not set
+
+#
+# Enable WiMAX (Networking options) to see the WiMAX drivers
+#
+
+#
+# USB Network Adapters
+#
+# CONFIG_USB_CATC is not set
+# CONFIG_USB_KAWETH is not set
+# CONFIG_USB_PEGASUS is not set
+# CONFIG_USB_RTL8150 is not set
+CONFIG_USB_USBNET=m
+CONFIG_USB_NET_AX8817X=m
+CONFIG_USB_NET_CDCETHER=m
+# CONFIG_USB_NET_CDC_EEM is not set
+CONFIG_USB_NET_CDC_NCM=m
+# CONFIG_USB_NET_DM9601 is not set
+# CONFIG_USB_NET_SMSC75XX is not set
+# CONFIG_USB_NET_SMSC95XX is not set
+# CONFIG_USB_NET_GL620A is not set
+CONFIG_USB_NET_NET1080=m
+# CONFIG_USB_NET_PLUSB is not set
+# CONFIG_USB_NET_MCS7830 is not set
+# CONFIG_USB_NET_RNDIS_HOST is not set
+CONFIG_USB_NET_CDC_SUBSET=m
+# CONFIG_USB_ALI_M5632 is not set
+# CONFIG_USB_AN2720 is not set
+CONFIG_USB_BELKIN=y
+CONFIG_USB_ARMLINUX=y
+# CONFIG_USB_EPSON2888 is not set
+# CONFIG_USB_KC2190 is not set
+CONFIG_USB_NET_ZAURUS=m
+# CONFIG_USB_NET_CX82310_ETH is not set
+# CONFIG_USB_NET_KALMIA is not set
+# CONFIG_USB_NET_INT51X1 is not set
+# CONFIG_USB_IPHETH is not set
+# CONFIG_USB_SIERRA_NET is not set
+# CONFIG_USB_VL600 is not set
+# CONFIG_WAN is not set
+
+#
+# CAIF transport drivers
+#
+# CONFIG_FDDI is not set
+# CONFIG_HIPPI is not set
+# CONFIG_PPP is not set
+# CONFIG_SLIP is not set
+# CONFIG_NET_FC is not set
+# CONFIG_NETPOLL is not set
+# CONFIG_NET_POLL_CONTROLLER is not set
+# CONFIG_VMXNET3 is not set
+# CONFIG_ISDN is not set
+# CONFIG_PHONE is not set
+
+#
+# Input device support
+#
+CONFIG_INPUT=y
+# CONFIG_INPUT_FF_MEMLESS is not set
+# CONFIG_INPUT_POLLDEV is not set
+# CONFIG_INPUT_SPARSEKMAP is not set
+
+#
+# Userland interfaces
+#
+CONFIG_INPUT_MOUSEDEV=y
+# CONFIG_INPUT_MOUSEDEV_PSAUX is not set
+CONFIG_INPUT_MOUSEDEV_SCREEN_X=1024
+CONFIG_INPUT_MOUSEDEV_SCREEN_Y=768
+# CONFIG_INPUT_JOYDEV is not set
+# CONFIG_INPUT_EVDEV is not set
+# CONFIG_INPUT_EVBUG is not set
+
+#
+# Input Device Drivers
+#
+# CONFIG_INPUT_KEYBOARD is not set
+# CONFIG_INPUT_MOUSE is not set
+# CONFIG_INPUT_JOYSTICK is not set
+# CONFIG_INPUT_TABLET is not set
+# CONFIG_INPUT_TOUCHSCREEN is not set
+# CONFIG_INPUT_MISC is not set
+
+#
+# Hardware I/O ports
+#
+CONFIG_SERIO=y
+CONFIG_SERIO_SERPORT=y
+# CONFIG_SERIO_PCIPS2 is not set
+# CONFIG_SERIO_RAW is not set
+# CONFIG_SERIO_ALTERA_PS2 is not set
+# CONFIG_SERIO_PS2MULT is not set
+# CONFIG_GAMEPORT is not set
+
+#
+# Character devices
+#
+CONFIG_VT=y
+CONFIG_CONSOLE_TRANSLATIONS=y
+CONFIG_VT_CONSOLE=y
+CONFIG_HW_CONSOLE=y
+# CONFIG_VT_HW_CONSOLE_BINDING is not set
+CONFIG_UNIX98_PTYS=y
+# CONFIG_DEVPTS_MULTIPLE_INSTANCES is not set
+CONFIG_LEGACY_PTYS=y
+CONFIG_LEGACY_PTY_COUNT=16
+# CONFIG_SERIAL_NONSTANDARD is not set
+# CONFIG_NOZOMI is not set
+# CONFIG_N_GSM is not set
+# CONFIG_TRACE_SINK is not set
+CONFIG_DEVKMEM=y
+
+#
+# Serial drivers
+#
+CONFIG_SERIAL_8250=y
+CONFIG_SERIAL_8250_CONSOLE=y
+CONFIG_SERIAL_8250_PCI=y
+CONFIG_SERIAL_8250_NR_UARTS=1
+CONFIG_SERIAL_8250_RUNTIME_UARTS=1
+CONFIG_SERIAL_8250_EXTENDED=y
+# CONFIG_SERIAL_8250_MANY_PORTS is not set
+# CONFIG_SERIAL_8250_SHARE_IRQ is not set
+# CONFIG_SERIAL_8250_DETECT_IRQ is not set
+# CONFIG_SERIAL_8250_RSA is not set
+
+#
+# Non-8250 serial port support
+#
+# CONFIG_SERIAL_MAX3100 is not set
+# CONFIG_SERIAL_MAX3107 is not set
+# CONFIG_SERIAL_MFD_HSU is not set
+CONFIG_SERIAL_CORE=y
+CONFIG_SERIAL_CORE_CONSOLE=y
+# CONFIG_SERIAL_JSM is not set
+# CONFIG_SERIAL_TIMBERDALE is not set
+# CONFIG_SERIAL_ALTERA_JTAGUART is not set
+# CONFIG_SERIAL_ALTERA_UART is not set
+# CONFIG_SERIAL_PCH_UART is not set
+# CONFIG_SERIAL_XILINX_PS_UART is not set
+# CONFIG_HVC_DCC is not set
+# CONFIG_IPMI_HANDLER is not set
+CONFIG_HW_RANDOM=y
+# CONFIG_HW_RANDOM_TIMERIOMEM is not set
+# CONFIG_R3964 is not set
+# CONFIG_APPLICOM is not set
+# CONFIG_RAW_DRIVER is not set
+# CONFIG_TCG_TPM is not set
+CONFIG_DEVPORT=y
+# CONFIG_RAMOOPS is not set
+CONFIG_I2C=y
+CONFIG_I2C_BOARDINFO=y
+CONFIG_I2C_COMPAT=y
+CONFIG_I2C_CHARDEV=y
+# CONFIG_I2C_MUX is not set
+CONFIG_I2C_HELPER_AUTO=y
+
+#
+# I2C Hardware Bus support
+#
+
+#
+# PC SMBus host controller drivers
+#
+# CONFIG_I2C_ALI1535 is not set
+# CONFIG_I2C_ALI1563 is not set
+# CONFIG_I2C_ALI15X3 is not set
+# CONFIG_I2C_AMD756 is not set
+# CONFIG_I2C_AMD8111 is not set
+# CONFIG_I2C_I801 is not set
+# CONFIG_I2C_ISCH is not set
+# CONFIG_I2C_PIIX4 is not set
+# CONFIG_I2C_NFORCE2 is not set
+# CONFIG_I2C_SIS5595 is not set
+# CONFIG_I2C_SIS630 is not set
+# CONFIG_I2C_SIS96X is not set
+# CONFIG_I2C_VIA is not set
+# CONFIG_I2C_VIAPRO is not set
+
+#
+# I2C system bus drivers (mostly embedded / system-on-chip)
+#
+# CONFIG_I2C_INTEL_MID is not set
+# CONFIG_I2C_OCORES is not set
+# CONFIG_I2C_PCA_PLATFORM is not set
+# CONFIG_I2C_PXA_PCI is not set
+# CONFIG_I2C_SIMTEC is not set
+CONFIG_I2C_TRANSCEDE=y
+# CONFIG_I2C_XILINX is not set
+# CONFIG_I2C_EG20T is not set
+
+#
+# External I2C/SMBus adapter drivers
+#
+# CONFIG_I2C_DIOLAN_U2C is not set
+# CONFIG_I2C_PARPORT_LIGHT is not set
+# CONFIG_I2C_TAOS_EVM is not set
+# CONFIG_I2C_TINY_USB is not set
+
+#
+# Other I2C/SMBus bus drivers
+#
+# CONFIG_I2C_STUB is not set
+# CONFIG_I2C_DEBUG_CORE is not set
+# CONFIG_I2C_DEBUG_ALGO is not set
+# CONFIG_I2C_DEBUG_BUS is not set
+CONFIG_SPI=y
+# CONFIG_SPI_DEBUG is not set
+CONFIG_SPI_MASTER=y
+
+#
+# SPI Master Controller Drivers
+#
+# CONFIG_SPI_ALTERA is not set
+# CONFIG_SPI_BITBANG is not set
+# CONFIG_SPI_PXA2XX_PCI is not set
+# CONFIG_SPI_TOPCLIFF_PCH is not set
+CONFIG_SPI_TRANSCEDE=y
+# CONFIG_SPI_TRANSCEDE_CS_GPIO is not set
+# CONFIG_SPI_XILINX is not set
+# CONFIG_SPI_DESIGNWARE is not set
+
+#
+# SPI Protocol Masters
+#
+CONFIG_SPI_SPIDEV=y
+# CONFIG_SPI_TLE62X0 is not set
+# CONFIG_SPI_CDCE62005 is not set
+
+#
+# PPS support
+#
+# CONFIG_PPS is not set
+
+#
+# PPS generators support
+#
+
+#
+# PTP clock support
+#
+
+#
+# Enable Device Drivers -> PPS to see the PTP clock options.
+#
+# CONFIG_W1 is not set
+# CONFIG_POWER_SUPPLY is not set
+# CONFIG_HWMON is not set
+# CONFIG_THERMAL is not set
+CONFIG_WATCHDOG=y
+# CONFIG_WATCHDOG_NOWAYOUT is not set
+
+#
+# Watchdog Device Drivers
+#
+# CONFIG_SOFT_WATCHDOG is not set
+CONFIG_MPCORE_WATCHDOG=y
+# CONFIG_MAX63XX_WATCHDOG is not set
+# CONFIG_ALIM7101_WDT is not set
+
+#
+# PCI-based Watchdog Cards
+#
+# CONFIG_PCIPCWATCHDOG is not set
+# CONFIG_WDTPCI is not set
+
+#
+# USB-based Watchdog Cards
+#
+# CONFIG_USBPCWATCHDOG is not set
+CONFIG_SSB_POSSIBLE=y
+
+#
+# Sonics Silicon Backplane
+#
+# CONFIG_SSB is not set
+CONFIG_BCMA_POSSIBLE=y
+
+#
+# Broadcom specific AMBA
+#
+# CONFIG_BCMA is not set
+CONFIG_MFD_SUPPORT=y
+# CONFIG_MFD_CORE is not set
+# CONFIG_MFD_88PM860X is not set
+# CONFIG_MFD_SM501 is not set
+# CONFIG_HTC_PASIC3 is not set
+# CONFIG_TPS6105X is not set
+# CONFIG_TPS6507X is not set
+# CONFIG_TWL4030_CORE is not set
+# CONFIG_MFD_STMPE is not set
+# CONFIG_MFD_TC3589X is not set
+# CONFIG_MFD_TMIO is not set
+# CONFIG_PMIC_DA903X is not set
+# CONFIG_PMIC_ADP5520 is not set
+# CONFIG_MFD_MAX8925 is not set
+# CONFIG_MFD_MAX8997 is not set
+# CONFIG_MFD_MAX8998 is not set
+# CONFIG_MFD_WM8400 is not set
+# CONFIG_MFD_WM831X_I2C is not set
+# CONFIG_MFD_WM831X_SPI is not set
+# CONFIG_MFD_WM8350_I2C is not set
+# CONFIG_MFD_WM8994 is not set
+# CONFIG_MFD_PCF50633 is not set
+# CONFIG_MFD_MC13XXX is not set
+# CONFIG_ABX500_CORE is not set
+# CONFIG_EZX_PCAP is not set
+# CONFIG_LPC_SCH is not set
+# CONFIG_MFD_RDC321X is not set
+# CONFIG_MFD_JANZ_CMODIO is not set
+# CONFIG_MFD_VX855 is not set
+# CONFIG_MFD_WL1273_CORE is not set
+# CONFIG_REGULATOR is not set
+# CONFIG_MEDIA_SUPPORT is not set
+
+#
+# Graphics support
+#
+CONFIG_VGA_ARB=y
+CONFIG_VGA_ARB_MAX_GPUS=16
+# CONFIG_DRM is not set
+# CONFIG_STUB_POULSBO is not set
+# CONFIG_VGASTATE is not set
+# CONFIG_VIDEO_OUTPUT_CONTROL is not set
+# CONFIG_FB is not set
+# CONFIG_BACKLIGHT_LCD_SUPPORT is not set
+
+#
+# Display device support
+#
+# CONFIG_DISPLAY_SUPPORT is not set
+
+#
+# Console display driver support
+#
+CONFIG_DUMMY_CONSOLE=y
+# CONFIG_SOUND is not set
+CONFIG_HID_SUPPORT=y
+CONFIG_HID=y
+# CONFIG_HIDRAW is not set
+
+#
+# USB Input Devices
+#
+# CONFIG_USB_HID is not set
+# CONFIG_HID_PID is not set
+
+#
+# Special HID drivers
+#
+CONFIG_USB_SUPPORT=y
+CONFIG_USB_ARCH_HAS_HCD=y
+CONFIG_USB_ARCH_HAS_OHCI=y
+CONFIG_USB_ARCH_HAS_EHCI=y
+CONFIG_USB=m
+# CONFIG_USB_DEBUG is not set
+# CONFIG_USB_ANNOUNCE_NEW_DEVICES is not set
+
+#
+# Miscellaneous USB options
+#
+# CONFIG_USB_DEVICEFS is not set
+CONFIG_USB_DEVICE_CLASS=y
+# CONFIG_USB_DYNAMIC_MINORS is not set
+# CONFIG_USB_MON is not set
+# CONFIG_USB_WUSB is not set
+# CONFIG_USB_WUSB_CBAF is not set
+
+#
+# USB Host Controller Drivers
+#
+# CONFIG_USB_C67X00_HCD is not set
+# CONFIG_USB_XHCI_HCD is not set
+CONFIG_USB_EHCI_HCD=m
+# CONFIG_USB_EHCI_ROOT_HUB_TT is not set
+CONFIG_USB_EHCI_TT_NEWSCHED=y
+# CONFIG_USB_OXU210HP_HCD is not set
+# CONFIG_USB_ISP116X_HCD is not set
+# CONFIG_USB_ISP1760_HCD is not set
+# CONFIG_USB_ISP1362_HCD is not set
+# CONFIG_USB_OHCI_HCD is not set
+# CONFIG_USB_UHCI_HCD is not set
+# CONFIG_USB_SL811_HCD is not set
+# CONFIG_USB_R8A66597_HCD is not set
+# CONFIG_USB_WHCI_HCD is not set
+# CONFIG_USB_HWA_HCD is not set
+
+#
+# Enable Host or Gadget support to see Inventra options
+#
+# CONFIG_USB_MUSB_HDRC is not set
+
+#
+# USB Device Class drivers
+#
+# CONFIG_USB_ACM is not set
+# CONFIG_USB_PRINTER is not set
+# CONFIG_USB_WDM is not set
+# CONFIG_USB_TMC is not set
+
+#
+# NOTE: USB_STORAGE depends on SCSI but BLK_DEV_SD may
+#
+
+#
+# also be needed; see USB_STORAGE Help for more info
+#
+CONFIG_USB_STORAGE=m
+# CONFIG_USB_STORAGE_DEBUG is not set
+# CONFIG_USB_STORAGE_REALTEK is not set
+# CONFIG_USB_STORAGE_DATAFAB is not set
+# CONFIG_USB_STORAGE_FREECOM is not set
+# CONFIG_USB_STORAGE_ISD200 is not set
+# CONFIG_USB_STORAGE_USBAT is not set
+# CONFIG_USB_STORAGE_SDDR09 is not set
+# CONFIG_USB_STORAGE_SDDR55 is not set
+# CONFIG_USB_STORAGE_JUMPSHOT is not set
+# CONFIG_USB_STORAGE_ALAUDA is not set
+# CONFIG_USB_STORAGE_ONETOUCH is not set
+# CONFIG_USB_STORAGE_KARMA is not set
+# CONFIG_USB_STORAGE_CYPRESS_ATACB is not set
+# CONFIG_USB_STORAGE_ENE_UB6250 is not set
+# CONFIG_USB_UAS is not set
+# CONFIG_USB_LIBUSUAL is not set
+
+#
+# USB Imaging devices
+#
+# CONFIG_USB_MDC800 is not set
+# CONFIG_USB_MICROTEK is not set
+
+#
+# USB port drivers
+#
+# CONFIG_USB_SERIAL is not set
+
+#
+# USB Miscellaneous drivers
+#
+# CONFIG_USB_EMI62 is not set
+# CONFIG_USB_EMI26 is not set
+# CONFIG_USB_ADUTUX is not set
+# CONFIG_USB_SEVSEG is not set
+# CONFIG_USB_RIO500 is not set
+# CONFIG_USB_LEGOTOWER is not set
+# CONFIG_USB_LCD is not set
+# CONFIG_USB_LED is not set
+# CONFIG_USB_CYPRESS_CY7C63 is not set
+# CONFIG_USB_CYTHERM is not set
+# CONFIG_USB_IDMOUSE is not set
+# CONFIG_USB_FTDI_ELAN is not set
+# CONFIG_USB_APPLEDISPLAY is not set
+# CONFIG_USB_SISUSBVGA is not set
+# CONFIG_USB_LD is not set
+# CONFIG_USB_TRANCEVIBRATOR is not set
+# CONFIG_USB_IOWARRIOR is not set
+# CONFIG_USB_TEST is not set
+# CONFIG_USB_ISIGHTFW is not set
+# CONFIG_USB_YUREX is not set
+# CONFIG_USB_GADGET is not set
+
+#
+# OTG and related infrastructure
+#
+# CONFIG_USB_ULPI is not set
+# CONFIG_NOP_USB_XCEIV is not set
+CONFIG_DWC_OTG=m
+CONFIG_DWC_OTG_HOST_ONLY=y
+# CONFIG_DWC_OTG_DEVICE_ONLY is not set
+CONFIG_DWC_OTG_TRANSCEDE=y
+# CONFIG_DWC_OTG_DEBUG is not set
+# CONFIG_UWB is not set
+# CONFIG_MMC is not set
+# CONFIG_MEMSTICK is not set
+# CONFIG_NEW_LEDS is not set
+# CONFIG_NFC_DEVICES is not set
+# CONFIG_ACCESSIBILITY is not set
+# CONFIG_INFINIBAND is not set
+CONFIG_RTC_LIB=y
+# CONFIG_RTC_CLASS is not set
+# CONFIG_DMADEVICES is not set
+# CONFIG_AUXDISPLAY is not set
+CONFIG_UIO=y
+# CONFIG_UIO_CIF is not set
+CONFIG_UIO_PDRV=y
+# CONFIG_UIO_PDRV_GENIRQ is not set
+# CONFIG_UIO_AEC is not set
+# CONFIG_UIO_SERCOS3 is not set
+# CONFIG_UIO_PCI_GENERIC is not set
+# CONFIG_UIO_NETX is not set
+# CONFIG_STAGING is not set
+
+#
+# File systems
+#
+# CONFIG_EXT2_FS is not set
+# CONFIG_EXT3_FS is not set
+# CONFIG_EXT4_FS is not set
+# CONFIG_REISERFS_FS is not set
+# CONFIG_JFS_FS is not set
+# CONFIG_XFS_FS is not set
+# CONFIG_GFS2_FS is not set
+# CONFIG_BTRFS_FS is not set
+# CONFIG_NILFS2_FS is not set
+CONFIG_FS_POSIX_ACL=y
+CONFIG_FILE_LOCKING=y
+CONFIG_FSNOTIFY=y
+CONFIG_DNOTIFY=y
+CONFIG_INOTIFY_USER=y
+# CONFIG_FANOTIFY is not set
+# CONFIG_QUOTA is not set
+# CONFIG_QUOTACTL is not set
+CONFIG_AUTOFS4_FS=y
+# CONFIG_FUSE_FS is not set
+
+#
+# Caches
+#
+# CONFIG_FSCACHE is not set
+
+#
+# CD-ROM/DVD Filesystems
+#
+# CONFIG_ISO9660_FS is not set
+# CONFIG_UDF_FS is not set
+
+#
+# DOS/FAT/NT Filesystems
+#
+CONFIG_FAT_FS=y
+CONFIG_MSDOS_FS=y
+CONFIG_VFAT_FS=y
+CONFIG_FAT_DEFAULT_CODEPAGE=437
+CONFIG_FAT_DEFAULT_IOCHARSET="iso8859-1"
+# CONFIG_NTFS_FS is not set
+
+#
+# Pseudo filesystems
+#
+CONFIG_PROC_FS=y
+CONFIG_PROC_SYSCTL=y
+CONFIG_PROC_PAGE_MONITOR=y
+CONFIG_SYSFS=y
+CONFIG_TMPFS=y
+# CONFIG_TMPFS_POSIX_ACL is not set
+# CONFIG_TMPFS_XATTR is not set
+# CONFIG_HUGETLB_PAGE is not set
+# CONFIG_CONFIGFS_FS is not set
+CONFIG_MISC_FILESYSTEMS=y
+# CONFIG_ADFS_FS is not set
+# CONFIG_AFFS_FS is not set
+# CONFIG_ECRYPT_FS is not set
+# CONFIG_HFS_FS is not set
+# CONFIG_HFSPLUS_FS is not set
+# CONFIG_BEFS_FS is not set
+# CONFIG_BFS_FS is not set
+# CONFIG_EFS_FS is not set
+CONFIG_JFFS2_FS=y
+CONFIG_JFFS2_FS_DEBUG=0
+CONFIG_JFFS2_FS_WRITEBUFFER=y
+# CONFIG_JFFS2_FS_WBUF_VERIFY is not set
+# CONFIG_JFFS2_SUMMARY is not set
+# CONFIG_JFFS2_FS_XATTR is not set
+# CONFIG_JFFS2_COMPRESSION_OPTIONS is not set
+CONFIG_JFFS2_ZLIB=y
+# CONFIG_JFFS2_LZO is not set
+CONFIG_JFFS2_RTIME=y
+# CONFIG_JFFS2_RUBIN is not set
+# CONFIG_LOGFS is not set
+# CONFIG_CRAMFS is not set
+# CONFIG_SQUASHFS is not set
+# CONFIG_VXFS_FS is not set
+# CONFIG_MINIX_FS is not set
+# CONFIG_OMFS_FS is not set
+# CONFIG_HPFS_FS is not set
+# CONFIG_QNX4FS_FS is not set
+# CONFIG_ROMFS_FS is not set
+# CONFIG_PSTORE is not set
+# CONFIG_SYSV_FS is not set
+# CONFIG_UFS_FS is not set
+CONFIG_NETWORK_FILESYSTEMS=y
+CONFIG_NFS_FS=y
+CONFIG_NFS_V3=y
+CONFIG_NFS_V3_ACL=y
+CONFIG_NFS_V4=y
+# CONFIG_NFS_V4_1 is not set
+CONFIG_ROOT_NFS=y
+# CONFIG_NFS_USE_LEGACY_DNS is not set
+CONFIG_NFS_USE_KERNEL_DNS=y
+# CONFIG_NFS_USE_NEW_IDMAPPER is not set
+# CONFIG_NFSD is not set
+CONFIG_LOCKD=y
+CONFIG_LOCKD_V4=y
+CONFIG_NFS_ACL_SUPPORT=y
+CONFIG_NFS_COMMON=y
+CONFIG_SUNRPC=y
+CONFIG_SUNRPC_GSS=y
+# CONFIG_CEPH_FS is not set
+# CONFIG_CIFS is not set
+# CONFIG_NCP_FS is not set
+# CONFIG_CODA_FS is not set
+# CONFIG_AFS_FS is not set
+
+#
+# Partition Types
+#
+# CONFIG_PARTITION_ADVANCED is not set
+CONFIG_MSDOS_PARTITION=y
+CONFIG_NLS=y
+CONFIG_NLS_DEFAULT="iso8859-1"
+CONFIG_NLS_CODEPAGE_437=y
+# CONFIG_NLS_CODEPAGE_737 is not set
+# CONFIG_NLS_CODEPAGE_775 is not set
+# CONFIG_NLS_CODEPAGE_850 is not set
+# CONFIG_NLS_CODEPAGE_852 is not set
+# CONFIG_NLS_CODEPAGE_855 is not set
+# CONFIG_NLS_CODEPAGE_857 is not set
+# CONFIG_NLS_CODEPAGE_860 is not set
+# CONFIG_NLS_CODEPAGE_861 is not set
+# CONFIG_NLS_CODEPAGE_862 is not set
+# CONFIG_NLS_CODEPAGE_863 is not set
+# CONFIG_NLS_CODEPAGE_864 is not set
+# CONFIG_NLS_CODEPAGE_865 is not set
+# CONFIG_NLS_CODEPAGE_866 is not set
+# CONFIG_NLS_CODEPAGE_869 is not set
+# CONFIG_NLS_CODEPAGE_936 is not set
+# CONFIG_NLS_CODEPAGE_950 is not set
+# CONFIG_NLS_CODEPAGE_932 is not set
+# CONFIG_NLS_CODEPAGE_949 is not set
+# CONFIG_NLS_CODEPAGE_874 is not set
+# CONFIG_NLS_ISO8859_8 is not set
+# CONFIG_NLS_CODEPAGE_1250 is not set
+# CONFIG_NLS_CODEPAGE_1251 is not set
+# CONFIG_NLS_ASCII is not set
+CONFIG_NLS_ISO8859_1=y
+# CONFIG_NLS_ISO8859_2 is not set
+# CONFIG_NLS_ISO8859_3 is not set
+# CONFIG_NLS_ISO8859_4 is not set
+# CONFIG_NLS_ISO8859_5 is not set
+# CONFIG_NLS_ISO8859_6 is not set
+# CONFIG_NLS_ISO8859_7 is not set
+# CONFIG_NLS_ISO8859_9 is not set
+# CONFIG_NLS_ISO8859_13 is not set
+# CONFIG_NLS_ISO8859_14 is not set
+# CONFIG_NLS_ISO8859_15 is not set
+# CONFIG_NLS_KOI8_R is not set
+# CONFIG_NLS_KOI8_U is not set
+# CONFIG_NLS_UTF8 is not set
+
+#
+# Kernel hacking
+#
+# CONFIG_PRINTK_TIME is not set
+CONFIG_DEFAULT_MESSAGE_LOGLEVEL=4
+CONFIG_ENABLE_WARN_DEPRECATED=y
+CONFIG_ENABLE_MUST_CHECK=y
+CONFIG_FRAME_WARN=1024
+# CONFIG_MAGIC_SYSRQ is not set
+# CONFIG_STRIP_ASM_SYMS is not set
+# CONFIG_UNUSED_SYMBOLS is not set
+CONFIG_DEBUG_FS=y
+# CONFIG_HEADERS_CHECK is not set
+# CONFIG_DEBUG_SECTION_MISMATCH is not set
+CONFIG_DEBUG_KERNEL=y
+# CONFIG_LOCKUP_DETECTOR is not set
+# CONFIG_HARDLOCKUP_DETECTOR is not set
+CONFIG_DETECT_HUNG_TASK=y
+CONFIG_DEFAULT_HUNG_TASK_TIMEOUT=120
+# CONFIG_BOOTPARAM_HUNG_TASK_PANIC is not set
+CONFIG_BOOTPARAM_HUNG_TASK_PANIC_VALUE=0
+# CONFIG_SCHED_DEBUG is not set
+# CONFIG_SCHEDSTATS is not set
+# CONFIG_TIMER_STATS is not set
+# CONFIG_DEBUG_OBJECTS is not set
+# CONFIG_DEBUG_SLAB is not set
+# CONFIG_DEBUG_KMEMLEAK is not set
+# CONFIG_DEBUG_PREEMPT is not set
+# CONFIG_DEBUG_RT_MUTEXES is not set
+# CONFIG_RT_MUTEX_TESTER is not set
+# CONFIG_DEBUG_SPINLOCK is not set
+# CONFIG_DEBUG_MUTEXES is not set
+# CONFIG_DEBUG_LOCK_ALLOC is not set
+# CONFIG_PROVE_LOCKING is not set
+# CONFIG_SPARSE_RCU_POINTER is not set
+# CONFIG_LOCK_STAT is not set
+# CONFIG_DEBUG_SPINLOCK_SLEEP is not set
+# CONFIG_DEBUG_LOCKING_API_SELFTESTS is not set
+# CONFIG_DEBUG_STACK_USAGE is not set
+# CONFIG_DEBUG_KOBJECT is not set
+CONFIG_DEBUG_BUGVERBOSE=y
+# CONFIG_DEBUG_INFO is not set
+# CONFIG_DEBUG_VM is not set
+# CONFIG_DEBUG_WRITECOUNT is not set
+CONFIG_DEBUG_MEMORY_INIT=y
+# CONFIG_DEBUG_LIST is not set
+# CONFIG_TEST_LIST_SORT is not set
+# CONFIG_DEBUG_SG is not set
+# CONFIG_DEBUG_NOTIFIERS is not set
+# CONFIG_DEBUG_CREDENTIALS is not set
+# CONFIG_BOOT_PRINTK_DELAY is not set
+# CONFIG_RCU_TORTURE_TEST is not set
+CONFIG_RCU_CPU_STALL_TIMEOUT=60
+CONFIG_RCU_CPU_STALL_VERBOSE=y
+# CONFIG_BACKTRACE_SELF_TEST is not set
+# CONFIG_DEBUG_BLOCK_EXT_DEVT is not set
+# CONFIG_DEBUG_FORCE_WEAK_PER_CPU is not set
+# CONFIG_DEBUG_PER_CPU_MAPS is not set
+# CONFIG_LKDTM is not set
+# CONFIG_FAULT_INJECTION is not set
+# CONFIG_SYSCTL_SYSCALL_CHECK is not set
+# CONFIG_DEBUG_PAGEALLOC is not set
+CONFIG_HAVE_FUNCTION_TRACER=y
+CONFIG_HAVE_FUNCTION_GRAPH_TRACER=y
+CONFIG_HAVE_DYNAMIC_FTRACE=y
+CONFIG_HAVE_FTRACE_MCOUNT_RECORD=y
+CONFIG_HAVE_C_RECORDMCOUNT=y
+CONFIG_TRACING_SUPPORT=y
+CONFIG_FTRACE=y
+# CONFIG_FUNCTION_TRACER is not set
+# CONFIG_IRQSOFF_TRACER is not set
+# CONFIG_PREEMPT_TRACER is not set
+# CONFIG_SCHED_TRACER is not set
+# CONFIG_MISSED_TIMER_OFFSETS_HIST is not set
+# CONFIG_ENABLE_DEFAULT_TRACERS is not set
+CONFIG_BRANCH_PROFILE_NONE=y
+# CONFIG_PROFILE_ANNOTATED_BRANCHES is not set
+# CONFIG_PROFILE_ALL_BRANCHES is not set
+# CONFIG_STACK_TRACER is not set
+# CONFIG_BLK_DEV_IO_TRACE is not set
+# CONFIG_DYNAMIC_DEBUG is not set
+# CONFIG_DMA_API_DEBUG is not set
+# CONFIG_ATOMIC64_SELFTEST is not set
+# CONFIG_SAMPLES is not set
+CONFIG_HAVE_ARCH_KGDB=y
+# CONFIG_KGDB is not set
+# CONFIG_TEST_KSTRTOX is not set
+# CONFIG_STRICT_DEVMEM is not set
+CONFIG_ARM_UNWIND=y
+CONFIG_DEBUG_USER=y
+CONFIG_DEBUG_LL=y
+CONFIG_EARLY_PRINTK=y
+# CONFIG_DEBUG_ICEDCC is not set
+# CONFIG_OC_ETM is not set
+
+#
+# Security options
+#
+CONFIG_KEYS=y
+# CONFIG_KEYS_DEBUG_PROC_KEYS is not set
+# CONFIG_SECURITY_DMESG_RESTRICT is not set
+# CONFIG_SECURITY is not set
+# CONFIG_SECURITYFS is not set
+CONFIG_DEFAULT_SECURITY_DAC=y
+CONFIG_DEFAULT_SECURITY=""
+CONFIG_CRYPTO=y
+
+#
+# Crypto core or helper
+#
+CONFIG_CRYPTO_ALGAPI=y
+CONFIG_CRYPTO_ALGAPI2=y
+CONFIG_CRYPTO_AEAD=y
+CONFIG_CRYPTO_AEAD2=y
+CONFIG_CRYPTO_BLKCIPHER=y
+CONFIG_CRYPTO_BLKCIPHER2=y
+CONFIG_CRYPTO_HASH=y
+CONFIG_CRYPTO_HASH2=y
+CONFIG_CRYPTO_RNG2=y
+CONFIG_CRYPTO_PCOMP2=y
+CONFIG_CRYPTO_MANAGER=y
+CONFIG_CRYPTO_MANAGER2=y
+CONFIG_CRYPTO_MANAGER_DISABLE_TESTS=y
+# CONFIG_CRYPTO_GF128MUL is not set
+CONFIG_CRYPTO_NULL=y
+# CONFIG_CRYPTO_PCRYPT is not set
+CONFIG_CRYPTO_WORKQUEUE=y
+# CONFIG_CRYPTO_CRYPTD is not set
+CONFIG_CRYPTO_AUTHENC=y
+# CONFIG_CRYPTO_TEST is not set
+
+#
+# Authenticated Encryption with Associated Data
+#
+# CONFIG_CRYPTO_CCM is not set
+# CONFIG_CRYPTO_GCM is not set
+# CONFIG_CRYPTO_SEQIV is not set
+
+#
+# Block modes
+#
+CONFIG_CRYPTO_CBC=y
+# CONFIG_CRYPTO_CTR is not set
+# CONFIG_CRYPTO_CTS is not set
+# CONFIG_CRYPTO_ECB is not set
+# CONFIG_CRYPTO_LRW is not set
+# CONFIG_CRYPTO_PCBC is not set
+# CONFIG_CRYPTO_XTS is not set
+
+#
+# Hash modes
+#
+CONFIG_CRYPTO_HMAC=y
+# CONFIG_CRYPTO_XCBC is not set
+# CONFIG_CRYPTO_VMAC is not set
+
+#
+# Digest
+#
+CONFIG_CRYPTO_CRC32C=y
+# CONFIG_CRYPTO_GHASH is not set
+CONFIG_CRYPTO_MD4=m
+CONFIG_CRYPTO_MD5=y
+# CONFIG_CRYPTO_MICHAEL_MIC is not set
+# CONFIG_CRYPTO_RMD128 is not set
+# CONFIG_CRYPTO_RMD160 is not set
+# CONFIG_CRYPTO_RMD256 is not set
+# CONFIG_CRYPTO_RMD320 is not set
+CONFIG_CRYPTO_SHA1=y
+CONFIG_CRYPTO_SHA256=y
+CONFIG_CRYPTO_SHA512=y
+CONFIG_CRYPTO_TGR192=m
+CONFIG_CRYPTO_WP512=m
+
+#
+# Ciphers
+#
+CONFIG_CRYPTO_AES=y
+CONFIG_CRYPTO_ANUBIS=m
+# CONFIG_CRYPTO_ARC4 is not set
+CONFIG_CRYPTO_BLOWFISH=m
+CONFIG_CRYPTO_CAMELLIA=m
+CONFIG_CRYPTO_CAST5=m
+CONFIG_CRYPTO_CAST6=m
+CONFIG_CRYPTO_DES=y
+CONFIG_CRYPTO_FCRYPT=m
+CONFIG_CRYPTO_KHAZAD=m
+# CONFIG_CRYPTO_SALSA20 is not set
+# CONFIG_CRYPTO_SEED is not set
+CONFIG_CRYPTO_SERPENT=m
+CONFIG_CRYPTO_TEA=m
+CONFIG_CRYPTO_TWOFISH=m
+CONFIG_CRYPTO_TWOFISH_COMMON=m
+
+#
+# Compression
+#
+CONFIG_CRYPTO_DEFLATE=y
+# CONFIG_CRYPTO_ZLIB is not set
+# CONFIG_CRYPTO_LZO is not set
+
+#
+# Random Number Generation
+#
+# CONFIG_CRYPTO_ANSI_CPRNG is not set
+# CONFIG_CRYPTO_USER_API_HASH is not set
+# CONFIG_CRYPTO_USER_API_SKCIPHER is not set
+CONFIG_CRYPTO_HW=y
+# CONFIG_CRYPTO_DEV_HIFN_795X is not set
+CONFIG_CRYPTO_DEV_TRANSCEDE=m
+
+#
+# OCF Configuration
+#
+# CONFIG_OCF_OCF is not set
+# CONFIG_BINARY_PRINTF is not set
+
+#
+# Library routines
+#
+CONFIG_BITREVERSE=y
+CONFIG_CRC_CCITT=m
+CONFIG_CRC16=y
+# CONFIG_CRC_T10DIF is not set
+# CONFIG_CRC_ITU_T is not set
+CONFIG_CRC32=y
+# CONFIG_CRC7 is not set
+CONFIG_LIBCRC32C=y
+CONFIG_ZLIB_INFLATE=y
+CONFIG_ZLIB_DEFLATE=y
+CONFIG_LZO_DECOMPRESS=y
+CONFIG_XZ_DEC=y
+CONFIG_XZ_DEC_X86=y
+CONFIG_XZ_DEC_POWERPC=y
+CONFIG_XZ_DEC_IA64=y
+CONFIG_XZ_DEC_ARM=y
+CONFIG_XZ_DEC_ARMTHUMB=y
+CONFIG_XZ_DEC_SPARC=y
+CONFIG_XZ_DEC_BCJ=y
+# CONFIG_XZ_DEC_TEST is not set
+CONFIG_DECOMPRESS_GZIP=y
+CONFIG_DECOMPRESS_BZIP2=y
+CONFIG_DECOMPRESS_LZMA=y
+CONFIG_DECOMPRESS_XZ=y
+CONFIG_DECOMPRESS_LZO=y
+CONFIG_TEXTSEARCH=y
+CONFIG_TEXTSEARCH_KMP=m
+CONFIG_TEXTSEARCH_BM=m
+CONFIG_TEXTSEARCH_FSM=m
+CONFIG_HAS_IOMEM=y
+CONFIG_HAS_IOPORT=y
+CONFIG_HAS_DMA=y
+CONFIG_CPU_RMAP=y
+CONFIG_NLATTR=y
+# CONFIG_AVERAGE is not set
diff --git a/arch/arm/configs/t4000_x2_defconfig b/arch/arm/configs/t4000_x2_defconfig
new file mode 100644
index 0000000..9fb00f8
--- /dev/null
+++ b/arch/arm/configs/t4000_x2_defconfig
@@ -0,0 +1,1505 @@
+#
+# Automatically generated make config: don't edit
+# Linux/arm 3.0.51 Kernel Configuration
+#
+CONFIG_ARM=y
+CONFIG_SYS_SUPPORTS_APM_EMULATION=y
+CONFIG_HAVE_SCHED_CLOCK=y
+# CONFIG_ARCH_USES_GETTIMEOFFSET is not set
+CONFIG_GENERIC_CLOCKEVENTS=y
+CONFIG_GENERIC_CLOCKEVENTS_BROADCAST=y
+CONFIG_KTIME_SCALAR=y
+CONFIG_HAVE_PROC_CPU=y
+CONFIG_STACKTRACE_SUPPORT=y
+CONFIG_LOCKDEP_SUPPORT=y
+CONFIG_TRACE_IRQFLAGS_SUPPORT=y
+CONFIG_HARDIRQS_SW_RESEND=y
+CONFIG_GENERIC_IRQ_PROBE=y
+CONFIG_GENERIC_LOCKBREAK=y
+CONFIG_RWSEM_GENERIC_SPINLOCK=y
+CONFIG_ARCH_HAS_CPU_IDLE_WAIT=y
+CONFIG_GENERIC_HWEIGHT=y
+CONFIG_GENERIC_CALIBRATE_DELAY=y
+CONFIG_NEED_DMA_MAP_STATE=y
+CONFIG_VECTORS_BASE=0xffff0000
+# CONFIG_ARM_PATCH_PHYS_VIRT is not set
+CONFIG_DEFCONFIG_LIST="/lib/modules/$UNAME_RELEASE/.config"
+CONFIG_HAVE_IRQ_WORK=y
+CONFIG_IRQ_WORK=y
+
+#
+# General setup
+#
+CONFIG_EXPERIMENTAL=y
+CONFIG_INIT_ENV_ARG_LIMIT=32
+CONFIG_CROSS_COMPILE=""
+CONFIG_LOCALVERSION=""
+# CONFIG_LOCALVERSION_AUTO is not set
+CONFIG_HAVE_KERNEL_GZIP=y
+CONFIG_HAVE_KERNEL_LZMA=y
+CONFIG_HAVE_KERNEL_LZO=y
+CONFIG_KERNEL_GZIP=y
+# CONFIG_KERNEL_LZMA is not set
+# CONFIG_KERNEL_LZO is not set
+CONFIG_DEFAULT_HOSTNAME="(none)"
+# CONFIG_SWAP is not set
+CONFIG_SYSVIPC=y
+CONFIG_SYSVIPC_SYSCTL=y
+CONFIG_POSIX_MQUEUE=y
+CONFIG_POSIX_MQUEUE_SYSCTL=y
+# CONFIG_BSD_PROCESS_ACCT is not set
+# CONFIG_FHANDLE is not set
+# CONFIG_TASKSTATS is not set
+# CONFIG_AUDIT is not set
+CONFIG_HAVE_GENERIC_HARDIRQS=y
+
+#
+# IRQ subsystem
+#
+CONFIG_GENERIC_HARDIRQS=y
+CONFIG_HAVE_SPARSE_IRQ=y
+CONFIG_GENERIC_IRQ_SHOW=y
+CONFIG_IRQ_FORCED_THREADING=y
+# CONFIG_SPARSE_IRQ is not set
+
+#
+# RCU Subsystem
+#
+CONFIG_TREE_PREEMPT_RCU=y
+CONFIG_PREEMPT_RCU=y
+# CONFIG_RCU_TRACE is not set
+CONFIG_RCU_FANOUT=32
+# CONFIG_RCU_FANOUT_EXACT is not set
+# CONFIG_TREE_RCU_TRACE is not set
+# CONFIG_RCU_BOOST is not set
+CONFIG_IKCONFIG=y
+CONFIG_IKCONFIG_PROC=y
+CONFIG_LOG_BUF_SHIFT=14
+CONFIG_CGROUPS=y
+# CONFIG_CGROUP_DEBUG is not set
+# CONFIG_CGROUP_FREEZER is not set
+# CONFIG_CGROUP_DEVICE is not set
+# CONFIG_CPUSETS is not set
+# CONFIG_CGROUP_CPUACCT is not set
+# CONFIG_RESOURCE_COUNTERS is not set
+# CONFIG_CGROUP_PERF is not set
+# CONFIG_CGROUP_SCHED is not set
+# CONFIG_BLK_CGROUP is not set
+CONFIG_NAMESPACES=y
+# CONFIG_UTS_NS is not set
+# CONFIG_IPC_NS is not set
+# CONFIG_USER_NS is not set
+# CONFIG_PID_NS is not set
+# CONFIG_NET_NS is not set
+# CONFIG_SCHED_AUTOGROUP is not set
+# CONFIG_SYSFS_DEPRECATED is not set
+# CONFIG_RELAY is not set
+# CONFIG_BLK_DEV_INITRD is not set
+CONFIG_CC_OPTIMIZE_FOR_SIZE=y
+CONFIG_SYSCTL=y
+CONFIG_ANON_INODES=y
+# CONFIG_EXPERT is not set
+CONFIG_UID16=y
+CONFIG_SYSCTL_SYSCALL=y
+CONFIG_KALLSYMS=y
+# CONFIG_KALLSYMS_ALL is not set
+CONFIG_HOTPLUG=y
+CONFIG_PRINTK=y
+CONFIG_BUG=y
+CONFIG_ELF_CORE=y
+CONFIG_BASE_FULL=y
+CONFIG_FUTEX=y
+CONFIG_EPOLL=y
+CONFIG_SIGNALFD=y
+CONFIG_TIMERFD=y
+CONFIG_EVENTFD=y
+CONFIG_SHMEM=y
+CONFIG_AIO=y
+# CONFIG_EMBEDDED is not set
+CONFIG_HAVE_PERF_EVENTS=y
+CONFIG_PERF_USE_VMALLOC=y
+
+#
+# Kernel Performance Events And Counters
+#
+CONFIG_PERF_EVENTS=y
+CONFIG_PERF_COUNTERS=y
+# CONFIG_DEBUG_PERF_USE_VMALLOC is not set
+CONFIG_VM_EVENT_COUNTERS=y
+CONFIG_COMPAT_BRK=y
+CONFIG_SLAB=y
+# CONFIG_PROFILING is not set
+CONFIG_HAVE_OPROFILE=y
+# CONFIG_KPROBES is not set
+CONFIG_HAVE_KPROBES=y
+CONFIG_HAVE_KRETPROBES=y
+CONFIG_USE_GENERIC_SMP_HELPERS=y
+CONFIG_HAVE_REGS_AND_STACK_ACCESS_API=y
+CONFIG_HAVE_DMA_API_DEBUG=y
+CONFIG_HAVE_HW_BREAKPOINT=y
+
+#
+# GCOV-based kernel profiling
+#
+# CONFIG_GCOV_KERNEL is not set
+CONFIG_HAVE_GENERIC_DMA_COHERENT=y
+CONFIG_SLABINFO=y
+CONFIG_RT_MUTEXES=y
+CONFIG_BASE_SMALL=0
+CONFIG_MODULES=y
+# CONFIG_MODULE_FORCE_LOAD is not set
+CONFIG_MODULE_UNLOAD=y
+# CONFIG_MODULE_FORCE_UNLOAD is not set
+# CONFIG_MODVERSIONS is not set
+# CONFIG_MODULE_SRCVERSION_ALL is not set
+CONFIG_STOP_MACHINE=y
+CONFIG_BLOCK=y
+CONFIG_LBDAF=y
+# CONFIG_BLK_DEV_BSG is not set
+# CONFIG_BLK_DEV_INTEGRITY is not set
+
+#
+# IO Schedulers
+#
+CONFIG_IOSCHED_NOOP=y
+CONFIG_IOSCHED_DEADLINE=y
+# CONFIG_IOSCHED_CFQ is not set
+CONFIG_DEFAULT_DEADLINE=y
+# CONFIG_DEFAULT_NOOP is not set
+CONFIG_DEFAULT_IOSCHED="deadline"
+# CONFIG_INLINE_SPIN_TRYLOCK is not set
+# CONFIG_INLINE_SPIN_TRYLOCK_BH is not set
+# CONFIG_INLINE_SPIN_LOCK is not set
+# CONFIG_INLINE_SPIN_LOCK_BH is not set
+# CONFIG_INLINE_SPIN_LOCK_IRQ is not set
+# CONFIG_INLINE_SPIN_LOCK_IRQSAVE is not set
+# CONFIG_INLINE_SPIN_UNLOCK is not set
+# CONFIG_INLINE_SPIN_UNLOCK_BH is not set
+# CONFIG_INLINE_SPIN_UNLOCK_IRQ is not set
+# CONFIG_INLINE_SPIN_UNLOCK_IRQRESTORE is not set
+# CONFIG_INLINE_READ_TRYLOCK is not set
+# CONFIG_INLINE_READ_LOCK is not set
+# CONFIG_INLINE_READ_LOCK_BH is not set
+# CONFIG_INLINE_READ_LOCK_IRQ is not set
+# CONFIG_INLINE_READ_LOCK_IRQSAVE is not set
+# CONFIG_INLINE_READ_UNLOCK is not set
+# CONFIG_INLINE_READ_UNLOCK_BH is not set
+# CONFIG_INLINE_READ_UNLOCK_IRQ is not set
+# CONFIG_INLINE_READ_UNLOCK_IRQRESTORE is not set
+# CONFIG_INLINE_WRITE_TRYLOCK is not set
+# CONFIG_INLINE_WRITE_LOCK is not set
+# CONFIG_INLINE_WRITE_LOCK_BH is not set
+# CONFIG_INLINE_WRITE_LOCK_IRQ is not set
+# CONFIG_INLINE_WRITE_LOCK_IRQSAVE is not set
+# CONFIG_INLINE_WRITE_UNLOCK is not set
+# CONFIG_INLINE_WRITE_UNLOCK_BH is not set
+# CONFIG_INLINE_WRITE_UNLOCK_IRQ is not set
+# CONFIG_INLINE_WRITE_UNLOCK_IRQRESTORE is not set
+# CONFIG_MUTEX_SPIN_ON_OWNER is not set
+# CONFIG_FREEZER is not set
+
+#
+# System Type
+#
+CONFIG_MMU=y
+# CONFIG_ARCH_INTEGRATOR is not set
+# CONFIG_ARCH_REALVIEW is not set
+# CONFIG_ARCH_VERSATILE is not set
+# CONFIG_ARCH_VEXPRESS is not set
+# CONFIG_ARCH_AT91 is not set
+# CONFIG_ARCH_BCMRING is not set
+# CONFIG_ARCH_CLPS711X is not set
+# CONFIG_ARCH_CNS3XXX is not set
+# CONFIG_ARCH_GEMINI is not set
+# CONFIG_ARCH_EBSA110 is not set
+# CONFIG_ARCH_EP93XX is not set
+# CONFIG_ARCH_FOOTBRIDGE is not set
+# CONFIG_ARCH_MXC is not set
+# CONFIG_ARCH_MXS is not set
+# CONFIG_ARCH_NETX is not set
+# CONFIG_ARCH_H720X is not set
+# CONFIG_ARCH_IOP13XX is not set
+# CONFIG_ARCH_IOP32X is not set
+# CONFIG_ARCH_IOP33X is not set
+# CONFIG_ARCH_IXP23XX is not set
+# CONFIG_ARCH_IXP2000 is not set
+# CONFIG_ARCH_IXP4XX is not set
+# CONFIG_ARCH_DOVE is not set
+# CONFIG_ARCH_KIRKWOOD is not set
+# CONFIG_ARCH_LOKI is not set
+# CONFIG_ARCH_LPC32XX is not set
+# CONFIG_ARCH_MV78XX0 is not set
+# CONFIG_ARCH_ORION5X is not set
+# CONFIG_ARCH_MMP is not set
+# CONFIG_ARCH_KS8695 is not set
+# CONFIG_ARCH_W90X900 is not set
+# CONFIG_ARCH_NUC93X is not set
+# CONFIG_ARCH_TEGRA is not set
+# CONFIG_ARCH_PNX4008 is not set
+# CONFIG_ARCH_PXA is not set
+# CONFIG_ARCH_MSM is not set
+# CONFIG_ARCH_SHMOBILE is not set
+# CONFIG_ARCH_RPC is not set
+# CONFIG_ARCH_SA1100 is not set
+# CONFIG_ARCH_S3C2410 is not set
+# CONFIG_ARCH_S3C64XX is not set
+# CONFIG_ARCH_S5P64X0 is not set
+# CONFIG_ARCH_S5PC100 is not set
+# CONFIG_ARCH_S5PV210 is not set
+# CONFIG_ARCH_EXYNOS4 is not set
+# CONFIG_ARCH_SHARK is not set
+# CONFIG_ARCH_TCC_926 is not set
+CONFIG_ARCH_TRANSCEDE=y
+# CONFIG_ARCH_U300 is not set
+# CONFIG_ARCH_U8500 is not set
+# CONFIG_ARCH_NOMADIK is not set
+# CONFIG_ARCH_DAVINCI is not set
+# CONFIG_ARCH_OMAP is not set
+# CONFIG_PLAT_SPEAR is not set
+# CONFIG_ARCH_VT8500 is not set
+
+#
+# System MMU
+#
+
+#
+# Transcede Implementation Options
+#
+CONFIG_MACH_M84XXX=y
+# CONFIG_MACH_M822XX is not set
+# CONFIG_TRANSCEDE_TDM_CLOCK is not set
+CONFIG_TRANSCEDE_DUALCORE=y
+CONFIG_TRANSCEDE_UART0_SUPPORT=y
+# CONFIG_TRANSCEDE_UART1_SUPPORT is not set
+# CONFIG_TRANSCEDE_UART2_SUPPORT is not set
+CONFIG_TRANSCEDE_IPSEC_HW_SUPPORT=y
+CONFIG_TRANSCEDE_ELP_CLP30=y
+CONFIG_IPSEC_DMA_MAP_HACK_TX=y
+CONFIG_CLP30_SW_FIFO=y
+CONFIG_TRANSCEDE_ELP_PDU=y
+CONFIG_TRANSCEDE_GEMAC_0=y
+# CONFIG_TRANSCEDE_GEMAC_1 is not set
+CONFIG_TRANSCEDE_GEM_PHY=y
+CONFIG_MTD_NAND_TRANSCEDE=y
+# CONFIG_MTD_TRANSCEDE_NOR_8 is not set
+CONFIG_MTD_TRANSCEDE_NOR_16=y
+# CONFIG_TRANSCEDE_MLOG is not set
+# CONFIG_TRANSCEDE_ELP_TRNG is not set
+
+#
+# Processor Type
+#
+CONFIG_CPU_V7=y
+CONFIG_CPU_32v6K=y
+CONFIG_CPU_32v7=y
+CONFIG_CPU_ABRT_EV7=y
+CONFIG_CPU_PABRT_V7=y
+CONFIG_CPU_CACHE_V7=y
+CONFIG_CPU_CACHE_VIPT=y
+CONFIG_CPU_COPY_V6=y
+CONFIG_CPU_TLB_V7=y
+CONFIG_CPU_HAS_ASID=y
+CONFIG_CPU_CP15=y
+CONFIG_CPU_CP15_MMU=y
+
+#
+# Processor Features
+#
+CONFIG_ARM_THUMB=y
+# CONFIG_ARM_THUMBEE is not set
+# CONFIG_SWP_EMULATE is not set
+# CONFIG_CPU_ICACHE_DISABLE is not set
+# CONFIG_CPU_DCACHE_DISABLE is not set
+# CONFIG_CPU_BPREDICT_DISABLE is not set
+CONFIG_OUTER_CACHE=y
+CONFIG_OUTER_CACHE_SYNC=y
+CONFIG_CACHE_L2X0=y
+CONFIG_CACHE_PL310=y
+CONFIG_ARM_L1_CACHE_SHIFT=5
+CONFIG_ARM_DMA_MEM_BUFFERABLE=y
+CONFIG_CPU_HAS_PMU=y
+# CONFIG_ARM_ERRATA_430973 is not set
+# CONFIG_ARM_ERRATA_458693 is not set
+# CONFIG_ARM_ERRATA_460075 is not set
+# CONFIG_ARM_ERRATA_742230 is not set
+# CONFIG_ARM_ERRATA_742231 is not set
+CONFIG_PL310_ERRATA_588369=y
+# CONFIG_ARM_ERRATA_720789 is not set
+CONFIG_PL310_ERRATA_727915=y
+# CONFIG_ARM_ERRATA_743622 is not set
+# CONFIG_ARM_ERRATA_751472 is not set
+CONFIG_ARM_ERRATA_753970=y
+# CONFIG_ARM_ERRATA_754322 is not set
+# CONFIG_ARM_ERRATA_754327 is not set
+# CONFIG_ARM_ERRATA_764369 is not set
+# CONFIG_PL310_ERRATA_769419 is not set
+# CONFIG_ARM_ERRATA_775420 is not set
+CONFIG_ARM_GIC=y
+
+#
+# Bus support
+#
+# CONFIG_PCI_SYSCALL is not set
+# CONFIG_ARCH_SUPPORTS_MSI is not set
+# CONFIG_PCCARD is not set
+
+#
+# Kernel Features
+#
+CONFIG_TICK_ONESHOT=y
+# CONFIG_NO_HZ is not set
+CONFIG_HIGH_RES_TIMERS=y
+CONFIG_GENERIC_CLOCKEVENTS_BUILD=y
+CONFIG_SMP=y
+CONFIG_SMP_ON_UP=y
+CONFIG_HAVE_ARM_SCU=y
+CONFIG_HAVE_ARM_TWD=y
+# CONFIG_VMSPLIT_3G is not set
+CONFIG_VMSPLIT_2G=y
+# CONFIG_VMSPLIT_1G is not set
+CONFIG_PAGE_OFFSET=0x80000000
+CONFIG_NR_CPUS=2
+CONFIG_LOCAL_TIMERS=y
+CONFIG_GLOBAL_POLLING=y
+CONFIG_PREEMPT=y
+CONFIG_PREEMPT_RT_BASE=y
+# CONFIG_PREEMPT_NONE is not set
+# CONFIG_PREEMPT_VOLUNTARY is not set
+# CONFIG_PREEMPT_LL is not set
+# CONFIG_PREEMPT_RTB is not set
+CONFIG_PREEMPT_RT_FULL=y
+CONFIG_HZ=1000
+# CONFIG_THUMB2_KERNEL is not set
+CONFIG_AEABI=y
+CONFIG_OABI_COMPAT=y
+# CONFIG_ARCH_SPARSEMEM_DEFAULT is not set
+# CONFIG_ARCH_SELECT_MEMORY_MODEL is not set
+CONFIG_HAVE_ARCH_PFN_VALID=y
+CONFIG_HW_PERF_EVENTS=y
+CONFIG_SELECT_MEMORY_MODEL=y
+CONFIG_FLATMEM_MANUAL=y
+CONFIG_FLATMEM=y
+CONFIG_FLAT_NODE_MEM_MAP=y
+CONFIG_HAVE_MEMBLOCK=y
+CONFIG_PAGEFLAGS_EXTENDED=y
+CONFIG_SPLIT_PTLOCK_CPUS=4
+# CONFIG_COMPACTION is not set
+# CONFIG_PHYS_ADDR_T_64BIT is not set
+CONFIG_ZONE_DMA_FLAG=0
+CONFIG_VIRT_TO_BUS=y
+# CONFIG_KSM is not set
+CONFIG_DEFAULT_MMAP_MIN_ADDR=4096
+# CONFIG_CLEANCACHE is not set
+CONFIG_FORCE_MAX_ZONEORDER=11
+CONFIG_ALIGNMENT_TRAP=y
+# CONFIG_UACCESS_WITH_MEMCPY is not set
+# CONFIG_SECCOMP is not set
+# CONFIG_CC_STACKPROTECTOR is not set
+# CONFIG_DEPRECATED_PARAM_STRUCT is not set
+
+#
+# Boot options
+#
+# CONFIG_USE_OF is not set
+CONFIG_ZBOOT_ROM_TEXT=0x0
+CONFIG_ZBOOT_ROM_BSS=0x0
+CONFIG_CMDLINE="root=/dev/nfs nfsroot=10.1.69.3:/work/nfsroot ip=dhcp console=ttyAMA0 mem=128M"
+CONFIG_CMDLINE_FROM_BOOTLOADER=y
+# CONFIG_CMDLINE_EXTEND is not set
+# CONFIG_CMDLINE_FORCE is not set
+# CONFIG_XIP_KERNEL is not set
+# CONFIG_KEXEC is not set
+# CONFIG_CRASH_DUMP is not set
+# CONFIG_AUTO_ZRELADDR is not set
+
+#
+# CPU Power Management
+#
+# CONFIG_CPU_IDLE is not set
+
+#
+# Floating point emulation
+#
+
+#
+# At least one emulation must be selected
+#
+# CONFIG_FPE_NWFPE is not set
+# CONFIG_FPE_FASTFPE is not set
+# CONFIG_VFP is not set
+
+#
+# Userspace binary formats
+#
+CONFIG_BINFMT_ELF=y
+# CONFIG_CORE_DUMP_DEFAULT_ELF_HEADERS is not set
+CONFIG_HAVE_AOUT=y
+# CONFIG_BINFMT_AOUT is not set
+# CONFIG_BINFMT_MISC is not set
+
+#
+# Power management options
+#
+# CONFIG_SUSPEND is not set
+# CONFIG_PM_RUNTIME is not set
+CONFIG_ARCH_SUSPEND_POSSIBLE=y
+CONFIG_NET=y
+
+#
+# Networking options
+#
+CONFIG_PACKET=y
+CONFIG_UNIX=y
+CONFIG_XFRM=y
+CONFIG_XFRM_USER=m
+# CONFIG_XFRM_SUB_POLICY is not set
+# CONFIG_XFRM_MIGRATE is not set
+# CONFIG_XFRM_STATISTICS is not set
+CONFIG_XFRM_IPCOMP=m
+CONFIG_NET_KEY=m
+# CONFIG_NET_KEY_MIGRATE is not set
+CONFIG_INET=y
+CONFIG_IP_MULTICAST=y
+# CONFIG_IP_ADVANCED_ROUTER is not set
+CONFIG_IP_ROUTE_CLASSID=y
+CONFIG_IP_PNP=y
+CONFIG_IP_PNP_DHCP=y
+CONFIG_IP_PNP_BOOTP=y
+# CONFIG_IP_PNP_RARP is not set
+CONFIG_NET_IPIP=y
+# CONFIG_NET_IPGRE_DEMUX is not set
+# CONFIG_IP_MROUTE is not set
+# CONFIG_ARPD is not set
+# CONFIG_SYN_COOKIES is not set
+CONFIG_INET_AH=m
+CONFIG_INET_ESP=m
+CONFIG_INET_IPCOMP=m
+CONFIG_INET_XFRM_TUNNEL=m
+CONFIG_INET_TUNNEL=y
+CONFIG_INET_XFRM_MODE_TRANSPORT=m
+CONFIG_INET_XFRM_MODE_TUNNEL=m
+CONFIG_INET_XFRM_MODE_BEET=m
+# CONFIG_INET_LRO is not set
+CONFIG_INET_DIAG=y
+CONFIG_INET_TCP_DIAG=y
+# CONFIG_TCP_CONG_ADVANCED is not set
+CONFIG_TCP_CONG_CUBIC=y
+CONFIG_DEFAULT_TCP_CONG="cubic"
+# CONFIG_TCP_MD5SIG is not set
+# CONFIG_IPV6 is not set
+# CONFIG_NETWORK_SECMARK is not set
+# CONFIG_NETWORK_PHY_TIMESTAMPING is not set
+CONFIG_NETFILTER=y
+# CONFIG_NETFILTER_DEBUG is not set
+CONFIG_NETFILTER_ADVANCED=y
+# CONFIG_BRIDGE_NETFILTER is not set
+
+#
+# Core Netfilter Configuration
+#
+# CONFIG_NETFILTER_NETLINK_QUEUE is not set
+# CONFIG_NETFILTER_NETLINK_LOG is not set
+# CONFIG_NF_CONNTRACK is not set
+# CONFIG_NETFILTER_XTABLES is not set
+# CONFIG_IP_VS is not set
+
+#
+# IP: Netfilter Configuration
+#
+# CONFIG_NF_DEFRAG_IPV4 is not set
+# CONFIG_IP_NF_QUEUE is not set
+# CONFIG_IP_NF_IPTABLES is not set
+# CONFIG_IP_NF_ARPTABLES is not set
+# CONFIG_IP_DCCP is not set
+CONFIG_IP_SCTP=m
+# CONFIG_SCTP_DBG_MSG is not set
+# CONFIG_SCTP_DBG_OBJCNT is not set
+# CONFIG_SCTP_HMAC_NONE is not set
+# CONFIG_SCTP_HMAC_SHA1 is not set
+CONFIG_SCTP_HMAC_MD5=y
+# CONFIG_RDS is not set
+# CONFIG_TIPC is not set
+# CONFIG_ATM is not set
+# CONFIG_L2TP is not set
+CONFIG_STP=y
+CONFIG_BRIDGE=y
+# CONFIG_BRIDGE_IGMP_SNOOPING is not set
+# CONFIG_NET_DSA is not set
+# CONFIG_VLAN_8021Q is not set
+# CONFIG_DECNET is not set
+CONFIG_LLC=y
+# CONFIG_LLC2 is not set
+# CONFIG_IPX is not set
+# CONFIG_ATALK is not set
+# CONFIG_X25 is not set
+# CONFIG_LAPB is not set
+# CONFIG_ECONET is not set
+# CONFIG_WAN_ROUTER is not set
+# CONFIG_PHONET is not set
+# CONFIG_IEEE802154 is not set
+CONFIG_NET_SCHED=y
+
+#
+# Queueing/Scheduling
+#
+# CONFIG_NET_SCH_CBQ is not set
+CONFIG_NET_SCH_HTB=m
+CONFIG_NET_SCH_HFSC=m
+CONFIG_NET_SCH_PRIO=m
+# CONFIG_NET_SCH_MULTIQ is not set
+CONFIG_NET_SCH_RED=m
+# CONFIG_NET_SCH_SFB is not set
+CONFIG_NET_SCH_SFQ=m
+CONFIG_NET_SCH_TEQL=m
+CONFIG_NET_SCH_TBF=m
+CONFIG_NET_SCH_GRED=m
+CONFIG_NET_SCH_DSMARK=m
+# CONFIG_NET_SCH_NETEM is not set
+# CONFIG_NET_SCH_DRR is not set
+# CONFIG_NET_SCH_MQPRIO is not set
+# CONFIG_NET_SCH_CHOKE is not set
+# CONFIG_NET_SCH_QFQ is not set
+CONFIG_NET_SCH_INGRESS=m
+
+#
+# Classification
+#
+CONFIG_NET_CLS=y
+CONFIG_NET_CLS_BASIC=m
+CONFIG_NET_CLS_TCINDEX=m
+CONFIG_NET_CLS_ROUTE4=m
+CONFIG_NET_CLS_FW=m
+CONFIG_NET_CLS_U32=m
+# CONFIG_CLS_U32_PERF is not set
+# CONFIG_CLS_U32_MARK is not set
+# CONFIG_NET_CLS_RSVP is not set
+# CONFIG_NET_CLS_RSVP6 is not set
+CONFIG_NET_CLS_FLOW=m
+# CONFIG_NET_CLS_CGROUP is not set
+CONFIG_NET_EMATCH=y
+CONFIG_NET_EMATCH_STACK=32
+CONFIG_NET_EMATCH_CMP=m
+CONFIG_NET_EMATCH_NBYTE=m
+CONFIG_NET_EMATCH_U32=m
+CONFIG_NET_EMATCH_META=m
+CONFIG_NET_EMATCH_TEXT=m
+CONFIG_NET_CLS_ACT=y
+CONFIG_NET_ACT_POLICE=m
+# CONFIG_NET_ACT_GACT is not set
+CONFIG_NET_ACT_MIRRED=m
+# CONFIG_NET_ACT_NAT is not set
+# CONFIG_NET_ACT_PEDIT is not set
+# CONFIG_NET_ACT_SIMP is not set
+CONFIG_NET_ACT_SKBEDIT=m
+# CONFIG_NET_ACT_CSUM is not set
+# CONFIG_NET_CLS_IND is not set
+CONFIG_NET_SCH_FIFO=y
+# CONFIG_DCB is not set
+CONFIG_DNS_RESOLVER=y
+# CONFIG_BATMAN_ADV is not set
+CONFIG_RPS=y
+CONFIG_RFS_ACCEL=y
+CONFIG_XPS=y
+
+#
+# Network testing
+#
+CONFIG_NET_PKTGEN=m
+# CONFIG_HAMRADIO is not set
+# CONFIG_CAN is not set
+# CONFIG_IRDA is not set
+# CONFIG_BT is not set
+# CONFIG_AF_RXRPC is not set
+CONFIG_WIRELESS=y
+# CONFIG_CFG80211 is not set
+# CONFIG_LIB80211 is not set
+
+#
+# CFG80211 needs to be enabled for MAC80211
+#
+# CONFIG_WIMAX is not set
+# CONFIG_RFKILL is not set
+# CONFIG_NET_9P is not set
+# CONFIG_CAIF is not set
+# CONFIG_CEPH_LIB is not set
+
+#
+# Device Drivers
+#
+
+#
+# Generic Driver Options
+#
+CONFIG_UEVENT_HELPER_PATH="/sbin/hotplug"
+# CONFIG_DEVTMPFS is not set
+CONFIG_STANDALONE=y
+CONFIG_PREVENT_FIRMWARE_BUILD=y
+CONFIG_FW_LOADER=y
+CONFIG_FIRMWARE_IN_KERNEL=y
+CONFIG_EXTRA_FIRMWARE=""
+# CONFIG_DEBUG_DRIVER is not set
+# CONFIG_DEBUG_DEVRES is not set
+# CONFIG_SYS_HYPERVISOR is not set
+# CONFIG_CONNECTOR is not set
+CONFIG_MTD=y
+# CONFIG_MTD_DEBUG is not set
+# CONFIG_MTD_TESTS is not set
+# CONFIG_MTD_REDBOOT_PARTS is not set
+CONFIG_MTD_CMDLINE_PARTS=y
+# CONFIG_MTD_AFS_PARTS is not set
+# CONFIG_MTD_AR7_PARTS is not set
+
+#
+# User Modules And Translation Layers
+#
+CONFIG_MTD_CHAR=y
+CONFIG_MTD_BLKDEVS=y
+CONFIG_MTD_BLOCK=y
+# CONFIG_FTL is not set
+CONFIG_NFTL=y
+# CONFIG_NFTL_RW is not set
+# CONFIG_INFTL is not set
+# CONFIG_RFD_FTL is not set
+# CONFIG_SSFDC is not set
+# CONFIG_SM_FTL is not set
+# CONFIG_MTD_OOPS is not set
+
+#
+# RAM/ROM/Flash chip drivers
+#
+CONFIG_MTD_CFI=y
+CONFIG_MTD_JEDECPROBE=y
+CONFIG_MTD_GEN_PROBE=y
+CONFIG_MTD_CFI_ADV_OPTIONS=y
+CONFIG_MTD_CFI_NOSWAP=y
+# CONFIG_MTD_CFI_BE_BYTE_SWAP is not set
+# CONFIG_MTD_CFI_LE_BYTE_SWAP is not set
+CONFIG_MTD_CFI_GEOMETRY=y
+CONFIG_MTD_MAP_BANK_WIDTH_1=y
+CONFIG_MTD_MAP_BANK_WIDTH_2=y
+CONFIG_MTD_MAP_BANK_WIDTH_4=y
+# CONFIG_MTD_MAP_BANK_WIDTH_8 is not set
+# CONFIG_MTD_MAP_BANK_WIDTH_16 is not set
+# CONFIG_MTD_MAP_BANK_WIDTH_32 is not set
+CONFIG_MTD_CFI_I1=y
+CONFIG_MTD_CFI_I2=y
+# CONFIG_MTD_CFI_I4 is not set
+# CONFIG_MTD_CFI_I8 is not set
+# CONFIG_MTD_OTP is not set
+CONFIG_MTD_CFI_INTELEXT=y
+CONFIG_MTD_CFI_AMDSTD=y
+# CONFIG_MTD_CFI_STAA is not set
+CONFIG_MTD_CFI_UTIL=y
+CONFIG_MTD_RAM=y
+CONFIG_MTD_ROM=y
+# CONFIG_MTD_ABSENT is not set
+
+#
+# Mapping drivers for chip access
+#
+# CONFIG_MTD_COMPLEX_MAPPINGS is not set
+CONFIG_MTD_PHYSMAP=y
+# CONFIG_MTD_PHYSMAP_COMPAT is not set
+# CONFIG_MTD_ARM_INTEGRATOR is not set
+# CONFIG_MTD_IMPA7 is not set
+CONFIG_MTD_PLATRAM=y
+
+#
+# Self-contained MTD device drivers
+#
+# CONFIG_MTD_DATAFLASH is not set
+# CONFIG_MTD_M25P80 is not set
+# CONFIG_MTD_SST25L is not set
+# CONFIG_MTD_SLRAM is not set
+# CONFIG_MTD_PHRAM is not set
+# CONFIG_MTD_MTDRAM is not set
+# CONFIG_MTD_BLOCK2MTD is not set
+
+#
+# Disk-On-Chip Device Drivers
+#
+# CONFIG_MTD_DOC2000 is not set
+# CONFIG_MTD_DOC2001 is not set
+# CONFIG_MTD_DOC2001PLUS is not set
+CONFIG_MTD_NAND_ECC=y
+# CONFIG_MTD_NAND_ECC_SMC is not set
+CONFIG_MTD_NAND=y
+# CONFIG_MTD_NAND_VERIFY_WRITE is not set
+# CONFIG_MTD_NAND_ECC_BCH is not set
+# CONFIG_MTD_SM_COMMON is not set
+CONFIG_MTD_NAND_MUSEUM_IDS=y
+CONFIG_MTD_NAND_IDS=y
+# CONFIG_MTD_NAND_DISKONCHIP is not set
+# CONFIG_MTD_NAND_NANDSIM is not set
+CONFIG_MTD_NAND_PLATFORM=y
+# CONFIG_MTD_ONENAND is not set
+
+#
+# LPDDR flash memory drivers
+#
+# CONFIG_MTD_LPDDR is not set
+# CONFIG_MTD_UBI is not set
+# CONFIG_PARPORT is not set
+CONFIG_BLK_DEV=y
+# CONFIG_BLK_DEV_COW_COMMON is not set
+# CONFIG_BLK_DEV_LOOP is not set
+
+#
+# DRBD disabled because PROC_FS, INET or CONNECTOR not selected
+#
+# CONFIG_BLK_DEV_NBD is not set
+# CONFIG_BLK_DEV_RAM is not set
+# CONFIG_CDROM_PKTCDVD is not set
+# CONFIG_ATA_OVER_ETH is not set
+# CONFIG_BLK_DEV_RBD is not set
+# CONFIG_SENSORS_LIS3LV02D is not set
+CONFIG_MISC_DEVICES=y
+# CONFIG_AD525X_DPOT is not set
+# CONFIG_INTEL_MID_PTI is not set
+# CONFIG_ICS932S401 is not set
+# CONFIG_ENCLOSURE_SERVICES is not set
+# CONFIG_APDS9802ALS is not set
+# CONFIG_ISL29003 is not set
+# CONFIG_ISL29020 is not set
+# CONFIG_SENSORS_TSL2550 is not set
+# CONFIG_SENSORS_BH1780 is not set
+# CONFIG_SENSORS_BH1770 is not set
+# CONFIG_SENSORS_APDS990X is not set
+# CONFIG_HMC6352 is not set
+# CONFIG_DS1682 is not set
+# CONFIG_TI_DAC7512 is not set
+# CONFIG_BMP085 is not set
+# CONFIG_C2PORT is not set
+
+#
+# EEPROM support
+#
+CONFIG_EEPROM_AT24=y
+# CONFIG_EEPROM_AT25 is not set
+# CONFIG_EEPROM_LEGACY is not set
+# CONFIG_EEPROM_MAX6875 is not set
+# CONFIG_EEPROM_93CX6 is not set
+
+#
+# Texas Instruments shared transport line discipline
+#
+# CONFIG_SENSORS_LIS3_SPI is not set
+# CONFIG_SENSORS_LIS3_I2C is not set
+CONFIG_HAVE_IDE=y
+# CONFIG_IDE is not set
+
+#
+# SCSI device support
+#
+CONFIG_SCSI_MOD=y
+# CONFIG_RAID_ATTRS is not set
+# CONFIG_SCSI is not set
+# CONFIG_SCSI_DMA is not set
+# CONFIG_SCSI_NETLINK is not set
+# CONFIG_ATA is not set
+# CONFIG_MD is not set
+CONFIG_NETDEVICES=y
+# CONFIG_IFB is not set
+# CONFIG_DUMMY is not set
+# CONFIG_BONDING is not set
+# CONFIG_MACVLAN is not set
+# CONFIG_EQUALIZER is not set
+# CONFIG_TUN is not set
+# CONFIG_VETH is not set
+CONFIG_MII=y
+CONFIG_PHYLIB=y
+
+#
+# MII PHY device drivers
+#
+# CONFIG_MARVELL_PHY is not set
+# CONFIG_DAVICOM_PHY is not set
+# CONFIG_QSEMI_PHY is not set
+# CONFIG_LXT_PHY is not set
+# CONFIG_CICADA_PHY is not set
+# CONFIG_VITESSE_PHY is not set
+# CONFIG_SMSC_PHY is not set
+# CONFIG_BROADCOM_PHY is not set
+# CONFIG_ICPLUS_PHY is not set
+# CONFIG_REALTEK_PHY is not set
+# CONFIG_NATIONAL_PHY is not set
+# CONFIG_STE10XP is not set
+# CONFIG_LSI_ET1011C_PHY is not set
+# CONFIG_MICREL_PHY is not set
+# CONFIG_FIXED_PHY is not set
+CONFIG_MDIO_BITBANG=y
+CONFIG_NET_ETHERNET=y
+CONFIG_TRANSCEDE_ETH=y
+# CONFIG_TRANSCEDE_ETH_CPRI is not set
+CONFIG_TRANSCEDE_ETH_QOSCOM=y
+CONFIG_TRANSCEDE_4000_ETH_ADM_BLOCK=y
+# CONFIG_AX88796 is not set
+# CONFIG_SMC91X is not set
+# CONFIG_DM9000 is not set
+# CONFIG_ENC28J60 is not set
+# CONFIG_ETHOC is not set
+# CONFIG_SMC911X is not set
+# CONFIG_SMSC911X is not set
+# CONFIG_DNET is not set
+# CONFIG_IBM_NEW_EMAC_ZMII is not set
+# CONFIG_IBM_NEW_EMAC_RGMII is not set
+# CONFIG_IBM_NEW_EMAC_TAH is not set
+# CONFIG_IBM_NEW_EMAC_EMAC4 is not set
+# CONFIG_IBM_NEW_EMAC_NO_FLOW_CTRL is not set
+# CONFIG_IBM_NEW_EMAC_MAL_CLR_ICINTSTAT is not set
+# CONFIG_IBM_NEW_EMAC_MAL_COMMON_ERR is not set
+# CONFIG_B44 is not set
+# CONFIG_KS8851 is not set
+# CONFIG_KS8851_MLL is not set
+# CONFIG_FTMAC100 is not set
+CONFIG_NETDEV_1000=y
+# CONFIG_STMMAC_ETH is not set
+CONFIG_NETDEV_10000=y
+CONFIG_WLAN=y
+# CONFIG_HOSTAP is not set
+
+#
+# Enable WiMAX (Networking options) to see the WiMAX drivers
+#
+# CONFIG_WAN is not set
+
+#
+# CAIF transport drivers
+#
+# CONFIG_PPP is not set
+# CONFIG_SLIP is not set
+# CONFIG_NETPOLL is not set
+# CONFIG_NET_POLL_CONTROLLER is not set
+# CONFIG_ISDN is not set
+# CONFIG_PHONE is not set
+
+#
+# Input device support
+#
+CONFIG_INPUT=y
+# CONFIG_INPUT_FF_MEMLESS is not set
+# CONFIG_INPUT_POLLDEV is not set
+# CONFIG_INPUT_SPARSEKMAP is not set
+
+#
+# Userland interfaces
+#
+CONFIG_INPUT_MOUSEDEV=y
+# CONFIG_INPUT_MOUSEDEV_PSAUX is not set
+CONFIG_INPUT_MOUSEDEV_SCREEN_X=1024
+CONFIG_INPUT_MOUSEDEV_SCREEN_Y=768
+# CONFIG_INPUT_JOYDEV is not set
+# CONFIG_INPUT_EVDEV is not set
+# CONFIG_INPUT_EVBUG is not set
+
+#
+# Input Device Drivers
+#
+# CONFIG_INPUT_KEYBOARD is not set
+# CONFIG_INPUT_MOUSE is not set
+# CONFIG_INPUT_JOYSTICK is not set
+# CONFIG_INPUT_TABLET is not set
+# CONFIG_INPUT_TOUCHSCREEN is not set
+# CONFIG_INPUT_MISC is not set
+
+#
+# Hardware I/O ports
+#
+CONFIG_SERIO=y
+CONFIG_SERIO_SERPORT=y
+# CONFIG_SERIO_RAW is not set
+# CONFIG_SERIO_ALTERA_PS2 is not set
+# CONFIG_SERIO_PS2MULT is not set
+# CONFIG_GAMEPORT is not set
+
+#
+# Character devices
+#
+CONFIG_VT=y
+CONFIG_CONSOLE_TRANSLATIONS=y
+CONFIG_VT_CONSOLE=y
+CONFIG_HW_CONSOLE=y
+# CONFIG_VT_HW_CONSOLE_BINDING is not set
+CONFIG_UNIX98_PTYS=y
+# CONFIG_DEVPTS_MULTIPLE_INSTANCES is not set
+CONFIG_LEGACY_PTYS=y
+CONFIG_LEGACY_PTY_COUNT=16
+# CONFIG_SERIAL_NONSTANDARD is not set
+# CONFIG_N_GSM is not set
+# CONFIG_TRACE_SINK is not set
+CONFIG_DEVKMEM=y
+
+#
+# Serial drivers
+#
+CONFIG_SERIAL_8250=y
+CONFIG_SERIAL_8250_CONSOLE=y
+CONFIG_SERIAL_8250_NR_UARTS=1
+CONFIG_SERIAL_8250_RUNTIME_UARTS=1
+CONFIG_SERIAL_8250_EXTENDED=y
+# CONFIG_SERIAL_8250_MANY_PORTS is not set
+# CONFIG_SERIAL_8250_SHARE_IRQ is not set
+# CONFIG_SERIAL_8250_DETECT_IRQ is not set
+# CONFIG_SERIAL_8250_RSA is not set
+
+#
+# Non-8250 serial port support
+#
+# CONFIG_SERIAL_MAX3100 is not set
+# CONFIG_SERIAL_MAX3107 is not set
+CONFIG_SERIAL_CORE=y
+CONFIG_SERIAL_CORE_CONSOLE=y
+# CONFIG_SERIAL_TIMBERDALE is not set
+# CONFIG_SERIAL_ALTERA_JTAGUART is not set
+# CONFIG_SERIAL_ALTERA_UART is not set
+# CONFIG_SERIAL_XILINX_PS_UART is not set
+# CONFIG_HVC_DCC is not set
+# CONFIG_IPMI_HANDLER is not set
+CONFIG_HW_RANDOM=y
+# CONFIG_HW_RANDOM_TIMERIOMEM is not set
+# CONFIG_R3964 is not set
+# CONFIG_RAW_DRIVER is not set
+# CONFIG_TCG_TPM is not set
+# CONFIG_RAMOOPS is not set
+CONFIG_I2C=y
+CONFIG_I2C_BOARDINFO=y
+CONFIG_I2C_COMPAT=y
+CONFIG_I2C_CHARDEV=y
+# CONFIG_I2C_MUX is not set
+CONFIG_I2C_HELPER_AUTO=y
+
+#
+# I2C Hardware Bus support
+#
+
+#
+# I2C system bus drivers (mostly embedded / system-on-chip)
+#
+# CONFIG_I2C_OCORES is not set
+# CONFIG_I2C_PCA_PLATFORM is not set
+# CONFIG_I2C_PXA_PCI is not set
+# CONFIG_I2C_SIMTEC is not set
+CONFIG_I2C_TRANSCEDE=y
+# CONFIG_I2C_XILINX is not set
+
+#
+# External I2C/SMBus adapter drivers
+#
+# CONFIG_I2C_PARPORT_LIGHT is not set
+# CONFIG_I2C_TAOS_EVM is not set
+
+#
+# Other I2C/SMBus bus drivers
+#
+# CONFIG_I2C_STUB is not set
+# CONFIG_I2C_DEBUG_CORE is not set
+# CONFIG_I2C_DEBUG_ALGO is not set
+# CONFIG_I2C_DEBUG_BUS is not set
+CONFIG_SPI=y
+# CONFIG_SPI_DEBUG is not set
+CONFIG_SPI_MASTER=y
+
+#
+# SPI Master Controller Drivers
+#
+# CONFIG_SPI_ALTERA is not set
+# CONFIG_SPI_BITBANG is not set
+# CONFIG_SPI_PXA2XX_PCI is not set
+CONFIG_SPI_TRANSCEDE=y
+CONFIG_SPI_TRANSCEDE_CS_GPIO=y
+# CONFIG_SPI_XILINX is not set
+# CONFIG_SPI_DESIGNWARE is not set
+
+#
+# SPI Protocol Masters
+#
+CONFIG_SPI_SPIDEV=y
+# CONFIG_SPI_TLE62X0 is not set
+# CONFIG_SPI_CDCE62005 is not set
+
+#
+# PPS support
+#
+# CONFIG_PPS is not set
+
+#
+# PPS generators support
+#
+
+#
+# PTP clock support
+#
+
+#
+# Enable Device Drivers -> PPS to see the PTP clock options.
+#
+# CONFIG_W1 is not set
+# CONFIG_POWER_SUPPLY is not set
+# CONFIG_HWMON is not set
+# CONFIG_THERMAL is not set
+CONFIG_WATCHDOG=y
+# CONFIG_WATCHDOG_NOWAYOUT is not set
+
+#
+# Watchdog Device Drivers
+#
+# CONFIG_SOFT_WATCHDOG is not set
+CONFIG_MPCORE_WATCHDOG=y
+# CONFIG_MAX63XX_WATCHDOG is not set
+CONFIG_SSB_POSSIBLE=y
+
+#
+# Sonics Silicon Backplane
+#
+# CONFIG_SSB is not set
+CONFIG_BCMA_POSSIBLE=y
+
+#
+# Broadcom specific AMBA
+#
+# CONFIG_BCMA is not set
+CONFIG_MFD_SUPPORT=y
+# CONFIG_MFD_CORE is not set
+# CONFIG_MFD_88PM860X is not set
+# CONFIG_MFD_SM501 is not set
+# CONFIG_HTC_PASIC3 is not set
+# CONFIG_TPS6105X is not set
+# CONFIG_TPS6507X is not set
+# CONFIG_TWL4030_CORE is not set
+# CONFIG_MFD_STMPE is not set
+# CONFIG_MFD_TC3589X is not set
+# CONFIG_MFD_TMIO is not set
+# CONFIG_PMIC_DA903X is not set
+# CONFIG_PMIC_ADP5520 is not set
+# CONFIG_MFD_MAX8925 is not set
+# CONFIG_MFD_MAX8997 is not set
+# CONFIG_MFD_MAX8998 is not set
+# CONFIG_MFD_WM8400 is not set
+# CONFIG_MFD_WM831X_I2C is not set
+# CONFIG_MFD_WM831X_SPI is not set
+# CONFIG_MFD_WM8350_I2C is not set
+# CONFIG_MFD_WM8994 is not set
+# CONFIG_MFD_PCF50633 is not set
+# CONFIG_MFD_MC13XXX is not set
+# CONFIG_ABX500_CORE is not set
+# CONFIG_EZX_PCAP is not set
+# CONFIG_MFD_WL1273_CORE is not set
+# CONFIG_REGULATOR is not set
+# CONFIG_MEDIA_SUPPORT is not set
+
+#
+# Graphics support
+#
+# CONFIG_DRM is not set
+# CONFIG_VGASTATE is not set
+# CONFIG_VIDEO_OUTPUT_CONTROL is not set
+# CONFIG_FB is not set
+# CONFIG_BACKLIGHT_LCD_SUPPORT is not set
+
+#
+# Display device support
+#
+# CONFIG_DISPLAY_SUPPORT is not set
+
+#
+# Console display driver support
+#
+CONFIG_DUMMY_CONSOLE=y
+# CONFIG_SOUND is not set
+CONFIG_HID_SUPPORT=y
+CONFIG_HID=y
+# CONFIG_HIDRAW is not set
+# CONFIG_HID_PID is not set
+
+#
+# Special HID drivers
+#
+CONFIG_USB_SUPPORT=y
+CONFIG_USB_ARCH_HAS_HCD=y
+# CONFIG_USB_ARCH_HAS_OHCI is not set
+# CONFIG_USB_ARCH_HAS_EHCI is not set
+# CONFIG_USB is not set
+
+#
+# Enable Host or Gadget support to see Inventra options
+#
+
+#
+# NOTE: USB_STORAGE depends on SCSI but BLK_DEV_SD may
+#
+# CONFIG_USB_GADGET is not set
+
+#
+# OTG and related infrastructure
+#
+# CONFIG_DWC_OTG is not set
+# CONFIG_MMC is not set
+# CONFIG_MEMSTICK is not set
+# CONFIG_NEW_LEDS is not set
+# CONFIG_NFC_DEVICES is not set
+# CONFIG_ACCESSIBILITY is not set
+CONFIG_RTC_LIB=y
+# CONFIG_RTC_CLASS is not set
+# CONFIG_DMADEVICES is not set
+# CONFIG_AUXDISPLAY is not set
+CONFIG_UIO=y
+CONFIG_UIO_PDRV=y
+# CONFIG_UIO_PDRV_GENIRQ is not set
+# CONFIG_STAGING is not set
+
+#
+# File systems
+#
+# CONFIG_EXT2_FS is not set
+# CONFIG_EXT3_FS is not set
+# CONFIG_EXT4_FS is not set
+# CONFIG_REISERFS_FS is not set
+# CONFIG_JFS_FS is not set
+# CONFIG_XFS_FS is not set
+# CONFIG_GFS2_FS is not set
+# CONFIG_BTRFS_FS is not set
+# CONFIG_NILFS2_FS is not set
+CONFIG_FS_POSIX_ACL=y
+CONFIG_FILE_LOCKING=y
+CONFIG_FSNOTIFY=y
+CONFIG_DNOTIFY=y
+CONFIG_INOTIFY_USER=y
+# CONFIG_FANOTIFY is not set
+# CONFIG_QUOTA is not set
+# CONFIG_QUOTACTL is not set
+CONFIG_AUTOFS4_FS=y
+# CONFIG_FUSE_FS is not set
+
+#
+# Caches
+#
+# CONFIG_FSCACHE is not set
+
+#
+# CD-ROM/DVD Filesystems
+#
+# CONFIG_ISO9660_FS is not set
+# CONFIG_UDF_FS is not set
+
+#
+# DOS/FAT/NT Filesystems
+#
+# CONFIG_MSDOS_FS is not set
+# CONFIG_VFAT_FS is not set
+# CONFIG_NTFS_FS is not set
+
+#
+# Pseudo filesystems
+#
+CONFIG_PROC_FS=y
+CONFIG_PROC_SYSCTL=y
+CONFIG_PROC_PAGE_MONITOR=y
+CONFIG_SYSFS=y
+CONFIG_TMPFS=y
+# CONFIG_TMPFS_POSIX_ACL is not set
+# CONFIG_TMPFS_XATTR is not set
+# CONFIG_HUGETLB_PAGE is not set
+# CONFIG_CONFIGFS_FS is not set
+CONFIG_MISC_FILESYSTEMS=y
+# CONFIG_ADFS_FS is not set
+# CONFIG_AFFS_FS is not set
+# CONFIG_ECRYPT_FS is not set
+# CONFIG_HFS_FS is not set
+# CONFIG_HFSPLUS_FS is not set
+# CONFIG_BEFS_FS is not set
+# CONFIG_BFS_FS is not set
+# CONFIG_EFS_FS is not set
+CONFIG_JFFS2_FS=y
+CONFIG_JFFS2_FS_DEBUG=0
+CONFIG_JFFS2_FS_WRITEBUFFER=y
+# CONFIG_JFFS2_FS_WBUF_VERIFY is not set
+# CONFIG_JFFS2_SUMMARY is not set
+# CONFIG_JFFS2_FS_XATTR is not set
+# CONFIG_JFFS2_COMPRESSION_OPTIONS is not set
+CONFIG_JFFS2_ZLIB=y
+# CONFIG_JFFS2_LZO is not set
+CONFIG_JFFS2_RTIME=y
+# CONFIG_JFFS2_RUBIN is not set
+# CONFIG_LOGFS is not set
+# CONFIG_CRAMFS is not set
+# CONFIG_SQUASHFS is not set
+# CONFIG_VXFS_FS is not set
+# CONFIG_MINIX_FS is not set
+# CONFIG_OMFS_FS is not set
+# CONFIG_HPFS_FS is not set
+# CONFIG_QNX4FS_FS is not set
+# CONFIG_ROMFS_FS is not set
+# CONFIG_PSTORE is not set
+# CONFIG_SYSV_FS is not set
+# CONFIG_UFS_FS is not set
+CONFIG_NETWORK_FILESYSTEMS=y
+CONFIG_NFS_FS=y
+CONFIG_NFS_V3=y
+CONFIG_NFS_V3_ACL=y
+CONFIG_NFS_V4=y
+# CONFIG_NFS_V4_1 is not set
+CONFIG_ROOT_NFS=y
+# CONFIG_NFS_USE_LEGACY_DNS is not set
+CONFIG_NFS_USE_KERNEL_DNS=y
+# CONFIG_NFS_USE_NEW_IDMAPPER is not set
+# CONFIG_NFSD is not set
+CONFIG_LOCKD=y
+CONFIG_LOCKD_V4=y
+CONFIG_NFS_ACL_SUPPORT=y
+CONFIG_NFS_COMMON=y
+CONFIG_SUNRPC=y
+CONFIG_SUNRPC_GSS=y
+# CONFIG_CEPH_FS is not set
+# CONFIG_CIFS is not set
+# CONFIG_NCP_FS is not set
+# CONFIG_CODA_FS is not set
+# CONFIG_AFS_FS is not set
+
+#
+# Partition Types
+#
+# CONFIG_PARTITION_ADVANCED is not set
+CONFIG_MSDOS_PARTITION=y
+# CONFIG_NLS is not set
+
+#
+# Kernel hacking
+#
+# CONFIG_PRINTK_TIME is not set
+CONFIG_DEFAULT_MESSAGE_LOGLEVEL=4
+CONFIG_ENABLE_WARN_DEPRECATED=y
+CONFIG_ENABLE_MUST_CHECK=y
+CONFIG_FRAME_WARN=1024
+# CONFIG_MAGIC_SYSRQ is not set
+# CONFIG_STRIP_ASM_SYMS is not set
+# CONFIG_UNUSED_SYMBOLS is not set
+CONFIG_DEBUG_FS=y
+# CONFIG_HEADERS_CHECK is not set
+# CONFIG_DEBUG_SECTION_MISMATCH is not set
+CONFIG_DEBUG_KERNEL=y
+# CONFIG_LOCKUP_DETECTOR is not set
+# CONFIG_HARDLOCKUP_DETECTOR is not set
+CONFIG_DETECT_HUNG_TASK=y
+CONFIG_DEFAULT_HUNG_TASK_TIMEOUT=120
+# CONFIG_BOOTPARAM_HUNG_TASK_PANIC is not set
+CONFIG_BOOTPARAM_HUNG_TASK_PANIC_VALUE=0
+# CONFIG_SCHED_DEBUG is not set
+# CONFIG_SCHEDSTATS is not set
+# CONFIG_TIMER_STATS is not set
+# CONFIG_DEBUG_OBJECTS is not set
+# CONFIG_DEBUG_SLAB is not set
+# CONFIG_DEBUG_KMEMLEAK is not set
+# CONFIG_DEBUG_PREEMPT is not set
+# CONFIG_DEBUG_RT_MUTEXES is not set
+# CONFIG_RT_MUTEX_TESTER is not set
+# CONFIG_DEBUG_SPINLOCK is not set
+# CONFIG_DEBUG_MUTEXES is not set
+# CONFIG_DEBUG_LOCK_ALLOC is not set
+# CONFIG_PROVE_LOCKING is not set
+# CONFIG_SPARSE_RCU_POINTER is not set
+# CONFIG_LOCK_STAT is not set
+# CONFIG_DEBUG_SPINLOCK_SLEEP is not set
+# CONFIG_DEBUG_LOCKING_API_SELFTESTS is not set
+# CONFIG_DEBUG_STACK_USAGE is not set
+# CONFIG_DEBUG_KOBJECT is not set
+CONFIG_DEBUG_BUGVERBOSE=y
+# CONFIG_DEBUG_INFO is not set
+# CONFIG_DEBUG_VM is not set
+# CONFIG_DEBUG_WRITECOUNT is not set
+CONFIG_DEBUG_MEMORY_INIT=y
+# CONFIG_DEBUG_LIST is not set
+# CONFIG_TEST_LIST_SORT is not set
+# CONFIG_DEBUG_SG is not set
+# CONFIG_DEBUG_NOTIFIERS is not set
+# CONFIG_DEBUG_CREDENTIALS is not set
+# CONFIG_BOOT_PRINTK_DELAY is not set
+# CONFIG_RCU_TORTURE_TEST is not set
+CONFIG_RCU_CPU_STALL_TIMEOUT=60
+CONFIG_RCU_CPU_STALL_VERBOSE=y
+# CONFIG_BACKTRACE_SELF_TEST is not set
+# CONFIG_DEBUG_BLOCK_EXT_DEVT is not set
+# CONFIG_DEBUG_FORCE_WEAK_PER_CPU is not set
+# CONFIG_DEBUG_PER_CPU_MAPS is not set
+# CONFIG_LKDTM is not set
+# CONFIG_FAULT_INJECTION is not set
+# CONFIG_SYSCTL_SYSCALL_CHECK is not set
+# CONFIG_DEBUG_PAGEALLOC is not set
+CONFIG_HAVE_FUNCTION_TRACER=y
+CONFIG_HAVE_FUNCTION_GRAPH_TRACER=y
+CONFIG_HAVE_DYNAMIC_FTRACE=y
+CONFIG_HAVE_FTRACE_MCOUNT_RECORD=y
+CONFIG_HAVE_C_RECORDMCOUNT=y
+CONFIG_TRACING_SUPPORT=y
+CONFIG_FTRACE=y
+# CONFIG_FUNCTION_TRACER is not set
+# CONFIG_IRQSOFF_TRACER is not set
+# CONFIG_PREEMPT_TRACER is not set
+# CONFIG_SCHED_TRACER is not set
+# CONFIG_MISSED_TIMER_OFFSETS_HIST is not set
+# CONFIG_ENABLE_DEFAULT_TRACERS is not set
+CONFIG_BRANCH_PROFILE_NONE=y
+# CONFIG_PROFILE_ANNOTATED_BRANCHES is not set
+# CONFIG_PROFILE_ALL_BRANCHES is not set
+# CONFIG_STACK_TRACER is not set
+# CONFIG_BLK_DEV_IO_TRACE is not set
+# CONFIG_DYNAMIC_DEBUG is not set
+# CONFIG_DMA_API_DEBUG is not set
+# CONFIG_ATOMIC64_SELFTEST is not set
+# CONFIG_SAMPLES is not set
+CONFIG_HAVE_ARCH_KGDB=y
+# CONFIG_KGDB is not set
+# CONFIG_TEST_KSTRTOX is not set
+# CONFIG_STRICT_DEVMEM is not set
+CONFIG_ARM_UNWIND=y
+CONFIG_DEBUG_USER=y
+CONFIG_DEBUG_LL=y
+CONFIG_EARLY_PRINTK=y
+# CONFIG_DEBUG_ICEDCC is not set
+# CONFIG_OC_ETM is not set
+
+#
+# Security options
+#
+CONFIG_KEYS=y
+# CONFIG_KEYS_DEBUG_PROC_KEYS is not set
+# CONFIG_SECURITY_DMESG_RESTRICT is not set
+# CONFIG_SECURITY is not set
+# CONFIG_SECURITYFS is not set
+CONFIG_DEFAULT_SECURITY_DAC=y
+CONFIG_DEFAULT_SECURITY=""
+CONFIG_CRYPTO=y
+
+#
+# Crypto core or helper
+#
+CONFIG_CRYPTO_ALGAPI=y
+CONFIG_CRYPTO_ALGAPI2=y
+CONFIG_CRYPTO_AEAD=y
+CONFIG_CRYPTO_AEAD2=y
+CONFIG_CRYPTO_BLKCIPHER=y
+CONFIG_CRYPTO_BLKCIPHER2=y
+CONFIG_CRYPTO_HASH=y
+CONFIG_CRYPTO_HASH2=y
+CONFIG_CRYPTO_RNG2=y
+CONFIG_CRYPTO_PCOMP2=y
+CONFIG_CRYPTO_MANAGER=y
+CONFIG_CRYPTO_MANAGER2=y
+CONFIG_CRYPTO_MANAGER_DISABLE_TESTS=y
+# CONFIG_CRYPTO_GF128MUL is not set
+CONFIG_CRYPTO_NULL=y
+# CONFIG_CRYPTO_PCRYPT is not set
+CONFIG_CRYPTO_WORKQUEUE=y
+# CONFIG_CRYPTO_CRYPTD is not set
+CONFIG_CRYPTO_AUTHENC=y
+# CONFIG_CRYPTO_TEST is not set
+
+#
+# Authenticated Encryption with Associated Data
+#
+# CONFIG_CRYPTO_CCM is not set
+# CONFIG_CRYPTO_GCM is not set
+# CONFIG_CRYPTO_SEQIV is not set
+
+#
+# Block modes
+#
+CONFIG_CRYPTO_CBC=y
+# CONFIG_CRYPTO_CTR is not set
+# CONFIG_CRYPTO_CTS is not set
+# CONFIG_CRYPTO_ECB is not set
+# CONFIG_CRYPTO_LRW is not set
+# CONFIG_CRYPTO_PCBC is not set
+# CONFIG_CRYPTO_XTS is not set
+
+#
+# Hash modes
+#
+CONFIG_CRYPTO_HMAC=y
+# CONFIG_CRYPTO_XCBC is not set
+# CONFIG_CRYPTO_VMAC is not set
+
+#
+# Digest
+#
+CONFIG_CRYPTO_CRC32C=y
+# CONFIG_CRYPTO_GHASH is not set
+CONFIG_CRYPTO_MD4=m
+CONFIG_CRYPTO_MD5=y
+# CONFIG_CRYPTO_MICHAEL_MIC is not set
+# CONFIG_CRYPTO_RMD128 is not set
+# CONFIG_CRYPTO_RMD160 is not set
+# CONFIG_CRYPTO_RMD256 is not set
+# CONFIG_CRYPTO_RMD320 is not set
+CONFIG_CRYPTO_SHA1=y
+CONFIG_CRYPTO_SHA256=y
+CONFIG_CRYPTO_SHA512=y
+CONFIG_CRYPTO_TGR192=m
+CONFIG_CRYPTO_WP512=m
+
+#
+# Ciphers
+#
+CONFIG_CRYPTO_AES=y
+CONFIG_CRYPTO_ANUBIS=m
+# CONFIG_CRYPTO_ARC4 is not set
+CONFIG_CRYPTO_BLOWFISH=m
+CONFIG_CRYPTO_CAMELLIA=m
+CONFIG_CRYPTO_CAST5=m
+CONFIG_CRYPTO_CAST6=m
+CONFIG_CRYPTO_DES=y
+CONFIG_CRYPTO_FCRYPT=m
+CONFIG_CRYPTO_KHAZAD=m
+# CONFIG_CRYPTO_SALSA20 is not set
+# CONFIG_CRYPTO_SEED is not set
+CONFIG_CRYPTO_SERPENT=m
+CONFIG_CRYPTO_TEA=m
+CONFIG_CRYPTO_TWOFISH=m
+CONFIG_CRYPTO_TWOFISH_COMMON=m
+
+#
+# Compression
+#
+CONFIG_CRYPTO_DEFLATE=y
+# CONFIG_CRYPTO_ZLIB is not set
+# CONFIG_CRYPTO_LZO is not set
+
+#
+# Random Number Generation
+#
+# CONFIG_CRYPTO_ANSI_CPRNG is not set
+# CONFIG_CRYPTO_USER_API_HASH is not set
+# CONFIG_CRYPTO_USER_API_SKCIPHER is not set
+CONFIG_CRYPTO_HW=y
+CONFIG_CRYPTO_DEV_TRANSCEDE=m
+
+#
+# OCF Configuration
+#
+# CONFIG_OCF_OCF is not set
+# CONFIG_BINARY_PRINTF is not set
+
+#
+# Library routines
+#
+CONFIG_BITREVERSE=y
+CONFIG_CRC_CCITT=m
+CONFIG_CRC16=y
+# CONFIG_CRC_T10DIF is not set
+# CONFIG_CRC_ITU_T is not set
+CONFIG_CRC32=y
+# CONFIG_CRC7 is not set
+CONFIG_LIBCRC32C=y
+CONFIG_ZLIB_INFLATE=y
+CONFIG_ZLIB_DEFLATE=y
+# CONFIG_XZ_DEC is not set
+# CONFIG_XZ_DEC_BCJ is not set
+CONFIG_TEXTSEARCH=y
+CONFIG_TEXTSEARCH_KMP=m
+CONFIG_TEXTSEARCH_BM=m
+CONFIG_TEXTSEARCH_FSM=m
+CONFIG_HAS_IOMEM=y
+CONFIG_HAS_IOPORT=y
+CONFIG_HAS_DMA=y
+CONFIG_CPU_RMAP=y
+CONFIG_NLATTR=y
+# CONFIG_AVERAGE is not set
diff --git a/arch/arm/configs/t4000_x4_defconfig b/arch/arm/configs/t4000_x4_defconfig
new file mode 100644
index 0000000..81f9210
--- /dev/null
+++ b/arch/arm/configs/t4000_x4_defconfig
@@ -0,0 +1,1505 @@
+#
+# Automatically generated make config: don't edit
+# Linux/arm 3.0.51 Kernel Configuration
+#
+CONFIG_ARM=y
+CONFIG_SYS_SUPPORTS_APM_EMULATION=y
+CONFIG_HAVE_SCHED_CLOCK=y
+# CONFIG_ARCH_USES_GETTIMEOFFSET is not set
+CONFIG_GENERIC_CLOCKEVENTS=y
+CONFIG_GENERIC_CLOCKEVENTS_BROADCAST=y
+CONFIG_KTIME_SCALAR=y
+CONFIG_HAVE_PROC_CPU=y
+CONFIG_STACKTRACE_SUPPORT=y
+CONFIG_LOCKDEP_SUPPORT=y
+CONFIG_TRACE_IRQFLAGS_SUPPORT=y
+CONFIG_HARDIRQS_SW_RESEND=y
+CONFIG_GENERIC_IRQ_PROBE=y
+CONFIG_GENERIC_LOCKBREAK=y
+CONFIG_RWSEM_GENERIC_SPINLOCK=y
+CONFIG_ARCH_HAS_CPU_IDLE_WAIT=y
+CONFIG_GENERIC_HWEIGHT=y
+CONFIG_GENERIC_CALIBRATE_DELAY=y
+CONFIG_NEED_DMA_MAP_STATE=y
+CONFIG_VECTORS_BASE=0xffff0000
+# CONFIG_ARM_PATCH_PHYS_VIRT is not set
+CONFIG_DEFCONFIG_LIST="/lib/modules/$UNAME_RELEASE/.config"
+CONFIG_HAVE_IRQ_WORK=y
+CONFIG_IRQ_WORK=y
+
+#
+# General setup
+#
+CONFIG_EXPERIMENTAL=y
+CONFIG_INIT_ENV_ARG_LIMIT=32
+CONFIG_CROSS_COMPILE=""
+CONFIG_LOCALVERSION=""
+# CONFIG_LOCALVERSION_AUTO is not set
+CONFIG_HAVE_KERNEL_GZIP=y
+CONFIG_HAVE_KERNEL_LZMA=y
+CONFIG_HAVE_KERNEL_LZO=y
+CONFIG_KERNEL_GZIP=y
+# CONFIG_KERNEL_LZMA is not set
+# CONFIG_KERNEL_LZO is not set
+CONFIG_DEFAULT_HOSTNAME="(none)"
+# CONFIG_SWAP is not set
+CONFIG_SYSVIPC=y
+CONFIG_SYSVIPC_SYSCTL=y
+CONFIG_POSIX_MQUEUE=y
+CONFIG_POSIX_MQUEUE_SYSCTL=y
+# CONFIG_BSD_PROCESS_ACCT is not set
+# CONFIG_FHANDLE is not set
+# CONFIG_TASKSTATS is not set
+# CONFIG_AUDIT is not set
+CONFIG_HAVE_GENERIC_HARDIRQS=y
+
+#
+# IRQ subsystem
+#
+CONFIG_GENERIC_HARDIRQS=y
+CONFIG_HAVE_SPARSE_IRQ=y
+CONFIG_GENERIC_IRQ_SHOW=y
+CONFIG_IRQ_FORCED_THREADING=y
+# CONFIG_SPARSE_IRQ is not set
+
+#
+# RCU Subsystem
+#
+CONFIG_TREE_PREEMPT_RCU=y
+CONFIG_PREEMPT_RCU=y
+# CONFIG_RCU_TRACE is not set
+CONFIG_RCU_FANOUT=32
+# CONFIG_RCU_FANOUT_EXACT is not set
+# CONFIG_TREE_RCU_TRACE is not set
+# CONFIG_RCU_BOOST is not set
+CONFIG_IKCONFIG=y
+CONFIG_IKCONFIG_PROC=y
+CONFIG_LOG_BUF_SHIFT=14
+CONFIG_CGROUPS=y
+# CONFIG_CGROUP_DEBUG is not set
+# CONFIG_CGROUP_FREEZER is not set
+# CONFIG_CGROUP_DEVICE is not set
+# CONFIG_CPUSETS is not set
+# CONFIG_CGROUP_CPUACCT is not set
+# CONFIG_RESOURCE_COUNTERS is not set
+# CONFIG_CGROUP_PERF is not set
+# CONFIG_CGROUP_SCHED is not set
+# CONFIG_BLK_CGROUP is not set
+CONFIG_NAMESPACES=y
+# CONFIG_UTS_NS is not set
+# CONFIG_IPC_NS is not set
+# CONFIG_USER_NS is not set
+# CONFIG_PID_NS is not set
+# CONFIG_NET_NS is not set
+# CONFIG_SCHED_AUTOGROUP is not set
+# CONFIG_SYSFS_DEPRECATED is not set
+# CONFIG_RELAY is not set
+# CONFIG_BLK_DEV_INITRD is not set
+CONFIG_CC_OPTIMIZE_FOR_SIZE=y
+CONFIG_SYSCTL=y
+CONFIG_ANON_INODES=y
+# CONFIG_EXPERT is not set
+CONFIG_UID16=y
+CONFIG_SYSCTL_SYSCALL=y
+CONFIG_KALLSYMS=y
+# CONFIG_KALLSYMS_ALL is not set
+CONFIG_HOTPLUG=y
+CONFIG_PRINTK=y
+CONFIG_BUG=y
+CONFIG_ELF_CORE=y
+CONFIG_BASE_FULL=y
+CONFIG_FUTEX=y
+CONFIG_EPOLL=y
+CONFIG_SIGNALFD=y
+CONFIG_TIMERFD=y
+CONFIG_EVENTFD=y
+CONFIG_SHMEM=y
+CONFIG_AIO=y
+# CONFIG_EMBEDDED is not set
+CONFIG_HAVE_PERF_EVENTS=y
+CONFIG_PERF_USE_VMALLOC=y
+
+#
+# Kernel Performance Events And Counters
+#
+CONFIG_PERF_EVENTS=y
+CONFIG_PERF_COUNTERS=y
+# CONFIG_DEBUG_PERF_USE_VMALLOC is not set
+CONFIG_VM_EVENT_COUNTERS=y
+CONFIG_COMPAT_BRK=y
+CONFIG_SLAB=y
+# CONFIG_PROFILING is not set
+CONFIG_HAVE_OPROFILE=y
+# CONFIG_KPROBES is not set
+CONFIG_HAVE_KPROBES=y
+CONFIG_HAVE_KRETPROBES=y
+CONFIG_USE_GENERIC_SMP_HELPERS=y
+CONFIG_HAVE_REGS_AND_STACK_ACCESS_API=y
+CONFIG_HAVE_DMA_API_DEBUG=y
+CONFIG_HAVE_HW_BREAKPOINT=y
+
+#
+# GCOV-based kernel profiling
+#
+# CONFIG_GCOV_KERNEL is not set
+CONFIG_HAVE_GENERIC_DMA_COHERENT=y
+CONFIG_SLABINFO=y
+CONFIG_RT_MUTEXES=y
+CONFIG_BASE_SMALL=0
+CONFIG_MODULES=y
+# CONFIG_MODULE_FORCE_LOAD is not set
+CONFIG_MODULE_UNLOAD=y
+# CONFIG_MODULE_FORCE_UNLOAD is not set
+# CONFIG_MODVERSIONS is not set
+# CONFIG_MODULE_SRCVERSION_ALL is not set
+CONFIG_STOP_MACHINE=y
+CONFIG_BLOCK=y
+CONFIG_LBDAF=y
+# CONFIG_BLK_DEV_BSG is not set
+# CONFIG_BLK_DEV_INTEGRITY is not set
+
+#
+# IO Schedulers
+#
+CONFIG_IOSCHED_NOOP=y
+CONFIG_IOSCHED_DEADLINE=y
+# CONFIG_IOSCHED_CFQ is not set
+CONFIG_DEFAULT_DEADLINE=y
+# CONFIG_DEFAULT_NOOP is not set
+CONFIG_DEFAULT_IOSCHED="deadline"
+# CONFIG_INLINE_SPIN_TRYLOCK is not set
+# CONFIG_INLINE_SPIN_TRYLOCK_BH is not set
+# CONFIG_INLINE_SPIN_LOCK is not set
+# CONFIG_INLINE_SPIN_LOCK_BH is not set
+# CONFIG_INLINE_SPIN_LOCK_IRQ is not set
+# CONFIG_INLINE_SPIN_LOCK_IRQSAVE is not set
+# CONFIG_INLINE_SPIN_UNLOCK is not set
+# CONFIG_INLINE_SPIN_UNLOCK_BH is not set
+# CONFIG_INLINE_SPIN_UNLOCK_IRQ is not set
+# CONFIG_INLINE_SPIN_UNLOCK_IRQRESTORE is not set
+# CONFIG_INLINE_READ_TRYLOCK is not set
+# CONFIG_INLINE_READ_LOCK is not set
+# CONFIG_INLINE_READ_LOCK_BH is not set
+# CONFIG_INLINE_READ_LOCK_IRQ is not set
+# CONFIG_INLINE_READ_LOCK_IRQSAVE is not set
+# CONFIG_INLINE_READ_UNLOCK is not set
+# CONFIG_INLINE_READ_UNLOCK_BH is not set
+# CONFIG_INLINE_READ_UNLOCK_IRQ is not set
+# CONFIG_INLINE_READ_UNLOCK_IRQRESTORE is not set
+# CONFIG_INLINE_WRITE_TRYLOCK is not set
+# CONFIG_INLINE_WRITE_LOCK is not set
+# CONFIG_INLINE_WRITE_LOCK_BH is not set
+# CONFIG_INLINE_WRITE_LOCK_IRQ is not set
+# CONFIG_INLINE_WRITE_LOCK_IRQSAVE is not set
+# CONFIG_INLINE_WRITE_UNLOCK is not set
+# CONFIG_INLINE_WRITE_UNLOCK_BH is not set
+# CONFIG_INLINE_WRITE_UNLOCK_IRQ is not set
+# CONFIG_INLINE_WRITE_UNLOCK_IRQRESTORE is not set
+# CONFIG_MUTEX_SPIN_ON_OWNER is not set
+# CONFIG_FREEZER is not set
+
+#
+# System Type
+#
+CONFIG_MMU=y
+# CONFIG_ARCH_INTEGRATOR is not set
+# CONFIG_ARCH_REALVIEW is not set
+# CONFIG_ARCH_VERSATILE is not set
+# CONFIG_ARCH_VEXPRESS is not set
+# CONFIG_ARCH_AT91 is not set
+# CONFIG_ARCH_BCMRING is not set
+# CONFIG_ARCH_CLPS711X is not set
+# CONFIG_ARCH_CNS3XXX is not set
+# CONFIG_ARCH_GEMINI is not set
+# CONFIG_ARCH_EBSA110 is not set
+# CONFIG_ARCH_EP93XX is not set
+# CONFIG_ARCH_FOOTBRIDGE is not set
+# CONFIG_ARCH_MXC is not set
+# CONFIG_ARCH_MXS is not set
+# CONFIG_ARCH_NETX is not set
+# CONFIG_ARCH_H720X is not set
+# CONFIG_ARCH_IOP13XX is not set
+# CONFIG_ARCH_IOP32X is not set
+# CONFIG_ARCH_IOP33X is not set
+# CONFIG_ARCH_IXP23XX is not set
+# CONFIG_ARCH_IXP2000 is not set
+# CONFIG_ARCH_IXP4XX is not set
+# CONFIG_ARCH_DOVE is not set
+# CONFIG_ARCH_KIRKWOOD is not set
+# CONFIG_ARCH_LOKI is not set
+# CONFIG_ARCH_LPC32XX is not set
+# CONFIG_ARCH_MV78XX0 is not set
+# CONFIG_ARCH_ORION5X is not set
+# CONFIG_ARCH_MMP is not set
+# CONFIG_ARCH_KS8695 is not set
+# CONFIG_ARCH_W90X900 is not set
+# CONFIG_ARCH_NUC93X is not set
+# CONFIG_ARCH_TEGRA is not set
+# CONFIG_ARCH_PNX4008 is not set
+# CONFIG_ARCH_PXA is not set
+# CONFIG_ARCH_MSM is not set
+# CONFIG_ARCH_SHMOBILE is not set
+# CONFIG_ARCH_RPC is not set
+# CONFIG_ARCH_SA1100 is not set
+# CONFIG_ARCH_S3C2410 is not set
+# CONFIG_ARCH_S3C64XX is not set
+# CONFIG_ARCH_S5P64X0 is not set
+# CONFIG_ARCH_S5PC100 is not set
+# CONFIG_ARCH_S5PV210 is not set
+# CONFIG_ARCH_EXYNOS4 is not set
+# CONFIG_ARCH_SHARK is not set
+# CONFIG_ARCH_TCC_926 is not set
+CONFIG_ARCH_TRANSCEDE=y
+# CONFIG_ARCH_U300 is not set
+# CONFIG_ARCH_U8500 is not set
+# CONFIG_ARCH_NOMADIK is not set
+# CONFIG_ARCH_DAVINCI is not set
+# CONFIG_ARCH_OMAP is not set
+# CONFIG_PLAT_SPEAR is not set
+# CONFIG_ARCH_VT8500 is not set
+
+#
+# System MMU
+#
+
+#
+# Transcede Implementation Options
+#
+CONFIG_MACH_M84XXX=y
+# CONFIG_MACH_M822XX is not set
+# CONFIG_TRANSCEDE_TDM_CLOCK is not set
+# CONFIG_TRANSCEDE_DUALCORE is not set
+CONFIG_TRANSCEDE_UART0_SUPPORT=y
+# CONFIG_TRANSCEDE_UART1_SUPPORT is not set
+# CONFIG_TRANSCEDE_UART2_SUPPORT is not set
+CONFIG_TRANSCEDE_IPSEC_HW_SUPPORT=y
+CONFIG_TRANSCEDE_ELP_CLP30=y
+CONFIG_IPSEC_DMA_MAP_HACK_TX=y
+CONFIG_CLP30_SW_FIFO=y
+CONFIG_TRANSCEDE_ELP_PDU=y
+CONFIG_TRANSCEDE_GEMAC_0=y
+# CONFIG_TRANSCEDE_GEMAC_1 is not set
+CONFIG_TRANSCEDE_GEM_PHY=y
+CONFIG_MTD_NAND_TRANSCEDE=y
+# CONFIG_MTD_TRANSCEDE_NOR_8 is not set
+CONFIG_MTD_TRANSCEDE_NOR_16=y
+# CONFIG_TRANSCEDE_MLOG is not set
+# CONFIG_TRANSCEDE_ELP_TRNG is not set
+
+#
+# Processor Type
+#
+CONFIG_CPU_V7=y
+CONFIG_CPU_32v6K=y
+CONFIG_CPU_32v7=y
+CONFIG_CPU_ABRT_EV7=y
+CONFIG_CPU_PABRT_V7=y
+CONFIG_CPU_CACHE_V7=y
+CONFIG_CPU_CACHE_VIPT=y
+CONFIG_CPU_COPY_V6=y
+CONFIG_CPU_TLB_V7=y
+CONFIG_CPU_HAS_ASID=y
+CONFIG_CPU_CP15=y
+CONFIG_CPU_CP15_MMU=y
+
+#
+# Processor Features
+#
+CONFIG_ARM_THUMB=y
+# CONFIG_ARM_THUMBEE is not set
+# CONFIG_SWP_EMULATE is not set
+# CONFIG_CPU_ICACHE_DISABLE is not set
+# CONFIG_CPU_DCACHE_DISABLE is not set
+# CONFIG_CPU_BPREDICT_DISABLE is not set
+CONFIG_OUTER_CACHE=y
+CONFIG_OUTER_CACHE_SYNC=y
+CONFIG_CACHE_L2X0=y
+CONFIG_CACHE_PL310=y
+CONFIG_ARM_L1_CACHE_SHIFT=5
+CONFIG_ARM_DMA_MEM_BUFFERABLE=y
+CONFIG_CPU_HAS_PMU=y
+# CONFIG_ARM_ERRATA_430973 is not set
+# CONFIG_ARM_ERRATA_458693 is not set
+# CONFIG_ARM_ERRATA_460075 is not set
+# CONFIG_ARM_ERRATA_742230 is not set
+# CONFIG_ARM_ERRATA_742231 is not set
+CONFIG_PL310_ERRATA_588369=y
+# CONFIG_ARM_ERRATA_720789 is not set
+CONFIG_PL310_ERRATA_727915=y
+# CONFIG_ARM_ERRATA_743622 is not set
+# CONFIG_ARM_ERRATA_751472 is not set
+CONFIG_ARM_ERRATA_753970=y
+# CONFIG_ARM_ERRATA_754322 is not set
+# CONFIG_ARM_ERRATA_754327 is not set
+# CONFIG_ARM_ERRATA_764369 is not set
+# CONFIG_PL310_ERRATA_769419 is not set
+# CONFIG_ARM_ERRATA_775420 is not set
+CONFIG_ARM_GIC=y
+
+#
+# Bus support
+#
+# CONFIG_PCI_SYSCALL is not set
+# CONFIG_ARCH_SUPPORTS_MSI is not set
+# CONFIG_PCCARD is not set
+
+#
+# Kernel Features
+#
+CONFIG_TICK_ONESHOT=y
+# CONFIG_NO_HZ is not set
+CONFIG_HIGH_RES_TIMERS=y
+CONFIG_GENERIC_CLOCKEVENTS_BUILD=y
+CONFIG_SMP=y
+CONFIG_SMP_ON_UP=y
+CONFIG_HAVE_ARM_SCU=y
+CONFIG_HAVE_ARM_TWD=y
+# CONFIG_VMSPLIT_3G is not set
+CONFIG_VMSPLIT_2G=y
+# CONFIG_VMSPLIT_1G is not set
+CONFIG_PAGE_OFFSET=0x80000000
+CONFIG_NR_CPUS=4
+CONFIG_LOCAL_TIMERS=y
+CONFIG_GLOBAL_POLLING=y
+CONFIG_PREEMPT=y
+CONFIG_PREEMPT_RT_BASE=y
+# CONFIG_PREEMPT_NONE is not set
+# CONFIG_PREEMPT_VOLUNTARY is not set
+# CONFIG_PREEMPT_LL is not set
+# CONFIG_PREEMPT_RTB is not set
+CONFIG_PREEMPT_RT_FULL=y
+CONFIG_HZ=1000
+# CONFIG_THUMB2_KERNEL is not set
+CONFIG_AEABI=y
+CONFIG_OABI_COMPAT=y
+# CONFIG_ARCH_SPARSEMEM_DEFAULT is not set
+# CONFIG_ARCH_SELECT_MEMORY_MODEL is not set
+CONFIG_HAVE_ARCH_PFN_VALID=y
+CONFIG_HW_PERF_EVENTS=y
+CONFIG_SELECT_MEMORY_MODEL=y
+CONFIG_FLATMEM_MANUAL=y
+CONFIG_FLATMEM=y
+CONFIG_FLAT_NODE_MEM_MAP=y
+CONFIG_HAVE_MEMBLOCK=y
+CONFIG_PAGEFLAGS_EXTENDED=y
+CONFIG_SPLIT_PTLOCK_CPUS=4
+# CONFIG_COMPACTION is not set
+# CONFIG_PHYS_ADDR_T_64BIT is not set
+CONFIG_ZONE_DMA_FLAG=0
+CONFIG_VIRT_TO_BUS=y
+# CONFIG_KSM is not set
+CONFIG_DEFAULT_MMAP_MIN_ADDR=4096
+# CONFIG_CLEANCACHE is not set
+CONFIG_FORCE_MAX_ZONEORDER=11
+CONFIG_ALIGNMENT_TRAP=y
+# CONFIG_UACCESS_WITH_MEMCPY is not set
+# CONFIG_SECCOMP is not set
+# CONFIG_CC_STACKPROTECTOR is not set
+# CONFIG_DEPRECATED_PARAM_STRUCT is not set
+
+#
+# Boot options
+#
+# CONFIG_USE_OF is not set
+CONFIG_ZBOOT_ROM_TEXT=0x0
+CONFIG_ZBOOT_ROM_BSS=0x0
+CONFIG_CMDLINE="root=/dev/nfs nfsroot=10.1.69.3:/work/nfsroot ip=dhcp console=ttyAMA0 mem=128M"
+CONFIG_CMDLINE_FROM_BOOTLOADER=y
+# CONFIG_CMDLINE_EXTEND is not set
+# CONFIG_CMDLINE_FORCE is not set
+# CONFIG_XIP_KERNEL is not set
+# CONFIG_KEXEC is not set
+# CONFIG_CRASH_DUMP is not set
+# CONFIG_AUTO_ZRELADDR is not set
+
+#
+# CPU Power Management
+#
+# CONFIG_CPU_IDLE is not set
+
+#
+# Floating point emulation
+#
+
+#
+# At least one emulation must be selected
+#
+# CONFIG_FPE_NWFPE is not set
+# CONFIG_FPE_FASTFPE is not set
+# CONFIG_VFP is not set
+
+#
+# Userspace binary formats
+#
+CONFIG_BINFMT_ELF=y
+# CONFIG_CORE_DUMP_DEFAULT_ELF_HEADERS is not set
+CONFIG_HAVE_AOUT=y
+# CONFIG_BINFMT_AOUT is not set
+# CONFIG_BINFMT_MISC is not set
+
+#
+# Power management options
+#
+# CONFIG_SUSPEND is not set
+# CONFIG_PM_RUNTIME is not set
+CONFIG_ARCH_SUSPEND_POSSIBLE=y
+CONFIG_NET=y
+
+#
+# Networking options
+#
+CONFIG_PACKET=y
+CONFIG_UNIX=y
+CONFIG_XFRM=y
+CONFIG_XFRM_USER=m
+# CONFIG_XFRM_SUB_POLICY is not set
+# CONFIG_XFRM_MIGRATE is not set
+# CONFIG_XFRM_STATISTICS is not set
+CONFIG_XFRM_IPCOMP=m
+CONFIG_NET_KEY=m
+# CONFIG_NET_KEY_MIGRATE is not set
+CONFIG_INET=y
+CONFIG_IP_MULTICAST=y
+# CONFIG_IP_ADVANCED_ROUTER is not set
+CONFIG_IP_ROUTE_CLASSID=y
+CONFIG_IP_PNP=y
+CONFIG_IP_PNP_DHCP=y
+CONFIG_IP_PNP_BOOTP=y
+# CONFIG_IP_PNP_RARP is not set
+CONFIG_NET_IPIP=y
+# CONFIG_NET_IPGRE_DEMUX is not set
+# CONFIG_IP_MROUTE is not set
+# CONFIG_ARPD is not set
+# CONFIG_SYN_COOKIES is not set
+CONFIG_INET_AH=m
+CONFIG_INET_ESP=m
+CONFIG_INET_IPCOMP=m
+CONFIG_INET_XFRM_TUNNEL=m
+CONFIG_INET_TUNNEL=y
+CONFIG_INET_XFRM_MODE_TRANSPORT=m
+CONFIG_INET_XFRM_MODE_TUNNEL=m
+CONFIG_INET_XFRM_MODE_BEET=m
+# CONFIG_INET_LRO is not set
+CONFIG_INET_DIAG=y
+CONFIG_INET_TCP_DIAG=y
+# CONFIG_TCP_CONG_ADVANCED is not set
+CONFIG_TCP_CONG_CUBIC=y
+CONFIG_DEFAULT_TCP_CONG="cubic"
+# CONFIG_TCP_MD5SIG is not set
+# CONFIG_IPV6 is not set
+# CONFIG_NETWORK_SECMARK is not set
+# CONFIG_NETWORK_PHY_TIMESTAMPING is not set
+CONFIG_NETFILTER=y
+# CONFIG_NETFILTER_DEBUG is not set
+CONFIG_NETFILTER_ADVANCED=y
+# CONFIG_BRIDGE_NETFILTER is not set
+
+#
+# Core Netfilter Configuration
+#
+# CONFIG_NETFILTER_NETLINK_QUEUE is not set
+# CONFIG_NETFILTER_NETLINK_LOG is not set
+# CONFIG_NF_CONNTRACK is not set
+# CONFIG_NETFILTER_XTABLES is not set
+# CONFIG_IP_VS is not set
+
+#
+# IP: Netfilter Configuration
+#
+# CONFIG_NF_DEFRAG_IPV4 is not set
+# CONFIG_IP_NF_QUEUE is not set
+# CONFIG_IP_NF_IPTABLES is not set
+# CONFIG_IP_NF_ARPTABLES is not set
+# CONFIG_IP_DCCP is not set
+CONFIG_IP_SCTP=m
+# CONFIG_SCTP_DBG_MSG is not set
+# CONFIG_SCTP_DBG_OBJCNT is not set
+# CONFIG_SCTP_HMAC_NONE is not set
+# CONFIG_SCTP_HMAC_SHA1 is not set
+CONFIG_SCTP_HMAC_MD5=y
+# CONFIG_RDS is not set
+# CONFIG_TIPC is not set
+# CONFIG_ATM is not set
+# CONFIG_L2TP is not set
+CONFIG_STP=y
+CONFIG_BRIDGE=y
+# CONFIG_BRIDGE_IGMP_SNOOPING is not set
+# CONFIG_NET_DSA is not set
+# CONFIG_VLAN_8021Q is not set
+# CONFIG_DECNET is not set
+CONFIG_LLC=y
+# CONFIG_LLC2 is not set
+# CONFIG_IPX is not set
+# CONFIG_ATALK is not set
+# CONFIG_X25 is not set
+# CONFIG_LAPB is not set
+# CONFIG_ECONET is not set
+# CONFIG_WAN_ROUTER is not set
+# CONFIG_PHONET is not set
+# CONFIG_IEEE802154 is not set
+CONFIG_NET_SCHED=y
+
+#
+# Queueing/Scheduling
+#
+# CONFIG_NET_SCH_CBQ is not set
+CONFIG_NET_SCH_HTB=m
+CONFIG_NET_SCH_HFSC=m
+CONFIG_NET_SCH_PRIO=m
+# CONFIG_NET_SCH_MULTIQ is not set
+CONFIG_NET_SCH_RED=m
+# CONFIG_NET_SCH_SFB is not set
+CONFIG_NET_SCH_SFQ=m
+CONFIG_NET_SCH_TEQL=m
+CONFIG_NET_SCH_TBF=m
+CONFIG_NET_SCH_GRED=m
+CONFIG_NET_SCH_DSMARK=m
+# CONFIG_NET_SCH_NETEM is not set
+# CONFIG_NET_SCH_DRR is not set
+# CONFIG_NET_SCH_MQPRIO is not set
+# CONFIG_NET_SCH_CHOKE is not set
+# CONFIG_NET_SCH_QFQ is not set
+CONFIG_NET_SCH_INGRESS=m
+
+#
+# Classification
+#
+CONFIG_NET_CLS=y
+CONFIG_NET_CLS_BASIC=m
+CONFIG_NET_CLS_TCINDEX=m
+CONFIG_NET_CLS_ROUTE4=m
+CONFIG_NET_CLS_FW=m
+CONFIG_NET_CLS_U32=m
+# CONFIG_CLS_U32_PERF is not set
+# CONFIG_CLS_U32_MARK is not set
+# CONFIG_NET_CLS_RSVP is not set
+# CONFIG_NET_CLS_RSVP6 is not set
+CONFIG_NET_CLS_FLOW=m
+# CONFIG_NET_CLS_CGROUP is not set
+CONFIG_NET_EMATCH=y
+CONFIG_NET_EMATCH_STACK=32
+CONFIG_NET_EMATCH_CMP=m
+CONFIG_NET_EMATCH_NBYTE=m
+CONFIG_NET_EMATCH_U32=m
+CONFIG_NET_EMATCH_META=m
+CONFIG_NET_EMATCH_TEXT=m
+CONFIG_NET_CLS_ACT=y
+CONFIG_NET_ACT_POLICE=m
+# CONFIG_NET_ACT_GACT is not set
+CONFIG_NET_ACT_MIRRED=m
+# CONFIG_NET_ACT_NAT is not set
+# CONFIG_NET_ACT_PEDIT is not set
+# CONFIG_NET_ACT_SIMP is not set
+CONFIG_NET_ACT_SKBEDIT=m
+# CONFIG_NET_ACT_CSUM is not set
+# CONFIG_NET_CLS_IND is not set
+CONFIG_NET_SCH_FIFO=y
+# CONFIG_DCB is not set
+CONFIG_DNS_RESOLVER=y
+# CONFIG_BATMAN_ADV is not set
+CONFIG_RPS=y
+CONFIG_RFS_ACCEL=y
+CONFIG_XPS=y
+
+#
+# Network testing
+#
+CONFIG_NET_PKTGEN=m
+# CONFIG_HAMRADIO is not set
+# CONFIG_CAN is not set
+# CONFIG_IRDA is not set
+# CONFIG_BT is not set
+# CONFIG_AF_RXRPC is not set
+CONFIG_WIRELESS=y
+# CONFIG_CFG80211 is not set
+# CONFIG_LIB80211 is not set
+
+#
+# CFG80211 needs to be enabled for MAC80211
+#
+# CONFIG_WIMAX is not set
+# CONFIG_RFKILL is not set
+# CONFIG_NET_9P is not set
+# CONFIG_CAIF is not set
+# CONFIG_CEPH_LIB is not set
+
+#
+# Device Drivers
+#
+
+#
+# Generic Driver Options
+#
+CONFIG_UEVENT_HELPER_PATH="/sbin/hotplug"
+# CONFIG_DEVTMPFS is not set
+CONFIG_STANDALONE=y
+CONFIG_PREVENT_FIRMWARE_BUILD=y
+CONFIG_FW_LOADER=y
+CONFIG_FIRMWARE_IN_KERNEL=y
+CONFIG_EXTRA_FIRMWARE=""
+# CONFIG_DEBUG_DRIVER is not set
+# CONFIG_DEBUG_DEVRES is not set
+# CONFIG_SYS_HYPERVISOR is not set
+# CONFIG_CONNECTOR is not set
+CONFIG_MTD=y
+# CONFIG_MTD_DEBUG is not set
+# CONFIG_MTD_TESTS is not set
+# CONFIG_MTD_REDBOOT_PARTS is not set
+CONFIG_MTD_CMDLINE_PARTS=y
+# CONFIG_MTD_AFS_PARTS is not set
+# CONFIG_MTD_AR7_PARTS is not set
+
+#
+# User Modules And Translation Layers
+#
+CONFIG_MTD_CHAR=y
+CONFIG_MTD_BLKDEVS=y
+CONFIG_MTD_BLOCK=y
+# CONFIG_FTL is not set
+CONFIG_NFTL=y
+# CONFIG_NFTL_RW is not set
+# CONFIG_INFTL is not set
+# CONFIG_RFD_FTL is not set
+# CONFIG_SSFDC is not set
+# CONFIG_SM_FTL is not set
+# CONFIG_MTD_OOPS is not set
+
+#
+# RAM/ROM/Flash chip drivers
+#
+CONFIG_MTD_CFI=y
+CONFIG_MTD_JEDECPROBE=y
+CONFIG_MTD_GEN_PROBE=y
+CONFIG_MTD_CFI_ADV_OPTIONS=y
+CONFIG_MTD_CFI_NOSWAP=y
+# CONFIG_MTD_CFI_BE_BYTE_SWAP is not set
+# CONFIG_MTD_CFI_LE_BYTE_SWAP is not set
+CONFIG_MTD_CFI_GEOMETRY=y
+CONFIG_MTD_MAP_BANK_WIDTH_1=y
+CONFIG_MTD_MAP_BANK_WIDTH_2=y
+CONFIG_MTD_MAP_BANK_WIDTH_4=y
+# CONFIG_MTD_MAP_BANK_WIDTH_8 is not set
+# CONFIG_MTD_MAP_BANK_WIDTH_16 is not set
+# CONFIG_MTD_MAP_BANK_WIDTH_32 is not set
+CONFIG_MTD_CFI_I1=y
+CONFIG_MTD_CFI_I2=y
+# CONFIG_MTD_CFI_I4 is not set
+# CONFIG_MTD_CFI_I8 is not set
+# CONFIG_MTD_OTP is not set
+CONFIG_MTD_CFI_INTELEXT=y
+CONFIG_MTD_CFI_AMDSTD=y
+# CONFIG_MTD_CFI_STAA is not set
+CONFIG_MTD_CFI_UTIL=y
+CONFIG_MTD_RAM=y
+CONFIG_MTD_ROM=y
+# CONFIG_MTD_ABSENT is not set
+
+#
+# Mapping drivers for chip access
+#
+# CONFIG_MTD_COMPLEX_MAPPINGS is not set
+CONFIG_MTD_PHYSMAP=y
+# CONFIG_MTD_PHYSMAP_COMPAT is not set
+# CONFIG_MTD_ARM_INTEGRATOR is not set
+# CONFIG_MTD_IMPA7 is not set
+CONFIG_MTD_PLATRAM=y
+
+#
+# Self-contained MTD device drivers
+#
+# CONFIG_MTD_DATAFLASH is not set
+# CONFIG_MTD_M25P80 is not set
+# CONFIG_MTD_SST25L is not set
+# CONFIG_MTD_SLRAM is not set
+# CONFIG_MTD_PHRAM is not set
+# CONFIG_MTD_MTDRAM is not set
+# CONFIG_MTD_BLOCK2MTD is not set
+
+#
+# Disk-On-Chip Device Drivers
+#
+# CONFIG_MTD_DOC2000 is not set
+# CONFIG_MTD_DOC2001 is not set
+# CONFIG_MTD_DOC2001PLUS is not set
+CONFIG_MTD_NAND_ECC=y
+# CONFIG_MTD_NAND_ECC_SMC is not set
+CONFIG_MTD_NAND=y
+# CONFIG_MTD_NAND_VERIFY_WRITE is not set
+# CONFIG_MTD_NAND_ECC_BCH is not set
+# CONFIG_MTD_SM_COMMON is not set
+CONFIG_MTD_NAND_MUSEUM_IDS=y
+CONFIG_MTD_NAND_IDS=y
+# CONFIG_MTD_NAND_DISKONCHIP is not set
+# CONFIG_MTD_NAND_NANDSIM is not set
+CONFIG_MTD_NAND_PLATFORM=y
+# CONFIG_MTD_ONENAND is not set
+
+#
+# LPDDR flash memory drivers
+#
+# CONFIG_MTD_LPDDR is not set
+# CONFIG_MTD_UBI is not set
+# CONFIG_PARPORT is not set
+CONFIG_BLK_DEV=y
+# CONFIG_BLK_DEV_COW_COMMON is not set
+# CONFIG_BLK_DEV_LOOP is not set
+
+#
+# DRBD disabled because PROC_FS, INET or CONNECTOR not selected
+#
+# CONFIG_BLK_DEV_NBD is not set
+# CONFIG_BLK_DEV_RAM is not set
+# CONFIG_CDROM_PKTCDVD is not set
+# CONFIG_ATA_OVER_ETH is not set
+# CONFIG_BLK_DEV_RBD is not set
+# CONFIG_SENSORS_LIS3LV02D is not set
+CONFIG_MISC_DEVICES=y
+# CONFIG_AD525X_DPOT is not set
+# CONFIG_INTEL_MID_PTI is not set
+# CONFIG_ICS932S401 is not set
+# CONFIG_ENCLOSURE_SERVICES is not set
+# CONFIG_APDS9802ALS is not set
+# CONFIG_ISL29003 is not set
+# CONFIG_ISL29020 is not set
+# CONFIG_SENSORS_TSL2550 is not set
+# CONFIG_SENSORS_BH1780 is not set
+# CONFIG_SENSORS_BH1770 is not set
+# CONFIG_SENSORS_APDS990X is not set
+# CONFIG_HMC6352 is not set
+# CONFIG_DS1682 is not set
+# CONFIG_TI_DAC7512 is not set
+# CONFIG_BMP085 is not set
+# CONFIG_C2PORT is not set
+
+#
+# EEPROM support
+#
+CONFIG_EEPROM_AT24=y
+# CONFIG_EEPROM_AT25 is not set
+# CONFIG_EEPROM_LEGACY is not set
+# CONFIG_EEPROM_MAX6875 is not set
+# CONFIG_EEPROM_93CX6 is not set
+
+#
+# Texas Instruments shared transport line discipline
+#
+# CONFIG_SENSORS_LIS3_SPI is not set
+# CONFIG_SENSORS_LIS3_I2C is not set
+CONFIG_HAVE_IDE=y
+# CONFIG_IDE is not set
+
+#
+# SCSI device support
+#
+CONFIG_SCSI_MOD=y
+# CONFIG_RAID_ATTRS is not set
+# CONFIG_SCSI is not set
+# CONFIG_SCSI_DMA is not set
+# CONFIG_SCSI_NETLINK is not set
+# CONFIG_ATA is not set
+# CONFIG_MD is not set
+CONFIG_NETDEVICES=y
+# CONFIG_IFB is not set
+# CONFIG_DUMMY is not set
+# CONFIG_BONDING is not set
+# CONFIG_MACVLAN is not set
+# CONFIG_EQUALIZER is not set
+# CONFIG_TUN is not set
+# CONFIG_VETH is not set
+CONFIG_MII=y
+CONFIG_PHYLIB=y
+
+#
+# MII PHY device drivers
+#
+# CONFIG_MARVELL_PHY is not set
+# CONFIG_DAVICOM_PHY is not set
+# CONFIG_QSEMI_PHY is not set
+# CONFIG_LXT_PHY is not set
+# CONFIG_CICADA_PHY is not set
+# CONFIG_VITESSE_PHY is not set
+# CONFIG_SMSC_PHY is not set
+# CONFIG_BROADCOM_PHY is not set
+# CONFIG_ICPLUS_PHY is not set
+# CONFIG_REALTEK_PHY is not set
+# CONFIG_NATIONAL_PHY is not set
+# CONFIG_STE10XP is not set
+# CONFIG_LSI_ET1011C_PHY is not set
+# CONFIG_MICREL_PHY is not set
+# CONFIG_FIXED_PHY is not set
+CONFIG_MDIO_BITBANG=y
+CONFIG_NET_ETHERNET=y
+CONFIG_TRANSCEDE_ETH=y
+# CONFIG_TRANSCEDE_ETH_CPRI is not set
+CONFIG_TRANSCEDE_ETH_QOSCOM=y
+CONFIG_TRANSCEDE_4000_ETH_ADM_BLOCK=y
+# CONFIG_AX88796 is not set
+# CONFIG_SMC91X is not set
+# CONFIG_DM9000 is not set
+# CONFIG_ENC28J60 is not set
+# CONFIG_ETHOC is not set
+# CONFIG_SMC911X is not set
+# CONFIG_SMSC911X is not set
+# CONFIG_DNET is not set
+# CONFIG_IBM_NEW_EMAC_ZMII is not set
+# CONFIG_IBM_NEW_EMAC_RGMII is not set
+# CONFIG_IBM_NEW_EMAC_TAH is not set
+# CONFIG_IBM_NEW_EMAC_EMAC4 is not set
+# CONFIG_IBM_NEW_EMAC_NO_FLOW_CTRL is not set
+# CONFIG_IBM_NEW_EMAC_MAL_CLR_ICINTSTAT is not set
+# CONFIG_IBM_NEW_EMAC_MAL_COMMON_ERR is not set
+# CONFIG_B44 is not set
+# CONFIG_KS8851 is not set
+# CONFIG_KS8851_MLL is not set
+# CONFIG_FTMAC100 is not set
+CONFIG_NETDEV_1000=y
+# CONFIG_STMMAC_ETH is not set
+CONFIG_NETDEV_10000=y
+CONFIG_WLAN=y
+# CONFIG_HOSTAP is not set
+
+#
+# Enable WiMAX (Networking options) to see the WiMAX drivers
+#
+# CONFIG_WAN is not set
+
+#
+# CAIF transport drivers
+#
+# CONFIG_PPP is not set
+# CONFIG_SLIP is not set
+# CONFIG_NETPOLL is not set
+# CONFIG_NET_POLL_CONTROLLER is not set
+# CONFIG_ISDN is not set
+# CONFIG_PHONE is not set
+
+#
+# Input device support
+#
+CONFIG_INPUT=y
+# CONFIG_INPUT_FF_MEMLESS is not set
+# CONFIG_INPUT_POLLDEV is not set
+# CONFIG_INPUT_SPARSEKMAP is not set
+
+#
+# Userland interfaces
+#
+CONFIG_INPUT_MOUSEDEV=y
+# CONFIG_INPUT_MOUSEDEV_PSAUX is not set
+CONFIG_INPUT_MOUSEDEV_SCREEN_X=1024
+CONFIG_INPUT_MOUSEDEV_SCREEN_Y=768
+# CONFIG_INPUT_JOYDEV is not set
+# CONFIG_INPUT_EVDEV is not set
+# CONFIG_INPUT_EVBUG is not set
+
+#
+# Input Device Drivers
+#
+# CONFIG_INPUT_KEYBOARD is not set
+# CONFIG_INPUT_MOUSE is not set
+# CONFIG_INPUT_JOYSTICK is not set
+# CONFIG_INPUT_TABLET is not set
+# CONFIG_INPUT_TOUCHSCREEN is not set
+# CONFIG_INPUT_MISC is not set
+
+#
+# Hardware I/O ports
+#
+CONFIG_SERIO=y
+CONFIG_SERIO_SERPORT=y
+# CONFIG_SERIO_RAW is not set
+# CONFIG_SERIO_ALTERA_PS2 is not set
+# CONFIG_SERIO_PS2MULT is not set
+# CONFIG_GAMEPORT is not set
+
+#
+# Character devices
+#
+CONFIG_VT=y
+CONFIG_CONSOLE_TRANSLATIONS=y
+CONFIG_VT_CONSOLE=y
+CONFIG_HW_CONSOLE=y
+# CONFIG_VT_HW_CONSOLE_BINDING is not set
+CONFIG_UNIX98_PTYS=y
+# CONFIG_DEVPTS_MULTIPLE_INSTANCES is not set
+CONFIG_LEGACY_PTYS=y
+CONFIG_LEGACY_PTY_COUNT=16
+# CONFIG_SERIAL_NONSTANDARD is not set
+# CONFIG_N_GSM is not set
+# CONFIG_TRACE_SINK is not set
+CONFIG_DEVKMEM=y
+
+#
+# Serial drivers
+#
+CONFIG_SERIAL_8250=y
+CONFIG_SERIAL_8250_CONSOLE=y
+CONFIG_SERIAL_8250_NR_UARTS=1
+CONFIG_SERIAL_8250_RUNTIME_UARTS=1
+CONFIG_SERIAL_8250_EXTENDED=y
+# CONFIG_SERIAL_8250_MANY_PORTS is not set
+# CONFIG_SERIAL_8250_SHARE_IRQ is not set
+# CONFIG_SERIAL_8250_DETECT_IRQ is not set
+# CONFIG_SERIAL_8250_RSA is not set
+
+#
+# Non-8250 serial port support
+#
+# CONFIG_SERIAL_MAX3100 is not set
+# CONFIG_SERIAL_MAX3107 is not set
+CONFIG_SERIAL_CORE=y
+CONFIG_SERIAL_CORE_CONSOLE=y
+# CONFIG_SERIAL_TIMBERDALE is not set
+# CONFIG_SERIAL_ALTERA_JTAGUART is not set
+# CONFIG_SERIAL_ALTERA_UART is not set
+# CONFIG_SERIAL_XILINX_PS_UART is not set
+# CONFIG_HVC_DCC is not set
+# CONFIG_IPMI_HANDLER is not set
+CONFIG_HW_RANDOM=y
+# CONFIG_HW_RANDOM_TIMERIOMEM is not set
+# CONFIG_R3964 is not set
+# CONFIG_RAW_DRIVER is not set
+# CONFIG_TCG_TPM is not set
+# CONFIG_RAMOOPS is not set
+CONFIG_I2C=y
+CONFIG_I2C_BOARDINFO=y
+CONFIG_I2C_COMPAT=y
+CONFIG_I2C_CHARDEV=y
+# CONFIG_I2C_MUX is not set
+CONFIG_I2C_HELPER_AUTO=y
+
+#
+# I2C Hardware Bus support
+#
+
+#
+# I2C system bus drivers (mostly embedded / system-on-chip)
+#
+# CONFIG_I2C_OCORES is not set
+# CONFIG_I2C_PCA_PLATFORM is not set
+# CONFIG_I2C_PXA_PCI is not set
+# CONFIG_I2C_SIMTEC is not set
+CONFIG_I2C_TRANSCEDE=y
+# CONFIG_I2C_XILINX is not set
+
+#
+# External I2C/SMBus adapter drivers
+#
+# CONFIG_I2C_PARPORT_LIGHT is not set
+# CONFIG_I2C_TAOS_EVM is not set
+
+#
+# Other I2C/SMBus bus drivers
+#
+# CONFIG_I2C_STUB is not set
+# CONFIG_I2C_DEBUG_CORE is not set
+# CONFIG_I2C_DEBUG_ALGO is not set
+# CONFIG_I2C_DEBUG_BUS is not set
+CONFIG_SPI=y
+# CONFIG_SPI_DEBUG is not set
+CONFIG_SPI_MASTER=y
+
+#
+# SPI Master Controller Drivers
+#
+# CONFIG_SPI_ALTERA is not set
+# CONFIG_SPI_BITBANG is not set
+# CONFIG_SPI_PXA2XX_PCI is not set
+CONFIG_SPI_TRANSCEDE=y
+CONFIG_SPI_TRANSCEDE_CS_GPIO=y
+# CONFIG_SPI_XILINX is not set
+# CONFIG_SPI_DESIGNWARE is not set
+
+#
+# SPI Protocol Masters
+#
+CONFIG_SPI_SPIDEV=y
+# CONFIG_SPI_TLE62X0 is not set
+# CONFIG_SPI_CDCE62005 is not set
+
+#
+# PPS support
+#
+# CONFIG_PPS is not set
+
+#
+# PPS generators support
+#
+
+#
+# PTP clock support
+#
+
+#
+# Enable Device Drivers -> PPS to see the PTP clock options.
+#
+# CONFIG_W1 is not set
+# CONFIG_POWER_SUPPLY is not set
+# CONFIG_HWMON is not set
+# CONFIG_THERMAL is not set
+CONFIG_WATCHDOG=y
+# CONFIG_WATCHDOG_NOWAYOUT is not set
+
+#
+# Watchdog Device Drivers
+#
+# CONFIG_SOFT_WATCHDOG is not set
+CONFIG_MPCORE_WATCHDOG=y
+# CONFIG_MAX63XX_WATCHDOG is not set
+CONFIG_SSB_POSSIBLE=y
+
+#
+# Sonics Silicon Backplane
+#
+# CONFIG_SSB is not set
+CONFIG_BCMA_POSSIBLE=y
+
+#
+# Broadcom specific AMBA
+#
+# CONFIG_BCMA is not set
+CONFIG_MFD_SUPPORT=y
+# CONFIG_MFD_CORE is not set
+# CONFIG_MFD_88PM860X is not set
+# CONFIG_MFD_SM501 is not set
+# CONFIG_HTC_PASIC3 is not set
+# CONFIG_TPS6105X is not set
+# CONFIG_TPS6507X is not set
+# CONFIG_TWL4030_CORE is not set
+# CONFIG_MFD_STMPE is not set
+# CONFIG_MFD_TC3589X is not set
+# CONFIG_MFD_TMIO is not set
+# CONFIG_PMIC_DA903X is not set
+# CONFIG_PMIC_ADP5520 is not set
+# CONFIG_MFD_MAX8925 is not set
+# CONFIG_MFD_MAX8997 is not set
+# CONFIG_MFD_MAX8998 is not set
+# CONFIG_MFD_WM8400 is not set
+# CONFIG_MFD_WM831X_I2C is not set
+# CONFIG_MFD_WM831X_SPI is not set
+# CONFIG_MFD_WM8350_I2C is not set
+# CONFIG_MFD_WM8994 is not set
+# CONFIG_MFD_PCF50633 is not set
+# CONFIG_MFD_MC13XXX is not set
+# CONFIG_ABX500_CORE is not set
+# CONFIG_EZX_PCAP is not set
+# CONFIG_MFD_WL1273_CORE is not set
+# CONFIG_REGULATOR is not set
+# CONFIG_MEDIA_SUPPORT is not set
+
+#
+# Graphics support
+#
+# CONFIG_DRM is not set
+# CONFIG_VGASTATE is not set
+# CONFIG_VIDEO_OUTPUT_CONTROL is not set
+# CONFIG_FB is not set
+# CONFIG_BACKLIGHT_LCD_SUPPORT is not set
+
+#
+# Display device support
+#
+# CONFIG_DISPLAY_SUPPORT is not set
+
+#
+# Console display driver support
+#
+CONFIG_DUMMY_CONSOLE=y
+# CONFIG_SOUND is not set
+CONFIG_HID_SUPPORT=y
+CONFIG_HID=y
+# CONFIG_HIDRAW is not set
+# CONFIG_HID_PID is not set
+
+#
+# Special HID drivers
+#
+CONFIG_USB_SUPPORT=y
+CONFIG_USB_ARCH_HAS_HCD=y
+# CONFIG_USB_ARCH_HAS_OHCI is not set
+# CONFIG_USB_ARCH_HAS_EHCI is not set
+# CONFIG_USB is not set
+
+#
+# Enable Host or Gadget support to see Inventra options
+#
+
+#
+# NOTE: USB_STORAGE depends on SCSI but BLK_DEV_SD may
+#
+# CONFIG_USB_GADGET is not set
+
+#
+# OTG and related infrastructure
+#
+# CONFIG_DWC_OTG is not set
+# CONFIG_MMC is not set
+# CONFIG_MEMSTICK is not set
+# CONFIG_NEW_LEDS is not set
+# CONFIG_NFC_DEVICES is not set
+# CONFIG_ACCESSIBILITY is not set
+CONFIG_RTC_LIB=y
+# CONFIG_RTC_CLASS is not set
+# CONFIG_DMADEVICES is not set
+# CONFIG_AUXDISPLAY is not set
+CONFIG_UIO=y
+CONFIG_UIO_PDRV=y
+# CONFIG_UIO_PDRV_GENIRQ is not set
+# CONFIG_STAGING is not set
+
+#
+# File systems
+#
+# CONFIG_EXT2_FS is not set
+# CONFIG_EXT3_FS is not set
+# CONFIG_EXT4_FS is not set
+# CONFIG_REISERFS_FS is not set
+# CONFIG_JFS_FS is not set
+# CONFIG_XFS_FS is not set
+# CONFIG_GFS2_FS is not set
+# CONFIG_BTRFS_FS is not set
+# CONFIG_NILFS2_FS is not set
+CONFIG_FS_POSIX_ACL=y
+CONFIG_FILE_LOCKING=y
+CONFIG_FSNOTIFY=y
+CONFIG_DNOTIFY=y
+CONFIG_INOTIFY_USER=y
+# CONFIG_FANOTIFY is not set
+# CONFIG_QUOTA is not set
+# CONFIG_QUOTACTL is not set
+CONFIG_AUTOFS4_FS=y
+# CONFIG_FUSE_FS is not set
+
+#
+# Caches
+#
+# CONFIG_FSCACHE is not set
+
+#
+# CD-ROM/DVD Filesystems
+#
+# CONFIG_ISO9660_FS is not set
+# CONFIG_UDF_FS is not set
+
+#
+# DOS/FAT/NT Filesystems
+#
+# CONFIG_MSDOS_FS is not set
+# CONFIG_VFAT_FS is not set
+# CONFIG_NTFS_FS is not set
+
+#
+# Pseudo filesystems
+#
+CONFIG_PROC_FS=y
+CONFIG_PROC_SYSCTL=y
+CONFIG_PROC_PAGE_MONITOR=y
+CONFIG_SYSFS=y
+CONFIG_TMPFS=y
+# CONFIG_TMPFS_POSIX_ACL is not set
+# CONFIG_TMPFS_XATTR is not set
+# CONFIG_HUGETLB_PAGE is not set
+# CONFIG_CONFIGFS_FS is not set
+CONFIG_MISC_FILESYSTEMS=y
+# CONFIG_ADFS_FS is not set
+# CONFIG_AFFS_FS is not set
+# CONFIG_ECRYPT_FS is not set
+# CONFIG_HFS_FS is not set
+# CONFIG_HFSPLUS_FS is not set
+# CONFIG_BEFS_FS is not set
+# CONFIG_BFS_FS is not set
+# CONFIG_EFS_FS is not set
+CONFIG_JFFS2_FS=y
+CONFIG_JFFS2_FS_DEBUG=0
+CONFIG_JFFS2_FS_WRITEBUFFER=y
+# CONFIG_JFFS2_FS_WBUF_VERIFY is not set
+# CONFIG_JFFS2_SUMMARY is not set
+# CONFIG_JFFS2_FS_XATTR is not set
+# CONFIG_JFFS2_COMPRESSION_OPTIONS is not set
+CONFIG_JFFS2_ZLIB=y
+# CONFIG_JFFS2_LZO is not set
+CONFIG_JFFS2_RTIME=y
+# CONFIG_JFFS2_RUBIN is not set
+# CONFIG_LOGFS is not set
+# CONFIG_CRAMFS is not set
+# CONFIG_SQUASHFS is not set
+# CONFIG_VXFS_FS is not set
+# CONFIG_MINIX_FS is not set
+# CONFIG_OMFS_FS is not set
+# CONFIG_HPFS_FS is not set
+# CONFIG_QNX4FS_FS is not set
+# CONFIG_ROMFS_FS is not set
+# CONFIG_PSTORE is not set
+# CONFIG_SYSV_FS is not set
+# CONFIG_UFS_FS is not set
+CONFIG_NETWORK_FILESYSTEMS=y
+CONFIG_NFS_FS=y
+CONFIG_NFS_V3=y
+CONFIG_NFS_V3_ACL=y
+CONFIG_NFS_V4=y
+# CONFIG_NFS_V4_1 is not set
+CONFIG_ROOT_NFS=y
+# CONFIG_NFS_USE_LEGACY_DNS is not set
+CONFIG_NFS_USE_KERNEL_DNS=y
+# CONFIG_NFS_USE_NEW_IDMAPPER is not set
+# CONFIG_NFSD is not set
+CONFIG_LOCKD=y
+CONFIG_LOCKD_V4=y
+CONFIG_NFS_ACL_SUPPORT=y
+CONFIG_NFS_COMMON=y
+CONFIG_SUNRPC=y
+CONFIG_SUNRPC_GSS=y
+# CONFIG_CEPH_FS is not set
+# CONFIG_CIFS is not set
+# CONFIG_NCP_FS is not set
+# CONFIG_CODA_FS is not set
+# CONFIG_AFS_FS is not set
+
+#
+# Partition Types
+#
+# CONFIG_PARTITION_ADVANCED is not set
+CONFIG_MSDOS_PARTITION=y
+# CONFIG_NLS is not set
+
+#
+# Kernel hacking
+#
+# CONFIG_PRINTK_TIME is not set
+CONFIG_DEFAULT_MESSAGE_LOGLEVEL=4
+CONFIG_ENABLE_WARN_DEPRECATED=y
+CONFIG_ENABLE_MUST_CHECK=y
+CONFIG_FRAME_WARN=1024
+# CONFIG_MAGIC_SYSRQ is not set
+# CONFIG_STRIP_ASM_SYMS is not set
+# CONFIG_UNUSED_SYMBOLS is not set
+CONFIG_DEBUG_FS=y
+# CONFIG_HEADERS_CHECK is not set
+# CONFIG_DEBUG_SECTION_MISMATCH is not set
+CONFIG_DEBUG_KERNEL=y
+# CONFIG_LOCKUP_DETECTOR is not set
+# CONFIG_HARDLOCKUP_DETECTOR is not set
+CONFIG_DETECT_HUNG_TASK=y
+CONFIG_DEFAULT_HUNG_TASK_TIMEOUT=120
+# CONFIG_BOOTPARAM_HUNG_TASK_PANIC is not set
+CONFIG_BOOTPARAM_HUNG_TASK_PANIC_VALUE=0
+# CONFIG_SCHED_DEBUG is not set
+# CONFIG_SCHEDSTATS is not set
+# CONFIG_TIMER_STATS is not set
+# CONFIG_DEBUG_OBJECTS is not set
+# CONFIG_DEBUG_SLAB is not set
+# CONFIG_DEBUG_KMEMLEAK is not set
+# CONFIG_DEBUG_PREEMPT is not set
+# CONFIG_DEBUG_RT_MUTEXES is not set
+# CONFIG_RT_MUTEX_TESTER is not set
+# CONFIG_DEBUG_SPINLOCK is not set
+# CONFIG_DEBUG_MUTEXES is not set
+# CONFIG_DEBUG_LOCK_ALLOC is not set
+# CONFIG_PROVE_LOCKING is not set
+# CONFIG_SPARSE_RCU_POINTER is not set
+# CONFIG_LOCK_STAT is not set
+# CONFIG_DEBUG_SPINLOCK_SLEEP is not set
+# CONFIG_DEBUG_LOCKING_API_SELFTESTS is not set
+# CONFIG_DEBUG_STACK_USAGE is not set
+# CONFIG_DEBUG_KOBJECT is not set
+CONFIG_DEBUG_BUGVERBOSE=y
+# CONFIG_DEBUG_INFO is not set
+# CONFIG_DEBUG_VM is not set
+# CONFIG_DEBUG_WRITECOUNT is not set
+CONFIG_DEBUG_MEMORY_INIT=y
+# CONFIG_DEBUG_LIST is not set
+# CONFIG_TEST_LIST_SORT is not set
+# CONFIG_DEBUG_SG is not set
+# CONFIG_DEBUG_NOTIFIERS is not set
+# CONFIG_DEBUG_CREDENTIALS is not set
+# CONFIG_BOOT_PRINTK_DELAY is not set
+# CONFIG_RCU_TORTURE_TEST is not set
+CONFIG_RCU_CPU_STALL_TIMEOUT=60
+CONFIG_RCU_CPU_STALL_VERBOSE=y
+# CONFIG_BACKTRACE_SELF_TEST is not set
+# CONFIG_DEBUG_BLOCK_EXT_DEVT is not set
+# CONFIG_DEBUG_FORCE_WEAK_PER_CPU is not set
+# CONFIG_DEBUG_PER_CPU_MAPS is not set
+# CONFIG_LKDTM is not set
+# CONFIG_FAULT_INJECTION is not set
+# CONFIG_SYSCTL_SYSCALL_CHECK is not set
+# CONFIG_DEBUG_PAGEALLOC is not set
+CONFIG_HAVE_FUNCTION_TRACER=y
+CONFIG_HAVE_FUNCTION_GRAPH_TRACER=y
+CONFIG_HAVE_DYNAMIC_FTRACE=y
+CONFIG_HAVE_FTRACE_MCOUNT_RECORD=y
+CONFIG_HAVE_C_RECORDMCOUNT=y
+CONFIG_TRACING_SUPPORT=y
+CONFIG_FTRACE=y
+# CONFIG_FUNCTION_TRACER is not set
+# CONFIG_IRQSOFF_TRACER is not set
+# CONFIG_PREEMPT_TRACER is not set
+# CONFIG_SCHED_TRACER is not set
+# CONFIG_MISSED_TIMER_OFFSETS_HIST is not set
+# CONFIG_ENABLE_DEFAULT_TRACERS is not set
+CONFIG_BRANCH_PROFILE_NONE=y
+# CONFIG_PROFILE_ANNOTATED_BRANCHES is not set
+# CONFIG_PROFILE_ALL_BRANCHES is not set
+# CONFIG_STACK_TRACER is not set
+# CONFIG_BLK_DEV_IO_TRACE is not set
+# CONFIG_DYNAMIC_DEBUG is not set
+# CONFIG_DMA_API_DEBUG is not set
+# CONFIG_ATOMIC64_SELFTEST is not set
+# CONFIG_SAMPLES is not set
+CONFIG_HAVE_ARCH_KGDB=y
+# CONFIG_KGDB is not set
+# CONFIG_TEST_KSTRTOX is not set
+# CONFIG_STRICT_DEVMEM is not set
+CONFIG_ARM_UNWIND=y
+CONFIG_DEBUG_USER=y
+CONFIG_DEBUG_LL=y
+CONFIG_EARLY_PRINTK=y
+# CONFIG_DEBUG_ICEDCC is not set
+# CONFIG_OC_ETM is not set
+
+#
+# Security options
+#
+CONFIG_KEYS=y
+# CONFIG_KEYS_DEBUG_PROC_KEYS is not set
+# CONFIG_SECURITY_DMESG_RESTRICT is not set
+# CONFIG_SECURITY is not set
+# CONFIG_SECURITYFS is not set
+CONFIG_DEFAULT_SECURITY_DAC=y
+CONFIG_DEFAULT_SECURITY=""
+CONFIG_CRYPTO=y
+
+#
+# Crypto core or helper
+#
+CONFIG_CRYPTO_ALGAPI=y
+CONFIG_CRYPTO_ALGAPI2=y
+CONFIG_CRYPTO_AEAD=y
+CONFIG_CRYPTO_AEAD2=y
+CONFIG_CRYPTO_BLKCIPHER=y
+CONFIG_CRYPTO_BLKCIPHER2=y
+CONFIG_CRYPTO_HASH=y
+CONFIG_CRYPTO_HASH2=y
+CONFIG_CRYPTO_RNG2=y
+CONFIG_CRYPTO_PCOMP2=y
+CONFIG_CRYPTO_MANAGER=y
+CONFIG_CRYPTO_MANAGER2=y
+CONFIG_CRYPTO_MANAGER_DISABLE_TESTS=y
+# CONFIG_CRYPTO_GF128MUL is not set
+CONFIG_CRYPTO_NULL=y
+# CONFIG_CRYPTO_PCRYPT is not set
+CONFIG_CRYPTO_WORKQUEUE=y
+# CONFIG_CRYPTO_CRYPTD is not set
+CONFIG_CRYPTO_AUTHENC=y
+# CONFIG_CRYPTO_TEST is not set
+
+#
+# Authenticated Encryption with Associated Data
+#
+# CONFIG_CRYPTO_CCM is not set
+# CONFIG_CRYPTO_GCM is not set
+# CONFIG_CRYPTO_SEQIV is not set
+
+#
+# Block modes
+#
+CONFIG_CRYPTO_CBC=y
+# CONFIG_CRYPTO_CTR is not set
+# CONFIG_CRYPTO_CTS is not set
+# CONFIG_CRYPTO_ECB is not set
+# CONFIG_CRYPTO_LRW is not set
+# CONFIG_CRYPTO_PCBC is not set
+# CONFIG_CRYPTO_XTS is not set
+
+#
+# Hash modes
+#
+CONFIG_CRYPTO_HMAC=y
+# CONFIG_CRYPTO_XCBC is not set
+# CONFIG_CRYPTO_VMAC is not set
+
+#
+# Digest
+#
+CONFIG_CRYPTO_CRC32C=y
+# CONFIG_CRYPTO_GHASH is not set
+CONFIG_CRYPTO_MD4=m
+CONFIG_CRYPTO_MD5=y
+# CONFIG_CRYPTO_MICHAEL_MIC is not set
+# CONFIG_CRYPTO_RMD128 is not set
+# CONFIG_CRYPTO_RMD160 is not set
+# CONFIG_CRYPTO_RMD256 is not set
+# CONFIG_CRYPTO_RMD320 is not set
+CONFIG_CRYPTO_SHA1=y
+CONFIG_CRYPTO_SHA256=y
+CONFIG_CRYPTO_SHA512=y
+CONFIG_CRYPTO_TGR192=m
+CONFIG_CRYPTO_WP512=m
+
+#
+# Ciphers
+#
+CONFIG_CRYPTO_AES=y
+CONFIG_CRYPTO_ANUBIS=m
+# CONFIG_CRYPTO_ARC4 is not set
+CONFIG_CRYPTO_BLOWFISH=m
+CONFIG_CRYPTO_CAMELLIA=m
+CONFIG_CRYPTO_CAST5=m
+CONFIG_CRYPTO_CAST6=m
+CONFIG_CRYPTO_DES=y
+CONFIG_CRYPTO_FCRYPT=m
+CONFIG_CRYPTO_KHAZAD=m
+# CONFIG_CRYPTO_SALSA20 is not set
+# CONFIG_CRYPTO_SEED is not set
+CONFIG_CRYPTO_SERPENT=m
+CONFIG_CRYPTO_TEA=m
+CONFIG_CRYPTO_TWOFISH=m
+CONFIG_CRYPTO_TWOFISH_COMMON=m
+
+#
+# Compression
+#
+CONFIG_CRYPTO_DEFLATE=y
+# CONFIG_CRYPTO_ZLIB is not set
+# CONFIG_CRYPTO_LZO is not set
+
+#
+# Random Number Generation
+#
+# CONFIG_CRYPTO_ANSI_CPRNG is not set
+# CONFIG_CRYPTO_USER_API_HASH is not set
+# CONFIG_CRYPTO_USER_API_SKCIPHER is not set
+CONFIG_CRYPTO_HW=y
+CONFIG_CRYPTO_DEV_TRANSCEDE=m
+
+#
+# OCF Configuration
+#
+# CONFIG_OCF_OCF is not set
+# CONFIG_BINARY_PRINTF is not set
+
+#
+# Library routines
+#
+CONFIG_BITREVERSE=y
+CONFIG_CRC_CCITT=m
+CONFIG_CRC16=y
+# CONFIG_CRC_T10DIF is not set
+# CONFIG_CRC_ITU_T is not set
+CONFIG_CRC32=y
+# CONFIG_CRC7 is not set
+CONFIG_LIBCRC32C=y
+CONFIG_ZLIB_INFLATE=y
+CONFIG_ZLIB_DEFLATE=y
+# CONFIG_XZ_DEC is not set
+# CONFIG_XZ_DEC_BCJ is not set
+CONFIG_TEXTSEARCH=y
+CONFIG_TEXTSEARCH_KMP=m
+CONFIG_TEXTSEARCH_BM=m
+CONFIG_TEXTSEARCH_FSM=m
+CONFIG_HAS_IOMEM=y
+CONFIG_HAS_IOPORT=y
+CONFIG_HAS_DMA=y
+CONFIG_CPU_RMAP=y
+CONFIG_NLATTR=y
+# CONFIG_AVERAGE is not set
diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 4fff837..4c32ad6 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -81,6 +81,15 @@ static inline void __dma_single_cpu_to_dev(const void *kaddr, size_t size,
 	extern void ___dma_single_cpu_to_dev(const void *, size_t,
 		enum dma_data_direction);
 
+	extern unsigned long (*__dma_cpu_sync)(const void * kaddr, size_t size, uint dir);
+
+	if (__dma_cpu_sync != NULL)
+	{
+	    uint  val = (uint)__dma_cpu_sync((const void*)kaddr, size, dir);
+	    if (val != 0)
+		return;
+	}
+
 	if (!arch_is_coherent())
 		___dma_single_cpu_to_dev(kaddr, size, dir);
 }
@@ -91,6 +100,15 @@ static inline void __dma_single_dev_to_cpu(const void *kaddr, size_t size,
 	extern void ___dma_single_dev_to_cpu(const void *, size_t,
 		enum dma_data_direction);
 
+	extern unsigned long (*__dma_cpu_sync)(const void * kaddr, size_t size, uint dir);
+
+	if (__dma_cpu_sync != NULL)
+	{
+	    uint  val = (uint)__dma_cpu_sync((const void*)kaddr, size, dir);
+	    if(val != 0)
+		return;
+	}
+
 	if (!arch_is_coherent())
 		___dma_single_dev_to_cpu(kaddr, size, dir);
 }
@@ -331,6 +349,16 @@ static inline int dmabounce_sync_for_device(struct device *d, dma_addr_t addr,
 static inline dma_addr_t __dma_map_single(struct device *dev, void *cpu_addr,
 		size_t size, enum dma_data_direction dir)
 {
+	extern unsigned long (*__dma_ext_map_mem)(const void * kaddr, size_t size, uint dir);
+
+	if (__dma_ext_map_mem != NULL)
+	{
+	    dma_addr_t addr = (dma_addr_t)__dma_ext_map_mem((const void*)cpu_addr, size, dir);
+	    if (addr != 0)
+		return addr;
+	}
+
+
 	__dma_single_cpu_to_dev(cpu_addr, size, dir);
 	return virt_to_dma(dev, cpu_addr);
 }
@@ -345,6 +373,15 @@ static inline dma_addr_t __dma_map_page(struct device *dev, struct page *page,
 static inline void __dma_unmap_single(struct device *dev, dma_addr_t handle,
 		size_t size, enum dma_data_direction dir)
 {
+	extern unsigned long (*__dma_ext_unmap_mem)(const void * kaddr, size_t size, uint dir);
+
+	if (__dma_ext_unmap_mem != NULL)
+	{
+	    unsigned long unmap = __dma_ext_unmap_mem((const void*)handle, size, dir);
+	    if (unmap != 0)
+		return;
+	}
+
 	__dma_single_dev_to_cpu(dma_to_virt(dev, handle), size, dir);
 }
 
@@ -503,6 +540,15 @@ static inline void dma_sync_single_range_for_device(struct device *dev,
 static inline void dma_sync_single_for_cpu(struct device *dev,
 		dma_addr_t handle, size_t size, enum dma_data_direction dir)
 {
+	extern unsigned long (*__dma_dev_sync)(const void * kaddr, size_t size, uint dir);
+
+        if (__dma_dev_sync != NULL)
+        {
+            uint  val = (uint)__dma_dev_sync((const void*)handle, size, dir);
+            if(val != 0)
+                return;
+        }
+
 	dma_sync_single_range_for_cpu(dev, handle, 0, size, dir);
 }
 
diff --git a/arch/arm/include/asm/entry-macro-multi.S b/arch/arm/include/asm/entry-macro-multi.S
index 2da8547..dc2297d 100644
--- a/arch/arm/include/asm/entry-macro-multi.S
+++ b/arch/arm/include/asm/entry-macro-multi.S
@@ -13,6 +13,12 @@
 	adrne	lr, BSYM(1b)
 	bne	asm_do_IRQ
 
+#ifdef CONFIG_MACH_M84XXX
+	/* Handle 4GMX IPI is usual interrupt */
+	cmp	r0, #7
+	bleq	asm_do_IRQ
+#endif  /* CONFIG_MACH_M84XXX */
+
 #ifdef CONFIG_SMP
 	/*
 	 * XXX
@@ -26,6 +32,13 @@
 	adrne	lr, BSYM(1b)
 	bne	do_IPI
 
+#ifdef CONFIG_GLOBAL_POLLING
+	test_for_gtirq r0, r6, r5, lr
+	movne	r1, sp
+	adrne	lr, BSYM(1b)
+	bne	do_global_timer
+#endif  /* CONFIG_GLOBAL_POLLING */
+
 #ifdef CONFIG_LOCAL_TIMERS
 	test_for_ltirq r0, r6, r5, lr
 	movne	r0, sp
diff --git a/arch/arm/include/asm/fixmap.h b/arch/arm/include/asm/fixmap.h
index bbae919..c8b08c7 100644
--- a/arch/arm/include/asm/fixmap.h
+++ b/arch/arm/include/asm/fixmap.h
@@ -20,7 +20,27 @@
 #define FIX_KMAP_BEGIN		0
 #define FIX_KMAP_END		(FIXADDR_SIZE >> PAGE_SHIFT)
 
+#ifndef CONFIG_ARCH_TRANSCEDE
 #define __fix_to_virt(x)	(FIXADDR_START + ((x) << PAGE_SHIFT))
+#else
+
+#if defined(CONFIG_MACH_M84XXX)
+
+#include <mach/transcede-4000.h>
+
+#elif defined(CONFIG_MACH_M822XX)
+
+#include <mach/transcede-2200.h>
+
+#else
+
+#error "Unsupported CPU"
+
+#endif
+
+#define __fix_to_virt(x)	AAB_XP_VADDR(x)
+#endif
+
 #define __virt_to_fix(x)	(((x) - FIXADDR_START) >> PAGE_SHIFT)
 
 extern void __this_fixmap_does_not_exist(void);
diff --git a/arch/arm/include/asm/hardirq.h b/arch/arm/include/asm/hardirq.h
index 89ad180..a2d685f 100644
--- a/arch/arm/include/asm/hardirq.h
+++ b/arch/arm/include/asm/hardirq.h
@@ -5,7 +5,11 @@
 #include <linux/threads.h>
 #include <asm/irq.h>
 
-#define NR_IPI	5
+#ifdef CONFIG_GLOBAL_POLLING
+#define NR_IPI	(5+1)
+#else
+#define NR_IPI	(5)
+#endif
 
 typedef struct {
 	unsigned int __softirq_pending;
diff --git a/arch/arm/include/asm/mach/map.h b/arch/arm/include/asm/mach/map.h
index d2fedb5..a62e019 100644
--- a/arch/arm/include/asm/mach/map.h
+++ b/arch/arm/include/asm/mach/map.h
@@ -30,6 +30,8 @@ struct map_desc {
 #define MT_MEMORY_DTCM		12
 #define MT_MEMORY_ITCM		13
 
+#define MT_MEMORY_STRONGLY_ORDERED	14
+
 #ifdef CONFIG_MMU
 extern void iotable_init(struct map_desc *, int);
 
diff --git a/arch/arm/include/asm/processor.h b/arch/arm/include/asm/processor.h
index b2d9df5..4372ceb 100644
--- a/arch/arm/include/asm/processor.h
+++ b/arch/arm/include/asm/processor.h
@@ -93,6 +93,9 @@ unsigned long get_wchan(struct task_struct *p);
  * Create a new kernel thread
  */
 extern int kernel_thread(int (*fn)(void *), void *arg, unsigned long flags);
+#ifdef CONFIG_AMP_STACK
+extern int kernel_amp_thread(int (*fn)(void *), void *arg, unsigned long flags, unsigned long cpu);
+#endif	/* CONFIG_AMP_STACK */
 
 #define task_pt_regs(p) \
 	((struct pt_regs *)(THREAD_START_SP + task_stack_page(p)) - 1)
diff --git a/arch/arm/include/asm/serial.h b/arch/arm/include/asm/serial.h
index ebb0490..3f441c0 100644
--- a/arch/arm/include/asm/serial.h
+++ b/arch/arm/include/asm/serial.h
@@ -14,6 +14,14 @@
 #ifndef __ASM_SERIAL_H
 #define __ASM_SERIAL_H
 
-#define BASE_BAUD	(1843200 / 16)
+#if defined(CONFIG_MACH_M822XX)
+# include 		<mach/clkrst.h>
+/* # define BASE_BAUD	(ClkRstGetFreq(CR_SYS_AXI) / 16) // not working, hangs on register access. So setting default rate for rev >= X2.1 */
+# define BASE_BAUD	(250000000 / 16)
+#elif defined(CONFIG_MACH_M84XXX)
+# define BASE_BAUD	(TRANSCEDE_AXICLK_HZ / 16)
+#else
+# define BASE_BAUD	(1843200 / 16)
+#endif
 
 #endif
diff --git a/arch/arm/include/asm/smp.h b/arch/arm/include/asm/smp.h
index e42d96a..cb04838 100644
--- a/arch/arm/include/asm/smp.h
+++ b/arch/arm/include/asm/smp.h
@@ -93,4 +93,8 @@ extern void arch_send_call_function_ipi_mask(const struct cpumask *mask);
  */
 extern void show_local_irqs(struct seq_file *, int);
 
+#ifdef CONFIG_GLOBAL_POLLING
+extern void smp_send_gtcore_sync(int cpu);
+#endif
+
 #endif /* ifndef __ASM_ARM_SMP_H */
diff --git a/arch/arm/include/asm/smp_twd.h b/arch/arm/include/asm/smp_twd.h
index fed9981..29aafad 100644
--- a/arch/arm/include/asm/smp_twd.h
+++ b/arch/arm/include/asm/smp_twd.h
@@ -21,6 +21,7 @@
 struct clock_event_device;
 
 extern void __iomem *twd_base;
+extern unsigned long twd_timer_rate;
 
 int twd_timer_ack(void);
 void twd_timer_setup(struct clock_event_device *);
diff --git a/arch/arm/include/asm/thread_info.h b/arch/arm/include/asm/thread_info.h
index 7b5cc8d..cf3d4fe 100644
--- a/arch/arm/include/asm/thread_info.h
+++ b/arch/arm/include/asm/thread_info.h
@@ -21,6 +21,10 @@
 
 #ifndef __ASSEMBLY__
 
+#ifdef CONFIG_AMP_STACK
+#include <linux/plist.h>
+#endif	/* CONFIG_AMP_STACK */
+
 struct task_struct;
 struct exec_domain;
 
@@ -43,11 +47,37 @@ struct cpu_context_save {
 	__u32	extra[2];		/* Xscale 'acc' register, etc */
 };
 
+#ifdef CONFIG_AMP_STACK
+/*
+ * This is the control structure for tasks blocked on a rt_mutex,
+ * which is allocated on the kernel stack on of the blocked task.
+ *
+ * @list_entry:		pi node to enqueue into the mutex waiters list
+ * @pi_list_entry:	pi node to enqueue into the mutex owner waiters list
+ * @task:		task reference to the blocked task
+ */
+struct amp_rt_mutex_waiter {
+	struct plist_node	list_entry;
+	struct plist_node	pi_list_entry;
+	struct task_struct	*task;
+	struct rt_mutex		*lock;
+	bool			savestate;
+#ifdef CONFIG_DEBUG_RT_MUTEXES
+	unsigned long		ip;
+	struct pid		*deadlock_task_pid;
+	struct rt_mutex		*deadlock_lock;
+#endif
+};
+#endif  /* CONFIG_AMP_STACK */
+
 /*
  * low level task data that entry.S needs immediate access to.
  * __switch_to() assumes cpu_context follows immediately after cpu_domain.
  */
 struct thread_info {
+#ifdef CONFIG_AMP_STACK
+	void *stack_smp;
+#endif	/* CONFIG_AMP_STACK */
 	unsigned long		flags;		/* low level flags */
 	int			preempt_count;	/* 0 => preemptable, <0 => bug */
 	mm_segment_t		addr_limit;	/* address limit */
@@ -66,8 +96,29 @@ struct thread_info {
 	unsigned long		thumbee_state;	/* ThumbEE Handler Base register */
 #endif
 	struct restart_block	restart_block;
+#ifdef CONFIG_AMP_STACK
+	struct amp_rt_mutex_waiter waiter_spinlock;
+	struct amp_rt_mutex_waiter waiter_mutex;
+#endif	/* CONFIG_AMP_STACK */
 };
 
+#ifdef CONFIG_AMP_STACK
+#define INIT_THREAD_INFO(tsk)						\
+{									\
+	.task		= &tsk,						\
+	.stack_smp = &init_thread_info, \
+	.exec_domain	= &default_exec_domain,				\
+	.flags		= 0,						\
+	.preempt_count	= INIT_PREEMPT_COUNT,				\
+	.addr_limit	= KERNEL_DS,					\
+	.cpu_domain	= domain_val(DOMAIN_USER, DOMAIN_MANAGER) |	\
+			  domain_val(DOMAIN_KERNEL, DOMAIN_MANAGER) |	\
+			  domain_val(DOMAIN_IO, DOMAIN_CLIENT),		\
+	.restart_block	= {						\
+		.fn	= do_no_restart_syscall,			\
+	},								\
+}
+#else   /* !CONFIG_AMP_STACK */
 #define INIT_THREAD_INFO(tsk)						\
 {									\
 	.task		= &tsk,						\
@@ -82,6 +133,7 @@ struct thread_info {
 		.fn	= do_no_restart_syscall,			\
 	},								\
 }
+#endif  /* CONFIG_AMP_STACK */
 
 #define init_thread_info	(init_thread_union.thread_info)
 #define init_stack		(init_thread_union.stack)
@@ -94,7 +146,12 @@ static inline struct thread_info *current_thread_info(void) __attribute_const__;
 static inline struct thread_info *current_thread_info(void)
 {
 	register unsigned long sp asm ("sp");
+
+#ifdef CONFIG_AMP_STACK
+	return (struct thread_info *)(((struct thread_info *)(sp & ~(THREAD_SIZE - 1)))->stack_smp);
+#else  /* !CONFIG_AMP_STACK */
 	return (struct thread_info *)(sp & ~(THREAD_SIZE - 1));
+#endif	/* CONFIG_AMP_STACK */
 }
 
 #define thread_saved_pc(tsk)	\
diff --git a/arch/arm/include/asm/uaccess.h b/arch/arm/include/asm/uaccess.h
index b293616..9d499a8 100644
--- a/arch/arm/include/asm/uaccess.h
+++ b/arch/arm/include/asm/uaccess.h
@@ -19,6 +19,10 @@
 #include <asm/system.h>
 #include <asm/unified.h>
 
+#ifdef CONFIG_MACH_M822XX
+#include <mach/hardware.h>
+#endif	/* CONFIG_MACH_M822XX */
+
 #define VERIFY_READ 0
 #define VERIFY_WRITE 1
 
@@ -68,6 +72,35 @@ static inline void set_fs(mm_segment_t fs)
 
 #define segment_eq(a,b)	((a) == (b))
 
+#if (defined CONFIG_MACH_M822XX) || (defined CONFIG_MACH_M84XXX)
+
+/* Here we make it possible to pass public addresses from/to user */
+/* TODO: make it safe */
+
+#define public_range_ok(addr,size) ((((unsigned long)addr >= TRANSCEDE_PUBLIC_VIRT) && ((unsigned long)addr + size < TRANSCEDE_PUBLIC_VIRT + TRANSCEDE_PUBLIC_SIZE)) == 0)
+#define public_addr_ok(addr) (public_range_ok(addr,0) == 0)
+#define __range_ok(addr,size) (public_range_ok(addr,size) && __safe_range_ok(addr,size))
+#define __addr_ok(addr) (public_addr_ok(addr) || __safe_addr_ok(addr))
+
+#define __safe_range_ok(addr,size) ({ \
+	unsigned long flag, roksum; \
+	__chk_user_ptr(addr);	\
+	__asm__("adds %1, %2, %3; sbcccs %1, %1, %0; movcc %0, #0" \
+		: "=&r" (flag), "=&r" (roksum) \
+		: "r" (addr), "Ir" (size), "0" (current_thread_info()->addr_limit) \
+		: "cc"); \
+	flag; })
+
+#define __safe_addr_ok(addr) ({ \
+	unsigned long flag; \
+	__asm__("cmp %2, %0; movlo %0, #0" \
+		: "=&r" (flag) \
+		: "0" (current_thread_info()->addr_limit), "r" (addr) \
+		: "cc"); \
+	(flag == 0); })
+
+#else  /* CONFIG_MACH_M822XX */
+
 #define __addr_ok(addr) ({ \
 	unsigned long flag; \
 	__asm__("cmp %2, %0; movlo %0, #0" \
@@ -76,6 +109,7 @@ static inline void set_fs(mm_segment_t fs)
 		: "cc"); \
 	(flag == 0); })
 
+
 /* We use 33-bit arithmetic here... */
 #define __range_ok(addr,size) ({ \
 	unsigned long flag, roksum; \
@@ -86,6 +120,8 @@ static inline void set_fs(mm_segment_t fs)
 		: "cc"); \
 	flag; })
 
+#endif	/* CONFIG_MACH_M822XX */
+
 /*
  * Single-value transfer routines.  They automatically use the right
  * size if we just have the right pointer type.  Note that the functions
diff --git a/arch/arm/kernel/asm-offsets.c b/arch/arm/kernel/asm-offsets.c
index 927522c..b9185f0 100644
--- a/arch/arm/kernel/asm-offsets.c
+++ b/arch/arm/kernel/asm-offsets.c
@@ -59,6 +59,9 @@ int main(void)
   DEFINE(TI_TP_VALUE,		offsetof(struct thread_info, tp_value));
   DEFINE(TI_FPSTATE,		offsetof(struct thread_info, fpstate));
   DEFINE(TI_VFPSTATE,		offsetof(struct thread_info, vfpstate));
+#ifdef CONFIG_AMP_STACK
+  DEFINE(TI_SMP_STACK, offsetof(struct thread_info, stack_smp));
+#endif	/* CONFIG_AMP_STACK */
 #ifdef CONFIG_ARM_THUMBEE
   DEFINE(TI_THUMBEE_STATE,	offsetof(struct thread_info, thumbee_state));
 #endif
diff --git a/arch/arm/kernel/entry-header.S b/arch/arm/kernel/entry-header.S
index 051166c..96ce7c9 100644
--- a/arch/arm/kernel/entry-header.S
+++ b/arch/arm/kernel/entry-header.S
@@ -111,6 +111,9 @@
 	.macro	get_thread_info, rd
 	mov	\rd, sp, lsr #13
 	mov	\rd, \rd, lsl #13
+#ifdef CONFIG_AMP_STACK
+	ldr	\rd, [ \rd, #TI_SMP_STACK ]
+#endif
 	.endm
 
 	@
diff --git a/arch/arm/kernel/irq.c b/arch/arm/kernel/irq.c
index 83bbad0..e7be727 100644
--- a/arch/arm/kernel/irq.c
+++ b/arch/arm/kernel/irq.c
@@ -62,6 +62,9 @@ int arch_show_interrupts(struct seq_file *p, int prec)
 #ifdef CONFIG_LOCAL_TIMERS
 	show_local_irqs(p, prec);
 #endif
+#ifdef CONFIG_GLOBAL_POLLING
+	show_global_irqs(p, prec);
+#endif	/* CONFIG_GLOBAL_POLLING */
 	seq_printf(p, "%*s: %10lu\n", prec, "Err", irq_err_count);
 	return 0;
 }
diff --git a/arch/arm/kernel/perf_event.c b/arch/arm/kernel/perf_event.c
index 7460d53..9c7839b 100644
--- a/arch/arm/kernel/perf_event.c
+++ b/arch/arm/kernel/perf_event.c
@@ -420,8 +420,20 @@ armpmu_reserve_hardware(void)
 			continue;
 
 		err = request_irq(irq, handle_irq,
-				  IRQF_NOBALANCING | IRQF_NO_THREAD,
-				  "armpmu", NULL);
+#if defined(CONFIG_MACH_M84XXX)
+	/* These devices have a single PMU IRQ for all cores, so we need to be able
+	 * to set the IRQ affinity from userspace to profile a specific core.
+	 * This is done in this way: echo 1 > /proc/irq/42/smp_affinity.
+	 * When IRQF_NOBALANCING is set, there is no way to execute 'echo 2 > /proc/irq/42/smp_affinity'
+	 * after IRQ42 has been processed on core #0. Only need to reboot.
+	 * Removing IRQF_NOBALANCING allows to profile each core separatelety but without rebooting.
+	 * There is no technical way to enable profiling of 2 cores simultaneously due to 1 PMU IRQ for all cores.
+	 */
+		                  IRQF_DISABLED | IRQF_NO_THREAD,
+#else
+		                  IRQF_NOBALANCING | IRQF_NO_THREAD,
+#endif
+		                  "armpmu", NULL);
 		if (err) {
 			pr_warning("unable to request IRQ%d for ARM perf "
 				"counters\n", irq);
diff --git a/arch/arm/kernel/pmu.c b/arch/arm/kernel/pmu.c
index 2c79eec..6b42a21 100644
--- a/arch/arm/kernel/pmu.c
+++ b/arch/arm/kernel/pmu.c
@@ -119,11 +119,13 @@ init_cpu_pmu(void)
 	if (irqs == 1 && !irq_can_set_affinity(platform_get_irq(pdev, 0)))
 		return 0;
 
+#if !defined(CONFIG_MACH_M84XXX) && !defined(CONFIG_MACH_M822XX)
 	for (i = 0; i < irqs; ++i) {
 		err = set_irq_affinity(platform_get_irq(pdev, i), i);
 		if (err)
 			break;
 	}
+#endif
 
 	return err;
 }
diff --git a/arch/arm/kernel/process.c b/arch/arm/kernel/process.c
index 2012ffe..b5bb79d 100644
--- a/arch/arm/kernel/process.c
+++ b/arch/arm/kernel/process.c
@@ -45,6 +45,10 @@ unsigned long __stack_chk_guard __read_mostly;
 EXPORT_SYMBOL(__stack_chk_guard);
 #endif
 
+#ifdef CONFIG_AMP_STACK
+#include <mach/mmu_protect.h>
+#endif	/* CONFIG_AMP_STACK */
+
 static const char *processor_modes[] = {
   "USER_26", "FIQ_26" , "IRQ_26" , "SVC_26" , "UK4_26" , "UK5_26" , "UK6_26" , "UK7_26" ,
   "UK8_26" , "UK9_26" , "UK10_26", "UK11_26", "UK12_26", "UK13_26", "UK14_26", "UK15_26",
@@ -353,9 +357,71 @@ void release_thread(struct task_struct *dead_task)
 
 asmlinkage void ret_from_fork(void) __asm__("ret_from_fork");
 
+#ifdef CONFIG_AMP_STACK
+
+int
+copy_thread(unsigned long clone_flags, unsigned long stack_start,
+            unsigned long stk_sz, struct task_struct *p, struct pt_regs *regs, unsigned long amp_cpu)
+{
+	struct thread_info *thread = task_thread_info(p);
+	struct pt_regs *childregs = task_pt_regs(p);
+	struct pt_regs *childregs_amp = NULL;
+
+	if (amp_cpu != MMU_CPU_SMP) {
+		struct smp_value smp;
+
+		MMU_DEBUG("p->stack = 0x%08lx", (unsigned long)p->stack);
+		MMU_DEBUG("p->stack_amp = 0x%08lx", (unsigned long)p->amp_stack);
+		MMU_DEBUG("stack_start = 0x%08lx", stack_start);
+
+		childregs_amp = (struct pt_regs *)(THREAD_START_SP + (p->amp_stack)) - 1;
+
+		MMU_DEBUG("childregs_amp = 0x%08lx", (unsigned long)childregs_amp);
+
+		*childregs_amp = *regs;
+
+		MMU_DEBUG("childregs->ARM_pc = 0x%08lx", (unsigned long)childregs_amp->ARM_pc);
+
+		childregs_amp->ARM_r0 = 0;
+		childregs_amp->ARM_sp = stack_start;
+
+		smp.src = (unsigned long *)(regs);
+		smp.dst = (unsigned long *)(childregs_amp);
+		smp.size = sizeof(*regs);
+
+		smp_call_function_single(0, smp_set_value, &smp, 1);
+		smp_call_function_single(1, smp_set_value, &smp, 1);
+	}
+
+	*childregs = *regs;
+	childregs->ARM_r0 = 0;
+	childregs->ARM_sp = stack_start;
+
+	memset(&thread->cpu_context, 0, sizeof(struct cpu_context_save));
+
+	if (amp_cpu != MMU_CPU_SMP) {
+		thread->cpu_context.sp = (unsigned long)childregs_amp;
+	} else {
+		thread->cpu_context.sp = (unsigned long)childregs;
+	}
+
+	thread->cpu_context.pc = (unsigned long)ret_from_fork;
+
+	clear_ptrace_hw_breakpoint(p);
+
+	if (clone_flags & CLONE_SETTLS)
+		thread->tp_value = regs->ARM_r3;
+
+	thread_notify(THREAD_NOTIFY_COPY, thread);
+
+	return 0;
+}
+
+#else  /* !CONFIG_AMP_STACK */
+
 int
 copy_thread(unsigned long clone_flags, unsigned long stack_start,
-	    unsigned long stk_sz, struct task_struct *p, struct pt_regs *regs)
+            unsigned long stk_sz, struct task_struct *p, struct pt_regs *regs)
 {
 	struct thread_info *thread = task_thread_info(p);
 	struct pt_regs *childregs = task_pt_regs(p);
@@ -378,6 +444,8 @@ copy_thread(unsigned long clone_flags, unsigned long stack_start,
 	return 0;
 }
 
+#endif	/* CONFIG_AMP_STACK */
+
 /*
  * Fill in the task's elfregs structure for a core dump.
  */
@@ -455,10 +523,33 @@ pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
 	regs.ARM_pc = (unsigned long)kernel_thread_helper;
 	regs.ARM_cpsr = regs.ARM_r7 | PSR_I_BIT;
 
+#ifdef CONFIG_AMP_STACK
+	return do_fork(flags|CLONE_VM|CLONE_UNTRACED, 0, &regs, 0, NULL, NULL, MMU_CPU_SMP);
+#else	/* CONFIG_AMP_STACK */
 	return do_fork(flags|CLONE_VM|CLONE_UNTRACED, 0, &regs, 0, NULL, NULL);
+#endif	/* CONFIG_AMP_STACK */
 }
 EXPORT_SYMBOL(kernel_thread);
 
+#ifdef CONFIG_AMP_STACK
+pid_t kernel_amp_thread(int (*fn)(void *), void *arg, unsigned long flags, unsigned long cpu)
+{
+	struct pt_regs regs;
+
+	memset(&regs, 0, sizeof(regs));
+
+	regs.ARM_r4 = (unsigned long)arg;
+	regs.ARM_r5 = (unsigned long)fn;
+	regs.ARM_r6 = (unsigned long)kernel_thread_exit;
+	regs.ARM_r7 = SVC_MODE | PSR_ENDSTATE | PSR_ISETSTATE;
+	regs.ARM_pc = (unsigned long)kernel_thread_helper;
+	regs.ARM_cpsr = regs.ARM_r7 | PSR_I_BIT;
+
+	return do_fork(flags|CLONE_VM|CLONE_UNTRACED, 0, &regs, 0, NULL, NULL, cpu);
+}
+EXPORT_SYMBOL(kernel_amp_thread);
+#endif	/* CONFIG_AMP_STACK */
+
 unsigned long get_wchan(struct task_struct *p)
 {
 	struct stackframe frame;
diff --git a/arch/arm/kernel/setup.c b/arch/arm/kernel/setup.c
index acbb447..5394e01 100644
--- a/arch/arm/kernel/setup.c
+++ b/arch/arm/kernel/setup.c
@@ -50,6 +50,10 @@
 #include <asm/traps.h>
 #include <asm/unwind.h>
 
+#ifdef CONFIG_MACH_M822XX
+#include <mach/sysheap.h>
+#endif
+
 #if defined(CONFIG_DEPRECATED_PARAM_STRUCT)
 #include "compat.h"
 #endif
@@ -513,6 +517,16 @@ static int __init early_mem(char *p)
 
 	arm_add_memory(start, size);
 
+#if (defined CONFIG_MACH_M822XX) || (defined CONFIG_MACH_M84XXX)
+	/*
+	 * define base of after-Linux RAM
+	 */
+	size -= start & ~PAGE_MASK;
+	start = PAGE_ALIGN(start);
+	size  = size & PAGE_MASK;
+	transcede_ddr_common_base = (unsigned long)(start + size);
+#endif
+
 	return 0;
 }
 early_param("mem", early_mem);
diff --git a/arch/arm/kernel/smp.c b/arch/arm/kernel/smp.c
index 5b8b222..0009825 100644
--- a/arch/arm/kernel/smp.c
+++ b/arch/arm/kernel/smp.c
@@ -40,6 +40,9 @@
 #include <asm/ptrace.h>
 #include <asm/localtimer.h>
 
+#ifdef CONFIG_GLOBAL_POLLING
+#include <mach/periodic_task.h>
+#endif
 /*
  * as from 2.5, kernels no longer have an init_tasks structure
  * so we need some other way of telling a new secondary core
@@ -53,6 +56,9 @@ enum ipi_msg_type {
 	IPI_CALL_FUNC,
 	IPI_CALL_FUNC_SINGLE,
 	IPI_CPU_STOP,
+#ifdef CONFIG_GLOBAL_POLLING
+	IPI_GTCORE_SYNC,
+#endif
 };
 
 int __cpuinit __cpu_up(unsigned int cpu)
@@ -414,6 +420,9 @@ static const char *ipi_types[NR_IPI] = {
 	S(IPI_CALL_FUNC, "Function call interrupts"),
 	S(IPI_CALL_FUNC_SINGLE, "Single function call interrupts"),
 	S(IPI_CPU_STOP, "CPU stop interrupts"),
+#ifdef CONFIG_GLOBAL_POLLING
+	S(IPI_GTCORE_SYNC, "Cores sync")
+#endif
 };
 
 void show_ipi_list(struct seq_file *p, int prec)
@@ -567,48 +576,60 @@ static void ipi_cpu_stop(unsigned int cpu)
 /*
  * Main handler for inter-processor interrupts
  */
-asmlinkage void __exception_irq_entry do_IPI(int ipinr, struct pt_regs *regs)
+void kernel_do_IPI(unsigned int cpu, int ipinr)
 {
-	unsigned int cpu = smp_processor_id();
-	struct pt_regs *old_regs = set_irq_regs(regs);
-
 	if (ipinr >= IPI_TIMER && ipinr < IPI_TIMER + NR_IPI)
 		__inc_irq_stat(cpu, ipi_irqs[ipinr - IPI_TIMER]);
 
 	switch (ipinr) {
 	case IPI_TIMER:
-		irq_enter();
-		ipi_timer();
-		irq_exit();
-		break;
-
-	case IPI_RESCHEDULE:
-		scheduler_ipi();
-		break;
-
-	case IPI_CALL_FUNC:
-		irq_enter();
-		generic_smp_call_function_interrupt();
-		irq_exit();
-		break;
+			irq_enter();
+			ipi_timer();
+			irq_exit();
+			break;
+
+		case IPI_RESCHEDULE:
+			scheduler_ipi();
+			break;
+
+		case IPI_CALL_FUNC:
+			irq_enter();
+			generic_smp_call_function_interrupt();
+			irq_exit();
+			break;
+
+		case IPI_CALL_FUNC_SINGLE:
+			irq_enter();
+			generic_smp_call_function_single_interrupt();
+			irq_exit();
+			break;
+
+		case IPI_CPU_STOP:
+			irq_enter();
+			ipi_cpu_stop(cpu);
+			irq_exit();
+			break;
+#ifdef CONFIG_GLOBAL_POLLING
+		case IPI_GTCORE_SYNC:
+			irq_enter();
+			ipi_gtcore_sync(cpu);
+			irq_exit();
+			break;
+#endif
+		default:
+			printk(KERN_CRIT "CPU%u: Unknown IPI message 0x%x\n",
+				cpu, ipinr);
+			break;
+		}
+	}
 
-	case IPI_CALL_FUNC_SINGLE:
-		irq_enter();
-		generic_smp_call_function_single_interrupt();
-		irq_exit();
-		break;
+asmlinkage void __exception_irq_entry do_IPI(int ipinr, struct pt_regs *regs)
+{
+	unsigned int cpu = smp_processor_id();
+	struct pt_regs *old_regs = set_irq_regs(regs);
 
-	case IPI_CPU_STOP:
-		irq_enter();
-		ipi_cpu_stop(cpu);
-		irq_exit();
-		break;
+	kernel_do_IPI(cpu, ipinr);
 
-	default:
-		printk(KERN_CRIT "CPU%u: Unknown IPI message 0x%x\n",
-		       cpu, ipinr);
-		break;
-	}
 	set_irq_regs(old_regs);
 }
 
@@ -644,3 +665,10 @@ int setup_profiling_timer(unsigned int multiplier)
 {
 	return -EINVAL;
 }
+
+#ifdef CONFIG_GLOBAL_POLLING
+void smp_send_gtcore_sync(int cpu)
+{
+    smp_cross_call(cpumask_of(cpu), IPI_GTCORE_SYNC);
+}
+#endif
diff --git a/arch/arm/kernel/smp_twd.c b/arch/arm/kernel/smp_twd.c
index 2c277d4..261545b 100644
--- a/arch/arm/kernel/smp_twd.c
+++ b/arch/arm/kernel/smp_twd.c
@@ -24,7 +24,7 @@
 /* set up by the platform code */
 void __iomem *twd_base;
 
-static unsigned long twd_timer_rate;
+unsigned long twd_timer_rate;
 
 static void twd_set_mode(enum clock_event_mode mode,
 			struct clock_event_device *clk)
@@ -127,8 +127,12 @@ void __cpuinit twd_timer_setup(struct clock_event_device *clk)
 	twd_calibrate_rate();
 
 	clk->name = "local_timer";
+#ifndef CONFIG_ARCH_TRANSCEDE
 	clk->features = CLOCK_EVT_FEAT_PERIODIC | CLOCK_EVT_FEAT_ONESHOT |
 			CLOCK_EVT_FEAT_C3STOP;
+#else
+	clk->features = CLOCK_EVT_FEAT_PERIODIC | CLOCK_EVT_FEAT_ONESHOT;
+#endif
 	clk->rating = 350;
 	clk->set_mode = twd_set_mode;
 	clk->set_next_event = twd_set_next_event;
diff --git a/arch/arm/kernel/sys_arm.c b/arch/arm/kernel/sys_arm.c
index 0264ab4..fad4a28 100644
--- a/arch/arm/kernel/sys_arm.c
+++ b/arch/arm/kernel/sys_arm.c
@@ -28,13 +28,21 @@
 #include <linux/uaccess.h>
 #include <linux/slab.h>
 
+#ifdef CONFIG_AMP_STACK
+#include <mach/mmu_protect.h>
+#endif	/* CONFIG_AMP_STACK */
+
 /* Fork a new task - this creates a new program thread.
  * This is called indirectly via a small wrapper
  */
 asmlinkage int sys_fork(struct pt_regs *regs)
 {
 #ifdef CONFIG_MMU
+#ifdef CONFIG_AMP_STACK
+	return do_fork(SIGCHLD, regs->ARM_sp, regs, 0, NULL, NULL, MMU_CPU_SMP);
+#else	/* !CONFIG_AMP_STACK */
 	return do_fork(SIGCHLD, regs->ARM_sp, regs, 0, NULL, NULL);
+#endif	/* CONFIG_AMP_STACK */
 #else
 	/* can not support in nommu mode */
 	return(-EINVAL);
@@ -51,12 +59,20 @@ asmlinkage int sys_clone(unsigned long clone_flags, unsigned long newsp,
 	if (!newsp)
 		newsp = regs->ARM_sp;
 
+#ifdef CONFIG_AMP_STACK
+	return do_fork(clone_flags, newsp, regs, 0, parent_tidptr, child_tidptr, MMU_CPU_SMP);
+#else	/* !CONFIG_AMP_STACK */
 	return do_fork(clone_flags, newsp, regs, 0, parent_tidptr, child_tidptr);
+#endif	/* CONFIG_AMP_STACK */
 }
 
 asmlinkage int sys_vfork(struct pt_regs *regs)
 {
+#ifdef CONFIG_AMP_STACK
+	return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, regs->ARM_sp, regs, 0, NULL, NULL, MMU_CPU_SMP);
+#else	/* !CONFIG_AMP_STACK */
 	return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, regs->ARM_sp, regs, 0, NULL, NULL);
+#endif	/* CONFIG_AMP_STACK */
 }
 
 /* sys_execve() executes a new program.
diff --git a/arch/arm/kernel/vmlinux.lds.S b/arch/arm/kernel/vmlinux.lds.S
index e5287f2..d192070 100644
--- a/arch/arm/kernel/vmlinux.lds.S
+++ b/arch/arm/kernel/vmlinux.lds.S
@@ -73,7 +73,9 @@ SECTIONS
 		INIT_CALLS
 		CON_INITCALL
 		SECURITY_INITCALL
+#ifndef CONFIG_ARCH_TRANSCEDE
 		INIT_RAM_FS
+#endif
 
 #ifndef CONFIG_XIP_KERNEL
 		__init_begin = _stext;
@@ -108,6 +110,7 @@ SECTIONS
 #endif
 	}
 
+	. = ALIGN(65536);
 	.text : {			/* Real text segment		*/
 		_text = .;		/* Text and read-only data	*/
 			__exception_text_start = .;
@@ -201,6 +204,17 @@ SECTIONS
 
 		_edata = .;
 	}
+
+#ifdef CONFIG_ARCH_TRANSCEDE
+	.initramfs : {
+		. = ALIGN(PAGE_SIZE);
+		__initramfs_begin = .;
+		INIT_RAM_FS
+		. = ALIGN(PAGE_SIZE);
+		__initramfs_end = .;
+	}
+#endif
+
 	_edata_loc = __data_loc + SIZEOF(.data);
 
 #ifdef CONFIG_HAVE_TCM
diff --git a/arch/arm/mach-transcede/Kconfig b/arch/arm/mach-transcede/Kconfig
new file mode 100644
index 0000000..baa8871
--- /dev/null
+++ b/arch/arm/mach-transcede/Kconfig
@@ -0,0 +1,244 @@
+if ARCH_TRANSCEDE
+
+menu "Transcede Implementation Options"
+
+choice
+	prompt "Transcede System Type"
+	default MACH_M84XXX
+
+config MACH_M84XXX
+	bool "M840xx"
+	select CPU_V7
+	select ARM_GIC
+	select SMP
+	select LOCAL_TIMERS
+	select TRANSCEDE_UART0_SUPPORT
+	help
+	  Say Y here if you intend to run this kernel with a Transcede 4000 device.
+
+config MACH_M822XX
+        bool "M822xx"
+        select CPU_V7
+        select ARM_GIC
+        select SMP
+        select LOCAL_TIMERS
+        select TRANSCEDE_UART0_SUPPORT
+	select MIGHT_HAVE_PCI
+	select ARCH_SUPPORTS_MSI
+	select ARCH_REQUIRE_GPIOLIB
+        help
+          Say Y here if you intend to run this kernel with a Transcede 2200 device.
+
+endchoice
+
+config TRANSCEDE_PCI_USE_APBB
+	bool "PCI Direct access through APBB"
+	depends on PCI && MACH_M822XX
+	default n
+
+config TRANSCEDE_PCI_TX_DMA_ARAM
+	bool "PCI TX DMA from ARAM"
+	depends on PCI && MACH_M822XX
+	default n
+	help
+	  Say Y if you want data to be copied to ARAM before the PCI device does Tx DMA.
+	  This will modify the pci_map_single()/pci_unmap_single API's to try to allocate
+	  a block of free ARAM memory and copy the data inside.
+
+config TRANSCEDE_PCI_SINGLE_ACCESS_TYPE
+	bool "HOST PCI does only Memory access through APBB"
+	depends on TRANSCEDE_PCI_USE_APBB
+	default n
+	help
+	  This is highly experimental. The idea is to configure the PCI
+	  in direct access mode and for one access type (ie memory access).
+	  This setting remove all the write/read and io_remap redirection done
+	  in io.h file. In order to use this mode, the PCI driver MUST only do
+	  memory accesses.
+	  If your are not sure just say no.
+
+config TRANSCEDE_PCI_DEBUG
+	bool "PCI Debugging"
+	depends on PCI
+	default n
+
+config TRANSCEDE_TDM_CLOCK
+        bool "Transcede device TDM clock and frame sync control through sysfs"
+	default y
+	help
+	  Say Y if you intend to use the Transcede TDM and be able to change
+	  different parameters through sysfs.
+
+config TRANSCEDE_DUALCORE
+	bool "Support Upper ARM on the Dual Core"
+	depends on MACH_M84XXX || (MACH_M822XX && !RTSM_ONLY)
+	default y
+	help
+	  Say Y if you intend to use the Transcede Dual Core ARM for Upper ARM (Linux) (Else use Quad Core for Upper ARM).
+
+config TRANSCEDE_UART0_SUPPORT
+	bool
+	default n
+
+config TRANSCEDE_UART1_SUPPORT
+	bool "Enable UART1"
+	default n
+	help
+	  Say Y if you plan to use UART1 exclusively from Linux (and it is not used from CEVA or L-ARM)
+
+config TRANSCEDE_UART2_SUPPORT
+	bool
+	default n
+
+config TRANSCEDE_IPSEC_HW_SUPPORT
+        bool "Support Hardware offload for IPSEC"
+	default y
+	help
+	  Say Y if you intend to use the onboard IPSEC hw device
+
+config TRANSCEDE_ELP_SPACC
+        bool "Elliptic SPAcc driver"
+	depends on MACH_M822XX && !MACH_M84XXX
+	default n
+	help
+	  Say Y if you intend to use the SPAcc SDK from Elliptic
+
+config TRANSCEDE_ELP_CLP30
+	bool "IPSEC driver (CLP30)"
+	depends on MACH_M822XX || MACH_M84XXX
+	default n
+	help
+	  Say Y if you want to see Elliptic CLP30 driver for IPSEC.
+
+config IPSEC_DMA_MAP_HACK_TX
+	bool "Unsafe performance hack for IPSec (don't perform DMA unmap/map on TX)"
+	depends on TRANSCEDE_ELP_CLP30
+	default n
+	help
+	  Say Y if you want to enable unsafe performance hack for IPSec.
+
+config IPSEC_DMA_MAP_HACK_RX
+	bool "Unsafe performance hack for IPSec (don't perform DMA unmap/map on RX)"
+	depends on TRANSCEDE_ELP_CLP30 && MACH_M822XX
+	default n
+	help
+	  Say Y if you want to enable unsafe performance hack for IPSec.
+
+config CLP30_SW_FIFO
+	bool "Enable SW FIFO"
+	depends on TRANSCEDE_ELP_CLP30
+	default n
+	help
+	    SW FIFO should decrease packet loss when traffic bursts happen.
+	    But on the other hand it might introduce extra latency and lower throughput.
+
+config TRANSCEDE_ELP_PDU
+	bool "Elliptic PDU module"
+	depends on MACH_M822XX || MACH_M84XXX
+	default n
+	help
+	  Say Y if you want to use Elliptic PDU module.
+
+config TRANSCEDE_GEMAC_0
+        bool "Ethernet #0 support"
+	depends on MACH_M84XXX || MACH_M822XX
+        default y
+        help
+          Say Y if you intend to use the Transcede GEMAC #0 interface
+
+config TRANSCEDE_GEMAC_1
+        bool "Ethernet #1 support"
+        default y
+        help
+          Say Y if you intend to use the Transcede GEMAC #1 interface
+
+config TRANSCEDE_GEM_PHY
+	bool "MDIO bus support"
+	default y
+	help
+	  Say Y if you intend to use the GEMAC MDIO interface
+
+config MTD_NAND_TRANSCEDE
+        bool "NAND onboard flash support"
+        default y
+        help
+          Say Y if you intend to use the onboard NAND flash device
+
+config MTD_TRANSCEDE_NOR_8
+        bool "NOR8 onboard flash support"
+        default n
+        help
+          Say Y if you intend to use the onboard NOR8 flash device
+
+config MTD_TRANSCEDE_NOR_16
+        bool "NOR16 onboard flash support"
+        default y
+        help
+          Say Y if you intend to use the onboard NOR16 flash device
+
+config ASMP_CORE_STARTUP
+	bool "Asymmetric Boot Loader"
+        depends on MACH_M822XX
+	default n
+	help
+	  Say y if you intend to use asymmetric boot loader on core #1
+
+config AMP_STACK
+	bool "Asymmetric stack for kernel threads"
+        depends on MACH_M822XX
+	default n
+	help
+	  Say y if you intend to use asymmetric stack for kernel threads
+
+config RTSM_ONLY
+	bool "RTSM only build"
+	depends on MACH_M822XX
+	default n
+	help
+		Say y if you need to build kernel for RTSM. If enabled then kernel would not work on real HW.
+
+config TRANSCEDE_MLOG
+	bool "Support for MLog functions from kernel"
+	depends on MACH_M822XX || MACH_M84XXX
+	default n
+	help
+	  Say Y if you intend to use the MLog for gathering debug information.
+
+config TRANSCEDE_MLOG_KERNEL_SCHEDULER
+        bool "Enable MLog in the kernel scheduler"
+        depends on TRANSCEDE_MLOG
+        default n
+        help
+          Say Y if you want to see mlog task from the kernel scheduler.
+
+config TRANSCEDE_MLOG_SEPARATE_IDLE
+	bool "Add separate idle process (id=-1) in MLog"
+	depends on TRANSCEDE_MLOG_KERNEL_SCHEDULER 
+	default n
+	help
+	  Say Y if you want to see separate process (id=-1) for idle loop, otherwise it will be seen as part of pid=0.
+
+config TRANSCEDE_ELP_TRNG
+	bool "Support for Elliptic True Random Generator (TRNG)"
+	depends on MACH_M822XX || MACH_M84XXX
+	default y
+	help
+	  Say Y if you intend to use the TRNG. This option adds platform device 'trng'. It requires driver to be loaded to work properly. 'spacc_load.sh' script should load TRNG dvires as well as SPAcc and IPSec. Driver creates /dev/hw_random device which returns random sequence when read. (e.g. with command 'dd if=/dev/hw_random of=out.bin count=1')
+
+config TRANSCEDE_MIPS_MONITOR
+	bool "Enable MIPS monitor module"
+	depends on MACH_M822XX  
+	default n
+	help
+	  Say Y if you want to enable monitor free MIPS.
+
+config TRANSCEDE_MTU_CTRL_ENABLED
+	bool "To allow change GEMAC MTU size"
+	depends on MACH_M822XX
+	default y
+	help
+	  Say Y if you intend to change MTU size and to use jumbo frames.
+
+endmenu
+
+endif
diff --git a/arch/arm/mach-transcede/Makefile b/arch/arm/mach-transcede/Makefile
new file mode 100644
index 0000000..5c60475
--- /dev/null
+++ b/arch/arm/mach-transcede/Makefile
@@ -0,0 +1,17 @@
+#
+# Makefile for the linux kernel.
+#
+
+obj-$(CONFIG_MACH_M84XXX)		+= transcede-4000.o l-arm.o u-arm.o sysheap.o syslib.o
+obj-$(CONFIG_MACH_M822XX)		+= transcede-2200.o l-arm.o u-arm.o sysheap.o iccom.o revision.o clkrst.o ddr_protection.o syslib.o
+obj-$(CONFIG_GLOBAL_POLLING)	+= periodic_task.o
+obj-$(CONFIG_SMP)			+= platsmp.o headsmp.o
+obj-$(CONFIG_ASMP_CORE_STARTUP)         += headamp.o
+obj-$(CONFIG_TRANSCEDE_MLOG)		+= mlog.o
+obj-$(CONFIG_AMP_STACK)                 += mmu_protect.o
+obj-$(CONFIG_TRANSCEDE_MIPS_MONITOR)    += mips_monitor.o
+
+ifeq ($(CONFIG_PCI),y)
+	obj-$(CONFIG_MACH_M822XX)   += pcie-t2200.o serdes-t2200.o
+endif
+
diff --git a/arch/arm/mach-transcede/Makefile.boot b/arch/arm/mach-transcede/Makefile.boot
new file mode 100644
index 0000000..67039c3
--- /dev/null
+++ b/arch/arm/mach-transcede/Makefile.boot
@@ -0,0 +1,3 @@
+   zreladdr-y	:= 0x00008000
+params_phys-y	:= 0x00000100
+initrd_phys-y	:= 0x00800000
diff --git a/arch/arm/mach-transcede/clkrst.c b/arch/arm/mach-transcede/clkrst.c
new file mode 100644
index 0000000..4599b4c
--- /dev/null
+++ b/arch/arm/mach-transcede/clkrst.c
@@ -0,0 +1,411 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#include <linux/kernel.h>
+#include <asm/div64.h>
+#include <mach/clkrst.h>
+#include <mach/transcede-2200.h>
+
+// Generic System Error Codes (0x00..0xFF)
+#define SYSRC_SUCCESS			0x00000000
+#define SYSRC_INVALID_DEVICE_ID		0x00000014
+
+#define _ASSERT_PTR(Ptr)	WARN_ON(!Ptr)
+#define _ASSERT(x)		WARN_ON(!x)
+#define _ASSERT_RC(rc)		WARN_ON(x)
+
+static UINT32 GetDevAddrReg(UINT32 nDevID, PUINT32 ctrl, PUINT32 ctrlDiv, PUINT32 rst);
+
+inline UINT32 ClkRstGetRefClock (void)
+{
+    return 25*1000*1000;
+}
+
+UINT32 ClkGetSrcPll(UINT32 nDevID) /* in progress */
+{
+    UINT32 rc;
+    UINT32 nPllID;
+    UINT32 clkCtrlReg;
+    PUINT32 pClkCtrlReg;
+
+
+    rc = GetDevAddrReg(nDevID, &clkCtrlReg, NULL, NULL);
+    if (rc != SYSRC_SUCCESS)
+    {
+        printk("GetDevAddrReg: failed\n");
+        _ASSERT(0);
+    }
+
+    pClkCtrlReg = (UINT32 *) clkCtrlReg;
+
+    nPllID = (*pClkCtrlReg >> 1) & 0x07;
+
+    return nPllID;
+}
+
+bool ClkRstIsPllInReset (UINT32 nDevID)
+{
+    UINT32 nCtrl = 0;
+
+    nCtrl = REG32(PLL_CNTRL(nDevID));
+
+    if (nCtrl & (1 << 0)/* || nCtrl & (1 << 3)*/)
+        return 1;
+
+    return 0;
+}
+
+
+UINT32 ClkRstGetPllFreq (UINT32 nPllID)
+{
+    UINT64 nFrq = 0;
+
+    UINT32 nCtrl = REG32(PLL_CNTRL(nPllID));
+
+    UINT32 nVal, p, s;
+
+    if (nPllID > CR_PLL_3)
+        return 0;
+
+    // if in reset or in PowerDown mode
+    if (ClkRstIsPllInReset(nPllID))
+        return 0;
+
+    nFrq = ClkRstGetRefClock();   /* need to check */
+
+    nVal = REG32(PLL_M_LSB (nPllID)) & 0xFF;
+    nVal |= (REG32(PLL_M_MSB (nPllID)) & 0x1F) << 8;
+
+
+    nFrq *= nVal;
+
+    p = REG32(PLL_P (nPllID)) & 0x3F;
+    s = (REG32(PLL_S (nPllID)) & 0x07) - 1;
+
+    // nFrq /= (p * (1 << s));
+    do_div(nFrq, (p * (1 << s)));
+
+    // if bypass mode
+    if (nCtrl & (1 << 4))
+        return ClkRstGetRefClock();
+
+    // in this case the PLL output freq. = REF * M / P / 2^S
+
+    return (UINT32)nFrq;
+
+}
+
+UINT32 ClkRstGetFreq (UINT32 nDevID)
+{
+    UINT32 nDiv = 1;
+    UINT32 rc;
+    UINT32 clkCtrlDivReg;
+    PUINT32 pClkCtrlDivReg;
+
+    if (nDevID <= CR_PLL_3)
+        return ClkRstGetPllFreq(nDevID-CR_PLL_0);
+
+    rc = GetDevAddrReg(nDevID, NULL, &clkCtrlDivReg, NULL);
+    if (rc != SYSRC_SUCCESS)
+    {
+        _ASSERT(0);
+        return 0;
+    }
+
+    pClkCtrlDivReg = (UINT32 *) clkCtrlDivReg;
+
+    switch(nDevID)
+    {
+        case CR_CA9_MC_MPU_ACP:
+            nDiv = (*pClkCtrlDivReg & (1 << 7))? 1 :( (*pClkCtrlDivReg >> 4) & 0x07);
+            break;
+
+        case CR_CA9_MC_MPU_PERIPH:
+            nDiv = (*pClkCtrlDivReg & (1 << 3))? 1 :( *pClkCtrlDivReg & 0x07);
+            break;
+
+
+        case CR_CEVA_BM_AXI:
+            nDiv = (*pClkCtrlDivReg & (1 << 3))? 1 :( *pClkCtrlDivReg & 0x07);
+            break;
+
+        case CR_CEVA_BM_AHB:
+            nDiv = (*pClkCtrlDivReg >> 4) & 0x07;
+            break;
+
+        case CR_CRP_BM:
+            nDiv = *pClkCtrlDivReg  & 0x07;
+            break;
+
+        case CR_FFT_BM:
+            nDiv = (*pClkCtrlDivReg & (1 << 3))? 1 :( *pClkCtrlDivReg & 0x07);
+            break;
+
+        case CR_CA9_MC_CPU0:
+        case CR_CA9_MC_CPU1:
+            nDiv = 1;
+            break;
+
+        case CR_FEC_DL:
+            // due to HW, the divider is 1 , even if it's read like 2 because bit 8 - bypass read as 0 always
+            nDiv = 1;//(*pClkCtrlDivReg & (1 << 7)) ? 1 :( *pClkCtrlDivReg & 0x1F);
+            break;
+#ifdef PLLS_SETUP_FOR_CHARTERIZATION
+        case CR_FEC_UL:
+        case CR_IPSEC:
+        case CR_SPACC:
+            // due to HW, the divider is 1 , even if it's read like 2 because bit 8 - bypass read as 0 always
+            nDiv = 1;
+            break;
+#endif
+        case CR_CA9_MC:
+            // due to HW, the divider is 1 always, even if it's read like 2
+            nDiv = 1;//(*pClkCtrlDivReg & (1 << 7)) ? 1 :( *pClkCtrlDivReg & 0x1F);
+            break;
+
+        case CR_CEVA:
+            // due to HW, the divider is 1 always, even if it's read like 2
+            nDiv = 1;//(*pClkCtrlDivReg & (1 << 7)) ? 1 :( *pClkCtrlDivReg & 0x1F);
+            break;
+
+        case CR_FFT:
+            // due to HW, the divider is 1 always, even if it's read like 2
+            nDiv = 1;
+            break;
+
+        default:
+            nDiv = (*pClkCtrlDivReg & (1 << 7)) ? 1 :(*pClkCtrlDivReg & 0x1F);
+            break;
+    }
+
+
+    return ClkRstGetPllFreq(ClkGetSrcPll(nDevID))/nDiv;
+}
+
+
+static UINT32 GetDevAddrReg(UINT32 nDevID, PUINT32 ctrl, PUINT32 ctrlDiv, PUINT32 rst)
+{
+    VUINT32 *pClkCtrlReg = NULL, *pClkCtrlDivReg = NULL, *pResetReg = NULL;
+
+#ifdef CLKRST_DEBUG
+    uart_printf("GetDevAddrReg: nDevID = %d ctrl = 0x%x ctrlDiv = 0x%x rst = 0x%x\n",  nDevID, ctrl, ctrlDiv, rst);
+#endif
+
+    switch(nDevID)
+    {
+        case CR_CA9_MC_MPU_ACP:
+        case CR_CA9_MC_MPU_PERIPH:
+            pClkCtrlReg =(UINT32 *) CA9_MC_CLK_CNTRL;
+            pClkCtrlDivReg =(UINT32 *) CA9_MC_MPU_CLK_DIV_CNTRL;
+            pResetReg = (UINT32 *) CA9_MC_MPU_RESET;
+            break;
+
+        case CR_CA9_MC_CPU0:
+        case CR_CA9_MC_CPU1:
+            pClkCtrlReg =(UINT32 *) CA9_MC_CPU_CLK_CNTRL;
+            pResetReg = (UINT32 *) CA9_MC_CPU_RESET;
+            break;
+
+        case CR_CA9_MC:
+            pClkCtrlReg = (UINT32 *) CA9_MC_CLK_CNTRL;
+            pClkCtrlDivReg = (UINT32 *) CA9_MC_CLK_DIV_CNTRL;
+            pResetReg = (UINT32 *) CA9_MC_RESET;
+            break;
+
+        case CR_L2CC:
+            pClkCtrlReg = (UINT32 *) L2CC_CLK_CNTRL;
+            pClkCtrlDivReg = (UINT32 *) L2CC_CLK_DIV_CNTRL;
+            pResetReg = (UINT32 *) L2CC_RESET;
+            break;
+
+        case CR_TPI:
+            pClkCtrlReg = (UINT32 *) TPI_CLK_CNTRL;
+            pClkCtrlDivReg = (UINT32 *) TPI_CLK_DIV_CNTRL;
+            pResetReg = (UINT32 *) TPI_RESET;
+            break;
+
+        case CR_CSYS:
+            pClkCtrlReg = (UINT32 *) CSYS_CLK_CNTRL;
+            pClkCtrlDivReg = (UINT32 *) CSYS_CLK_DIV_CNTRL;
+            pResetReg = (UINT32 *) CSYS_RESET;
+            break;
+
+        case CR_EXTPHY0:
+            pClkCtrlReg = (UINT32 *) EXTPHY0_CLK_CNTRL;
+            pClkCtrlDivReg = (UINT32 *) EXTPHY0_CLK_DIV_CNTRL;
+            pResetReg = (UINT32 *) EXTPHY0_RESET;
+            break;
+
+        case CR_EXTPHY1:
+            pClkCtrlReg = (UINT32 *) EXTPHY1_CLK_CNTRL;
+            pClkCtrlDivReg = (UINT32 *) EXTPHY1_CLK_DIV_CNTRL;
+            pResetReg = (UINT32 *) EXTPHY1_RESET;
+            break;
+
+        case CR_FEC_UL:
+            pClkCtrlReg = (UINT32 *) FEC_UL_CLK_CNTRL;
+            pClkCtrlDivReg = (UINT32 *) FEC_UL_CLK_DIV_CNTRL;
+            pResetReg = (UINT32 *) FEC_UL_RESET;
+            break;
+
+        case CR_FEC_DL:
+            pClkCtrlReg = (UINT32 *) FEC_DL_CLK_CNTRL;
+            pClkCtrlDivReg = (UINT32 *) FEC_DL_CLK_DIV_CNTRL;
+            pResetReg = (UINT32 *) FEC_DL_RESET;
+            break;
+
+        case CR_FFT:
+            pClkCtrlReg = (UINT32 *) FFT_CLK_CNTRL;
+            pClkCtrlDivReg = (UINT32 *) FFT_CLK_DIV_CNTRL;
+            pResetReg = (UINT32 *) FFT_RESET;
+            break;
+
+        case CR_IPSEC:
+            pClkCtrlReg = (UINT32 *) IPSEC_CLK_CNTRL;
+            pClkCtrlDivReg = (UINT32 *) IPSEC_CLK_DIV_CNTRL;
+            pResetReg = (UINT32 *) IPSEC_RESET;
+            break;
+
+        case CR_DDR3:
+            pClkCtrlReg = (UINT32 *) DDR3_CLK_CNTRL;
+            pClkCtrlDivReg = (UINT32 *) DDR3_CLK_DIV_CNTRL;
+            pResetReg = (UINT32 *) DDR3_RESET;
+            break;
+
+        case CR_GEMTX:
+            pClkCtrlReg = (UINT32 *) GEMTX_CLK_CNTRL;
+            pClkCtrlDivReg = (UINT32 *) GEMTX_CLK_DIV_CNTRL;
+            pResetReg = (UINT32 *) GEMTX_RESET;
+            break;
+
+        case CR_TDMNTG:
+            pClkCtrlReg = (UINT32 *) TDMNTG_REF_CLK_CNTRL;
+            pClkCtrlDivReg = (UINT32 *) TDMNTG_REF_CLK_DIV_CNTRL;
+            pResetReg = (UINT32 *) TDMNTG_RESET;
+            break;
+
+        case CR_TSUNTG:
+            pClkCtrlReg = (UINT32 *) TSUNTG_REF_CLK_CNTRL;
+            pClkCtrlDivReg = (UINT32 *) TSUNTG_REF_CLK_DIV_CNTRL;
+            pResetReg = (UINT32 *) TSUNTG_RESET;
+            break;
+
+        case CR_CRP:
+           pClkCtrlReg = (UINT32 *) CRP_CLK_CNTRL;
+            pClkCtrlDivReg = (UINT32 *) CRP_CLK_DIV_CNTRL;
+            pResetReg = (UINT32 *) CRP_CLK_RESET;
+            break;
+
+        case CR_CEVA:
+            pClkCtrlReg = (UINT32 *) CEVA_CLK_CNTRL;
+            pClkCtrlDivReg = (UINT32 *) CEVA_CLK_DIV_CNTRL;
+            pResetReg = (UINT32 *) CEVA_CLK_RESET;
+            break;
+
+        case CR_SPACC:
+            pClkCtrlReg = (UINT32 *) SPACC_CLK_CNTRL;
+            pClkCtrlDivReg = (UINT32 *) SPACC_CLK_DIV_CNTRL;
+            pResetReg = (UINT32 *) SPACC_RESET;
+            break;
+
+        case CR_SASPA:
+            pClkCtrlReg = (UINT32 *) SASPA_CLK_CNTRL;
+            pClkCtrlDivReg = (UINT32 *) SASPA_CLK_DIV_CNTRL;
+            pResetReg = (UINT32 *) SASPA_RESET;
+            break;
+
+        case CR_SYS_AXI:
+            pClkCtrlReg = (UINT32 *) AXI_CLK_CNTRL_0;
+            pClkCtrlDivReg = (UINT32 *) AXI_CLK_DIV_CNTRL;
+            break;
+
+        case CR_MDMA_SYS1:
+        case CR_MDMA_SYS0:
+            pResetReg = (UINT32 *) AXI_RESET_0;
+            pClkCtrlReg = (UINT32 *) AXI_CLK_CNTRL_0;
+            pClkCtrlDivReg = (UINT32 *) AXI_CLK_DIV_CNTRL;
+            break;
+
+        case CR_TIMERS:
+        case CR_UART:
+        case CR_I2C_SPI:
+        case CR_DUS:
+            pResetReg = (UINT32 *) AXI_RESET_1;
+            pClkCtrlReg = (UINT32 *) AXI_CLK_CNTRL_1;
+            pClkCtrlDivReg = (UINT32 *) AXI_CLK_DIV_CNTRL;
+            break;
+
+        case CR_TBD:
+        case CR_GEM0:
+        case CR_GEM1:
+        case CR_USB_MISC:
+        case CR_USB:
+        case CR_JESD207:
+        case CR_CPRI:
+        case CR_PCI:
+            pResetReg = (UINT32 *) AXI_RESET_2;
+            pClkCtrlReg = (UINT32 *) AXI_CLK_CNTRL_1;
+            pClkCtrlDivReg = (UINT32 *) AXI_CLK_DIV_CNTRL;
+            break;
+
+        case CR_CEVA_BM_AHB:
+        case CR_CEVA_BM_AXI:
+
+            pClkCtrlReg = (UINT32 *) CEVA_CLK_CNTRL;
+            pClkCtrlDivReg =(UINT32 *) CEVA_BM_CLK_DIV_CNTRL;
+            break;
+
+        case CR_CRP_BM:
+            pClkCtrlReg =(UINT32 *) CRP_BM_CLK_CNTRL;
+            pClkCtrlDivReg =(UINT32 *) CRP_BM_CLK_DIV_CNTRL;
+            break;
+
+        case CR_FFT_BM:
+            pClkCtrlReg =(UINT32 *) FFT_BM_CLK_CNTRL;
+            pClkCtrlDivReg =(UINT32 *) FFT_BM_CLK_DIV_CNTRL;
+            break;
+
+        default:
+             return  SYSRC_INVALID_DEVICE_ID;
+
+    }
+
+    if (ctrl != NULL)
+    {
+        _ASSERT_PTR(pClkCtrlReg);
+        *ctrl = (UINT32) pClkCtrlReg;
+    }
+
+    if (ctrlDiv != NULL)
+    {
+        _ASSERT_PTR(pClkCtrlDivReg);
+        *ctrlDiv = (UINT32) pClkCtrlDivReg;
+    }
+
+    if (rst != NULL)
+    {
+        _ASSERT_PTR(pResetReg);
+        *rst = (UINT32) pResetReg;
+    }
+
+    return SYSRC_SUCCESS;
+}
diff --git a/arch/arm/mach-transcede/core.h b/arch/arm/mach-transcede/core.h
new file mode 100644
index 0000000..7a2c7bd
--- /dev/null
+++ b/arch/arm/mach-transcede/core.h
@@ -0,0 +1,30 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef __ASM_ARCH_CORE_H
+#define __ASM_ARCH_CORE_H
+
+#include <linux/io.h>
+
+extern void __iomem *gic_cpu_base_addr;
+
+#endif
diff --git a/arch/arm/mach-transcede/ddr_protection.c b/arch/arm/mach-transcede/ddr_protection.c
new file mode 100644
index 0000000..62d96aa
--- /dev/null
+++ b/arch/arm/mach-transcede/ddr_protection.c
@@ -0,0 +1,645 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#include <linux/module.h>
+#include <linux/io.h>
+#include <linux/proc_fs.h>
+#include <linux/slab.h>
+#include <linux/interrupt.h>
+#include <linux/bitops.h>
+
+#include <asm/uaccess.h>
+
+#include <mach/hardware.h>
+#include <mach/irqs.h>
+#include <mach/revision.h>
+#include <mach/ddr_protection.h>
+
+static unsigned int ddr_protection_state = 0;
+static struct proc_dir_entry *ddr_proc;
+static struct proc_dir_entry *state_proc;
+static uint32_t acs = 1;	// should be 4 for X1
+
+/**
+ * 
+ *  Master BM  SYS BM AXI   HW blocks                                    DDR port fixed/configurable  Assigned DDR port  
+ *             master port                                               and route                    (fixed or default) 
+ * 
+ *  RAD BM     4            CPRI, JESD (2x)                              Fixed via SYS BM             0                  
+ *  AHB BM     5            USB, A9 Corsight, UART0 DMA, SPI0 DMA, TDMA  Config-gp1[8] via SYS BM     0                  
+ *  PCIe BM    7            4 lane PCIe, 1 lane PCIe, GEM0 (SGMII)       Config-gp1[10] vi SYS BM     0                  
+ *  FEC BM     2            FEC UL, FEC DL,                              Config-gp1[9] via SYS BM     1                  
+ *  GEM BM     6            GEM1(RGMII), SYS-MDMA (2x), Security         Fixed via SYS BM             1                  
+ *  Ceva BM    1            CEVA, FFT, BP, CRP-DMA                       Fixed via SYS BM             2                  
+ *  CA9 BM     3            L2CC M0                                      Fixed and direct             3                  
+ * 
+ **/
+
+#define L2CC_BASE                       0xFFF10000
+#define L2CC_FILTER_END                 (L2CC_BASE + 0xC04)
+
+static unsigned int ddr_prot_limit = 1 * 1024*1024*1024; // 1G is default limitation despite on uboot configuration
+
+static int __init ddr_prot_limit_change(char *str)
+{
+    ddr_prot_limit = memparse(str, NULL);
+    if (ddr_prot_limit == 0)
+	printk ("DDR protection: DDR limit is removed, uboot cfg is used\n");
+    else
+	printk ("DDR protection: DDR limit size is set to %dMB\n", ddr_prot_limit/(1024*1024));
+    return 0;
+}
+
+__setup("ddr_limit=", ddr_prot_limit_change);
+
+static uint GET_DDR_PHYS_END(void)
+{
+    uint end = REG32(L2CC_FILTER_END);
+    return end;
+}
+
+/** @brief This function calculates the end of DDR memory by using HW registers and real configuration,
+	    also this function is designed to take into account <bootarg:ddr_limit=xxx> parameter used like ddr limitation
+	    for testing and emulating of customer boards with 1G ddr for example
+*/
+static uint GET_DDR_LOGIC_END(void)
+{
+    uint end = GET_DDR_PHYS_END();
+
+    if (ddr_prot_limit != 0)
+	return min(end, ddr_prot_limit);
+
+    return end;
+}
+
+/**
+ * @brief  Set protection state for all ports.
+ *
+ * @param  state  0-disable, 1-enable
+ */
+static void ddr_protect_set_state(unsigned int state)
+{
+	if (get_t2200_rev() <= T2200_REV_X2) {
+		writeb(!!state, DDR0_CONTROLLER_BASE + (0x18F * 4));
+	} else {
+		writeb(!!state, DDR0_CONTROLLER_BASE + 0x18F);
+	}
+}
+
+static int ddr_proc_read_state(char *page, char **start, off_t off,
+                               int count, int *eof, void *data)
+{
+	int len = 0;
+
+	if (off > 0) {
+		return 0;
+	}
+
+	len += sprintf(page + len, "%d\n", ddr_protection_state);
+
+	*start = NULL;
+
+	if (count <= len) {
+		*eof = 1;
+	}
+
+	return len;
+}
+
+static int ddr_proc_write_state(struct file *file, const char __user *buffer,
+                                unsigned long count, void *data)
+{
+	char *tmp = kmalloc(count + 1, GFP_KERNEL);
+
+	if (copy_from_user(tmp, buffer, count)) {
+		printk(KERN_ERR "copy failed: 0x%08lx\n", (unsigned long)buffer);
+		kfree(tmp);
+		return 0;
+	}
+
+	tmp[count] = '\0';
+
+	ddr_protection_state = !!(simple_strtoul(tmp, NULL, 10));
+	ddr_protect_set_state(ddr_protection_state);
+
+	kfree(tmp);
+
+	return count;
+}
+
+/**
+ * @brief  Setup /proc DDR protection port
+*/
+int ddr_protect_set_proc(void)
+{
+	ddr_proc = proc_mkdir("ddr_protection", NULL);
+	state_proc = create_proc_entry("state", 0600, ddr_proc);
+
+	state_proc->read_proc = ddr_proc_read_state;
+	state_proc->write_proc = ddr_proc_write_state;
+	state_proc->data = NULL;
+
+	return 0;
+}
+
+/**
+ * @brief  Set DDR region protection.
+ * Regions on the same port must not overlap!
+ *
+ * @param  port_no      DDR port number to set protection for (0..3)
+ * @param  range_no     range (region) number (0..3)
+ * @param  start_addr   start address of memory region, granularity 16KiB
+ * @param  end_address  end address of region (e.g., 0x03ffffff), granularity 16K
+ * @param  range_type   0-no access, 1-read only, 2-write only, 3-read&write
+ * @param  range_prot   0-privileged&secure, 1-secure, 2-privileged, 3-full access
+*/
+static int protect_region(uint32_t port_no,
+                          uint32_t range_no,
+                          uint32_t start_addr,
+                          uint32_t end_addr,
+                          uint32_t range_type,
+                          uint32_t range_prot)
+{
+	volatile uint8_t *range;
+
+	printk(KERN_DEBUG
+	       "DDR protect: port:%u range:%u type:%u prot:%u = 0x%08x..0x%08x\n",
+	       port_no,
+	       range_no,
+	       range_type,
+	       range_prot,
+	       start_addr,
+	       end_addr);
+
+	if (port_no >= 4 || range_no >= 4 || start_addr > end_addr) {
+		printk(KERN_ERR "error: invalid arg for %s()\n", __func__);
+
+		return -1;
+	}
+
+	/* start of range */
+	range = (volatile uint8_t *) (DDR0_CONTROLLER_BASE + acs *
+	                              (0x190 + port_no * 40 + range_no * 10));
+
+	/* start addr, granularity - 16KB */
+	writeb((start_addr >> 14) & 0xff, &range[acs * 0]);
+	writeb((start_addr >> 22) & 0xff, &range[acs * 1]);
+	writeb((start_addr >> 30) & 0xff, &range[acs * 2]);
+	/* end addr, granularity - 16KB */
+	writeb((end_addr   >> 14) & 0xff, &range[acs * 3]);
+	writeb((end_addr   >> 22) & 0xff, &range[acs * 4]);
+	writeb((end_addr   >> 30) & 0xff, &range[acs * 5]);
+
+	writeb(range_type & 3, &range[acs * 6]);
+	writeb(range_prot & 3, &range[acs * 7]);
+
+	/* same roule for all AXI ID */
+	writeb(0xff, &range[acs * 8]);
+	writeb(0xff, &range[acs * 9]);
+
+	return 0;
+}
+
+/**
+ * @brief  Setup DDR Port0 protection regions:
+ * RAD BM: CPRI, JESD (2x),
+ * AHB BM: TDMA, USB, UART-SPI-DMA, SPI0 DMA, A9 CoreSight
+ * PCIe BM: PCIe (4&1 lane), GEM0 (SGMII),
+ */
+static void ddr_protect_port0(void)
+{
+	/*
+	 * Range 0: Full Access: up to TRANSCEDE_ICC_BASE
+	 * Range 1: Full Access: +TRANSCEDE_ICC_SIZE
+	 * Range 2: Full Access: up to GET_DDR_LOGIC_END() this is needed for PCIe x4 DMA controled by CEVA processor
+	 * Range 3: No access if DDR limit is used
+	 */
+	protect_region(DDR_PORT0, DDR_RANGE0,
+	               TRANSCEDE_DDR_BASE,
+	               TRANSCEDE_ICC_BASE - 1,
+	               3, 3);
+	protect_region(DDR_PORT0, DDR_RANGE1,
+	               TRANSCEDE_ICC_BASE,
+	               TRANSCEDE_ICC_BASE + TRANSCEDE_ICC_SIZE - 1,
+	               3, 3);
+	protect_region(DDR_PORT0, DDR_RANGE2,
+	               TRANSCEDE_ICC_BASE + TRANSCEDE_ICC_SIZE,
+	               GET_DDR_LOGIC_END() - 1,
+	               3, 3);
+
+	if (GET_DDR_LOGIC_END() < GET_DDR_PHYS_END())
+	{
+	protect_region(DDR_PORT0, DDR_RANGE3,
+			GET_DDR_LOGIC_END(),
+			GET_DDR_PHYS_END() - 1,
+	               0, 0);
+	}
+}
+
+/**
+ * @brief  Setup DDR Port1 protection regions:
+ * FEC BM: FEC DL/UL
+ * GEM BM: GEM1 (RGMII), SYS-MDMA (2x), Security
+ */
+static void ddr_protect_port1(void)
+{
+	/*
+	 * Range 0: Full Access: up to TRANSCEDE_ICC_BASE
+	 * Range 1: Full Access: +TRANSCEDE_ICC_SIZE
+	 * Range 2: No Access:   up to GET_DDR_LOGIC_END()
+	 * Range 3: No Access:   up to PHYSICAL END if DDR limit is used
+	 */
+	protect_region(DDR_PORT1, DDR_RANGE0,
+	               TRANSCEDE_DDR_BASE,
+	               TRANSCEDE_ICC_BASE - 1,
+	               3, 3);
+	protect_region(DDR_PORT1, DDR_RANGE1,
+	               TRANSCEDE_ICC_BASE,
+	               TRANSCEDE_ICC_BASE + TRANSCEDE_ICC_SIZE - 1,
+	               3, 3);
+	protect_region(DDR_PORT1, DDR_RANGE2,
+	               TRANSCEDE_ICC_BASE + TRANSCEDE_ICC_SIZE,
+	               GET_DDR_LOGIC_END() - 1,
+	               0, 0);
+	if (GET_DDR_LOGIC_END() < GET_DDR_PHYS_END())
+	{
+		protect_region(DDR_PORT1, DDR_RANGE3,
+			GET_DDR_LOGIC_END(),
+			GET_DDR_PHYS_END() - 1,
+	               0, 0);
+	}
+}
+/**
+ * @brief  Setup DDR Port2 protection regions:
+ * Ceva BM: CEVA, FFT, BP, CRP-DMA
+ */
+static void ddr_protect_port2(void)
+{
+	/*
+	 * Range 0: No Access:   up to TRANSCEDE_DDR_CEVA_SHARED_BASE
+	 * Range 1: Full Access: +TRANSCEDE_DDR_CEVA_SHARED_SIZE
+	 * Range 2: No Access:   up to TRANSCEDE_DDRHEAP_BASE
+	 * Range 3: Full Access: up to Logical DDR end
+	 */
+	protect_region(DDR_PORT2, DDR_RANGE0,
+	               TRANSCEDE_DDR_BASE,
+	               TRANSCEDE_DDR_CEVA_SHARED_BASE - 1,
+	               0, 0);
+	protect_region(DDR_PORT2, DDR_RANGE1,
+	               TRANSCEDE_DDR_CEVA_SHARED_BASE,
+	               TRANSCEDE_DDR_CEVA_SHARED_BASE + TRANSCEDE_DDR_CEVA_SHARED_SIZE - 1,
+	               3, 3);
+	protect_region(DDR_PORT2, DDR_RANGE2,
+	               TRANSCEDE_DDR_CEVA_SHARED_BASE + TRANSCEDE_DDR_CEVA_SHARED_SIZE,
+	               TRANSCEDE_DDRHEAP_BASE - 1,
+	               0, 0);
+	protect_region(DDR_PORT2, DDR_RANGE3,
+	               TRANSCEDE_DDRHEAP_BASE,
+	               GET_DDR_LOGIC_END() - 1,
+	               3, 3);
+}
+
+
+/**
+ * @brief  Setup DDR Port2 protection regions:
+ * CA9 BM: ARM CPU (L1 and L2 cache)
+ */
+static void ddr_protect_port3(void)
+{
+	/*
+	 * Range 0: Full Access: up to TRANSCEDE_DDRHEAP_BASE
+	 * Range 1: Full Access: up to TRANSCEDE_DDRCBHEAP_BASE
+	 * Range 2: Full Access: up to Logical DDR end
+	 * Range 3: No Access:   up to Physical DDR end
+	 */
+	protect_region(DDR_PORT3, DDR_RANGE0,
+	               TRANSCEDE_DDR_BASE,
+	               TRANSCEDE_DDRHEAP_BASE - 1,
+	               3, 3);
+	protect_region(DDR_PORT3, DDR_RANGE1,
+	               TRANSCEDE_DDRHEAP_BASE,
+	               TRANSCEDE_DDRCBHEAP_BASE - 1,
+	               3, 3);
+	protect_region(DDR_PORT3, DDR_RANGE2,
+	               TRANSCEDE_DDRCBHEAP_BASE,
+	               GET_DDR_LOGIC_END() - 1,
+	               3, 3);
+	
+	if (GET_DDR_LOGIC_END() < GET_DDR_PHYS_END())
+	{
+		protect_region(DDR_PORT3, DDR_RANGE3,
+				GET_DDR_LOGIC_END(),
+				GET_DDR_PHYS_END() - 1,
+				0, 0);
+	}
+}
+
+/**
+ * @brief  Converts 'out_of_rang_type' value to readable message.
+ *
+ * @param[in] val	'out_of_rang_type' value to convert
+ *
+ * @return const char*	Decoded message c-string.
+ */
+static const char* out_of_rang_type_message(const unsigned int val)
+{
+	switch (val) {
+	case 0b000000:
+		return "Non-Exclusive Write\n";
+	case 0b000010:
+		return "Non-Exclusive Masked Write";
+	case 0b000100:
+		return "Wrapped Write";
+	case 0b000101:
+		return "Wrapped Read";
+	case 0b000110:
+		return "Wrapped Masked Write";
+	case 0b001000:
+		return "Exclusive Write";
+	case 0b001001:
+		return "Exclusive Read";
+	case 0b001010:
+		return "Exclusive Masked Write";
+	case 0b010000:
+		return "Flushed Write";
+	case 0b100000:
+		return "Non-Exclusive Write with Auto-Precharge";
+	case 0b100001:
+		return "Non-Exclusive Read with Auto-Precharge";
+	case 0b100010:
+		return "Non-Exclusive Masked Write with Auto-Precharge";
+	case 0b100100:
+		return "Wrapped Write with Auto-Precharge";
+	case 0b100101:
+		return "Wrapped Read with Auto-Precharge";
+	case 0b100110:
+		return "Wrapped Masked Write with Auto-Precharge";
+	case 0b101000:
+		return "Exclusive Write with Auto-Precharge";
+	case 0b101001:
+		return "Exclusive Read with Auto-Precharge";
+	case 0b101010:
+		return "Exclusive Masked Write with Auto-Precharge";
+	case 0b110000:
+		return "Flushed Write with Auto-Precharge";
+	default:
+		return "Unexpected reserved value\n";
+	}
+}
+
+/**
+ * @brief  Prints interrupt status register bits meaning if they are active.
+ *
+ * @param[in] int_status[3]	int_status[3] array to get active bits.
+ */
+static void int_status_bits_print(const unsigned char int_status[3])
+{
+	if (BIT(21 - 16) & int_status[2]) {
+		printk("Bit[21] = Logical OR of all lower bits.\n");
+
+		if (BIT(20 - 16) & int_status[2]) printk("Bit[20] = The user-initiated DLL resync has completed.\n");
+		if (BIT(19 - 16) & int_status[2]) printk("Bit[19] = A state change has been detected on the dfi_init_complete signal after initialization.\n");
+		if (BIT(18 - 16) & int_status[2]) printk("Bit[18] = The assertion of the INHIBIT_DRAM_CMD parameter has successfully inhibited the command queue.\n");
+		if (BIT(17 - 16) & int_status[2]) printk("Bit[17] = The register interfaceinitiated mode register write has completed and another mode register write may be issued.\n");
+		if (BIT(16 - 16) & int_status[2]) printk("Bit[16] = The leveling operation has completed.\n");
+
+		if (BIT(15 - 8) & int_status[1]) printk("Bit[15] = A leveling operation has been requested.\n");
+		if (BIT(14 - 8) & int_status[1]) printk("Bit[14] = A DFI update error has occurred. Error information can be found in the UPDATE_ERROR_STATUS parameter.\n");
+		if (BIT(13 - 8) & int_status[1]) printk("Bit[13] = A write leveling error has occurred. Error information can be found in the WRLVL_ERROR_STATUS parameter.\n");
+		if (BIT(12 - 8) & int_status[1]) printk("Bit[12] = A read leveling gate training error has occurred. Error information can be found in the RDLVL_ERROR_STATUS parameter.\n");
+		if (BIT(11 - 8) & int_status[1]) printk("Bit[11] = A read leveling error has occurred. Error information can be found in the RDLVL_ERROR_STATUS parameter.\n");
+		if (BIT(10 - 8) & int_status[1]) printk("Bit[10] = The low power operation has been completed.\n");
+		if (BIT(9  - 8) & int_status[1]) printk("Bit[9] = The MC initialization has been completed.\n");
+		if (BIT(8  - 8) & int_status[1]) printk("Bit[8] = An error occurred on the port data channel.\n");
+
+		if (BIT(7) & int_status[0]) printk("Bit[7] = An error occurred on the port command channel.\n");
+		if (BIT(6) & int_status[0]) printk("Bit[6] = Multiple uncorrectable ECC events have been detected.\n");
+		if (BIT(5) & int_status[0]) printk("Bit[5] = An uncorrectable ECC event has been detected.\n");
+		if (BIT(4) & int_status[0]) printk("Bit[4] = Multiple correctable ECC events have been detected.\n");
+		if (BIT(3) & int_status[0]) printk("Bit[3] = A correctable ECC event has been detected.\n");
+		if (BIT(2) & int_status[0]) printk("Bit[2] = Multiple accesses outside the defined PHYSICAL memory space have occurred.\n");
+		if (BIT(1) & int_status[0]) printk("Bit[1] = A memory access outside the defined PHYSICAL memory space has occurred.\n");
+		if (BIT(0) & int_status[0]) printk("Bit[0] = The memory reset is valid on the DFI bus.\n");
+	}
+}
+
+/**
+ * @brief  Prints 'port_cmd_error' bits explanation.
+ *
+ * @param[in] port_cmd_error_type	Value to read bits from.
+ */
+static void port_cmd_error_type_print(const unsigned char port_cmd_error_type)
+{
+	if (BIT(2) & port_cmd_error_type) printk("Bit[2]: Indicates that a protection error occurred. The error may relate to the address, access type, transaction type, or range ID.\n");
+	if (BIT(1) & port_cmd_error_type) printk("Bit[1]: Narrow transfer requested for a requestor from port Y whose axiY_en_size_lt_width_instr parameter is clear.\n");
+	if (BIT(0) & port_cmd_error_type) printk("Bit[0]: Indicates that a FIXED command was requested on the axiY_ARBURST or axiY_AWBURST signal.\n");
+}
+
+/**
+ * @brief  Prints 'port_cmd_error_id' explanation.
+ *
+ * @param[in] port_cmd_error_id	Value to read Port and Requestor ID from.
+ */
+static void port_cmd_error_id_print(const unsigned char port_cmd_error_id[3])
+{
+	printk("Port ID: 0x%02x", port_cmd_error_id[2]);
+	switch (port_cmd_error_id[2]) {
+	case 0x00:
+		printk(" - RAD BM (CPRI, JESD), PCIe BM (PCIe, GEM0:SGMII), AHB BM (TDMA, USB, UART-SPI-DMA), CoreSight\n");
+		break;
+	case 0x01:
+		printk(" - GEM BM (GEM1:RGMII), SYS-MDMA, Security, FEC BM (FEC DL/UL)\n");
+		break;
+	case 0x02:
+		printk(" - CEVA, FFT\n");
+		break;
+	case 0x03:
+		printk(" - ARM CPU (L1 and L2 cache)\n");
+		break;
+	}
+
+	printk("Requestor ID: 0x%02x%02x\n", port_cmd_error_id[1], port_cmd_error_id[0]);
+}
+
+/**
+ * @brief  Acknowledges all interrupts.
+ */
+static void ddr_controller_int_ack(void)
+{
+	// INT_ACK
+	// DENALI_CTL_224-226:	AHB: 0xe0-e2
+	writeb(0xff, DDR0_CONTROLLER_BASE + acs * 0xe0);
+	writeb(0xff, DDR0_CONTROLLER_BASE + acs * 0xe1);
+	writeb(0xff, DDR0_CONTROLLER_BASE + acs * 0xe2);
+}
+
+/**
+ * @brief  DDR controller IRQ handler.
+ * Dumps relevant registers with explanations.
+ *
+ * @param[in]	irq	IRQ number that we've received.
+ * @param[in]	context	Handler context
+ */
+static irqreturn_t ddr_control_irq_handler(int irq, void *context)
+{
+	unsigned char int_status[3], tmp[3];
+
+	printk(KERN_ERR "DDR controller IRQ received. Dumping related registers...\n");
+
+	// INT_STATUS
+	// DENALI_CTL_221-223	AHB: 0xdd-0xdf
+	printk("INT_STATUS [21:0]\t0x%02x_%02x%02x\n",
+			int_status[2] = readb(DDR0_CONTROLLER_BASE + acs * 0xdf) & 0x3f,
+			int_status[1] = readb(DDR0_CONTROLLER_BASE + acs * 0xde),
+			int_status[0] = readb(DDR0_CONTROLLER_BASE + acs * 0xdd)
+			);
+	int_status_bits_print(int_status);
+
+	// INT_MASK
+	// DENALI_CTL_227-229:	AHB: 0xe3-0xe5
+	printk("INT_MASK [21:0]\t\t0x%02x_%02x%02x\n",
+			readb(DDR0_CONTROLLER_BASE + acs * 0xe5) & 0x3f,
+			readb(DDR0_CONTROLLER_BASE + acs * 0xe4),
+			readb(DDR0_CONTROLLER_BASE + acs * 0xe3)
+			);
+
+	if (BIT(1) & int_status[0]) { // int_status Bit [1] = A memory access outside the defined PHYSICAL memory space has occurred.
+		// OUT_OF_RANGE_ADDR
+		// DENALI_CTL_230-234,	AHB: 0xe6-0xea
+		printk("OUT_OF_RANGE_ADDR [33:0]\t0x%02x_%02x%02x_%02x%02x\n",
+				readb(DDR0_CONTROLLER_BASE + acs * 0xea) & 0x03,
+				readb(DDR0_CONTROLLER_BASE + acs * 0xe9),
+				readb(DDR0_CONTROLLER_BASE + acs * 0xe8),
+				readb(DDR0_CONTROLLER_BASE + acs * 0xe7),
+				readb(DDR0_CONTROLLER_BASE + acs * 0xe6)
+				);
+
+		// OUT_OF_RANGE_LENGTH
+		// DENALI_CTL_235,	AHB: 0xeb
+		printk("OUT_OF_RANGE_LENGTH [6:0]\t0x%02x\n",
+				readb(DDR0_CONTROLLER_BASE + acs * 0xeb) & 0x7f
+				);
+
+		// OUT_OF_RANGE_TYPE
+		// DENALI_CTL_236,	AHB: 0xec
+		tmp[0] = readb(DDR0_CONTROLLER_BASE + acs * 0xec) & 0x3f;
+		printk("OUT_OF_RANGE_TYPE [5:0]\t\t0x%02x (%s)\n",
+				tmp[0],
+				out_of_rang_type_message(tmp[0])
+				);
+
+		// OUT_OF_RANGE_SOURCE_ID
+		// DENALI_CTL_237-239,	AHB: 0xed-0xef
+		printk("OUT_OF_RANGE_SOURCE_ID [17:0]\t0x%02x_%02x%02x\n",
+				readb(DDR0_CONTROLLER_BASE + acs * 0xef) & 0x3,
+				readb(DDR0_CONTROLLER_BASE + acs * 0xee),
+				readb(DDR0_CONTROLLER_BASE + acs * 0xed)
+				);
+	}
+
+	if (BIT(7) & int_status[0]) { // int_status Bit [7] = An error occurred on the port command channel.
+		// PORT_CMD_ERROR_ADDR
+		// DENALI_CTL_240-244,	AHB: 0xf0-0xf4
+		printk("PORT_CMD_ERROR_ADDR [33:0]\t0x%02x_%02x%02x_%02x%02x\n",
+				readb(DDR0_CONTROLLER_BASE + acs * 0xf4) & 0x03,
+				readb(DDR0_CONTROLLER_BASE + acs * 0xf3),
+				readb(DDR0_CONTROLLER_BASE + acs * 0xf2),
+				readb(DDR0_CONTROLLER_BASE + acs * 0xf1),
+				readb(DDR0_CONTROLLER_BASE + acs * 0xf0)
+				);
+
+		// PORT_CMD_ERROR_ID
+		// DENALI_CTL_245-247,	AHB: 0xf5-0xf7
+		printk("PORT_CMD_ERROR_ID [17:0]\t0x%02x_%02x%02x\n",
+				tmp[2] = readb(DDR0_CONTROLLER_BASE + acs * 0xf7) & 0x03,
+				tmp[1] = readb(DDR0_CONTROLLER_BASE + acs * 0xf6),
+				tmp[0] = readb(DDR0_CONTROLLER_BASE + acs * 0xf5)
+				);
+		port_cmd_error_id_print(tmp);
+
+		// PORT_CMD_ERROR_TYPE
+		// DENALI_CTL_248,	AHB: 0xf8
+		printk("PORT_CMD_ERROR_TYPE [2:0]\t0x%02x\n",
+				tmp[0] = readb(DDR0_CONTROLLER_BASE + acs * 0xf8) & 0x07
+				);
+		port_cmd_error_type_print(tmp[0]);
+	}
+
+	if (BIT(8) & int_status[0]) { // int_status Bit [8] = An error occurred on the port data channel.
+		// PORT_DATA_ERROR_ID
+		// DENALI_CTL_249-251,	AHB: 0xf9-0xfb
+		printk("PORT_DATA_ERROR_ID [7:0]\t0x%02x_%02x%02x\n",
+				readb(DDR0_CONTROLLER_BASE + acs * 0xfb) & 0x03,
+				readb(DDR0_CONTROLLER_BASE + acs * 0xfa),
+				readb(DDR0_CONTROLLER_BASE + acs * 0xf9)
+				);
+	}
+
+	ddr_controller_int_ack();
+
+	return IRQ_HANDLED;
+}
+
+void ddr_protect_ports(void)
+{
+	// This function should be called first so that 'acs' is set to valid value
+	if (get_t2200_rev() <= T2200_REV_X2) {
+		acs = 4;
+	} else {
+		acs = 1;
+	}
+
+	printk(KERN_INFO "DDR protect ports for rev: %d\n", get_t2200_rev());
+	printk(KERN_INFO "DDR protection limit is set to %dMB, DDR logical end:0x%x, DDR physical end:0x%x\n", ddr_prot_limit / (1024*1024), GET_DDR_LOGIC_END(), GET_DDR_PHYS_END());
+
+	ddr_protect_disable();
+	ddr_protect_port0();
+	ddr_protect_port1();
+	ddr_protect_port2();
+	ddr_protect_port3();
+
+	ddr_controller_int_ack();	// Clean all previous interrupts we might have.
+	printk(KERN_INFO "DDR protection IRQ registering\n");
+	if (request_irq(IRQ_DDR_CTRL, ddr_control_irq_handler, IRQF_NO_SOFTIRQ_CALL, "ddr_ctrl", NULL)) {
+		printk(KERN_INFO "DDR CTRL IRQ (%d) requesting failed\n", IRQ_DDR_CTRL);
+	}
+
+	ddr_protect_enable();
+}
+
+/**
+ * @brief  Enable protection for all ports.
+ */
+void ddr_protect_enable(void)
+{
+	ddr_protection_state = 1;
+	ddr_protect_set_state(ddr_protection_state);
+}
+
+/**
+ * @brief  Disable protection for all ports.
+ */
+void ddr_protect_disable(void)
+{
+	ddr_protection_state = 0;
+	ddr_protect_set_state(ddr_protection_state);
+}
diff --git a/arch/arm/mach-transcede/headamp.S b/arch/arm/mach-transcede/headamp.S
new file mode 100644
index 0000000..7e0dec2
--- /dev/null
+++ b/arch/arm/mach-transcede/headamp.S
@@ -0,0 +1,113 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#include <linux/linkage.h>
+#include <linux/init.h>
+
+#include <asm/assembler.h>
+#include <asm/domain.h>
+#include <asm/asm-offsets.h>
+#include <asm/memory.h>
+#include <asm/system.h>
+
+	.text
+
+	/*
+	 * Asymmetric core entry point
+	 *
+	 * The core configured as AMP goes here just after Boot Monitor
+	 *  so it operated on Boot Monitor Stack, supervisor mode
+	 *  no MMU translation, thus no direct access to kernel data
+	 */
+ENTRY(asmp_core_startup)
+	mov	r0, #-1			@ release holding pen
+	str	r0, [r6]
+	mov	r0, #0
+	mcr	p15, 0, r0, c7, c10, 5	@ write memory barrier
+
+	adr	r4, 4f
+	ldmia	r4, {r5, r6}
+	sub	r4, r4, r5
+	add	r6, r6, r4
+	adr	r0, reset
+	str	r0, [r6]		@ store asmp_core_reset with reset handler
+
+
+	/*
+         * Common entry point for low arm CPUs.
+	 *
+	 * Ensure that we're in SVC mode, and IRQs are disabled.
+         * Clean caches and disable it including mmu
+	 */
+reset:
+	msr     cpsr_c, #PSR_F_BIT | PSR_I_BIT | SVC_MODE
+	cpsid	if			@ disable irq and fiq
+	mov     r4, #0
+	mcr     p15, 0, r4, c7, c5, 0	@ clean I-Cache
+	mrc     p15, 0, r1, c1, c0, 1
+	orr     r1, r1, #0x20
+	mcr     p15, 0, r1, c1, c0, 1	@ set symmetric mp mode
+
+	adr	r4, 3f			@ address asmp_core_boot in r7
+	ldmia	r4, {r5, r7}
+	sub	r4, r4, r5
+	add	r7, r7, r4
+	mov	r4, #0
+	str	r4, [r7]		@ reset asmp_core_boot entry
+	mrc	p15, 0, r0, c0, c0, 5	@ read cpu that has been reset
+	adr	r4, 2f			@ mark asmp_core_ready with cpuid
+	ldmia	r4, {r5, r6}
+	sub	r4, r4, r5
+	add	r6, r6, r4
+	str	r0, [r6]
+
+        mrc     p15, 0, r1, c1, c0, 0
+	bic     r1, r1, #CR_C 		@ disable mmu and caches
+	bic	r1, r1, #CR_I
+	bic	r1, r1, #CR_M
+	mcr     p15, 0, r1, c1, c0, 0
+
+
+	mov	r4,#0
+wait_for_boot:
+	mcr     p15, 0, r0, c7, c10, 5  @ read memory barrier
+	nop
+	ldr	r3, [r7]		@ waiting for loader to write a boot entry
+	cmp	r3, r4
+	beq	wait_for_boot
+	str	r4, [r6]		@ set asmp_core_ready with 0
+	mcr     p15, 0, r0, c7, c10, 5  @ write memory barrier
+
+;;	mrc     p15, 0, r1, c1, c0, 1
+;;      bic     r1, r1, #0x20
+;;	mcr     p15, 0, r1, c1, c0, 1	@ set asymmetric mp mode
+
+	mov	pc, r3			@ run the code loaded
+
+2:	.long	.
+	.long	asmp_core_ready
+
+3:	.long	.
+	.long	asmp_core_boot
+
+4:	.long	.
+	.long	asmp_core_reset
diff --git a/arch/arm/mach-transcede/headsmp.S b/arch/arm/mach-transcede/headsmp.S
new file mode 100644
index 0000000..c6efa90
--- /dev/null
+++ b/arch/arm/mach-transcede/headsmp.S
@@ -0,0 +1,58 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#include <linux/linkage.h>
+#include <linux/init.h>
+
+	__INIT
+
+/*
+ * Realview specific entry point for secondary CPUs.  This provides
+ * a "holding pen" into which all secondary cores are held until we're
+ * ready for them to initialise.
+ */
+ENTRY(transcede_secondary_startup)
+//	b	transcede_secondary_startup
+	mrc	p15, 0, r0, c0, c0, 5
+	and	r0, r0, #15
+	adr	r4, 1f
+	ldmia	r4, {r5, r6}
+	sub	r4, r4, r5
+	add	r6, r6, r4
+pen:	ldr	r7, [r6]
+	cmp	r7, r0
+	bne	pen
+
+#ifdef CONFIG_ASMP_CORE_STARTUP
+        mov     r7, #1
+        cmp     r7, r0
+        beq     asmp_core_startup
+	mov	r7, r0
+#endif
+	/*
+	 * we've been released from the holding pen: secondary_stack
+	 * should now contain the SVC stack for this core
+	 */
+	b	secondary_startup
+
+1:	.long	.
+	.long	pen_release
diff --git a/arch/arm/mach-transcede/iccom.c b/arch/arm/mach-transcede/iccom.c
new file mode 100644
index 0000000..2a8b34b
--- /dev/null
+++ b/arch/arm/mach-transcede/iccom.c
@@ -0,0 +1,512 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#include <linux/cdev.h>
+#include <linux/device.h>
+#include <linux/interrupt.h>
+#include <linux/fs.h>
+#include <linux/kernel.h>
+#include <linux/poll.h>
+#include <linux/sched.h>
+#include <asm/delay.h>
+#include <asm/io.h>
+#include <mach/irqs.h>
+#include <mach/gpio.h>
+
+#define ICCOM_VERSION		0x0300
+
+#define ICCOM_BLOCK_RX_SIZE	(16*1024)
+#define ICCOM_BLOCK_TX_SIZE	(16*1024)
+#define ICCOM_BLOCK_RX_NUM	(128)
+#define ICCOM_BLOCK_TX_NUM	(128)
+
+#define ICCOM_QUEUE_NUM		6
+
+#define ICCOM_RX_FREE_QUEUE	0   // This queue contain indexes of free blocks in TX partition
+#define ICCOM_RX_HI_QUEUE	1   // Indexes of HI priority messages
+#define ICCOM_RX_REG_QUEUE	2   // Indexes of regular priority messages
+#define ICCOM_TX_FREE_QUEUE	3   // This queue contain indexes of free blocks in RX partition
+#define ICCOM_TX_HI_QUEUE	4   // Indexes of HI priority messages
+#define ICCOM_TX_REG_QUEUE	5   // Indexes of regular priority messages
+
+/* Same consts but for secondary cluster side */
+#define ICCOM_S_BLOCK_TX_SIZE	(16*1024)
+#define ICCOM_S_BLOCK_RX_SIZE	(16*1024)
+#define ICCOM_S_BLOCK_TX_NUM	(128)
+#define ICCOM_S_BLOCK_RX_NUM	(128)
+
+#define ICCOM_S_TX_FREE_QUEUE	0   // This queue contain indexes of free blocks in TX partition
+#define ICCOM_S_TX_HI_QUEUE	1   // Indexes of HI priority messages
+#define ICCOM_S_TX_REG_QUEUE	2   // Indexes of regular priority messages
+#define ICCOM_S_RX_FREE_QUEUE	3   // This queue contain indexes of free blocks in RX partition
+#define ICCOM_S_RX_HI_QUEUE	4   // Indexes of HI priority messages
+#define ICCOM_S_RX_REG_QUEUE	5   // Indexes of regular priority messages
+
+#ifndef ROUND
+#define ROUND(x, dx) (((x) + ((dx) - 1) ) & ~((dx) - 1))
+#endif
+
+static struct class	*iccom_class;
+static dev_t		iccom_dev_t;
+static int		iccom_major;
+
+
+static struct device	*iccom_dev;
+static struct cdev	iccom_cdev;
+static atomic_t		iccom_refs;
+static int		iccom_irq=-1;
+static u32		iccom_irq_count;
+static u32		iccom_context_phys;
+static u32		iccom_gpio_in;
+static u32		iccom_gpio_out;
+static struct iccom_queue {
+	volatile u32	*ptr_phys;
+	volatile u32	put;
+	volatile u32	get;
+	volatile u32	num;		/* number of elements in the queue */
+	volatile u32	stat_put;	/* put counter */
+	volatile u32	stat_get;	/* get counter */
+} *iccom_rxq, *iccom_rxq_priority;
+
+static struct iccom_context {
+	u32	version;	/* version of I-CCOM */
+	u32	status;		/* bit 0: 1 == the I-CPU is initialized */
+//	u32	rx_irq;		/* inter-ARM ID (0..7) of interrupt to notify remote side */
+//	u32	tx_irq;		/* inter-ARM ID (0..7) of interrupt to notify local side */
+	u32	rx_storage;	/* pointer to the array where TX blocks are located, physical address */
+	u32	tx_storage;	/* pointer to the array where RX blocks are located, physical address */
+	u32	rx_blocks_num;	/* number of blocks in TX array */
+	u32	rx_block_size;	/* size of block in bytes of TX array */
+	u32	tx_blocks_num;	/* number of blocks in RX array */
+	u32	tx_block_size;	/* size of block in bytes of RX array */
+	u32	rx_queue_num;	/* number of TX queues */
+	u32	tx_queue_num;	/* number of RX queues */
+	struct iccom_queue	queues[ICCOM_QUEUE_NUM];
+} *iccom_context;
+
+/* ICCOM context on secondary cluster (TX\RX are swapped) */
+static struct iccom_context_swap {
+	u32	version;	/* version of I-CCOM */
+	u32	status;		/* bit 0: 1 == the I-CPU is initialized */
+//	u32	rx_irq;		/* inter-ARM ID (0..7) of interrupt to notify remote side */
+//	u32	tx_irq;		/* inter-ARM ID (0..7) of interrupt to notify local side */
+	u32	tx_storage;	/* pointer to the array where RX blocks are located, physical address */
+	u32	rx_storage;	/* pointer to the array where TX blocks are located, physical address */
+	u32	tx_blocks_num;	/* number of blocks in RX array */
+	u32	tx_block_size;	/* size of block in bytes of RX array */
+	u32	rx_blocks_num;	/* number of blocks in TX array */
+	u32	rx_block_size;	/* size of block in bytes of TX array */
+	u32	tx_queue_num;	/* number of RX queues */
+	u32	rx_queue_num;	/* number of TX queues */
+	struct iccom_queue	queues[ICCOM_QUEUE_NUM];
+} *iccom_context_swap;
+
+static DECLARE_WAIT_QUEUE_HEAD(iccom_wait_queue);
+
+
+#define ICCOM_IOCTL_WAIT		_IO('I', 1)
+#define ICCOM_IOCTL_WAIT_SECONDARY	_IO('I', 2)
+#define ICCOM_IOCTL_INIT		_IOWR('I', 3, void *)
+#define ICCOM_IOCTL_INIT_SECONDARY	_IOWR('I', 4, void *)
+
+
+void ICComAddFreeBlocks(struct iccom_queue *pQ, u32 nElmNum)
+{
+	u32 i = 0;
+
+	for (i = 0; i < nElmNum; i++)
+	{
+		((u32*)((u32)pQ->ptr_phys + (u32)iccom_context_swap))[pQ->put] = i;
+
+		pQ->put++;
+		pQ->stat_put++;
+
+		if (pQ->put >= pQ->num)
+			pQ->put = 0;
+	}
+}
+
+static irqreturn_t iccom_irq_handler(int irq, void *arg);
+
+/* Initialization on secondary clustor side. (Maybe should be moved to user space lib) */
+int iccom_init_secondary(void *pStorage)
+{
+	u32 nSize;
+	volatile u32 *pQ;
+	volatile u8 *pP;
+
+	if (!pStorage) {
+		printk(KERN_ERR "Can't use NULL pointer for pSporage.\n");
+		return -ENXIO;
+	}
+
+	if ((u32)pStorage & 0xFFFFF) {
+		printk(KERN_ERR "Pointer align error.\n");
+		return -ENXIO;
+	}
+
+	iccom_context = (struct iccom_context*)pStorage;
+	iccom_context_swap = (struct iccom_context_swap*)pStorage;
+	if (!iccom_context_swap)
+		return -ENXIO;
+
+	memset(iccom_context_swap, 0, sizeof(struct iccom_context_swap));
+
+	iccom_context_swap->version = ICCOM_VERSION;
+
+	iccom_context_swap->tx_queue_num = 3; // FreeQ + REG-Q + HI-Q; it can be increased for example with Hi-hi-Q :)
+	iccom_context_swap->rx_queue_num = 3; // FreeQ + REG-Q + HI-Q; it can be increased
+
+	nSize = ROUND(sizeof(struct iccom_context_swap), 32);
+
+	nSize += iccom_context_swap->tx_queue_num * sizeof (u32) * (ICCOM_BLOCK_TX_NUM + 1);
+	nSize += iccom_context_swap->rx_queue_num * sizeof (u32) * (ICCOM_BLOCK_RX_NUM + 1);
+
+	if (nSize >= 1024*1024) {
+		printk("ICCOM Storage overwrite.\n");
+		return -1;
+	}
+
+	// Queue initialization
+	pQ = (u32*)ROUND(sizeof (struct iccom_context_swap), 32); // store only offset
+
+	iccom_context_swap->queues[ICCOM_S_TX_FREE_QUEUE].ptr_phys = pQ;
+	iccom_context_swap->queues[ICCOM_S_TX_FREE_QUEUE].num = ICCOM_S_BLOCK_TX_NUM + 1;
+	pQ+= ICCOM_S_BLOCK_TX_NUM + 1;
+
+	iccom_context_swap->queues[ICCOM_S_TX_HI_QUEUE].ptr_phys = pQ;
+	iccom_context_swap->queues[ICCOM_S_TX_HI_QUEUE].num = ICCOM_S_BLOCK_TX_NUM + 1;
+	pQ+= ICCOM_S_BLOCK_TX_NUM + 1;
+
+	iccom_context_swap->queues[ICCOM_S_TX_REG_QUEUE].ptr_phys = pQ;
+	iccom_context_swap->queues[ICCOM_S_TX_REG_QUEUE].num = ICCOM_S_BLOCK_TX_NUM + 1;
+	pQ+= ICCOM_S_BLOCK_TX_NUM + 1;
+
+	iccom_context_swap->queues[ICCOM_S_RX_FREE_QUEUE].ptr_phys = pQ;
+	iccom_context_swap->queues[ICCOM_S_RX_FREE_QUEUE].num = ICCOM_S_BLOCK_RX_NUM + 1;
+	pQ+= ICCOM_S_BLOCK_TX_NUM + 1;
+
+	iccom_context_swap->queues[ICCOM_S_RX_HI_QUEUE].ptr_phys = pQ;
+	iccom_context_swap->queues[ICCOM_S_RX_HI_QUEUE].num = ICCOM_S_BLOCK_RX_NUM + 1;
+	pQ+= ICCOM_S_BLOCK_TX_NUM + 1;
+
+	iccom_context_swap->queues[ICCOM_S_RX_REG_QUEUE].ptr_phys = pQ;
+	iccom_context_swap->queues[ICCOM_S_RX_REG_QUEUE].num = ICCOM_S_BLOCK_RX_NUM + 1;
+	pQ+= ICCOM_S_BLOCK_TX_NUM + 1;
+
+	// Partition parameters initialization
+	// The partition storage is 1MB aligned
+	//                          ~~~
+
+	pP = (u8*)(1024*1024); //store only offset
+
+	iccom_context_swap->tx_storage = (u32)pP; pP += (ICCOM_S_BLOCK_TX_NUM * ICCOM_S_BLOCK_TX_SIZE);
+	iccom_context_swap->rx_storage = (u32)pP; pP += (ICCOM_S_BLOCK_RX_NUM * ICCOM_S_BLOCK_RX_SIZE);
+
+	iccom_context_swap->tx_blocks_num = ICCOM_S_BLOCK_TX_NUM;
+	iccom_context_swap->tx_block_size = ICCOM_S_BLOCK_TX_SIZE;
+
+	iccom_context_swap->rx_blocks_num = ICCOM_S_BLOCK_RX_NUM;
+	iccom_context_swap->rx_block_size = ICCOM_S_BLOCK_RX_SIZE;
+
+	ICComAddFreeBlocks(&iccom_context_swap->queues[ICCOM_S_TX_FREE_QUEUE], ICCOM_S_BLOCK_TX_NUM);
+	ICComAddFreeBlocks(&iccom_context_swap->queues[ICCOM_S_RX_FREE_QUEUE], ICCOM_S_BLOCK_RX_NUM);
+
+	iccom_gpio_in = ICCOM_S_GPIO_RX;
+	iccom_gpio_out = ICCOM_S_GPIO_TX;
+
+	if (iccom_irq < 0) {	// locks shold be added to avoid possible races
+		iccom_irq = IRQ_GPIO(iccom_gpio_in);
+		if (request_irq(iccom_irq, iccom_irq_handler, IRQF_DISABLED, "iccom", iccom_dev)) {
+			return -ENXIO;
+		}
+	}
+
+
+	iccom_irq = IRQ_GPIO(iccom_gpio_in);
+	iccom_irq_count = 0;
+
+	iccom_context_swap->status = 1;
+
+	return 0;
+}
+EXPORT_SYMBOL(iccom_init_secondary);
+
+int iccom_start(void *context)
+{
+	int i;
+
+	if (iccom_context)
+		iounmap(iccom_context);
+
+	iccom_context = (struct iccom_context*)ioremap_nocache((u32)context, sizeof(*iccom_context));
+	iccom_context_swap = (struct iccom_context_swap*)iccom_context;
+	if (!iccom_context) {
+		printk(KERN_ERR "ICCOM: ioremap_nocache fails. (addr = %p)\n", context);
+		return -ENXIO;
+	}
+
+	for (i = 0; i < 5*1000; i++) {
+		if (iccom_context->status & 1)
+			break;
+		udelay(1000);
+	}
+
+	if ((iccom_context->status & 1) == 0) {
+		iounmap(iccom_context);
+		iccom_context = NULL;
+		printk(KERN_ERR "ICCOM: status bit not set, aborting\n");
+		return -ENODEV;
+	}
+
+	iccom_context_phys = (u32)context;
+
+	printk(KERN_ERR "ICCOM: v%u.%u, context at 0x%08X\n", iccom_context->version>>8, iccom_context->version&255, (u32)context);
+
+	iccom_gpio_in = ICCOM_GPIO_RX;
+	iccom_gpio_out = ICCOM_GPIO_TX;
+
+	if (iccom_irq < 0) {	// locks shold be added to avoid possible races
+		iccom_irq = IRQ_GPIO(iccom_gpio_in);
+		if (request_irq(iccom_irq, iccom_irq_handler, IRQF_DISABLED, "iccom", iccom_dev)) {
+			return -ENXIO;
+		}
+	}
+
+	iccom_irq = IRQ_GPIO(iccom_gpio_in);
+	iccom_irq_count = 0;
+
+	iccom_rxq_priority = &iccom_context->queues[ICCOM_RX_HI_QUEUE];
+	iccom_rxq = &iccom_context->queues[ICCOM_RX_REG_QUEUE];
+
+	return 0;
+}
+EXPORT_SYMBOL(iccom_start);
+
+int iccom_stop(void)
+{
+	if (iccom_context)
+		iounmap(iccom_context);
+
+	return 0;
+}
+EXPORT_SYMBOL(iccom_stop);
+
+static irqreturn_t iccom_irq_handler(int irq, void *arg)
+{
+	wake_up_interruptible(&iccom_wait_queue);
+
+/* TODO: VIVA check this for t2200 */
+#if defined(CONFIG_MACH_M84XXX)
+	writel(readl(TRANSCEDE_ARM_IRQ_CLR) | (1 << (irq - 32)), TRANSCEDE_ARM_IRQ_CLR);
+#endif
+
+	// clear GPIO irq
+	*(volatile u32*)TRANSCEDE_GPIO_INT_CLEAR_REG = 1 << iccom_gpio_in;
+
+	iccom_irq_count++;
+
+	return IRQ_HANDLED;
+}
+
+static int iccom_open(struct inode *inode, struct file *file)
+{
+	if ((atomic_inc_return(&iccom_refs) == 1) && iccom_irq != -1) {
+		if (request_irq(iccom_irq, iccom_irq_handler, IRQF_DISABLED, "iccom", iccom_dev)) {
+			atomic_dec(&iccom_refs);
+			return -ENXIO;
+		}
+	}
+
+	return 0;
+}
+
+static int iccom_close(struct inode *inode, struct file *file)
+{
+	if (atomic_dec_return(&iccom_refs) == 0 && iccom_irq != -1) {
+		if (iccom_context)
+			free_irq(iccom_irq, iccom_dev);
+	}
+
+	return 0;
+}
+
+static long iccom_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	struct iccom_queue	*iccom_secondary_rxq = &iccom_context->queues[ICCOM_S_RX_REG_QUEUE],
+				*iccom_secondary_rxq_priority = &iccom_context->queues[ICCOM_S_RX_HI_QUEUE];
+
+	switch (cmd) {
+	case ICCOM_IOCTL_WAIT:
+		if (iccom_rxq->get == iccom_rxq->put && iccom_rxq_priority->get == iccom_rxq_priority->put) {
+			if (wait_event_interruptible(iccom_wait_queue, iccom_rxq->get != iccom_rxq->put || iccom_rxq_priority->get != iccom_rxq_priority->put))
+				return -ERESTARTSYS;
+		}
+
+		return 0;
+	case ICCOM_IOCTL_WAIT_SECONDARY:
+		if (	iccom_secondary_rxq->get == iccom_secondary_rxq->put
+			&& iccom_secondary_rxq_priority->get == iccom_secondary_rxq_priority->put)
+		{
+			if (wait_event_interruptible(iccom_wait_queue,
+							iccom_secondary_rxq->get != iccom_secondary_rxq->put
+							|| iccom_secondary_rxq_priority->get != iccom_secondary_rxq_priority->put))
+			{
+				return -ERESTARTSYS;
+			}
+		}
+
+		return 0;
+	case ICCOM_IOCTL_INIT:
+		iccom_start((void*)arg);
+		return 0;
+	case ICCOM_IOCTL_INIT_SECONDARY:
+		iccom_init_secondary((void*)arg);
+		return 0;
+	}
+
+	return -EOPNOTSUPP;
+}
+
+static unsigned int iccom_poll(struct file *file, struct poll_table_struct *wait)
+{
+	poll_wait(file, &iccom_wait_queue, wait);
+
+	if (iccom_rxq->get != iccom_rxq->put || iccom_rxq_priority->get != iccom_rxq_priority->put)
+		return (POLLIN | POLLRDNORM);
+
+	return 0;
+}
+
+static u32 iccom_queue_used(struct iccom_queue *q)
+{
+	u32 free;
+
+	if (q->get > q->put)
+		free = q->get - q->put;
+	else
+		free = q->num - q->put + q->get;
+
+	return q->num - free;
+}
+
+static ssize_t iccom_info_show(struct device *dev, struct device_attribute *attr, char *buf)
+{
+	int n = 0, i;
+
+	if (!iccom_context)
+		return sprintf(buf, "Not available\n");
+
+	n = sprintf(buf,
+			"Version: %u.%u\n"
+			"Context: 0x%08X\n"
+			"TX/RX block sizes: %u/%u\n"
+			"RX GPIO/IRQ:       %u/%u, count: %u\n"
+			"TX GPIO:           %u\n"
+			"TX/RX storage:     0x%08X/0x%08X\n"
+			"===\n"
+			"Queue # put get num used\tstats put/get\n",
+			iccom_context->version>>8, iccom_context->version&255,
+			iccom_context_phys,
+			iccom_context->tx_block_size, iccom_context->rx_block_size,
+			iccom_gpio_in, iccom_irq, iccom_irq_count,
+			iccom_gpio_out,
+			iccom_context->tx_storage, iccom_context->rx_storage
+	);
+
+	for (i = 0; i < ARRAY_SIZE(iccom_context->queues); i++)
+		n += sprintf(buf+n,
+			"%d\t%3u %3u %3u %3u\t\t%10u %10u\n",
+			i,
+			iccom_context->queues[i].put, iccom_context->queues[i].get,
+			iccom_context->queues[i].num,
+			iccom_queue_used(&iccom_context->queues[i]),
+			iccom_context->queues[i].stat_put, iccom_context->queues[i].stat_get);
+
+	return n;
+}
+
+struct file_operations iccom_fops = {
+	.open		= iccom_open,
+	.release	= iccom_close,
+	.unlocked_ioctl	= iccom_ioctl,
+	.poll		= iccom_poll,
+};
+
+struct device_attribute iccom_info = {
+	.attr = {
+		.name = "info",
+		.mode = S_IRUGO,
+	},
+	.show   = iccom_info_show,
+	.store  = NULL,
+};
+
+static int __init iccom_init(void)
+{
+	int err;
+
+	iccom_class = class_create(THIS_MODULE, "iccom");
+	if (IS_ERR(iccom_class))
+		return PTR_ERR(iccom_class);
+
+	err = alloc_chrdev_region(&iccom_dev_t, 0, 1, "iccom");
+	if (err)
+		goto err0;
+
+	iccom_major = MAJOR(iccom_dev_t);
+
+	cdev_init(&iccom_cdev, &iccom_fops);
+	if (cdev_add(&iccom_cdev, iccom_dev_t, 1))
+		goto err1;
+
+	iccom_dev = device_create(iccom_class, NULL, MKDEV(iccom_major, 0), NULL, "iccom");
+	if (IS_ERR(iccom_dev))
+		goto err2;
+
+	if (device_create_file(iccom_dev, &iccom_info))
+		goto err3;
+
+	return 0;
+
+err3:
+	device_destroy(iccom_class, MKDEV(iccom_major, 0));
+err2:
+	unregister_chrdev_region(iccom_dev_t, 1);
+err1:
+	cdev_del(&iccom_cdev);
+err0:
+	class_destroy(iccom_class);
+	return -1;
+}
+
+static void __exit iccom_exit(void)
+{
+	device_remove_file(iccom_dev, &iccom_info);
+	device_destroy(iccom_class, MKDEV(iccom_major, 0));
+	cdev_del(&iccom_cdev);
+	unregister_chrdev_region(iccom_dev_t, 1);
+	class_destroy(iccom_class);
+}
+
+module_init(iccom_init);
+module_exit(iccom_exit);
diff --git a/arch/arm/mach-transcede/include/mach/clk-rst.h b/arch/arm/mach-transcede/include/mach/clk-rst.h
new file mode 100644
index 0000000..d7a1199
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/clk-rst.h
@@ -0,0 +1,112 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef __ASM_ARCH_CLKRST_H
+#define __ASM_ARCH_CLKRST_H
+
+#define CLUSTER_RESETS		AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x00)
+#define PLL_EXT_BYPASS		AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x01)
+#define GP_CONFIG		AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x02)
+#define GP_STAT			AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x03)
+#define AXI_CLK_CNTRL_0		AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x04)
+#define AXI_CLK_CNTRL_1		AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x05)
+#define AXI_CLK_DIV_CNTRL_0	AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x06)
+#define AXI_CLK_DIV_CNTRL_1	AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x07)
+#define AXI_CLK_DIV_CNTRL_2	AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x08)
+#define CEVA_CLK_CNTRL		AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x0C)
+#define CEVA_CLK_DIV_CNTRL	AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x0D)
+#define FFT_CLK_CNTRL		AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x10)
+#define FFT_CLK_DIV_CNTRL	AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x11)
+#define ARMQP_CLK_CNTRL		AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x14)
+#define ARMQP_CLK_DIV_CNTRL	AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x15)
+#define ARMDP_CLK_CNTRL		AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x18)
+#define ARMDP_CLK_DIV_CNTRL	AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x19)
+#define DDR_CLK_CNTRL		AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x1C)
+#define DDR_CLK_DIV_CNTRL	AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x1D)
+#define GEMPHY_CLK_CNTRL	AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x20)
+#define GEMPHY_CLK_DIV_CNTRL	AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x21)
+#define FEC_DL_CLK_CNTRL	AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x24)
+#define FEC_DL_CLK_DIV_CNTRL	AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x25)
+#define TPI_CLK_CNTRL		AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x28)
+#define TPI_CLK_DIV_CNTRL	AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x29)
+#define TDMNTG_CLK_CNTRL	AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x2C)
+#define TDMNTG_CLK_DIV_CNTRL	AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x2D)
+#define GEMNTG_CLK_CNTRL	AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x30)
+#define GEMNTG_CLK_DIV_CNTRL	AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x31)
+#define FEC_UL_CLK_CNTRL	AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x34)
+#define FEC_UL_CLK_DIV_CNTRL	AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST+4*0x35)
+
+
+/*
+ *  System cluster clocks and resets
+ */
+#define TRANSCEDE_SYS_CLUSTER_CNTRL		AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x00)
+#define TRANSCEDE_SYS_A9DP_CORE_CLK_DIV_CNTRL	AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x01)
+#define TRANSCEDE_SYS_A9DP_CORE_CLK_CNTRL	AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x02)
+#define TRANSCEDE_SYS_A9DP_AXI_CLK_DIV_CNTRL 	AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x03)
+#define TRANSCEDE_SYS_A9DP_L2_AXI_CLK_CNTRL	AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x04)
+#define TRANSCEDE_SYS_A9DP_RESET_0		AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x05)
+#define TRANSCEDE_SYS_A9DP_RESET_1		AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x06)
+#define TRANSCEDE_SYS_A9DP_RESET_2		AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x07)
+#define TRANSCEDE_SYS_A9DP_WD_CNTRL		AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x08)
+#define TRANSCEDE_SYS_A9QP_CORE_CLK_DIV_CNTRL	AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x09)
+#define TRANSCEDE_SYS_A9QP_CORE_CLK_CNTRL	AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x0A)
+#define TRANSCEDE_SYS_A9QP_AXI_CLK_DIV_CNTRL	AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x0B)
+#define TRANSCEDE_SYS_A9QP_L2_AXI_CLK_CNTRL	AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x0C)
+#define	TRANSCEDE_SYS_A9QP_RESET_0		AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x0D)
+#define TRANSCEDE_SYS_A9QP_RESET_1		AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x0E)
+#define TRANSCEDE_SYS_A9QP_RESET_2		AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x0F)
+#define TRANSCEDE_SYS_A9QP_WD_CNTRL		AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x10)
+#define TRANSCEDE_SYS_AXI_CLK_DIV_CNTRL		AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x11)
+#define TRANSCEDE_SYS_AXI_CLK_CNTRL_0		AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x12)
+#define TRANSCEDE_SYS_AXI_CLK_CNTRL_1		AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x13)
+#define TRANSCEDE_SYS_AXI_RESET_0		AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x14)
+#define TRANSCEDE_SYS_AXI_RESET_1		AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x15)
+#define TRANSCEDE_SYS_DDR_CLK_DIV_CNTRL		AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x17)
+#define TRANSCEDE_SYS_DDR_CLK_CNTRL		AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x18)
+#define TRANSCEDE_SYS_DDR_RESET			AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x19)
+#define TRANSCEDE_SYS_FEC_CLK_DIV_CNTRL		AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x1A)
+#define TRANSCEDE_SYS_FEC_CLK_CNTRL		AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x1B)
+#define TRANSCEDE_SYS_FEC_RESET			AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x1C)
+#define TRANSCEDE_SYS_TPI_CLK_DIV_CNTRL		AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x1D)
+#define TRANSCEDE_SYS_CSYS_DBG_CLK_CNTRL	AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x1E)
+#define TRANSCEDE_SYS_CSYS_DBG_RESET		AAB_XP_VADDR(TRANSCEDE_CLKRST_SYS+4*0x1F)
+
+/*
+ *  Expansion cluster clocks and resets
+ */
+#define TRANSCEDE_EXP_CLUSTER_CNTRL	AAB_XP_VADDR(TRANSCEDE_EXP_CLKRST+4*0x00)
+#define TRANSCEDE_EXP_AXI_CLK_CNTRL_0 	AAB_XP_VADDR(TRANSCEDE_EXP_CLKRST+4*0x02)
+#define TRANSCEDE_EXP_AXI_CLK_CNTRL_1 	AAB_XP_VADDR(TRANSCEDE_EXP_CLKRST+4*0x03)
+#define TRANSCEDE_EXP_AXI_RESET_0	AAB_XP_VADDR(TRANSCEDE_EXP_CLKRST+4*0x04)
+#define TRANSCEDE_EXP_AXI_RESET_1	AAB_XP_VADDR(TRANSCEDE_EXP_CLKRST+4*0x05)
+#define TRANSCEDE_EXP_PHY_CLK_DIV_CNTRL AAB_XP_VADDR(TRANSCEDE_EXP_CLKRST+4*0x06)
+#define TRANSCEDE_EXP_PHY_CLK_CNTRL	AAB_XP_VADDR(TRANSCEDE_EXP_CLKRST+4*0x07)
+#define TRANSCEDE_EXP_PHY_RESET		AAB_XP_VADDR(TRANSCEDE_EXP_CLKRST+4*0x08)
+#define TRANSCEDE_EXP_TDM_NTG_CNTRL	AAB_XP_VADDR(TRANSCEDE_EXP_CLKRST+4*0x09)
+#define TRANSCEDE_EXP_GEM_NTG_CNTRL	AAB_XP_VADDR(TRANSCEDE_EXP_CLKRST+4*0x0A)
+#define TRANSCEDE_EXP_SRDS_CNTRL	AAB_XP_VADDR(TRANSCEDE_EXP_CLKRST+4*0x0B)
+#define TRANSCEDE_EXP_EXTPHY_CLK_CNTRL 	AAB_XP_VADDR(TRANSCEDE_EXP_CLKRST+4*0x0C)
+#define TRANSCEDE_EXP_TDM_NTG_BASE	AAB_XP_VADDR(TRANSCEDE_EXP_CLKRST+4*0x40)
+#define TRANSCEDE_EXP_GEM_NTG_BASE	AAB_XP_VADDR(TRANSCEDE_EXP_CLKRST+4*0x80)
+
+#endif
diff --git a/arch/arm/mach-transcede/include/mach/clkrst.h b/arch/arm/mach-transcede/include/mach/clkrst.h
new file mode 100644
index 0000000..bfb941d
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/clkrst.h
@@ -0,0 +1,119 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef _CLKRST_H_
+#define _CLKRST_H_
+
+// #include "systypes.h"
+#include <linux/types.h>
+
+#ifndef _UINT32_T_
+#define _UINT32_T_
+typedef uint32_t		UINT32;
+#endif
+
+#ifndef _UINT64_T_
+#define _UINT64_T_
+typedef uint64_t		UINT64;
+#endif
+
+#ifndef _VUINT32_T_
+#define _VUINT32_T_
+typedef volatile uint32_t	VUINT32;
+#endif
+
+#ifndef _PUINT32_T_
+#define _PUINT32_T_
+typedef uint32_t		*PUINT32;
+#endif
+
+typedef enum
+{
+    CR_PLL_0 = 0,
+    CR_PLL_1 = 1,
+    CR_PLL_2 = 2,
+    CR_PLL_3 = 3,
+
+    CR_PLL_REF_CLOCK = 4,
+
+    CR_CA9_MC_MPU_ACP,
+    CR_CA9_MC_MPU_PERIPH,
+
+    CR_CA9_MC_CPU0,
+    CR_CA9_MC_CPU1,
+
+    // devices have own clk&rst block
+    CR_CA9_MC,
+    CR_L2CC,
+    CR_TPI,
+    CR_CSYS,
+    CR_EXTPHY0,
+    CR_EXTPHY1,
+    CR_FEC_UL,
+    CR_FEC_DL,
+    CR_FFT,
+    CR_IPSEC,
+    CR_DDR3,
+    CR_GEMTX,
+    CR_TDMNTG,
+    CR_TSUNTG,
+    CR_CRP,
+    CR_CEVA,
+    CR_SPACC,
+    CR_SASPA,
+
+    // AXI_CNTRL Group start
+    // axi clk&rst 0
+    CR_MDMA_SYS1,
+    CR_MDMA_SYS0,
+
+    // axi clk&rst 1
+    CR_SYS_AXI,
+    CR_TIMERS,
+    CR_UART,
+    CR_I2C_SPI,
+    CR_PCI,
+    CR_TDM,
+    CR_FEC,
+    CR_DUS,
+
+    // axi clk&rst 2
+    CR_TBD,
+    CR_GEM0,
+    CR_GEM1,
+    CR_USB_MISC,
+    CR_USB,
+    CR_JESD207,
+    CR_CPRI,
+    // AXI_CNTRL Group end
+
+    CR_CEVA_BM_AXI,
+    CR_CEVA_BM_AHB,
+    CR_CRP_BM,
+    CR_FFT_BM,
+}ClkRstDev;
+
+#define FREQ_MHZ(nDevID) (ClkRstGetFreq (nDevID)/1000000)
+
+UINT32 ClkRstGetFreq (UINT32 nDevID);
+
+#endif
diff --git a/arch/arm/mach-transcede/include/mach/ddr_protection.h b/arch/arm/mach-transcede/include/mach/ddr_protection.h
new file mode 100644
index 0000000..0c3f352
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/ddr_protection.h
@@ -0,0 +1,47 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef _TRANSCEDE_DDR_PROTECTION_H_
+#define _TRANSCEDE_DDR_PROTECTION_H_
+
+#include <linux/seq_file.h>
+
+enum {
+	DDR_PORT0,
+	DDR_PORT1,
+	DDR_PORT2,
+	DDR_PORT3,
+};
+
+enum {
+	DDR_RANGE0,
+	DDR_RANGE1,
+	DDR_RANGE2,
+	DDR_RANGE3,
+};
+
+extern void ddr_protect_ports(void);
+extern void ddr_protect_enable(void);
+extern void ddr_protect_disable(void);
+extern int ddr_protect_set_proc(void);
+
+#endif	/* _TRANSCEDE_DDR_PROTECTION_H_ */
diff --git a/arch/arm/mach-transcede/include/mach/debug-macro.S b/arch/arm/mach-transcede/include/mach/debug-macro.S
new file mode 100644
index 0000000..368ad2b
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/debug-macro.S
@@ -0,0 +1,54 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#include <linux/serial_reg.h>
+#include <mach/hardware.h>
+
+	.macro	addruart, rp, rv
+#ifdef CONFIG_TRANSCEDE_UART0_SUPPORT
+	ldr	\rp, =TRANSCEDE_UART0
+#elif CONFIG_TRANSCEDE_UART1_SUPPORT
+	ldr	\rp, =TRANSCEDE_UART1
+#elif CONFIG_TRANSCEDE_UART2_SUPPORT
+	ldr	\rp, =TRANSCEDE_UART2
+#else
+	#error No UAR defined
+#endif
+	mov	\rv, \rp	/* 1:1 mapping */
+	.endm
+
+	.macro	senduart, rd, rx
+	strb	\rd, [\rx]
+	.endm
+
+	.macro	waituart, rd, rx
+1001:	ldrb	\rd, [\rx, #UART_LSR*4]
+	tst	\rd, #UART_LSR_THRE		@ wait for THRE
+	beq	1001b
+	.endm
+
+	.macro	busyuart, rd, rx
+1001:	ldrb	\rd, [\rx, #UART_LSR*4]
+	and	\rd, \rd, #(UART_LSR_TEMT|UART_LSR_THRE)
+	teq	\rd, #(UART_LSR_TEMT|UART_LSR_THRE)		@ wait for TEMT and THRE
+	bne	1001b
+	.endm
diff --git a/arch/arm/mach-transcede/include/mach/drv_if.h b/arch/arm/mach-transcede/include/mach/drv_if.h
new file mode 100644
index 0000000..d4c756d
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/drv_if.h
@@ -0,0 +1,51 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef _DRV_IF_
+#define _DRV_IF_
+
+// the struct carries a set of ICC interface functions
+// usable from kernel space
+typedef struct tag_DRV_ICC_KDII
+{
+    u32   pUaii;
+
+    void* (*alloc)(void*, u32 size);
+    int   (*free)(void*, void* pBlk);
+    int   (*put)(void*, void* pMsg, u32 MsgSize, u16 MsgTypeID, u16 Priority);
+    void* (*get)(void*, u32 *MsgSize);
+    int   (*wait)(void*);
+    void* (*wget)(void*, u32 *MsgSize);
+    void* (*exget)(void*, u32 *MsgSize, u32 *Flags);
+    void* (*exwget)(void*, u32 *MsgSize, u32 *Flags);
+    u32   (*memmap)(void* vaddr, u32 MsgSize, u32 dir);
+
+    void* (*get_trash)(void*, u32 *MsgSize);
+    u32   (*get_statistics)(void*, char *buf, u32 BufSize);
+    u32   (*get_cluster_id)(void);
+    u8    OwnerName[32];
+} DRV_ICC_KDII, *PDRV_ICC_KDII;
+
+extern void* (*ICC_KDII_Create)(const char *name, u32 QueueSize, u32 BlockSize, u32 *Ports, u32 mode);
+extern int   (*ICC_KDII_Destroy)(void* h);
+
+#endif // _DRV_IF_
\ No newline at end of file
diff --git a/arch/arm/mach-transcede/include/mach/entry-macro.S b/arch/arm/mach-transcede/include/mach/entry-macro.S
new file mode 100644
index 0000000..a65a2fd
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/entry-macro.S
@@ -0,0 +1,183 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#include <mach/hardware.h>
+#include <mach/irqs.h>
+#include <asm/hardware/gic.h>
+
+		.macro	disable_fiq
+		.endm
+
+		.macro  get_irqnr_preamble, base, tmp
+		ldr	\base, =gic_cpu_base_addr
+		ldr	\base, [\base]
+		.endm
+
+		.macro  arch_ret_to_user, tmp1, tmp2
+		.endm
+
+#ifdef CONFIG_TRANSCEDE_DUALCORE
+		/*
+		 * The interrupt numbering scheme is defined in the
+		 * interrupt controller spec.  To wit:
+		 *
+		 * Interrupts 0-15 are IPI
+		 * 16-28 are reserved
+		 * 29-31 are local.  We allow 30 to be used for the watchdog.
+		 * 32-1020 are global
+		 * 1021-1022 are reserved
+		 * 1023 is "spurious" (no interrupt)
+		 *
+		 * For now, we ignore all local interrupts so only return an interrupt if it's
+		 * between 30 and 1020.  The test_for_ipi routine below will pick up on IPIs.
+		 *
+		 * A simple read from the controller will tell us the number of the highest
+		 * priority enabled interrupt.  We then just need to check whether it is in the
+		 * valid range for an IRQ (30-1020 inclusive).
+		 */
+
+		.macro	get_irqnr_and_base, irqnr, irqstat, base, tmp
+		ldr	\irqstat, [\base, #GIC_CPU_HIGHPRI]	/* bits 12-10 = src CPU, 9-0 = int # */
+		ldr	\tmp, [\base, #GIC_CPU_HIGHPRI]
+		cmp	\irqstat, \tmp
+		ldreq	\tmp, [\base, #GIC_CPU_HIGHPRI]
+		cmpeq	\irqstat, \tmp
+
+		movne	\irqstat, #156		/* must be the higher than we support to roll out */
+		moveq	\tmp, #155		/* real limit of supported IRQ numbers */
+
+		bic	\irqnr, \irqstat, #0x1C00
+		cmp	\irqnr, #IRQ_LOCALTIMER	/* if IRQ number matches local timer - set EQ condition to avoid asm_do_IRQ */
+		cmpcc	\irqnr, \irqnr		/* if IRQ number is lower than local timer - the same as above (could be IPI) */
+		cmpne	\irqnr, \tmp		/* if IRQ number doesn't match local timer - check the upper boundary of IRQ numbers */
+		cmpcs	\irqnr, \irqnr		/* if IRQ number is higher than we support - avoid asm_do_IRQ */
+		.endm
+
+		/* irqstat and base must be preserved */
+
+		.macro test_for_ipi, irqnr, irqstat, base, tmp
+		cmp	\irqnr, \irqnr		/* IPI goes via a 'normal' IRQ */
+		.endm
+
+#ifdef CONFIG_GLOBAL_POLLING
+		.macro test_for_gtirq, irqnr, irqstat, base, tmp
+		bic	\irqnr, \irqstat, #0x1c00
+		mov	\tmp, #(1<<IRQ_GLOBALTIMER)
+		cmp	\irqnr, #IRQ_GLOBALTIMER
+		streq	\irqstat, [\base, #GIC_CPU_EOI]
+		moveq	\irqnr, #0x1180
+		streq	\tmp, [\base, \irqnr]
+		movne	\irqnr, \tmp
+		cmp	\tmp, \irqnr
+		.endm
+#endif  /* CONFIG_GLOBAL_POLLING */
+
+		/* irqstat and base must be preserved */
+
+		.macro test_for_ltirq, irqnr, irqstat, base, tmp
+		bic	\irqnr, \irqstat, #0x1C00
+		cmp	\irqnr, #IRQ_LOCALTIMER
+		moveq	\tmp, #0x20000000
+		streq	\irqstat, [\base, #GIC_CPU_EOI]
+		moveq	\irqnr, #0x1180
+		streq	\tmp, [\base, \irqnr]	/* GIC CPU base 0xFFF00100 + 0x1180 == 0xFFF01280 == distributor clear pending */
+		movne	\irqnr, \tmp		/* needed invert condition to avoid call do_local_timer */
+		cmp	\tmp, \irqnr
+		.endm
+
+#else
+
+		/*
+		 * The interrupt numbering scheme is defined in the
+		 * interrupt controller spec.  To wit:
+		 *
+		 * Interrupts 0-15 are IPI
+		 * 16-28 are reserved
+		 * 29-31 are local.  We allow 30 to be used for the watchdog.
+		 * 32-1020 are global
+		 * 1021-1022 are reserved
+		 * 1023 is "spurious" (no interrupt)
+		 *
+		 * For now, we ignore all local interrupts so only return an interrupt if it's
+		 * between 30 and 1020.  The test_for_ipi routine below will pick up on IPIs.
+		 *
+		 * A simple read from the controller will tell us the number of the highest
+                 * priority enabled interrupt.  We then just need to check whether it is in the
+		 * valid range for an IRQ (30-1020 inclusive).
+		 */
+
+		.macro  get_irqnr_and_base, irqnr, irqstat,  base, tmp
+		ldr     \irqstat, [\base, #GIC_CPU_INTACK] /* bits 12-10 = src CPU, 9-0 = int # */
+		bic     \irqnr, \irqstat, #0x1c00
+
+		ldr	\tmp, =255
+		cmp     \irqnr, #IRQ_LOCALTIMER
+		it	cc
+		cmpcc	\irqnr, \irqnr
+		it	ne
+		cmpne	\irqnr, \tmp
+		it	cs
+		cmpcs	\irqnr, \irqnr
+
+		.endm
+
+		/* We assume that irqstat (the raw value of the IRQ acknowledge
+		 * register) is preserved from the macro above.
+		 * If there is an IPI, we immediately signal end of interrupt on the
+		 * controller, since this requires the original irqstat value which
+		 * we won't easily be able to recreate later.
+		 */
+
+		.macro test_for_ipi, irqnr, irqstat, base, tmp
+		bic	\irqnr, \irqstat, #0x1c00
+		cmp	\irqnr, #9
+		it	cc
+		strcc	\irqstat, [\base, #GIC_CPU_EOI]
+		cmp	\irqnr, #8
+		it	cs
+		cmpcs	\irqnr, \irqnr
+		.endm
+
+#ifdef CONFIG_GLOBAL_POLLING
+		.macro test_for_gtirq, irqnr, irqstat, base, tmp
+		bic	\irqnr, \irqstat, #0x1c00
+		mov 	\tmp, #0
+		cmp	\irqnr, #IRQ_GLOBALTIMER
+		itt	eq
+		moveq	\tmp, #1
+		streq	\irqstat, [\base, #GIC_CPU_EOI]
+		cmp	\tmp, #0
+		.endm
+#endif  /* CONFIG_GLOBAL_POLLING */
+
+		/* As above, this assumes that irqstat and base are preserved.. */
+
+		.macro test_for_ltirq, irqnr, irqstat, base, tmp
+		bic	\irqnr, \irqstat, #0x1c00
+		mov 	\tmp, #0
+		cmp	\irqnr, #IRQ_LOCALTIMER
+		itt	eq
+		moveq	\tmp, #1
+		streq	\irqstat, [\base, #GIC_CPU_EOI]
+		cmp	\tmp, #0
+		.endm
+#endif
diff --git a/arch/arm/mach-transcede/include/mach/exp-bus.h b/arch/arm/mach-transcede/include/mach/exp-bus.h
new file mode 100644
index 0000000..09417f2
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/exp-bus.h
@@ -0,0 +1,89 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef __ASM_ARCH_EXP_BUS_H
+#define __ASM_ARCH_EXP_BUS_H
+
+#define TRANSCEDE_EXP_CS0_BASE			0x20000000
+#define TRANSCEDE_EXP_CS1_BASE			0x24000000
+#define TRANSCEDE_EXP_CS2_BASE			0x28000000
+#define TRANSCEDE_EXP_CS3_BASE			0x2C000000
+#define TRANSCEDE_EXP_CS4_BASE			0x30000000
+
+/***** Registers address *****/
+
+#define TRANSCEDE_EXP_SW_RST_R			AAB_XP_VADDR(TRANSCEDE_EXP_BUS + 0x000)
+#define TRANSCEDE_EXP_CS_EN_R			AAB_XP_VADDR(TRANSCEDE_EXP_BUS + 0x004)
+#define TRANSCEDE_EXP_CSx_BASE_R(x)		AAB_XP_VADDR(TRANSCEDE_EXP_BUS + 0x008 + (0x4 *x))
+#define TRANSCEDE_EXP_CSx_SEG_R(x)		AAB_XP_VADDR(TRANSCEDE_EXP_BUS + 0x01C + (0x4 *x))
+#define TRANSCEDE_EXP_CSx_CFG_R(x)		AAB_XP_VADDR(TRANSCEDE_EXP_BUS + 0x030 + (0x4 *x))
+#define TRANSCEDE_EXP_CSx_TMG1_R(x)		AAB_XP_VADDR(TRANSCEDE_EXP_BUS + 0x044 + (0x4 *x))
+#define TRANSCEDE_EXP_CSx_TMG2_R(x)		AAB_XP_VADDR(TRANSCEDE_EXP_BUS + 0x058 + (0x4 *x))
+#define TRANSCEDE_EXP_CSx_TMG3_R(x)		AAB_XP_VADDR(TRANSCEDE_EXP_BUS + 0x06C + (0x4 *x))
+#define TRANSCEDE_EXP_CLOCK_DIV_R		AAB_XP_VADDR(TRANSCEDE_EXP_BUS + 0x080)
+
+#define TRANSCEDE_EXP_MFSM_R			AAB_XP_VADDR(TRANSCEDE_EXP_BUS + 0x100)
+#define TRANSCEDE_EXP_CSFSM_R			AAB_XP_VADDR(TRANSCEDE_EXP_BUS + 0x104)
+#define TRANSCEDE_EXP_WRSM_R			AAB_XP_VADDR(TRANSCEDE_EXP_BUS + 0x108)
+#define TRANSCEDE_EXP_RDSM_R			AAB_XP_VADDR(TRANSCEDE_EXP_BUS + 0x10C)
+
+
+/***** Masks *****/
+
+/* EXP_SWRST_R register*/
+#define EXP_SW_RST				0x00000001
+
+/* EXP_CS_EN_R register*/
+#define EXP_CS4_EN				0x00000020
+#define EXP_CS3_EN				0x00000010
+#define EXP_CS2_EN				0x00000008
+#define EXP_CS1_EN				0x00000004
+#define EXP_CS0_EN				0x00000002
+#define EXP_CLK_EN				0x00000001
+
+/* EXP_CSx_CFG_R register*/
+#define EXP_RDY_EDG				0x00000800
+#define EXP_RDY_EN				0x00000400
+#define EXP_NAND_MODE				0x00000200
+#define EXP_DM_MODE				0x00000100
+#define EXP_STRB_MODE				0x00000080
+#define EXP_ALE_MODE				0x00000040
+#define EXP_RE_CMD_LVL				0x00000020
+#define EXP_WE_CMD_LVL				0x00000010
+#define EXP_CS_LEVEL				0x00000008
+#define EXP_MEM_BUS_SIZE			0x00000006
+#define EXP_MEM_BUS_SIZE_32			0x00000004
+#define EXP_MEM_BUS_SIZE_16			0x00000002
+#define EXP_MEM_BUS_SIZE_8			0x00000000
+
+/* EXP_CSx_TMG1_R register */
+/* EXP_CSx_TMG2_R register */
+/* EXP_CSx_TMG3_R register */
+
+/* EXP_CLOCK_DIV_R register */
+
+/* EXP_MFSM_R register*/
+/* EXP_CSFSM_R register*/
+/* EXP_WRFSM_R register*/
+/* EXP_RDFSM_R register*/
+
+#endif
diff --git a/arch/arm/mach-transcede/include/mach/gpio-2200.h b/arch/arm/mach-transcede/include/mach/gpio-2200.h
new file mode 100644
index 0000000..be36fd8
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/gpio-2200.h
@@ -0,0 +1,178 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef __ASM_ARCH_GPIO_2200_H
+#define __ASM_ARCH_GPIO_2200_H
+
+#define ARCH_NR_GPIOS (32)
+#define TRANSCEDE_GPIO_NR_GPIOS ARCH_NR_GPIOS
+
+/***** GPIO  *****/
+#define TRANSCEDE_GPIO_OUTPUT_REG		AAB_XP_VADDR((TRANSCEDE_GPIO + 0x00))
+#define TRANSCEDE_GPIO_OE_REG			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x04))
+#define TRANSCEDE_GPIO_INT_CFG_REG		AAB_XP_VADDR((TRANSCEDE_GPIO + 0x08))
+#define TRANSCEDE_GPIO_INT_CFG_REG_2		AAB_XP_VADDR((TRANSCEDE_GPIO + 0x0C))
+#define TRANSCEDE_GPIO_INPUT_REG		AAB_XP_VADDR((TRANSCEDE_GPIO + 0x10))
+#define TRANSCEDE_GPIO_APB_WS			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x14))
+#define SYSCONF_STAT_REG			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x18))
+#define TRANSCEDE_GPIO_INT_STAT_REG		AAB_XP_VADDR((TRANSCEDE_GPIO + 0x1C))
+#define TRANSCEDE_GPIO_INT_MASK_REG		AAB_XP_VADDR((TRANSCEDE_GPIO + 0x20))
+#define TRANSCEDE_GPIO_INT_CLEAR_REG		AAB_XP_VADDR((TRANSCEDE_GPIO + 0x24))
+#define TRANSCEDE_GPIO_TDM_MUX			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x28))
+#define TRANSCEDE_GPIO_MISC_CTRL		AAB_XP_VADDR((TRANSCEDE_GPIO + 0x34))
+#define TRANSCEDE_GPIO_DDRC_AXI			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x38))
+#define TRANSCEDE_GPIO_BOOTSTRAP_STATUS		AAB_XP_VADDR((TRANSCEDE_GPIO + 0x40))
+#define TRANSCEDE_GPIO_BOOTSTRAP_OVERRIDE	AAB_XP_VADDR((TRANSCEDE_GPIO + 0x44))
+#define TRANSCEDE_GPIO_GENERAL_CONTROL_REG	AAB_XP_VADDR((TRANSCEDE_GPIO + 0x4C))
+#define TRANSCEDE_GPIO_DEVICE_ID_REG		AAB_XP_VADDR((TRANSCEDE_GPIO + 0x50))
+#define TRANSCEDE_GPIO_PIN_SELECT		AAB_XP_VADDR((TRANSCEDE_GPIO + 0x58))
+#define TRANSCEDE_GPIO_PIN_SELECT1		AAB_XP_VADDR((TRANSCEDE_GPIO + 0x5C))
+#define TRANSCEDE_MISC_PIN_SELECT		AAB_XP_VADDR((TRANSCEDE_GPIO + 0x60))
+#define TRANSCEDE_A9_AUTH_CTRL			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x70))
+#define TRANSCEDE_ACP_CONFIG			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x74))
+#define TRANSCEDE_IRQ_SET			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x98))
+#define TRANSCEDE_IRQ_CLEAR			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x9C))
+#define TRANSCEDE_IRQ_MASK			AAB_XP_VADDR((TRANSCEDE_GPIO + 0xA0))
+#define TRANSCEDE_TSU_CONFIG_REG		AAB_XP_VADDR((TRANSCEDE_GPIO + 0xB0))
+#define TRANSCEDE_THERMAL_CTRL_STATUS		AAB_XP_VADDR((TRANSCEDE_GPIO + 0xD4))
+#define TRANSCEDE_PAD_CONFIG0			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x100))
+#define TRANSCEDE_PAD_CONFIG1			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x104))
+#define TRANSCEDE_PAD_CONFIG2			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x108))
+#define TRANSCEDE_PAD_CONFIG3			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x10C))
+#define TRANSCEDE_PAD_CONFIG4			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x110))
+#define TRANSCEDE_PAD_CONFIG5			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x114))
+#define TRANSCEDE_PAD_CONFIG6			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x118))
+#define TRANSCEDE_PAD_CONFIG7			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x11C))
+#define TRANSCEDE_PAD_CONFIG8			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x120))
+#define TRANSCEDE_PAD_CONFIG9			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x124))
+#define TRANSCEDE_PAD_CONFIG10			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x128))
+#define TRANSCEDE_PAD_CONFIG11			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x12C))
+#define TRANSCEDE_PAD_CONFIG12			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x130))
+#define TRANSCEDE_PAD_CONFIG13			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x134))
+#define TRANSCEDE_PAD_CONFIG14			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x138))
+#define TRANSCEDE_PMU_IRQ_CLEAR			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x160))
+#define TRANSCEDE_PMU_IRQ_SET			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x164))
+#define TRANSCEDE_PMU_IRQ_MASK0			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x168))
+#define TRANSCEDE_PMU_IRQ_MASK1			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x16C))
+
+
+#define GPIO_0		0x00000001
+#define GPIO_1		0x00000002
+#define GPIO_2		0x00000004
+#define GPIO_3		0x00000008
+#define GPIO_4		0x00000010
+#define GPIO_5		0x00000020
+#define GPIO_6	        0x00000040
+#define GPIO_7		0x00000080
+#define GPIO_8		0x00000100
+#define GPIO_9		0x00000200
+#define GPIO_10		0x00000400
+#define GPIO_11		0x00000800
+#define GPIO_12		0x00001000
+#define GPIO_13		0x00002000
+#define GPIO_14		0x00004000
+#define GPIO_15		0x00008000
+#define GPIO_16		0x00010000
+#define GPIO_17		0x00020000
+#define GPIO_18		0x00040000
+#define GPIO_19		0x00080000
+#define GPIO_20		0x00100000
+#define GPIO_21		0x00200000
+#define GPIO_22		0x00400000
+#define GPIO_23		0x00800000
+#define GPIO_24		0x01000000
+#define GPIO_25		0x02000000
+#define GPIO_26		0x04000000
+#define GPIO_27		0x08000000
+#define GPIO_28		0x10000000
+#define GPIO_29		0x20000000
+#define GPIO_30		0x40000000
+#define GPIO_31		0x80000000
+
+#define PIN_EXP_NAND_RDY	GPIO_29
+#define PIN_EXP_NAND_CS		GPIO_28
+
+#define ICCOM_GPIO_RX            0
+#define ICCOM_GPIO_TX            2
+#define ICCOM_S_GPIO_RX          1
+#define ICCOM_S_GPIO_TX          0
+#define RADIO_GPIO_RESET         3
+#define NAND_GPIO_CS             28
+#define NAND_GPIO_RDY            29
+#define PCIE_GPIO_RESET          8
+
+/* Generic Pad Control */
+#define PAD_TDM_PULL_UP (1 << 18)
+#define PAD_TDM_POWER 	(1 << 17)
+#define PAD_SPI_POWER 	(1 << 16)
+#define PAD_I2C_PULL_UP	(1 << 15)
+#define PAD_I2C_POWER	(1 << 14)
+#define PAD_STRAP_POWER	(1 << 13)
+#define PAD_EXP_ADDRESS	(1 << 12)
+#define PAD_EXP_BUS	(1 << 11)
+#define PAD_EXP_HI	(1 << 10)
+#define PAD_EXP_HI_CLK	(1 << 9)
+#define PAD_GEM1_IF	(1 << 8)
+#define PAD_GEM1_CLKIN	(1 << 7)
+#define PAD_GEM0_IF	(1 << 6)
+#define PAD_GEM0_CLKIN	(1 << 5)
+#define PAD_MDIO	(1 << 4)
+#define PAD_TSU_VCXO	(1 << 3)
+#define PAD_TSU_NTG	(1 << 2)
+#define PAD_TSU_CLKIN	(1 << 1)
+#define PAD_TSU_RGMII	(1 << 0)
+
+/* GPIO Pin Select Pins */
+#define SPI_RXD         (3 << 30)
+#define SPI_TXD         (3 << 28)
+#define EXP_RDY_BSY     (3 << 26) // reserved
+#define EXP_NAND_CS     (3 << 24) // reserved
+#define EXP_ALE         (3 << 22)
+#define EXP_RDY         (3 << 20)
+#define EXP_CS3_N       (3 << 18)
+#define EXP_CS2_N       (3 << 16)
+#define UART1_TX        (3 << 14)
+#define UART1_RX        (3 << 12)
+#define UART0_CTS_N     (3 << 10)
+#define UART0_RTS_N     (3 << 8)
+#define SPI_SS1_N       (3 << 6)
+#define SPI_SS0_N       (3 << 4)
+#define I2C_SDA         (3 << 2)
+#define I2C_SCL         (3 << 0)
+
+#include <asm/gpio.h>
+#define gpio_to_irq	__gpio_to_irq
+#define gpio_set_value	__gpio_set_value
+#define gpio_get_value	__gpio_get_value
+
+#ifndef __ASSEMBLY__
+#include <asm-generic/gpio.h>
+
+struct t2200_gpio_pin_stat_info {
+	uint32_t t2200_gpio_pins;
+};
+
+extern struct t2200_gpio_pin_stat_info t2200_gpio_pin_stat;
+
+#endif	/* __ASSEMBLY__ */
+
+#endif
diff --git a/arch/arm/mach-transcede/include/mach/gpio-4000.h b/arch/arm/mach-transcede/include/mach/gpio-4000.h
new file mode 100644
index 0000000..9f66bb0
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/gpio-4000.h
@@ -0,0 +1,138 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef __ASM_ARCH_GPIO_4000_H
+#define __ASM_ARCH_GPIO_4000_H
+
+/***** GPIO  *****/
+#define TRANSCEDE_GPIO_OUTPUT_REG		AAB_XP_VADDR((TRANSCEDE_GPIO + 0x00))
+#define TRANSCEDE_GPIO_OE_REG			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x04))
+#define TRANSCEDE_GPIO_INT_CFG_REG		AAB_XP_VADDR((TRANSCEDE_GPIO + 0x08))
+#define TRANSCEDE_GPIO_INT_CFG_REG_2		AAB_XP_VADDR((TRANSCEDE_GPIO + 0x0C))
+#define TRANSCEDE_GPIO_INPUT_REG		AAB_XP_VADDR((TRANSCEDE_GPIO + 0x10))
+#define TRANSCEDE_GPIO_APB_WS			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x14))
+#define TRANSCEDE_GPIO_SYSTEM_CONFIG		AAB_XP_VADDR((TRANSCEDE_GPIO + 0x1C))
+#define TRANSCEDE_GPIO_EXP_MUX			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x24))
+#define TRANSCEDE_GPIO_TDM_MUX			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x28))
+#define TRANSCEDE_GPIO_GENERIC_PAD		AAB_XP_VADDR((TRANSCEDE_GPIO + 0x2C))
+#define TRANSCEDE_GPIO_IRQ_STATUS		AAB_XP_VADDR((TRANSCEDE_GPIO + 0x30))
+#define TRANSCEDE_GPIO_IRQ_CLEAR		AAB_XP_VADDR((TRANSCEDE_GPIO + 0x34))
+#define TRANSCEDE_GPIO_IRQ_MASK			AAB_XP_VADDR((TRANSCEDE_GPIO + 0x38))
+#define TRANSCEDE_GPIO_BOOTSTRAP_STATUS		AAB_XP_VADDR((TRANSCEDE_GPIO + 0x40))
+#define TRANSCEDE_GPIO_BOOTSTRAP_OVERRIDE	AAB_XP_VADDR((TRANSCEDE_GPIO + 0x44))
+#define TRANSCEDE_GPIO_GENERAL_CONTROL_REG	AAB_XP_VADDR((TRANSCEDE_GPIO + 0x4C))
+#define TRANSCEDE_GPIO_DEVICE_ID_REG		AAB_XP_VADDR((TRANSCEDE_GPIO + 0x50))
+#define TRANSCEDE_GPIO_PIN_SELECT		AAB_XP_VADDR((TRANSCEDE_GPIO + 0x58))
+#define TRANSCEDE_GPIO_SERDES_LANE_CTRL		AAB_XP_VADDR((TRANSCEDE_GPIO + 0x60))
+#define TRANSCEDE_GPIO_SERDES_PLL_CTRL		AAB_XP_VADDR((TRANSCEDE_GPIO + 0x64))
+#define TRANSCEDE_GPIO_SERDES_MGMT_ENABLE	AAB_XP_VADDR((TRANSCEDE_GPIO + 0x68))
+
+#define GPIO_0		0x00000001
+#define GPIO_1		0x00000002
+#define GPIO_2		0x00000004
+#define GPIO_3		0x00000008
+#define GPIO_4		0x00000010
+#define GPIO_5		0x00000020
+#define GPIO_6	        0x00000040
+#define GPIO_7		0x00000080
+#define GPIO_8		0x00000100
+#define GPIO_9		0x00000200
+#define GPIO_10		0x00000400
+#define GPIO_11		0x00000800
+#define GPIO_12		0x00001000
+#define GPIO_13		0x00002000
+#define GPIO_14		0x00004000
+#define GPIO_15		0x00008000
+#define GPIO_16		0x00010000
+#define GPIO_17		0x00020000
+#define GPIO_18		0x00040000
+#define GPIO_19		0x00080000
+#define GPIO_20		0x00100000
+#define GPIO_21		0x00200000
+#define GPIO_22		0x00400000
+#define GPIO_23		0x00800000
+#define GPIO_24		0x01000000
+#define GPIO_25		0x02000000
+#define GPIO_26		0x04000000
+#define GPIO_27		0x08000000
+#define GPIO_28		0x10000000
+#define GPIO_29		0x20000000
+#define GPIO_30		0x40000000
+#define GPIO_31		0x80000000
+
+/* Generic Pad Control */
+#define PAD_TDM_PULL_UP (1 << 18)
+#define PAD_TDM_POWER 	(1 << 17)
+#define PAD_SPI_POWER 	(1 << 16)
+#define PAD_I2C_PULL_UP	(1 << 15)
+#define PAD_I2C_POWER	(1 << 14)
+#define PAD_STRAP_POWER	(1 << 13)
+#define PAD_EXP_ADDRESS	(1 << 12)
+#define PAD_EXP_BUS	(1 << 11)
+#define PAD_EXP_HI	(1 << 10)
+#define PAD_EXP_HI_CLK	(1 << 9)
+#define PAD_GEM1_IF	(1 << 8)
+#define PAD_GEM1_CLKIN	(1 << 7)
+#define PAD_GEM0_IF	(1 << 6)
+#define PAD_GEM0_CLKIN	(1 << 5)
+#define PAD_MDIO	(1 << 4)
+#define PAD_TSU_VCXO	(1 << 3)
+#define PAD_TSU_NTG	(1 << 2)
+#define PAD_TSU_CLKIN	(1 << 1)
+#define PAD_TSU_RGMII	(1 << 0)
+
+/* GPIO Pin Select Pins */
+#define UART2_TX	(1 << 31)
+#define UART1_RX	(1 << 30)
+#define UART1_TX	(1 << 39)
+#define I2C_SDA		(1 << 28)
+#define I2C_SCL		(1 << 27)
+#define SPI_RXD		(1 << 26)
+#define SPI_TXD		(1 << 25)
+#define SPI_SS3_N	(1 << 24)
+#define SPI_SS2_N	(1 << 23)
+#define SPI_SS1_N	(1 << 22)
+#define SPI_SS0_N	(1 << 21)
+#define UART0_TX	(1 << 20)
+#define UART0_RX	(1 << 19)
+#define EXP_NAND_CS_N	(1 << 18)
+#define EXP_NAND_CLE	(1 << 17)
+#define EXP_RDY_BSY	(1 << 16)
+#define UART2_RX	(1 << 15)
+#define RAD_FRAMESYNC	(1 << 14)
+#define RAD_COM_OUT	(1 << 13)
+#define RAD_COM_IN	(1 << 12)
+
+
+#define SPI_BUS (SPI_RXD | SPI_RXD | SPI_TXD)
+#define SPI_SSN (SPI_SS0_N | SPI_SS1_N | SPI_SS2_N | SPI_SS3_N)	/* TODO move these to board file, enable only used ones */
+
+#define UART1_BUS	(UART1_RX | UART1_TX)
+#define I2C_BUS		(I2C_SCL | I2C_SDA)
+#define NAND_BUS	(EXP_RDY_BUSY | NAND_CS_N | EXP_NAND_CLE | EXP_NAND_CLE)
+
+
+
+/* general control register values */
+#define UART_LOOPBACK_ENABLE	(1 << 0)
+
+#endif
diff --git a/arch/arm/mach-transcede/include/mach/gpio.h b/arch/arm/mach-transcede/include/mach/gpio.h
new file mode 100644
index 0000000..b2ae8ec
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/gpio.h
@@ -0,0 +1,34 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef __ASM_ARCH_GPIO_H
+#define __ASM_ARCH_GPIO_H
+
+#if defined(CONFIG_MACH_M84XXX)
+#include "gpio-4000.h"
+#elif defined(CONFIG_MACH_M822XX)
+#include "gpio-2200.h"
+#else
+#error "Unsupported CPU"
+#endif
+
+#endif
diff --git a/arch/arm/mach-transcede/include/mach/hardware.h b/arch/arm/mach-transcede/include/mach/hardware.h
new file mode 100644
index 0000000..4ee2dc4
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/hardware.h
@@ -0,0 +1,40 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef __ASM_ARCH_HARDWARE_H
+#define __ASM_ARCH_HARDWARE_H
+
+#if defined(CONFIG_MACH_M84XXX)
+
+#include <mach/transcede-4000.h>
+
+#elif defined(CONFIG_MACH_M822XX)
+
+#include <mach/transcede-2200.h>
+
+#else
+
+#error "Unsupported CPU" 
+
+#endif
+
+#endif
diff --git a/arch/arm/mach-transcede/include/mach/i2c.h b/arch/arm/mach-transcede/include/mach/i2c.h
new file mode 100644
index 0000000..60b08b3
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/i2c.h
@@ -0,0 +1,60 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef __ASM_ARCH_I2C_H
+#define __ASM_ARCH_I2C_H
+
+#define TRANSCEDE_I2C_ADDR		(0x00*4)
+#define TRANSCEDE_I2C_DATA		(0x01*4)
+#define TRANSCEDE_I2C_CNTR		(0x02*4)
+#define TRANSCEDE_I2C_STAT		(0x03*4)
+#define TRANSCEDE_I2C_CCRFS		(0x03*4)
+#define TRANSCEDE_I2C_XADDR		(0x04*4)
+#define TRANSCEDE_I2C_CCRH		(0x05*4)
+#define TRANSCEDE_I2C_RESET		(0x07*4)
+
+/* CNTR - Control register bits */
+#define CNTR_IEN			(1<<7)
+#define CNTR_ENAB			(1<<6)
+#define CNTR_STA			(1<<5)
+#define CNTR_STP			(1<<4)
+#define CNTR_IFLG			(1<<3)
+#define CNTR_AAK			(1<<2)
+
+/* STAT - Status codes */
+#define STAT_BUS_ERROR			0x00	/* Bus error in master mode only */
+#define STAT_START			0x08	/* Start condition transmitted */
+#define STAT_START_REPEATED		0x10	/* Repeated Start condition transmited */
+#define STAT_ADDR_WR_ACK		0x18	/* Address + Write bit transmitted, ACK received */
+#define STAT_ADDR_WR_NACK		0x20	/* Address + Write bit transmitted, NACK received */
+#define STAT_DATA_WR_ACK		0x28	/* Data byte transmitted in master mode , ACK received */
+#define STAT_DATA_WR_NACK		0x30	/* Data byte transmitted in master mode , NACK received */
+#define STAT_ARBIT_LOST			0x38	/* Arbitration lost in address or data byte */
+#define STAT_ADDR_RD_ACK		0x40	/* Address + Read bit transmitted, ACK received  */
+#define STAT_ADDR_RD_NACK		0x48	/* Address + Read bit transmitted, NACK received  */
+#define STAT_DATA_RD_ACK		0x50	/* Data byte received in master mode, ACK transmitted  */
+#define STAT_DATA_RD_NACK		0x58	/* Data byte received in master mode, NACK transmitted*/
+#define STAT_ARBIT_LOST_ADDR		0x68	/* Arbitration lost in address  */
+#define STAT_GENERAL_CALL		0x70	/* General Call, ACK transmitted */
+#define STAT_NO_RELEVANT_INFO		0xF8	/* No relevant status information, IFLF=0 */
+
+#endif
diff --git a/arch/arm/mach-transcede/include/mach/io.h b/arch/arm/mach-transcede/include/mach/io.h
new file mode 100644
index 0000000..d6b474a
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/io.h
@@ -0,0 +1,84 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef __ASM_ARCH_IO_H
+#define __ASM_ARCH_IO_H
+
+#define CONFIG_FIX_EARLYCON_MEM
+#define FIX_EARLYCON_MEM_BASE 		TRANSCEDE_UART0
+#define set_fixmap_nocache(idx,phys)
+#define __fix_to_virt(x) 		AAB_XP_VADDR(x)
+
+#define IO_SPACE_LIMIT 0xFFFFFFFF
+
+#if !defined(CONFIG_PCI)
+
+#define __io(a)			((void __iomem *)(a))
+#define __mem_pci(a)	(a)
+
+#else
+
+#define __mem_pci(a)	(a)
+
+/* IO ports are not supported */
+#define outb(v,p)	__readwrite_bug("outb")
+#define outw(v,p)	__readwrite_bug("outw")
+#define outl(v,p)	__readwrite_bug("outl")
+
+#define inb(p)		(__readwrite_bug("inb"), 0)
+#define inw(p)		(__readwrite_bug("inw"), 0)
+#define inl(p)		(__readwrite_bug("inl"), 0)
+
+#define outsb(p,d,l)	__readwrite_bug("outsb")
+#define outsw(p,d,l)	__readwrite_bug("outsw")
+#define outsl(p,d,l)	__readwrite_bug("outsl")
+
+#define insb(p,d,l)	(__readwrite_bug("insb"), 0)
+#define insw(p,d,l)	(__readwrite_bug("insw"), 0)
+#define insl(p,d,l)	(__readwrite_bug("insl"), 0)
+
+/*
+ * io{read,write}{8,16,32} macros
+ */
+
+#define ioread8(p)	({ unsigned int __v = __raw_readb(p); __v; })
+#define ioread16(p)	({ unsigned int __v = le16_to_cpu(__raw_readw(p)); __v; })
+#define ioread32(p)	({ unsigned int __v = le32_to_cpu(__raw_readl(p)); __v; })
+
+#define iowrite8(v,p)	__raw_writeb(v, p)
+#define iowrite16(v,p)	__raw_writew(cpu_to_le16(v), p)
+#define iowrite32(v,p)	__raw_writel(cpu_to_le32(v), p)
+
+#define ioread8_rep(p,d,c)	__raw_readsb(p,d,c)
+#define ioread16_rep(p,d,c)	__raw_readsw(p,d,c)
+#define ioread32_rep(p,d,c)	__raw_readsl(p,d,c)
+
+#define iowrite8_rep(p,s,c)	__raw_writesb(p,s,c)
+#define iowrite16_rep(p,s,c)	__raw_writesw(p,s,c)
+#define iowrite32_rep(p,s,c)	__raw_writesl(p,s,c)
+
+#define ioport_map(c,s)		(__readwrite_bug("ioport_map"), NULL)
+#define ioport_unmap(addr)	__readwrite_bug("ioport_unmap")
+
+#endif /* !defined(CONFIG_PCI) */
+
+#endif
diff --git a/arch/arm/mach-transcede/include/mach/irqs-2200.h b/arch/arm/mach-transcede/include/mach/irqs-2200.h
new file mode 100644
index 0000000..7d6876a
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/irqs-2200.h
@@ -0,0 +1,151 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef __ASM_ARCH_IRQS_2200_H
+#define __ASM_ARCH_IRQS_2200_H
+
+#define IRQ_INTER_ARM0		(IRQ_GIC_START + 0)
+#define IRQ_INTER_ARM1		(IRQ_GIC_START + 1)
+#define IRQ_INTER_ARM2		(IRQ_GIC_START + 2)
+#define IRQ_INTER_ARM3		(IRQ_GIC_START + 3)
+#define IRQ_INTER_ARM(n)	(IRQ_INTER_ARM_0 + ((n)&3))
+#define IRQ_ARM2_L2_CACHE	(IRQ_GIC_START + 8)
+#define IRQ_PMU0		(IRQ_GIC_START + 10)
+#define IRQ_PMU1		(IRQ_GIC_START + 11)
+#define IRQ_FEC_DL_DMA		(IRQ_GIC_START + 12)
+#define IRQ_FEC_UL_DMA		(IRQ_GIC_START + 14)
+#define IRQ_FEC_RM_DONE		(IRQ_GIC_START + 14)
+#define IRQ_FEC_MISC		(IRQ_GIC_START + 15)
+#define IRQ_SPAcc_IRQ0		(IRQ_GIC_START + 16)
+#define IRQ_SPAcc_IRQ1		(IRQ_GIC_START + 17)
+#define IRQ_ESPAH		(IRQ_GIC_START + 18)
+#define IRQ_SASPA		(IRQ_GIC_START + 19)
+#define IRQ_DDR_CTRL		(IRQ_GIC_START + 20)
+#define IRQ_MDMA_SYS0_IRQ0	(IRQ_GIC_START + 22)
+#define IRQ_MDMA_SYS0_IRQ1	(IRQ_GIC_START + 23)
+#define IRQ_MDMA_SYS1_IRQ0	(IRQ_GIC_START + 24)
+#define IRQ_MDMA_SYS1_IRQ1	(IRQ_GIC_START + 25)
+#define IRQ_CEVA0		(IRQ_GIC_START + 30)
+#define IRQ_CEVA1		(IRQ_GIC_START + 31)
+#define IRQ_CEVA(n)		(IRQ_CEVA0 + ((n)&1))
+#define IRQ_TX_TIMER		(IRQ_GIC_START + 37)
+#define IRQ_BPDMA3_MEM2DMA	(IRQ_GIC_START + 38)
+#define IRQ_BPDMA2_MEM2DMA	(IRQ_GIC_START + 39)
+#define IRQ_BPDMA1_MEM2DMA	(IRQ_GIC_START + 40)
+#define IRQ_BPDMA0_DMA2MEM	(IRQ_GIC_START + 41)
+#define IRQ_BPDMA0_MEM2DMA	(IRQ_GIC_START + 42)
+#define IRQ_CVCOMB2ARM		(IRQ_GIC_START + 43)
+#define IRQ_FPDMA_IN		(IRQ_GIC_START + 44)
+#define IRQ_FPDMA_OUT		(IRQ_GIC_START + 45)
+#define IRQ_C2C_DIST0		(IRQ_GIC_START + 46)
+#define IRQ_C2C_DIST1		(IRQ_GIC_START + 47)
+#define IRQ_FP_TO_ARM		(IRQ_GIC_START + 48)
+#define IRQ_CPRI0		(IRQ_GIC_START + 50)
+#define IRQ_CPDMA_TX0		(IRQ_GIC_START + 53)
+#define IRQ_CPDMA_RX0		(IRQ_GIC_START + 54)
+#define IRQ_TDONE2ARM0		(IRQ_GIC_START + 56)
+#define IRQ_TDONE2ARM1		(IRQ_GIC_START + 57)
+#define IRQ_TDONE2ARM2		(IRQ_GIC_START + 58)
+#define IRQ_TDONE2ARM3		(IRQ_GIC_START + 59)
+#define IRQ_TDONE2ARM4		(IRQ_GIC_START + 60)
+#define IRQ_TDONE2ARM5		(IRQ_GIC_START + 61)
+#define IRQ_TDONE2ARM6		(IRQ_GIC_START + 62)
+#define IRQ_TDONE2ARM7		(IRQ_GIC_START + 63)
+#define IRQ_CRP_ERR		(IRQ_GIC_START + 64)
+#define IRQ_SYNCNET2ARM		(IRQ_GIC_START + 65)
+#define IRQ_IQCNTR2ARM0		(IRQ_GIC_START + 66)
+#define IRQ_IQCNTR2ARM1		(IRQ_GIC_START + 67)
+#define IRQ_IQCNTR2ARM2		(IRQ_GIC_START + 68)
+#define IRQ_IQCNTR_OVF		(IRQ_GIC_START + 69)
+#define IRQ_PCIE_X1		(IRQ_GIC_START + 72)
+#define IRQ_PCIE_X4		(IRQ_GIC_START + 73)
+#define IRQ_PCIE_X4_DMA		(IRQ_GIC_START + 74)
+#define IRQ_RAD_TIMER0		(IRQ_GIC_START + 76)
+#define IRQ_RAD_TIMER1		(IRQ_GIC_START + 77)
+#define IRQ_RAD_TIMER2		(IRQ_GIC_START + 78)
+#define IRQ_RAD_TIMER3		(IRQ_GIC_START + 79)
+#define IRQ_JESD0		(IRQ_GIC_START + 84)
+#define IRQ_JESD0_DPLANE	(IRQ_GIC_START + 85)
+#define IRQ_JESD1		(IRQ_GIC_START + 86)
+#define IRQ_JESD1_DPLANE	(IRQ_GIC_START + 87)
+#define IRQ_JDMA0_TX		(IRQ_GIC_START + 88)
+#define IRQ_JDMA0_RX		(IRQ_GIC_START + 89)
+#define IRQ_JDMA1_TX		(IRQ_GIC_START + 90)
+#define IRQ_JDMA1_RX		(IRQ_GIC_START + 91)
+#define IRQ_JDMA_TX(id)		(IRQ_JDMA0_TX + 2*(id))
+#define IRQ_JDMA_RX(id)		(IRQ_JDMA0_RX + 2*(id))
+#define IRQ_GEM0		(IRQ_GIC_START + 92)
+#define IRQ_GEM0_1		(IRQ_GIC_START + 93)
+#define IRQ_GEM0_2		(IRQ_GIC_START + 94)
+#define IRQ_GEM0_3		(IRQ_GIC_START + 95)
+#define IRQ_USIM		(IRQ_GIC_START + 96)
+#define IRQ_USB			(IRQ_GIC_START + 97)
+#define IRQ_GEM0		(IRQ_GIC_START + 92)
+#define IRQ_GEM1		(IRQ_GIC_START + 98)
+#define IRQ_TSU_NTG		(IRQ_GIC_START + 102)
+#define IRQ_GPIO_0		(IRQ_GIC_START + 104)
+#define IRQ_GPIO_1		(IRQ_GIC_START + 105)
+#define IRQ_GPIO_2		(IRQ_GIC_START + 106)
+#define IRQ_GPIO_3		(IRQ_GIC_START + 107)
+#define IRQ_GPIO_4		(IRQ_GIC_START + 108)
+#define IRQ_GPIO_5		(IRQ_GIC_START + 109)
+#define IRQ_GPIO_6		(IRQ_GIC_START + 110)
+#define IRQ_GPIO_7		(IRQ_GIC_START + 111)
+#define IRQ_GPIO(n)		(IRQ_GPIO_0 + (n))
+#define IRQ_TIMER		(IRQ_GIC_START + 112)
+#define IRQ_TDM_TIMER		(IRQ_GIC_START + 113)
+#define IRQ_UART0		(IRQ_GIC_START + 114)
+#define IRQ_UART1		(IRQ_GIC_START + 115)
+#define IRQ_SPI0                (IRQ_GIC_START + 116)
+#define IRQ_SPI1                (IRQ_GIC_START + 117)
+#define IRQ_DUS			(IRQ_GIC_START + 118)
+#define IRQ_I2C			(IRQ_GIC_START + 119)
+#define IRQ_TDM_NTG		(IRQ_GIC_START + 120)
+#define IRQ_TDMA_TXCHK0		(IRQ_GIC_START + 121)
+#define IRQ_TDMA_TXCHK1		(IRQ_GIC_START + 122)
+#define IRQ_TDMA_TXCHK2		(IRQ_GIC_START + 123)
+#define IRQ_TDMA_TXCHK3		(IRQ_GIC_START + 124)
+#define IRQ_TDMA_ERR_RX		(IRQ_GIC_START + 125)
+#define IRQ_TDMA_ERR_TX		(IRQ_GIC_START + 126)
+#define IRQ_TDMA		(IRQ_GIC_START + 127)
+
+/* Software decoded interrupts used by PCIe */
+#define TRANSCEDE_IRQ_MAX    (IRQ_GIC_START + 128)
+
+/* PCIE MSI virtual IRQs */
+#define PCIE_NUM_MSI_IRQS   32
+#define PCIE0_MSI_INT_BASE  (TRANSCEDE_IRQ_MAX + 0)
+#define PCIE0_MSI_INT_END   (PCIE0_MSI_INT_BASE + PCIE_NUM_MSI_IRQS)
+#define PCIE1_MSI_INT_BASE  (PCIE0_MSI_INT_END + 0)
+#define PCIE1_MSI_INT_END   (PCIE1_MSI_INT_BASE + PCIE_NUM_MSI_IRQS)
+
+#define PCIE_NUM_INTX_IRQS  4
+#define PCIE0_INTX_BASE     (PCIE1_MSI_INT_END + 0)
+#define PCIE0_INTX_END      (PCIE0_INTX_BASE + PCIE_NUM_INTX_IRQS)
+#define PCIE1_INTX_BASE     (PCIE0_INTX_END + 0)
+#define PCIE1_INTX_END      (PCIE1_INTX_BASE + PCIE_NUM_INTX_IRQS)
+
+#define VIRQ_END            PCIE1_INTX_END
+
+#define NR_IRQS			256
+
+#endif
diff --git a/arch/arm/mach-transcede/include/mach/irqs-4000.h b/arch/arm/mach-transcede/include/mach/irqs-4000.h
new file mode 100644
index 0000000..4a31246
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/irqs-4000.h
@@ -0,0 +1,119 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef __ASM_ARCH_IRQS_4000_H
+#define __ASM_ARCH_IRQS_4000_H
+
+#define IRQ_INTER_ARM0		(IRQ_GIC_START + 0)
+#define IRQ_INTER_ARM1		(IRQ_GIC_START + 1)
+#define IRQ_INTER_ARM2		(IRQ_GIC_START + 2)
+#define IRQ_INTER_ARM3		(IRQ_GIC_START + 3)
+#define IRQ_INTER_ARM4		(IRQ_GIC_START + 4)
+#define IRQ_INTER_ARM5		(IRQ_GIC_START + 5)
+#define IRQ_INTER_ARM6		(IRQ_GIC_START + 6)
+#define IRQ_INTER_ARM7		(IRQ_GIC_START + 7)
+#define IRQ_INTER_ARM(n)	(IRQ_INTER_ARM_0 + (n))
+#define IRQ_ARM2_L2_CACHE	(IRQ_GIC_START + 8)
+#define IRQ_ARM4_L2_CACHE	(IRQ_GIC_START + 9)
+#define IRQ_ARM2_MONITOR	(IRQ_GIC_START + 10)
+#define IRQ_ARM4_MONITOR	(IRQ_GIC_START + 11)
+#define IRQ_FEC0		(IRQ_GIC_START + 12)
+#define IRQ_FEC1		(IRQ_GIC_START + 13)
+#define IRQ_SPAcc		(IRQ_GIC_START + 14)
+#define IRQ_ESPAH		(IRQ_GIC_START + 15)
+#define IRQ_DDR_CTRL		(IRQ_GIC_START + 16)
+#define IRQ_MDMA		(IRQ_GIC_START + 17)
+#define IRQ_CEVA0		(IRQ_GIC_START + 30)
+#define IRQ_CEVA1		(IRQ_GIC_START + 31)
+#define IRQ_CEVA2		(IRQ_GIC_START + 32)
+#define IRQ_CEVA3		(IRQ_GIC_START + 33)
+#define IRQ_CEVA4		(IRQ_GIC_START + 34)
+#define IRQ_CEVA5		(IRQ_GIC_START + 35)
+#define IRQ_CEVA6		(IRQ_GIC_START + 36)
+#define IRQ_CEVA7		(IRQ_GIC_START + 37)
+#define IRQ_CEVA8		(IRQ_GIC_START + 38)
+#define IRQ_CEVA9		(IRQ_GIC_START + 39)
+#define IRQ_CEVA(n)		(IRQ_CEVA_0 + (n))
+#define IRQ_FFT_OUT		(IRQ_GIC_START + 46)
+#define IRQ_FFT_IN		(IRQ_GIC_START + 47)
+#define IRQ_FP_TO_ARM		(IRQ_GIC_START + 48)
+#define IRQ_MDMA_SPU		(IRQ_GIC_START + 49)
+#define IRQ_CPRI0		(IRQ_GIC_START + 50)
+#define IRQ_CPRI1		(IRQ_GIC_START + 51)
+#define IRQ_CPRI2		(IRQ_GIC_START + 52)
+#define IRQ_CPRI3		(IRQ_GIC_START + 53)
+#define IRQ_CPRI4		(IRQ_GIC_START + 54)
+#define IRQ_CPRI5		(IRQ_GIC_START + 55)
+#define IRQ_CPDMA_TX0		(IRQ_GIC_START + 56)
+#define IRQ_CPDMA_TX1		(IRQ_GIC_START + 57)
+#define IRQ_CPDMA_TX2		(IRQ_GIC_START + 58)
+#define IRQ_CPDMA_TX3		(IRQ_GIC_START + 59)
+#define IRQ_CPDMA_TX4		(IRQ_GIC_START + 60)
+#define IRQ_CPDMA_TX5		(IRQ_GIC_START + 61)
+#define IRQ_CPDMA_RX0		(IRQ_GIC_START + 62)
+#define IRQ_CPDMA_RX1		(IRQ_GIC_START + 63)
+#define IRQ_CPDMA_RX2		(IRQ_GIC_START + 64)
+#define IRQ_CPDMA_RX3		(IRQ_GIC_START + 65)
+#define IRQ_CPDMA_RX4		(IRQ_GIC_START + 66)
+#define IRQ_CPDMA_RX5		(IRQ_GIC_START + 67)
+#define IRQ_CPRI(n)		(IRQ_CPRI_0 + (n))
+#define IRQ_CPDMA_TX(n)		(IRQ_CPDMA_TX_0 + (n))
+#define IRQ_CPDMA_RX(n)		(IRQ_CPDMA_RX_0 + (n))
+#define IRQ_SRIO0		(IRQ_GIC_START + 68)
+#define IRQ_SRIO1		(IRQ_GIC_START + 69)
+#define IRQ_PCIE0		(IRQ_GIC_START + 70)
+#define IRQ_PCIE1		(IRQ_GIC_START + 71)
+#define IRQ_PCIE2		(IRQ_GIC_START + 72)
+#define IRQ_PCIE3		(IRQ_GIC_START + 73)
+#define IRQ_MDMA_RAD		(IRQ_GIC_START + 74)
+#define IRQ_GEM0		(IRQ_GIC_START + 100)
+#define IRQ_ADM0		(IRQ_GIC_START + 101)
+#define IRQ_GEM1		(IRQ_GIC_START + 102)
+#define IRQ_ADM1		(IRQ_GIC_START + 103)
+#define IRQ_SGMII_SERDES	(IRQ_GIC_START + 104)
+#define IRQ_GEM_NTG		(IRQ_GIC_START + 105)
+#define IRQ_GPIO0		(IRQ_GIC_START + 106)
+#define IRQ_GPIO1		(IRQ_GIC_START + 107)
+#define IRQ_GPIO2		(IRQ_GIC_START + 108)
+#define IRQ_GPIO3		(IRQ_GIC_START + 109)
+#define IRQ_GPIO(n)		(IRQ_GPIO_0 + (n))
+#define IRQ_GPIO_COMB		(IRQ_GIC_START + 110)
+#define IRQ_TIMER		(IRQ_GIC_START + 112)
+#define IRQ_TDM_TIMER		(IRQ_GIC_START + 113)
+#define IRQ_UART0		(IRQ_GIC_START + 114)
+#define IRQ_UART1		(IRQ_GIC_START + 115)
+#define IRQ_UART2		(IRQ_GIC_START + 116)
+#define IRQ_UART(n)		(IRQ_UART_0 + (n))
+#define IRQ_SPI			(IRQ_GIC_START + 117)
+#define IRQ_I2C			(IRQ_GIC_START + 118)
+#define IRQ_HBI			(IRQ_GIC_START + 119)
+#define IRQ_TDM_NTG		(IRQ_GIC_START + 121)
+#define IRQ_TDM_SIG		(IRQ_GIC_START + 122)
+#define IRQ_TDM_ERR_RX		(IRQ_GIC_START + 123)
+#define IRQ_TDM_ERR_TX		(IRQ_GIC_START + 124)
+#define IRQ_TDMA		(IRQ_GIC_START + 125)
+#define IRQ_TDM_TX		(IRQ_GIC_START + 126)
+#define IRQ_TDM_RX		(IRQ_GIC_START + 127)
+
+#define NR_IRQS			256
+
+#endif
diff --git a/arch/arm/mach-transcede/include/mach/irqs.h b/arch/arm/mach-transcede/include/mach/irqs.h
new file mode 100644
index 0000000..4168405
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/irqs.h
@@ -0,0 +1,50 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef __ASM_ARCH_IRQS_H
+#define __ASM_ARCH_IRQS_H
+
+#define IRQ_GLOBALTIMER		27
+#define IRQ_LOCALTIMER		29
+#define IRQ_LOCALWDOG		30
+
+#define IRQ_GIC_START		32
+
+#if defined(CONFIG_MACH_M84XXX)
+
+#include "irqs-4000.h"
+
+#elif defined(CONFIG_MACH_M822XX)
+
+#include "irqs-2200.h"
+
+#else
+
+#error "Unsupported CPU"
+
+#endif
+
+#ifndef NR_IRQS
+#error "NR_IRQS not defined by the board-specific files"
+#endif
+
+#endif
diff --git a/arch/arm/mach-transcede/include/mach/memory.h b/arch/arm/mach-transcede/include/mach/memory.h
new file mode 100644
index 0000000..12dffaa
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/memory.h
@@ -0,0 +1,36 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef __ASM_ARCH_MEMORY_H
+#define __ASM_ARCH_MEMORY_H
+
+#include <mach/hardware.h>
+
+#define PHYS_OFFSET		TRANSCEDE_SDRAM_BASE
+
+#define __virt_to_bus(x)	__virt_to_phys(x)
+#define __bus_to_virt(x)	__phys_to_virt(x)
+
+#define __pfn_to_bus(x)		(__pfn_to_phys(x) - PHYS_OFFSET)
+#define __bus_to_pfn(x)		__phys_to_pfn((x) + PHYS_OFFSET)
+
+#endif
diff --git a/arch/arm/mach-transcede/include/mach/mips_monitor.h b/arch/arm/mach-transcede/include/mach/mips_monitor.h
new file mode 100644
index 0000000..e8d803e
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/mips_monitor.h
@@ -0,0 +1,81 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifdef CONFIG_TRANSCEDE_MIPS_MONITOR
+
+#ifndef _MIPS_MONITOR_H_
+#define _MIPS_MONITOR_H_
+
+#include <linux/threads.h>
+#include <asm/sizes.h>
+
+#define MMTicksDiff(CurrTick, LastTick) ((CurrTick >= LastTick) ? (CurrTick - LastTick) : (0xFFFFFFFF - LastTick + CurrTick))
+
+typedef struct idle_mips_monitor_t {
+    u32 last_avg_ratio[NR_CPUS];         /* avg mips % */
+    u32 last_ratio[NR_CPUS];             /* instant mips % */
+
+    u32 accum_idle_time_ticks [NR_CPUS]; /* sum of IDLE ticks within interval of measurments */
+    u32 currnt_idle_start[NR_CPUS];      /* start of latest IDLE  task */
+    u32 tick_start[NR_CPUS];             /* local timer start */
+    u32 total_time_ticks [NR_CPUS];      /* total ticks within interval */
+    u32 tick_period [NR_CPUS];           /* period of global timer in ticks */
+    u32 last_total_time[NR_CPUS];        /* lats measurmenmt of total time */
+    u32 meas_counter [NR_CPUS];          /* current counter */
+    u32 meas_interval;                   /* measurmenmt period in ticks of global timer */
+} idle_mips_monitor_t;
+
+idle_mips_monitor_t* mmonitor_get_ctx(void);
+
+static inline unsigned int mips_monitor_idle_start(unsigned int cpu)
+{
+    unsigned int         idle_start = 0;
+    idle_mips_monitor_t* mmonitor_p;
+
+    mmonitor_p = mmonitor_get_ctx();
+
+    if (mmonitor_p){
+        idle_start = mmonitor_p->currnt_idle_start[cpu];
+    }
+
+    return idle_start;
+}
+
+static inline unsigned int mips_monitor_idle_start_update(unsigned int cpu, unsigned int timemark)
+{
+    idle_mips_monitor_t *mmonitor_p;
+
+    mmonitor_p = mmonitor_get_ctx();
+
+    if (mmonitor_p){
+        mmonitor_p->currnt_idle_start[cpu] = timemark;
+    }
+
+	return 0;
+}
+
+unsigned int mips_monitor_idle_update(unsigned int start, unsigned int endtime, unsigned int cpu);
+unsigned int mips_monitor_tick_timer_update(unsigned int time, unsigned int cpu);
+
+#endif
+#endif
+
diff --git a/arch/arm/mach-transcede/include/mach/mlog.h b/arch/arm/mach-transcede/include/mach/mlog.h
new file mode 100644
index 0000000..0ec3eaf
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/mlog.h
@@ -0,0 +1,490 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifdef CONFIG_TRANSCEDE_MLOG
+
+#ifndef _MLOG_H_
+#define _MLOG_H_
+
+#include <linux/threads.h>
+#include <asm/sizes.h>
+
+#define TASK_ID_SELECT (35000)
+#define TASK_ID_IOCTL (35001)
+#define TASK_ID_PSELECT (35002)
+#define TASK_ID_IRQ (36000)
+
+//#define MLOG_SUBTASK_ENABLE    //allows for ceva sub task profiling for some tasks.
+//#define MLOG_ADD_DEPENDENCY    // allows to add dependencies in MLOG
+//#define MLOG_IRQ_SUP_ENABLE    //allows  for monitiring time spent by arm in irq or supervisor more ( heavy logging)
+
+#if defined(_WIN32) || defined(__linux__) || defined(__KERNEL__)
+#define MX_OK 0
+#endif
+
+#ifndef _WIN32
+#define MLOG_ENABLED
+#endif
+
+#define MLOG_INLINE __always_inline
+
+#define MLOG_TRUE             1
+#define MLOG_FALSE            0
+
+#define MLOG_DWORD unsigned int
+#define MLOG_UINTSIZE       4
+#define MLOG_BUFFERS_COUNT  4
+
+
+#define MLOG_NOP              0x00000000
+#define MLOG_NEWLOG           0x01000000
+#define MLOG_INFO             0x08000000
+#define MLOG_ERROR            0x09000000
+#define MLOG_END              0x0A000000
+#define MLOG_CONTEXT          0x14000000
+#define MLOG_INPUT            0x15000000
+#define MLOG_OUTPUT           0x16000000
+#define MLOG_BUFFERIN         0x17000000
+#define MLOG_BUFFEROUT        0x18000000
+#define MLOG_BUFFER           0x19000000
+
+#define MLOG_VAR_SELECT_0     0xDEAD0000
+#define MLOG_VAR_SELECT_1     0xDEAD0002
+#define MLOG_VAR_IRQ_0        0xDEAD1000
+#define MLOG_VAR_IRQ_1        0xDEAD1001
+#define MLOG_VAR_DABT         0xDEAD1002
+#define MLOG_VAR_IOCTL        0xDEAD0001
+
+#define MLOG_VAR_MIPS         0xDEAD2000
+
+#define MLOG_SIZE_MASK        0x00FFFFFF
+#define MLOG_ID_MASK          0xFF000000
+
+#define MLOG_SIZE_EXESTART      12
+#define MLOG_EXESTART           0x0C000000
+#define MLOG_ID_EXESTART        0x0C00000C // MLOG_EXESTART & MLOG_SIZE_EXESTART
+
+#define MLOG_SIZE_EXEFINISH     20
+//#define MLOG_SIZE_EXEFINISH     16
+//#define MLOG_EXEFINISH          0x0D000000
+#define MLOG_ID_EXEFINISH       0x0D000014 // MLOG_EXEFINISH & MLOG_SIZE_EXEFINISH
+//#define MLOG_ID_EXEFINISH       0x0D000010 // MLOG_EXEFINISH & MLOG_SIZE_EXEFINISH
+
+#define MLOG_SIZE_NEWTASK       12
+#define MLOG_NEWTASK            0x0F000000
+#define MLOG_ID_NEWTASK         0x0F00000C // MLOG_NEWTASK  & MLOG_SIZE_NEWTASK
+
+#define MLOG_SIZE_SUBTASK       12
+#define MLOG_SUBTASK            0x20000000
+#define MLOG_ID_SUBTASK         0x2000000C // MLOG_SUBTASK & MLOG_SIZE_SUBTASK
+
+#define MLOG_SIZE_FREQ          12
+#define MLOG_FREQ               0x21000000
+#define MLOG_ID_FREQ            0x2100000C // MLOG_FREQ   & MLOG_SIZE_FREQ
+
+#define MLOG_SIZE_TASK          20
+#define MLOG_TASK               0x22000000
+#define MLOG_ID_TASK            0x22000014 // MLOG_TASK   & MLOG_SIZE_TASK
+
+#define MLOG_SIZE_MARK          8
+#define MLOG_MARK               0x23000000
+#define MLOG_ID_MARK            0x23000008 // MLOG_FREQ   & MLOG_SIZE_FREQ
+
+#define MLOG_SIZE_LIST          4
+#define MLOG_LIST               0x25000000
+#define MLOG_ID_LIST            0x25000004
+
+#define MLOG_LISTSTART          0x26000000
+#define MLOG_ID_LISTSTART       0x26000008
+
+#define MLOG_LISTSTOP           0x27000000
+#define MLOG_ID_LISTSTOP        0x27000008
+
+#define MLOG_SIZE_ADDTOLIST     8
+#define MLOG_ADDTOLIST          0x28000000
+#define MLOG_ID_ADDTOLIST       0x28000008
+
+#define MLOG_SIZE_RESCONDITION  12
+#define MLOG_RESCONDITION       0x33000000
+#define MLOG_ID_RESCONDITION    0x3300000C
+
+#define MLOG_SIZE_MARK_TCB_CTRL 8
+#define MLOG_MARK_TCB_CTRL      0x34000000
+#define MLOG_ID_MARK_TCB_CTRL   0x34000008
+
+
+#define MLOG_SIZE_DEPENDS       8
+#define MLOG_DEPENDS            0x02000000
+#define MLOG_ID_DEPENDS         0x02000008 // MLOG_DEPENDS  & MLOG_SIZE_DEPENDS
+#define MLOG_DEPENDS_TCB_TCB    MLOG_ID_DEPENDS
+
+#define MLOG_TASKDEPENDS        0x24000000
+#define MLOG_ID_TASKDEPENDS     0x24000008 // MLOG_TASKDEPENDS  & MLOG_SIZE_DEPENDS
+#define MLOG_DEPENDS_KEY_KEY    MLOG_ID_TASKDEPENDS
+
+#define MLOG_LTDEPENDS          0x29000000
+#define MLOG_ID_LTDEPENDS       0x29000008
+#define MLOG_DEPENDS_LISTID_TCB MLOG_ID_LTDEPENDS
+
+#define MLOG_TLDEPENDS          0x30000000
+#define MLOG_ID_TLDEPENDS       0x30000008
+#define MLOG_DEPENDS_TCB_LISTID MLOG_ID_TLDEPENDS
+
+#define MLOG_LKDEPENDS          0x31000000
+#define MLOG_ID_LKDEPENDS       0x31000008
+#define MLOG_DEPENDS_LISTID_KEY MLOG_ID_LKDEPENDS
+
+#define MLOG_KLDEPENDS          0x32000000
+#define MLOG_ID_KLDEPENDS       0x32000008
+#define MLOG_DEPENDS_KEY_LISTID MLOG_ID_KLDEPENDS
+
+#define MLOG_SIZE_FR_SUBFR_NUM  8
+#define MLOG_FR_SUBFR_NUM       0x36000000
+#define MLOG_ID_FR_SUBFR        0x36000008
+
+#define MLOG_ID_ADD_VARIABLES   0x37000000
+
+#define MLOG_SIZE_DEVICE_INFO   4
+#define MLOG_DEVICE_INFO        0x38000000
+#define MLOG_ID_DEVICE_INFO     0x38000004
+
+#define MLOG_SIZE_CACHE_MIPS_STATS         ((CPU_NUM*7)+2)*4  // 2 = CPU_NUM and GetTicks
+#define MLOG_SIZE_CACHE_MIPS_STATS_SINGLE  ((1*8)+2)*4                   // 2 = CPU_NUM and GetTicks
+#define MLOG_CACHE_MIPS_STATS               0x39000000
+#define MLOG_ID_CACHE_MIPS_STATS            MLOG_CACHE_MIPS_STATS | MLOG_SIZE_CACHE_MIPS_STATS
+#define MLOG_ID_CACHE_MIPS_STATS_SINGLE     MLOG_CACHE_MIPS_STATS | MLOG_SIZE_CACHE_MIPS_STATS_SINGLE
+
+
+
+#define MLOG_MAGIC                 0x474f4c4d  // "MLOG"
+#define MLOG_VERSION               30
+
+#define MLOG_FILENAME "mlog.bin"
+
+
+#define MLOG_MARK_NOTHING        0
+#define MLOG_MARK_FRAMEBORDER    1
+
+#define MLOG_MAX_LOGGING_SPACE   1000000 // Ticks (Some arbitrary large Tick Counter)
+#define MLOG_DONT_LOCK           0
+#define MLOG_PUT_LOCK            1
+
+
+
+#define MLOG_MODE_UNKNOWN        0
+#define MLOG_MODE_IRQ            1
+#define MLOG_MODE_SUPERVISOR     2
+#define MLOG_MODE_IDLE           3
+
+#if !defined(_WIN32) && !defined(__linux__) && !defined(__KERNEL__)
+#define RC_MLOG_ALLOC_ERROR         DEF_USRRC(APP_MODULE, MLOG_SVSR,  1)
+#define RC_MLOG_CTX_ERROR           DEF_USRRC(APP_MODULE, MLOG_SVSR,  2)
+#define MLOG_DEVICE_COUNT           1
+
+#else
+#define RC_MLOG_ALLOC_ERROR         1
+#define RC_MLOG_CTX_ERROR           2
+#define RC_MLOG_NO_SPACE            3
+
+#if defined(__KERNEL__)
+#if defined(CONFIG_MACH_M822XX)
+#define CPU_NUM         2
+#elif defined(CONFIG_MACH_M84XXX)
+#if defined(CONFIG_TRANSCEDE_DUALCORE)
+#define CPU_NUM         2
+#else
+#define CPU_NUM         4
+#endif /* CONFIG_TRANSCEDE_DUALCORE */
+#else /* CONFIG_MACH_M84XXX */
+#error "Unsupported CPU"
+#endif /* CONFIG_MACH_M822XX */
+#else /* __KERNEL__ */
+#define CPU_NUM         1
+#endif
+
+#define MLOG_DEVICE_COUNT           0
+
+#ifdef CONFIG_MACH_M84XXX
+/* Frame limit is decreased if number of CPU is increased.
+ * This is done for MLog storage size to fit into system allocation limits.
+ * Usually there are problems allocating more than 4Mb with kmalloc().
+ */
+#define MLOG_FRAME_LIMIT (400 / CPU_NUM)
+#else
+#define MLOG_FRAME_LIMIT 200
+#endif
+
+#define MLOG_FRAME_SIZE  (7*1024)//3584  //4*1024
+#define MLOG_STORAGE_SIZE (MLOG_FRAME_LIMIT * MLOG_FRAME_SIZE * (CPU_NUM + MLOG_DEVICE_COUNT))
+
+#if defined(CONFIG_MACH_M84XXX) && (MLOG_STORAGE_SIZE >= SZ_4M)
+#error On T4000 MLOG_STORAGE_SIZE should be less than 4Mb in order to kmalloc() to operate correctly!
+#endif
+
+#define _memcpy memcpy
+#ifndef INLINE
+#define INLINE
+#endif
+
+typedef struct _SYSFPART_
+{
+    MLOG_DWORD *storage;
+    MLOG_DWORD *freeblk;
+    MLOG_DWORD blksize;
+    MLOG_DWORD blkcnt;
+    MLOG_DWORD AllocCnt;
+} SYSFPART, *PSYSFPART;
+
+#endif
+
+
+typedef struct tagMLogFHPart
+{
+    MLOG_DWORD Index;
+    MLOG_DWORD Size;
+    MLOG_DWORD StaticData;
+    MLOG_DWORD DynamicData;
+}MLOGFHPART;
+
+
+typedef struct tagMLogFileHeader
+{
+    MLOG_DWORD Magic;
+    MLOG_DWORD Version;
+    MLOG_DWORD BasePointer;
+    MLOG_DWORD PartNum;
+} MLOGFILEHEADER;
+
+
+typedef struct MLOG_FRAME_BLOCK_HEADER MLOG_FRAME_BLOCK_HEADER;
+
+struct MLOG_FRAME_BLOCK_HEADER
+{
+    MLOG_FRAME_BLOCK_HEADER *next;
+    MLOG_DWORD payload;
+    MLOG_DWORD size;
+    unsigned char *cur_data;
+};
+
+typedef struct _MLOG_STORAGE_HEADER
+{
+    SYSFPART mem_part;
+    MLOG_DWORD mempart_size;
+    unsigned char *    ptr;
+    MLOG_FRAME_BLOCK_HEADER * static_block;
+    MLOG_FRAME_BLOCK_HEADER * first_block;
+    MLOG_FRAME_BLOCK_HEADER * current_block;
+    MLOG_DWORD block_num;
+    MLOG_DWORD tti_counter;
+    MLOG_DWORD payload;
+    MLOGFHPART *hdr;
+}MLOG_STORAGE_HEADER;
+
+typedef unsigned int (*write_fn) (unsigned int cpu, unsigned char *rec, unsigned int size);
+
+typedef struct _MLOG_STORAGE_CTX
+{
+    MLOG_STORAGE_HEADER *storage[CPU_NUM + MLOG_DEVICE_COUNT];
+    MLOG_DWORD tti_counter;
+    MLOG_DWORD buff_num;
+    unsigned char *        ptr;
+    MLOGFILEHEADER *hdr;
+    write_fn   wr_dyn[CPU_NUM];
+    write_fn   wr_stat[CPU_NUM];
+}MLOG_STORAGE_CTX;
+
+typedef struct _MLOG_CTX_
+{
+            MLOG_DWORD      Opened;
+            MLOG_DWORD      Finalized;
+            MLOG_DWORD      Started;
+            MLOG_DWORD      EnabledMaskProg;
+            MLOG_DWORD      EnabledMask;
+            MLOG_STORAGE_CTX *StoragePtr;
+volatile    MLOG_DWORD      StorageSize;
+volatile    MLOG_DWORD      Mlogtaskkeycounter;
+            MLOG_DWORD      Mlogenablerc;
+            unsigned char*  DevStoragePtr;
+} MLOGCTX;
+
+typedef struct tagMLogListRec
+{
+    MLOG_DWORD code;
+    MLOG_DWORD listid;
+    MLOG_DWORD ticks;
+}MLOGLISTREC;
+
+typedef struct tagMLogTCBRec
+{
+    MLOG_DWORD code;
+    MLOG_DWORD ptcb;
+    MLOG_DWORD taskID;
+    MLOG_DWORD resID;
+}MLOGTCBREC;
+
+typedef struct tagMLogEXERec
+{
+    MLOG_DWORD code;
+    MLOG_DWORD ptcb;
+    MLOG_DWORD ticks;
+    MLOG_DWORD resID;
+    MLOG_DWORD execticks;
+    MLOG_DWORD errcode;
+}MLOGEXEREC;
+
+typedef struct tagMLogSubTaskRec
+{
+    MLOG_DWORD code;
+    MLOG_DWORD ptcb;
+    MLOG_DWORD subID;
+    MLOG_DWORD ticks;
+}MLOGSUBTASKREC;
+
+typedef struct tagMLogTaskRec
+{
+    MLOG_DWORD code;
+    MLOG_DWORD taskID;
+    MLOG_DWORD resID;
+    MLOG_DWORD tickstart;
+    MLOG_DWORD tickstop;
+    MLOG_DWORD key;
+}MLOGTASKREC;
+
+typedef struct tagMLogFreqRec
+{
+    MLOG_DWORD code;
+    MLOG_DWORD resID;
+    MLOG_DWORD freqID;
+    MLOG_DWORD freqval;
+}MLOGFREQREC;
+
+typedef struct tagMLogMarkRec
+{
+    MLOG_DWORD code;
+    MLOG_DWORD markID;
+    MLOG_DWORD ticks;
+}MLOGMARKREC;
+
+typedef struct tagMLogDepRec
+{
+    MLOG_DWORD mode;
+    MLOG_DWORD param1;
+    MLOG_DWORD param2;
+}MLOGDEPREC;
+
+typedef struct tagMLogTCBCntrRec
+{
+    MLOG_DWORD code;
+    MLOG_DWORD ptcb;
+    MLOG_DWORD markid;
+}MLOGTCBCNTRREC;
+
+typedef struct tagMLogResCondRec
+{
+    MLOG_DWORD id;
+    MLOG_DWORD code;
+    MLOG_DWORD resid_index;
+    MLOG_DWORD ticks;
+}MLOGRESCONDREC;
+
+typedef struct tagMLogAddListRec
+{
+    MLOG_DWORD code;
+    MLOG_DWORD ptcb;
+    MLOG_DWORD listid;
+}MLOGADDLISTREC;
+
+typedef struct tagMLogDevInfoRec
+{
+    MLOG_DWORD code;
+    MLOG_DWORD devinfo;
+}MOGDEVINFOREC;
+
+typedef struct tagMLogFrameSFRec
+{
+    MLOG_DWORD code;
+    MLOG_DWORD framenum;
+    MLOG_DWORD subframenum;
+}MLOGFRAMESFREC;
+
+typedef struct tagCacheMipsHeadRec
+{
+    MLOG_DWORD code;
+    MLOG_DWORD ticks;
+    MLOG_DWORD buffcount;
+}MLOGCACHEMIPSHEADREC;
+
+typedef struct tagCacheMipsRec
+{
+    MLOG_DWORD buffnum;
+    MLOG_DWORD cachemiss;
+    MLOG_DWORD cachehit;
+    MLOG_DWORD irqcycles;
+    MLOG_DWORD idlecycles;
+    MLOG_DWORD supervisiorcycles;
+    MLOG_DWORD irqcount;
+}MLOGCACHEMIPSREC;
+
+typedef struct tagCacheMipsSRec
+{
+    MLOG_DWORD buffnum;
+    MLOG_DWORD cachemiss;
+    MLOG_DWORD cachehit;
+    MLOG_DWORD cachemissinst;
+    MLOG_DWORD irqcycles;
+    MLOG_DWORD idlecycles;
+    MLOG_DWORD supervisiorcycles;
+    MLOG_DWORD irqcount;
+}MLOGCACHEMIPSSREC;
+
+typedef struct tagMLogTimeEx {
+	MLOG_DWORD code;
+	MLOG_DWORD type;
+    MLOG_DWORD time_ms;
+	MLOG_DWORD ticks;
+} MLOGTIMEEXREC;
+
+#define MLOG_RESOURCE_KERNEL_MASK (28)
+#define MLOG_CHECK_TICK_ENABLED   (1)
+#define MLOG_CHECK_TICK_DISABLED  (0)
+#define MLOG_SIZE_EXTIME        8
+#define MLOG_EXTIME             0x40000000
+#define MLOG_ID_EXTIME          0x4000000B
+#define MLOG_EX_TIME_TYPE_MSEC  (1)
+
+#define MLOG_MARK_KERNEL_START      (101)
+#define MLOG_MARK_KERNEL_OVERFLOW   (102)
+#define MLOG_MARK_KERNEL_5SEC       (103)
+#define MLOG_MARK_KERNEL_STOP       (199)
+
+int MLogIsEnabled(void);
+unsigned int MLogSetMask (MLOG_DWORD nMask);
+unsigned int MLogTask(unsigned int taskid, unsigned int resourceid , unsigned int ticksstart,unsigned int ticksstop);
+void MLogExTime(unsigned int resourceId, unsigned int type, unsigned int time_ms, unsigned int event, unsigned int ticks);
+void MLogAddVariables(unsigned int numVar, unsigned int *variables, unsigned int ticks);
+void clear_mlog_counter(void);
+void init_mlog_counter(void);
+
+#endif
+
+#endif
+
diff --git a/arch/arm/mach-transcede/include/mach/mmu_protect.h b/arch/arm/mach-transcede/include/mach/mmu_protect.h
new file mode 100644
index 0000000..1ce1a82
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/mmu_protect.h
@@ -0,0 +1,82 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+#ifndef _TRANSCEDE_MMU_PROTECT_H_
+#define _TRANSCEDE_MMU_PROTECT_H_
+
+#include <linux/dma-mapping.h>
+#include <linux/seq_file.h>
+
+/* #define MMU_DEBUG(fmt, args...) printk(KERN_DEBUG "MMU: %s(): " fmt "\n", __func__, ## args) */
+#define MMU_INFO(fmt, args...) printk(KERN_INFO "MMU: " fmt "\n", ## args)
+#define MMU_ERROR(fmt, args...) printk(KERN_ERR "MMU: error: " fmt "\n", ## args)
+
+#define MMU_DEBUG(fmt, args...) do { /* nothing */ } while (0)
+/* #define MMU_INFO(fmt, args...) do { /\* nothing *\/ } while (0) */
+/* #define MMU_ERROR(fmt, args...) do { /\* nothing *\/ } while (0) */
+
+#define MMU_CPU_SMP (~0UL)
+
+struct mmu_mem {
+	unsigned long phys;
+	unsigned long virt;
+	unsigned long size;
+};
+
+struct smp_value {
+	unsigned long *dst;
+	unsigned long *src;
+	size_t size;
+};
+
+extern void v7_dma_map_area(void *, int, int);
+extern int mmu_protect_set_proc(void);
+extern void smp_set_value(void *);
+extern void *amp_stack_alloc(void);
+extern void amp_stack_free(void *p);
+extern int amp_check_addr(void *addr);
+
+struct task_struct *kthread_amp_create(int cpu, int (*threadfn)(void *data),
+                                       void *data,
+                                       const char namefmt[]);
+
+extern struct mmu_mem amp_mem;
+
+static inline int is_mmu_shared(unsigned long addr)
+{
+	unsigned long flags;
+	int is_shared = 0;
+
+	local_irq_save(flags);
+
+	asm volatile ("bic r1, %1, #0x1f\n"
+	              "mcr p15,0,r1,c7,c8,0\n"
+	              "mrc p15,0,%0,c7,c4,0\n"
+	              : "=r" (is_shared)
+	              : "r" (addr)
+	              : "r0", "r1");
+
+	local_irq_restore(flags);
+
+	return is_shared & (1<<7);
+}
+
+#endif	/* _TRANSCEDE_MMU_PROTECT_H_ */
diff --git a/arch/arm/mach-transcede/include/mach/mmu_protect_v7.h b/arch/arm/mach-transcede/include/mach/mmu_protect_v7.h
new file mode 100644
index 0000000..148d46e
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/mmu_protect_v7.h
@@ -0,0 +1,145 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+#ifndef _TRANSCEDE_MMU_PROTECT_V7_H_
+#define _TRANSCEDE_MMU_PROTECT_V7_H_
+
+#define TTB0_SHIFT (14)
+#define TTB1_SHIFT (10)
+#define TTBCR_N (0x3)
+
+#define SCTLR_TRE (1 << 28)
+#define SCTLR_AFE (1 << 29)
+
+#define SZ_1M_SHIFT (20)
+#define SZ_1M_MASK (~((1UL << SZ_1M_SHIFT) - 1))
+
+#define TT_FAULT (0x0)
+#define TT_PAGE (0x1)
+#define TT_SECTION (0x2)
+#define TT_MASK (0x3)
+
+#define TT_PAGE_BASE_SHIFT (10)
+#define TT_PAGE_BASE_MASK (~((1 << TT_PAGE_BASE_SHIFT) - 1))
+
+#define TT_SECTION_BASE_SHIFT (20)
+#define TT_SECTION_BASE_MASK (~((1 << TT_SECTION_BASE_SHIFT) - 1))
+
+#define TT_SECTION_ATR_PXN (1 << 0)
+#define TT_SECTION_ATR_B (1 << 2)
+#define TT_SECTION_ATR_C (1 << 3)
+#define TT_SECTION_ATR_XN (1 << 4)
+#define TT_SECTION_ATR_DOMAIN (1 << 5)
+#define TT_SECTION_ATR_AP(x) (x << 10)
+#define TT_SECTION_ATR_TEX (1 << 12)
+#define TT_SECTION_ATR_AP2 (1 << 15)
+#define TT_SECTION_ATR_S (1 << 16)
+#define TT_SECTION_ATR_NG (1 << 17)
+#define TT_SECTION_ATR_NS (1 << 19)
+
+#define TT_LARGE (0x2)
+#define TT_SMALL (0x3)
+
+#define TT_LARGE_BASE_SHIFT (16)
+#define TT_LARGE_BASE_MASK (~((1 << TT_LARGE_BASE_SHIFT) - 1))
+#define TT_LARGE_ATR_XN (1 << 15)
+#define TT_LARGE_ATR_B (1 << 2)
+#define TT_LARGE_ATR_C (1 << 3)
+#define TT_LARGE_ATR_AP(x) (x << 4)
+#define TT_LARGE_ATR_TEX(x) (x << 12)
+#define TT_LARGE_ATR_AP2 (1 << 9)
+#define TT_LARGE_ATR_S (1 << 10)
+#define TT_LARGE_ATR_NG (1 << 11)
+
+#define TT_SMALL_BASE_SHIFT (12)
+#define TT_SMALL_BASE_MASK (~((1 << TT_SMALL_BASE_SHIFT) - 1))
+#define TT_SMALL_ATR_XN (1 << 0)
+#define TT_SMALL_ATR_TEX(x) (x << 6)
+#define TT_SMALL_ATR_B (TT_LARGE_ATR_B)
+#define TT_SMALL_ATR_C (TT_LARGE_ATR_C)
+#define TT_SMALL_ATR_AP(x) (TT_LARGE_ATR_AP(x))
+#define TT_SMALL_ATR_AP2 (TT_LARGE_ATR_AP2)
+#define TT_SMALL_ATR_S (TT_LARGE_ATR_S)
+#define TT_SMALL_ATR_NG (TT_LARGE_ATR_NG)
+
+#define AP_PERM_FAULT (0x0)
+#define AP_PL0_FAULT (0x1)
+#define AP_PL0_RDONLY (0x2)
+#define AP_FULL_ACCESS (0x3)
+#define AP_MASK (0x3)
+
+#define TEX_MEM_STRONGLY (0x0)
+#define TEX_MEM_DEVICE (0x1)
+#define TEX_MEM_NORMAL (0x2)
+#define TEX_MEM_RESERVED (0x3)
+#define TEX_MEM_MASK (0x3)
+
+#define TEX_REGION_NCNB (0x0)
+#define TEX_REGION_WB_WA (0x1)
+#define TEX_REGION_WRITETHROUGH (0x2)
+#define TEX_REGION_WRITEBACK (0x3)
+#define TEX_REGION_MASK (0x3)
+
+#define TEX_MASK (0x7)
+
+/* #define virt_to_icc(v)	  \ */
+/* 	((unsigned long)v - TRANSCEDE_ICC_VIRT + (unsigned long)TRANSCEDE_ICC_BASE) */
+/* #define icc_to_virt(p)	  \ */
+/* 	((unsigned long)p - (unsigned long)TRANSCEDE_ICC_BASE + TRANSCEDE_ICC_VIRT) */
+
+/* #define ICC_HEAP_VIRT (TRANSCEDE_ICC_VIRT) */
+/* #define ICC_HEAP_SIZE (TRANSCEDE_ICCHEAP_SIZE) */
+
+#define RD15_SCTLR(r)	  \
+	asm volatile ("mrc p15, 0, %0, c1, c0, 0\n" \
+	              : "=r" (r));
+#define RD15_PRRR(r)	  \
+	asm volatile ("mrc p15, 0, %0, c10, c2, 0\n" \
+	              : "=r" (r));
+#define RD15_NRRR(r)	  \
+	asm volatile ("mrc p15, 0, %0, c10, c2, 1\n" \
+	              : "=r" (r));
+#define RD15_TTBCR(r)	  \
+	asm volatile ("mrc p15, 0, %0, c2, c0, 2\n" \
+	              : "=r" (r));
+#define RD15_TTBR0(r)	  \
+	asm volatile ("mrc p15, 0, %0, c2, c0, 0\n" \
+	              : "=r" (r));
+#define RD15_TTBR1(r)	  \
+	asm volatile ("mrc p15, 0, %0, c2, c0, 1\n" \
+	              : "=r" (r));
+
+enum mem_control {
+	MEM_LOCK,
+	MEM_UNLOCK,
+	MEM_UNLOCK_AMP,
+	MEM_RDONLY,
+};
+
+enum cache_type {
+	REGION_IGNORE,
+	REGION_NCNB,
+	REGION_WB_WA,
+	REGION_WRITETHROUGH,
+	REGION_WRITEBACK,
+};
+
+#endif	/* _TRANSCEDE_MMU_PROTECT_V7_H_ */
diff --git a/arch/arm/mach-transcede/include/mach/pcie-t2200.h b/arch/arm/mach-transcede/include/mach/pcie-t2200.h
new file mode 100644
index 0000000..7103b79
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/pcie-t2200.h
@@ -0,0 +1,81 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef __ASM_ARCH_PCIE_TRANSCEDE_2200_H
+#define __ASM_ARCH_PCIE_TRANSCEDE_2200_H
+
+#define NUM_PCIE_PORTS	2
+#define MAX_PCIE_PORTS  2
+
+
+#define PCIE0_PORT_ID   0 /* PCIe 0 ID */
+#define PCIE1_PORT_ID   1 /* PCIe 1 ID */
+
+struct iatu_region {
+	unsigned long base;
+	unsigned long size;
+};
+
+struct pcie_port {
+	u8              port;		/* PCIe port ID */
+	u8              root_bus_nr;
+	u8              intx_enable;  /* flag, set to 1 if INTx enabled */
+	u8              intx_mask;	  /* flag, set to 1 if INTx masked */
+	unsigned long   base;
+	unsigned long   remote_mem_baseaddr;
+	unsigned long   app_base;
+	void __iomem    *va_app_base;
+	void __iomem    *va_dbi_base;
+	unsigned long   cfg0_base;
+	void __iomem    *va_cfg0_base;
+	unsigned long   cfg1_base;
+	void __iomem    *va_cfg1_base;
+	unsigned int    cfg0_prev_taddr;
+	unsigned int    cfg1_prev_taddr;
+	unsigned int    cmd_reg_val;
+	spinlock_t      conf_lock;
+	spinlock_t      intr_lock;
+	spinlock_t      msi_map_lock;
+	char            mem_space_name[16];
+	char            io_space_name[16];
+	int             port_mode; /* RC only supported */
+	int             link_state;
+	int             intx_base;
+	int             msi_base;
+	int             irq;
+	dma_addr_t      msi_mbox_handle;
+	void            *msi_mbox_baseaddr;
+	struct resource res[2];
+	struct clk      *ref_clock;
+	unsigned long   link_status_reg;
+
+	struct iatu_region  iatu_mem;
+	struct iatu_region  iatu_io;
+	struct iatu_region  iatu_cfg0;
+	struct iatu_region  iatu_cfg1;
+};
+
+#define PCIE_PORT_MODE_NONE (-1)
+#define PCIE_PORT_MODE_EP   CFG0_DEV_TYPE_EP
+#define PCIE_PORT_MODE_RC   CFG0_DEV_TYPE_RC
+
+#endif	/* __ASM_ARCH_PCIE_TRANSCEDE_2200_H */
diff --git a/arch/arm/mach-transcede/include/mach/pcie.h b/arch/arm/mach-transcede/include/mach/pcie.h
new file mode 100644
index 0000000..1231b27
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/pcie.h
@@ -0,0 +1,180 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef __ASM_ARCH_PCIE_H
+#define __ASM_ARCH_PCIE_H
+
+/* CFG0 Register definitions */
+#define DWC_CFG0_DEV_TYPE_MASK  0x0F
+#define CFG0_DEV_TYPE_EP        0x00
+#define CFG0_DEV_TYPE_LEP       0x01
+#define CFG0_DEV_TYPE_RC        0x04
+#define CFG0_DEV_TYPE_UP_SW     0x05
+#define CFG0_DEV_TYPE_DWN_SW    0x06
+
+/* CFG1,2 Register definitions */
+/*ID definitions of ARMISC */
+#define AXI_OP_TYPE_ID_MASK     0x0000001F
+#define AXI_OP_TYPE_ID_BIT      0
+#define AXI_OP_BCM_ID           0x00000020
+#define AXI_OP_EP_ID            0x00000040
+#define AXI_OP_TD_ID            0x00000080
+#define AXI_OP_ATTRIBUTE_ID_MASK    0x00000300
+#define AXI_OP_ATTRIBUTE_ID_BIT     8
+#define AXI_OP_TC_ID_MASK           0x00001C00
+#define AXI_OP_TC_ID_BIT            10
+#define AXI_OP_MSG_CODE_ID_MASK     0x001FE000
+#define AXI_OP_MSG_CODE_ID_BIT      13
+#define AXI_OP_DBI_ACCESS_ID        0x00200000
+#define AXI_OP_TYPE_MASK                0x1F
+#define AXI_OP_TYPE_MEM_RDRW            0
+#define AXI_OP_TYPE_MEM_RDRW_LOCKED     1
+#define AXI_OP_TYPE_IO_RDRW             2
+#define AXI_OP_TYPE_CONFIG_RDRW_TYPE0   4
+#define AXI_OP_TYPE_CONFIG_RDRW_TYPE1   5
+#define AXI_OP_TYPE_MSG_REQ             16
+#define AXI_OP_TYPE_COMPLETION          10
+#define AXI_OP_TYPE_COMPLETION_LOCKED   11
+#define AXI_OP_TYPE_DBI_ELBI_ENABLE     1
+
+
+
+/* CFG5 Register definitions */
+#define     CFG5_APP_INIT_RST   0x01
+#define     CFG5_LTSSM_ENABLE   0x02
+#define     CFG5_APP_RDY_L23    0x04
+
+
+/* STS0 Register definitions */
+#define     STS0_XMLH_LINK_UP   0x8000
+#define     STS0_RDLH_LINK_UP   0x10000
+
+/* INTR_STS and INTR_EN Register definitions */
+#define     INTR_CTRL_INTA_ASSERT   0x0001
+#define     INTR_CTRL_INTA_DEASSERT 0x0002
+#define     INTR_CTRL_INTB_ASSERT   0x0004
+#define     INTR_CTRL_INTB_DEASSERT 0x0008
+#define     INTR_CTRL_INTC_ASSERT   0x0010
+#define     INTR_CTRL_INTC_DEASSERT 0x0020
+#define     INTR_CTRL_INTD_ASSERT   0x0040
+#define     INTR_CTRL_INTD_DEASSERT 0x0080
+#define     INTR_CTRL_AER       0x0100
+#define     INTR_CTRL_PME       0x0200
+#define     INTR_CTRL_HP        0x0400
+#define     INTR_CTRL_LINK_AUTO_BW  0x0800
+#define     INTR_CTRL_MSI       0x1000
+
+/* synopsis specific PCIE configuration registers*/
+
+/* Port Logic Registers */
+#define PCIE_ALRT_REG              0x700
+#define PCIE_AFL0L1_REG            0x70C
+#define PCIE_CTL_REG               0x710
+#define PCIE_SYMNUM_REG            0x718
+#define PCIE_G2CTRL_REG            0x80C
+
+#define PCIE_CAP_BASE           0x70
+#define PCIE_DCNT_REG           (PCIE_CAP_BASE + 0x08)
+#define PCIE_LCAP_REG           (PCIE_CAP_BASE + 0x0C)
+#define PCIE_LCNT2_REG          (PCIE_CAP_BASE + 0x30)
+
+/* MSI interface registers */
+#define PCIE_MSI_ADDR_LO    0x820   /* 32 bits */
+#define PCIE_MSI_ADDR_HI    0x824   /* 32 bits */
+#define PCIE_MSI_INTR0_ENABLE   0x828   /* 32 bits */
+#define PCIE_MSI_INTR0_MASK 0x82C   /* 32 bits */
+#define PCIE_MSI_INTR0_STATUS   0x830   /* 32 bits */
+
+/* iATU interface registers */
+#define PCIE_iATU_VIEW_PORT 0x900   /* 32 bits */
+#define PCIE_iATU_CTRL1     0x904   /* 32 bits */
+#define PCIE_iATU_CTRL2     0x908   /* 32 bits */
+#define PCIE_iATU_SRC_LOW   0x90C   /* 32 bits */
+#define PCIE_iATU_SRC_HIGH  0x910   /* 32 bits */
+#define PCIE_iATU_LIMIT     0x914   /* 32 bits */
+#define PCIE_iATU_TRGT_LOW  0x918   /* 32 bits */
+#define PCIE_iATU_TRGT_HIGH 0x91C   /* 32 bits */
+
+/*******************************************************/
+/* BAR MASK Registers (uses dbi_cs2)                   */
+/*******************************************************/
+#define PCIE_BAR0_MASK_REG              0x100010
+
+/* iATU viewport register definitions */
+#define iATU_VIEW_PORT_ID_MASK  0x0F
+#define iATU_VIEW_PORT_ID_BIT   0
+#define iATU_VIEW_PORT_IN_BOUND 0X80000000
+
+/* iATU control1 register definitions */
+#define iATU_CTRL1_TYPE_MASK    0x001F
+#define iATU_CTRL1_TYPE_BIT 0
+#define iATU_CTRL1_TC_MASK  0x00E0
+#define iATU_CTRL1_TC_BIT   5
+#define iATU_CTRL1_TD       0x100
+
+
+/* iATU control2 register definitions */
+#define iATU_CTRL2_ID_EN            0x80000000
+#define iATU_CTRL2_IB_MEM_BAR_MATCH 0x40000000
+#define iATU_CTRL2_IB_CFG0_ACCEPT   0x40000000
+#define iATU_CTRL2_IB_MATCH_BAR0    0x00000000
+#define iATU_CTRL2_IB_MATCH_BAR1    0x00000100
+#define iATU_CTRL2_IB_MATCH_BAR2    0x00000200
+#define iATU_CTRL2_IB_MATCH_BAR3    0x00000300
+#define iATU_CTRL2_IB_MATCH_BAR4    0x00000400
+#define iATU_CTRL2_IB_MATCH_BAR5    0x00000500
+#define iATU_CTRL2_IB_MATCH_ROM     0x00000600
+
+#define iATU_ENTRY_MAX      8
+#define iATU_ENTRY_MEM      0
+#define iATU_ENTRY_IO       1
+#define iATU_ENTRY_CNF0     2
+#define iATU_ENTRY_CNF1     3
+#define iATU_ENTRY_MSG      4
+
+/* MAX 64 MB for PCIe0 */
+#define TRANSCEDE_PCIE0_MEM_SIZE  (62 << 20)
+#define TRANSCEDE_PCIE0_MSG_SIZE  (1 << 20)
+#define TRANSCEDE_PCIE0_IO_SIZE   (SZ_64K)
+#define TRANSCEDE_PCIE0_CFG0_SIZE (SZ_64K)
+#define TRANSCEDE_PCIE0_CFG1_SIZE (SZ_64K)
+
+/* MAX 46 MB for PCIe1 */
+#define TRANSCEDE_PCIE1_MEM_SIZE  (44 << 20)
+#define TRANSCEDE_PCIE1_MSG_SIZE  (1 << 20)
+#define TRANSCEDE_PCIE1_IO_SIZE   (SZ_64K)
+#define TRANSCEDE_PCIE1_CFG0_SIZE (SZ_64K)
+#define TRANSCEDE_PCIE1_CFG1_SIZE (SZ_64K)
+
+#define iATU_MEM_SIZE(_id)  ((_id) ? TRANSCEDE_PCIE1_MEM_SIZE : TRANSCEDE_PCIE0_MEM_SIZE)
+#define iATU_MSG_SIZE(_id)  ((_id) ? TRANSCEDE_PCIE1_MSG_SIZE : TRANSCEDE_PCIE0_MSG_SIZE)
+#define iATU_IO_SIZE(_id)   ((_id) ? TRANSCEDE_PCIE1_IO_SIZE : TRANSCEDE_PCIE0_IO_SIZE)
+#define iATU_CFG0_SIZE(_id) ((_id) ? TRANSCEDE_PCIE1_CFG0_SIZE : TRANSCEDE_PCIE0_CFG0_SIZE)
+#define iATU_CFG1_SIZE(_id) ((_id) ? TRANSCEDE_PCIE1_CFG1_SIZE : TRANSCEDE_PCIE0_CFG1_SIZE)
+
+#define iATU_GET_MEM_BASE(_base, _id)  ((_base) + 0)
+#define iATU_GET_MSG_BASE(_base, _id)  (iATU_GET_MEM_BASE(_base,_id) + iATU_MEM_SIZE(_id))
+#define iATU_GET_IO_BASE(_base, _id)   (iATU_GET_MSG_BASE(_base,_id) + iATU_MSG_SIZE(_id))
+#define iATU_GET_CFG0_BASE(_base, _id) (iATU_GET_IO_BASE(_base,_id) + iATU_IO_SIZE(_id))
+#define iATU_GET_CFG1_BASE(_base, _id) (iATU_GET_CFG0_BASE(_base,_id) + iATU_CFG0_SIZE(_id))
+
+#endif	/* __ASM_ARCH_PCIE_H */
diff --git a/arch/arm/mach-transcede/include/mach/periodic_task.h b/arch/arm/mach-transcede/include/mach/periodic_task.h
new file mode 100644
index 0000000..f9f9e65
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/periodic_task.h
@@ -0,0 +1,44 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef _TRANSCEDE_PERIODIC_TASK_H_
+#define _TRANSCEDE_PERIODIC_TASK_H_
+
+#include <linux/seq_file.h>
+
+#define PERIODIC_NAME_LEN (15)
+
+extern void show_global_irqs(struct seq_file *, int);
+extern int periodic_task_setup(void);
+extern void *periodic_task_add(unsigned int cpu,
+                               unsigned long period, /* ms */
+                               void (*action)(unsigned long),
+                               unsigned long data,
+                               const char *name);
+extern int periodic_task_remove(void *task);
+extern int periodic_task_remove_all(unsigned int cpu);
+extern unsigned long periodic_task_get_period(void *task);
+extern unsigned long periodic_task_get_cpu(void *task);
+extern int periodic_task_get_name(void *task, char *name);
+extern void ipi_gtcore_sync(int cpu);
+
+#endif	/* _TRANSCEDE_PERIODIC_TASK_H_ */
diff --git a/arch/arm/mach-transcede/include/mach/revision.h b/arch/arm/mach-transcede/include/mach/revision.h
new file mode 100644
index 0000000..d919377
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/revision.h
@@ -0,0 +1,36 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef __TRANSCEDE_REVISION_H
+#define __TRANSCEDE_REVISION_H
+
+#define T2200_REV_UNDEF		-1U
+#define T2200_REV_UNKNOWN	0
+#define T2200_REV_X1		1
+#define T2200_REV_X2		2
+#define T2200_REV_X2_1		3
+
+#define T2200_REV_PROCFS_NAME	"transcede-rev"
+
+int get_t2200_rev(void);
+
+#endif
diff --git a/arch/arm/mach-transcede/include/mach/serdes-t2200.h b/arch/arm/mach-transcede/include/mach/serdes-t2200.h
new file mode 100644
index 0000000..05fbf5f
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/serdes-t2200.h
@@ -0,0 +1,27 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#define SERDES0_ID 0
+#define SERDES1_ID 1
+
+
+int serdes_pcie_phy_init(int id);
diff --git a/arch/arm/mach-transcede/include/mach/serdes.h b/arch/arm/mach-transcede/include/mach/serdes.h
new file mode 100644
index 0000000..dab682b
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/serdes.h
@@ -0,0 +1,51 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef __TRANSCEDE_SERDES_H
+#define __TRANSCEDE_SERDES_H
+
+#include "hardware.h"
+
+/* SER-DES Address space */
+#define SD_COMMON_CMU     0x000
+#define SD_LANE0          0x200
+#define SD_LANE1          0x400
+#define SD_LANE2          0x600
+#define SD_LANE3          0x800
+#define SD_COMMON_LANE    0xA00
+
+#define SD_DEV_TYPE_PCIE    0
+#define SD_DEV_TYPE_SATA    1
+#define SD_DEV_TYPE_SGMII   2
+
+#define TRANSCEDE_DWC1_CFG_BASE         APB_VADDR(TRANSCEDE_APB_USBPHY_SERDES_STAT_BASE)
+#define TRANSCEDE_DWC1_SERDES_CFG_BASE  TRANSCEDE_DWC1_CFG_BASE + 0x2C
+#define SD_PHY_STS_REG_OFST     0x00
+#define SD_PHY_CTRL1_REG_OFST   0x04
+#define SD_PHY_CTRL2_REG_OFST   0x08
+#define SD_PHY_CTRL3_REG_OFST   0x0C
+
+
+#define TRANSCEDE_SERDES_REG( _num, _ofst) ((TRANSCEDE_SERDES0 + (0x10000 * _num)) + _ofst)
+#define TRANSCEDE_SERDES_DWC_CFG_REG( _num, _ofst) ((TRANSCEDE_DWC1_SERDES_CFG_BASE + (_num <<4)) + _ofst)
+
+#endif /* __TRANSCEDE_SERDES_H */
diff --git a/arch/arm/mach-transcede/include/mach/spa.h b/arch/arm/mach-transcede/include/mach/spa.h
new file mode 100644
index 0000000..5f6de19
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/spa.h
@@ -0,0 +1,67 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef _TRANSCEDE_SPA_H_
+#define _TRANSCEDE_SPA_H_
+
+/*
+ * Memory layout
+ * Security Protocol Accelerator
+ */
+#ifdef CONFIG_MACH_M84XXX
+#define TRANSCEDE_SPA_REG_BASE		0xFBD00000
+#elif CONFIG_MACH_M822XX
+#define TRANSCEDE_SPA_REG_BASE		0xFDE00000
+#endif
+
+#ifdef CONFIG_MACH_M822XX
+/* SPA TRNG Block - True Random Number Generator */
+#define SAPSA_BASEADDR			(TRANSCEDE_SPA_REG_BASE + 0x000C0000)
+#define TRANSCEDE_SPA_TRNG_REG_OFFSET	0x00004000
+#define TRANSCEDE_SPA_TRNG_REG_SIZE	0x00000100
+#define TRNG_BASEADDR			(SAPSA_BASEADDR + TRANSCEDE_SPA_TRNG_REG_OFFSET)
+#elif CONFIG_MACH_M84XXX
+#define TRANSCEDE_SPA_TRNG_REG_OFFSET	0x00018000
+#define TRANSCEDE_SPA_TRNG_REG_SIZE	0x00000100
+#define TRNG_BASEADDR			(TRANSCEDE_SPA_REG_BASE + TRANSCEDE_SPA_TRNG_REG_OFFSET)
+#endif
+
+/* SPA PKA Block - Publick Key Accelerator */
+#define TRANSCEDE_SPA_PKA_REG_OFFSET	0x00020000
+#define TRANSCEDE_SPA_PKA_REG_SIZE	0x00010000
+
+/* SPA SPAcc Block - Security Protocol Accelerator Engine */
+#ifdef CONFIG_MACH_M84XXX
+#define TRANSCEDE_SPA_SPACC_REG_OFFSET	0x00040000
+#elif CONFIG_MACH_M822XX
+#define TRANSCEDE_SPA_SPACC_REG_OFFSET	0x00000000
+#endif
+#define TRANSCEDE_SPA_SPACC_REG_SIZE	0x00020000
+
+/* SPA ESPAH Block - IPSec */
+#define TRANSCEDE_SPA_ESPAH_REG_OFFSET	0x00080000
+#define TRANSCEDE_SPA_ESPAH_REG_SIZE	0x00080000
+
+#define TRANSCEDE_IPSEC_REG_BASE	(TRANSCEDE_SPA_REG_BASE + TRANSCEDE_SPA_ESPAH_REG_OFFSET)
+
+#endif
+
diff --git a/arch/arm/mach-transcede/include/mach/spi.h b/arch/arm/mach-transcede/include/mach/spi.h
new file mode 100644
index 0000000..aa57991
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/spi.h
@@ -0,0 +1,70 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef __ASM_ARCH_SPI_H
+#define __ASM_ARCH_SPI_H
+
+#ifdef CONFIG_ARCH_M823V2
+#define TRANSCEDE_SPI_FIFO_DEPTH	128
+#else
+#define TRANSCEDE_SPI_FIFO_DEPTH	8
+#endif
+
+/* SPI core registers */
+#define TRANSCEDE_SPI_CTRLR0		0x00
+#define TRANSCEDE_SPI_CTRLR1		0x04
+#define TRANSCEDE_SPI_SSIENR		0x08
+#define TRANSCEDE_SPI_MWCR		0x0C
+#define TRANSCEDE_SPI_SER		0x10
+#define TRANSCEDE_SPI_BAUDR		0x14
+#define TRANSCEDE_SPI_TXFTLR		0x18
+#define TRANSCEDE_SPI_RXFTLR		0x1C
+#define TRANSCEDE_SPI_TXFLR		0x20
+#define TRANSCEDE_SPI_RXFLR		0x24
+#define TRANSCEDE_SPI_SR		0x28
+#define TRANSCEDE_SPI_IMR		0x2C
+#define TRANSCEDE_SPI_ISR		0x30
+#define TRANSCEDE_SPI_RISR		0x34
+#define TRANSCEDE_SPI_TXOICR		0x38
+#define TRANSCEDE_SPI_RXOICR		0x3C
+#define TRANSCEDE_SPI_RXUICR		0x40
+#define TRANSCEDE_SPI_MSTICR		0x44
+#define TRANSCEDE_SPI_ICR		0x48
+#define TRANSCEDE_SPI_IDR		0x58
+#define TRANSCEDE_SPI_DR		0x60
+
+/* CTRLR0 - control register 0 bits/masks (incomplete) */
+#define SPI_CTRLR0_DFS_MASK		(15<<0)	/* Data frame size mask */
+#define SPI_CTRLR0_SCPOL		(1<<7)	/* Serial clock polarity */
+#define SPI_CTRLR0_SCPH			(1<<6)	/* Serial clock phase */
+#define SPI_CTRLR0_SRL			(1<<11)	/* Shift register loop */
+
+/* SR - status register bits */
+#define SPI_SR_BUSY			(1<<0)	/* SSI busy flag, serial transfer in progress */
+#define SPI_SR_TFNF			(1<<1)	/* Transmit FIFO not full */
+#define SPI_SR_TFE			(1<<2)	/* Transmit FIFO empty */
+#define SPI_SR_RFNE			(1<<3)	/* Receive FIFO not empty */
+#define SPI_SR_RFF			(1<<4)	/* Receive FIFO full */
+#define SPI_SR_TXE			(1<<5)	/* Transmission error */
+#define SPI_SR_DCOL			(1<<6)	/* Data collision error */
+
+#endif
diff --git a/arch/arm/mach-transcede/include/mach/syscfg.h b/arch/arm/mach-transcede/include/mach/syscfg.h
new file mode 100644
index 0000000..8c4d5cf
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/syscfg.h
@@ -0,0 +1,551 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef __ASM_ARCH_SYSCFG_H
+#define __ASM_ARCH_SYSCFG_H
+/*
+ * System configuration registers
+ */
+
+#define TRANSCEDE_CFG_0           AAB_XP_VADDR(TRANSCEDE_CFG_SYS + 0x00)
+#define TRANSCEDE_CFG_1           AAB_XP_VADDR(TRANSCEDE_CFG_SYS + 0x04)
+#define TRANSCEDE_CFG_2           AAB_XP_VADDR(TRANSCEDE_CFG_SYS + 0x08)
+#define TRANSCEDE_CFG_3           AAB_XP_VADDR(TRANSCEDE_CFG_SYS + 0x0C)
+#define TRANSCEDE_STAT_0          AAB_XP_VADDR(TRANSCEDE_CFG_SYS + 0x10)
+#define TRANSCEDE_STAT_1          AAB_XP_VADDR(TRANSCEDE_CFG_SYS + 0x14)
+#define TRANSCEDE_STAT_2          AAB_XP_VADDR(TRANSCEDE_CFG_SYS + 0x18)
+#define TRANSCEDE_STAT_3          AAB_XP_VADDR(TRANSCEDE_CFG_SYS + 0x1C)
+#define TRANSCEDE_BUS_CFG         AAB_XP_VADDR(TRANSCEDE_CFG_SYS + 0x20)
+#define TRANSCEDE_BUS_PRIO        AAB_XP_VADDR(TRANSCEDE_CFG_SYS + 0x24)
+/*TODO: fixit doubled def in transecede-2200.h */
+#if defined(CONFIG_MACH_M84XXX)
+#define TRANSCEDE_DDR_CFG         AAB_XP_VADDR(TRANSCEDE_CFG_SYS + 0x28)
+#endif
+#define TRANSCEDE_DDR_LL0         AAB_XP_VADDR(TRANSCEDE_CFG_SYS + 0x2C)
+#define TRANSCEDE_DDR_HL0         AAB_XP_VADDR(TRANSCEDE_CFG_SYS + 0x30)
+#define TRANSCEDE_DDR_LL1         AAB_XP_VADDR(TRANSCEDE_CFG_SYS + 0x34)
+#define TRANSCEDE_DDR_HL1         AAB_XP_VADDR(TRANSCEDE_CFG_SYS + 0x38)
+#if defined(CONFIG_MACH_M84XXX)
+#define TRANSCEDE_ARM_IRQ_SET     AAB_XP_VADDR(TRANSCEDE_CFG_SYS + 0x3C)
+#define TRANSCEDE_ARM_IRQ_CLR     AAB_XP_VADDR(TRANSCEDE_CFG_SYS + 0x40)
+#endif
+
+/*
+ * Radio configuration registers
+ */
+
+#define TRANSCEDE_RAD_CFG_PSTBL           				AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x00)
+#define TRANSCEDE_RAD_CFG_SRDS_MUX           			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x04)
+#define TRANSCEDE_RAD_CFG_CPRI2SPU           			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x08)
+#define TRANSCEDE_RAD_CFG_T_RWM0           			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x0C)
+#define TRANSCEDE_RAD_CFG_T_RWM1          			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x10)
+#define TRANSCEDE_RAD_CFG_CPRI_PRIOR          		AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x14)
+#define TRANSCEDE_RAD_CFG_SRIO_PRIOR          		AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x18)
+#define TRANSCEDE_RAD_CFG_PCIE_PRIOR          		AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x1C)
+#define TRANSCEDE_RAD_CFG_RAD_DMA_PRIOR 			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x20)
+#define TRANSCEDE_RAD_CFG_PCIE_RAD_DMA   			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x24)
+#define TRANSCEDE_RAD_CFG_SRDS_MMD         			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x28)
+#define TRANSCEDE_RAD_CFG_SRDS_LANE         			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x2C)
+#define TRANSCEDE_RAD_CFG_SRIO0_MODE        			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x30)
+#define TRANSCEDE_RAD_CFG_SRIO1_MODE        			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x34)
+#define TRANSCEDE_RAD_CFG_CPRI_RATE         			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x38)
+#define TRANSCEDE_RAD_CFG_CPRI_IRQ_MODE  			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x3C)
+#define TRANSCEDE_RAD_CFG_PCIE_SET_IRQ     			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x40)
+#define TRANSCEDE_RAD_CFG_PCIE_PEND_IRQ   			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x44)
+#define TRANSCEDE_RAD_CFG_PCIE_CFG           			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x48)
+#define TRANSCEDE_RAD_CFG_PCIE_FNUM          			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x4C)
+#define TRANSCEDE_RAD_CFG_PCIE_DBG          			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x50)
+#define TRANSCEDE_RAD_CFG_PCI_ROOT_COMP_ID   		AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x54)
+#define TRANSCEDE_RAD_CFG_PCIE_MRC_WR_ID 		AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x58)
+#define TRANSCEDE_RAD_CFG_PCIE_MRC_WC_ID 		AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x5C)
+#define TRANSCEDE_RAD_CFG_PCIE_MRC_RR_ID 			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x60)
+#define TRANSCEDE_RAD_CFG_PCIE_MRC_RC_ID 			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x64)
+#define TRANSCEDE_RAD_CFG_PCIE_IRQ_ACK     			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x68)
+#define TRANSCEDE_RAD_CFG_PCIE_STATUS      			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x6C)
+#define TRANSCEDE_RAD_CFG_PCIE_WRITE_ERR 			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x70)
+#define TRANSCEDE_RAD_CFG_PCIE_READ_ERR 			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x74)
+#define TRANSCEDE_RAD_SRDS0_INIT_CFG_R0  			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x78)
+#define TRANSCEDE_RAD_SRDS0_INIT_CFG_R1  			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x7C)
+#define TRANSCEDE_RAD_SRDS0_INIT_CFG_R2           	AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x80)
+#define TRANSCEDE_RAD_SRDS0_INIT_CFG_R3           	AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x84)
+#define TRANSCEDE_RAD_SRDS1_INIT_CFG_R0           	AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x88)
+#define TRANSCEDE_RAD_SRDS1_INIT_CFG_R1           	AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x8C)
+#define TRANSCEDE_RAD_SRDS2_INIT_CFG_R0          		AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x90)
+#define TRANSCEDE_RAD_SRDS2_INIT_CFG_R1          		AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x94)
+#define TRANSCEDE_RAD_SRDS2_INIT_CFG_R2          		AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x98)
+#define TRANSCEDE_RAD_SRDS2_INIT_CFG_R3          		AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0x9C)
+#define TRANSCEDE_RAD_SRIO0_DEV_VID_DID 			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0xA0)
+#define TRANSCEDE_RAD_SRIO0_DEV_VER   				AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0xA4)
+#define TRANSCEDE_RAD_SRIO0_ASSEMBLY_VID_AID		AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0xA8)
+#define TRANSCEDE_RAD_SRIO0_ASSEMBLY_VER        		AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0xAC)
+#define TRANSCEDE_RAD_SRIO1_DEV_VID_DID 			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0xB0)
+#define TRANSCEDE_RAD_SRIO1_DEV_VER   				AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0xB4)
+#define TRANSCEDE_RAD_SRIO1_ASSEMBLY_VID_AID		AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0xB8)
+#define TRANSCEDE_RAD_SRIO1_ASSEMBLY_VER        		AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0xBC)
+#define TRANSCEDE_RAD_SRDS0_STATUS     				AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0xC0)
+#define TRANSCEDE_RAD_SRDS1_STATUS   				AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0xC4)
+#define TRANSCEDE_RAD_SRDS2_STATUS           			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0xC8)
+#define TRANSCEDE_RAD_CPRI_RST_REQ          			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0xCC)
+#define TRANSCEDE_RAD_CFG_CPRI_FSYNC_MODE     		AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0xEC)
+#define TRANSCEDE_RAD_CFG_SRIO_LOOPBACK   		AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0xF0)
+#define TRANSCEDE_RAD_CFG_PDEN_REG 				AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0xF4)
+#define TRANSCEDE_RAD_CFG_CPRI_MODE_REG 			AAB_XP_VADDR(TRANSCEDE_CONFIG_RAD + 0xF8)
+
+/*
+ * PCIE SYS - PCIe Registers
+*/
+#define TRANSCEDE_RAD_PCIE_L0S_TIMEOUT            		AAB_XP_VADDR(TRANSCEDE_PCIE_SYS + 0x24)
+#define TRANSCEDE_RAD_PCIE_SYS_MIRQ_CLR           	AAB_XP_VADDR(TRANSCEDE_PCIE_SYS + 0xFFC)
+#define TRANSCEDE_RAD_PCIE_SYS_INT_ACK            		AAB_XP_VADDR(TRANSCEDE_PCIE_SYS + 0xFF8)
+
+
+
+/*
+ * Serdes PHY Management blocks
+ */
+#define TRANSCEDE_SERDES0_PMA_IEEE_PERLANE_CFG    			(TRANSCEDE_SERDES0_4CH + 0x0000)
+#define TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_CFG    			(TRANSCEDE_SERDES0_4CH + 0xC000)
+#define TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG    			(TRANSCEDE_SERDES0_4CH + 0x8000)
+#define TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_MON    			(TRANSCEDE_SERDES0_4CH + 0xC020)
+#define TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_MON    		(TRANSCEDE_SERDES0_4CH + 0x8020)
+#define TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_10GBASE_CFG  	(TRANSCEDE_SERDES0_4CH + 0x8030)
+#define TRANSCEDE_SERDES0_PCS_IEEE_PERLANE_CFG    			(TRANSCEDE_SERDES0_4CH + 0x0000)
+#define TRANSCEDE_SERDES0_PCS_COMMON_VENDOR_GLOBAL_CFG    	(TRANSCEDE_SERDES0_4CH + 0xC000)
+#define TRANSCEDE_SERDES0_PCS_COMMON_VENDOR_PERLANE_CFG   	(TRANSCEDE_SERDES0_4CH + 0x8000)
+#define TRANSCEDE_SERDES0_PCS_XAUI_VENDOR_PERLANE_CFG    	(TRANSCEDE_SERDES0_4CH + 0x8060)
+#define TRANSCEDE_SERDES0_PCS_PCIE_VENDOR_PERLANE_CFG    	(TRANSCEDE_SERDES0_4CH + 0x80C0)
+#define TRANSCEDE_SERDES0_PCS_10GBASEX_VENDOR_PERLANE_CFG	(TRANSCEDE_SERDES0_4CH + 0x8040)
+#define TRANSCEDE_SERDES0_PCS_10GBASER_VENDOR_PERLANE_CFG	(TRANSCEDE_SERDES0_4CH + 0x8080)
+
+#define TRANSCEDE_SERDES1_PMA_IEEE_PERLANE_CFG    			(TRANSCEDE_SERDES1_2CH + 0x0000)
+#define TRANSCEDE_SERDES1_PMA_VENDOR_GLOBAL_CFG    			(TRANSCEDE_SERDES1_2CH + 0xC000)
+#define TRANSCEDE_SERDES1_PMA_VENDOR_PERLANE_CFG    			(TRANSCEDE_SERDES1_2CH + 0x8000)
+#define TRANSCEDE_SERDES1_PMA_VENDOR_GLOBAL_MON    			(TRANSCEDE_SERDES1_2CH + 0xC020)
+#define TRANSCEDE_SERDES1_PMA_VENDOR_PERLANE_MON    		(TRANSCEDE_SERDES1_2CH + 0x8020)
+#define TRANSCEDE_SERDES1_PMA_VENDOR_PERLANE_10GBASE_CFG  	(TRANSCEDE_SERDES1_2CH + 0x8030)
+#define TRANSCEDE_SERDES1_PCS_IEEE_PERLANE_CFG    			(TRANSCEDE_SERDES1_2CH + 0x0000)
+#define TRANSCEDE_SERDES1_PCS_COMMON_VENDOR_GLOBAL_CFG    	(TRANSCEDE_SERDES1_2CH + 0xC000)
+#define TRANSCEDE_SERDES1_PCS_COMMON_VENDOR_PERLANE_CFG   	(TRANSCEDE_SERDES1_2CH + 0x8000)
+#define TRANSCEDE_SERDES1_PCS_XAUI_VENDOR_PERLANE_CFG    	(TRANSCEDE_SERDES1_2CH + 0x8060)
+#define TRANSCEDE_SERDES1_PCS_PCIE_VENDOR_PERLANE_CFG    	(TRANSCEDE_SERDES1_2CH + 0x80C0)
+#define TRANSCEDE_SERDES1_PCS_10GBASEX_VENDOR_PERLANE_CFG	(TRANSCEDE_SERDES1_2CH + 0x8040)
+#define TRANSCEDE_SERDES1_PCS_10GBASER_VENDOR_PERLANE_CFG	(TRANSCEDE_SERDES1_2CH + 0x8080)
+
+#define TRANSCEDE_SERDES2_PMA_IEEE_PERLANE_CFG    			(TRANSCEDE_SERDES2_4CH + 0x0000)
+#define TRANSCEDE_SERDES2_PMA_VENDOR_GLOBAL_CFG    			(TRANSCEDE_SERDES2_4CH + 0xC000)
+#define TRANSCEDE_SERDES2_PMA_VENDOR_PERLANE_CFG    			(TRANSCEDE_SERDES2_4CH + 0x8000)
+#define TRANSCEDE_SERDES2_PMA_VENDOR_GLOBAL_MON    			(TRANSCEDE_SERDES2_4CH + 0xC020)
+#define TRANSCEDE_SERDES2_PMA_VENDOR_PERLANE_MON    		(TRANSCEDE_SERDES2_4CH + 0x8020)
+#define TRANSCEDE_SERDES2_PMA_VENDOR_PERLANE_10GBASE_CFG  	(TRANSCEDE_SERDES2_4CH + 0x8030)
+#define TRANSCEDE_SERDES2_PCS_IEEE_PERLANE_CFG    			(TRANSCEDE_SERDES2_4CH + 0x0000)
+#define TRANSCEDE_SERDES2_PCS_COMMON_VENDOR_GLOBAL_CFG    	(TRANSCEDE_SERDES2_4CH + 0xC000)
+#define TRANSCEDE_SERDES2_PCS_COMMON_VENDOR_PERLANE_CFG   	(TRANSCEDE_SERDES2_4CH + 0x8000)
+#define TRANSCEDE_SERDES2_PCS_XAUI_VENDOR_PERLANE_CFG    	(TRANSCEDE_SERDES2_4CH + 0x8060)
+#define TRANSCEDE_SERDES2_PCS_PCIE_VENDOR_PERLANE_CFG    	(TRANSCEDE_SERDES2_4CH + 0x80C0)
+#define TRANSCEDE_SERDES2_PCS_10GBASEX_VENDOR_PERLANE_CFG	(TRANSCEDE_SERDES2_4CH + 0x8040)
+#define TRANSCEDE_SERDES2_PCS_10GBASER_VENDOR_PERLANE_CFG	(TRANSCEDE_SERDES2_4CH + 0x8080)
+
+
+/* "SRDS.MMD = 0x1"  for  "PMA/PMD_x Registers Mapping" */
+/* Table 6-2 PMA/PMD IEEE-defined per-lane registers (45.2.1) */
+#define TRANSCEDE_RAD_SRDS0_PMA_CTRL_1			AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_IEEE_PERLANE_CFG + 4*0x0)
+#define TRANSCEDE_RAD_SRDS0_PMA_STATUS_1		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_IEEE_PERLANE_CFG + 4*0x1)
+#define TRANSCEDE_RAD_SRDS0_PMA_DEVID_1			AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_IEEE_PERLANE_CFG + 4*0x2)
+#define TRANSCEDE_RAD_SRDS0_PMA_DEVID_2			AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_IEEE_PERLANE_CFG + 4*0x3)
+#define TRANSCEDE_RAD_SRDS0_PMA_SPEED_ABILITY	AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_IEEE_PERLANE_CFG + 4*0x4)
+#define TRANSCEDE_RAD_SRDS0_PMA_DEV_IN_PKG_1	AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_IEEE_PERLANE_CFG + 4*0x5)
+#define TRANSCEDE_RAD_SRDS0_PMA_DEV_IN_PKG_2	AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_IEEE_PERLANE_CFG + 4*0x6)
+#define TRANSCEDE_RAD_SRDS0_PMA_CTRL_2			AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_IEEE_PERLANE_CFG + 4*0x7)
+#define TRANSCEDE_RAD_SRDS0_PMA_STATUS_2		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_IEEE_PERLANE_CFG + 4*0x8)
+#define TRANSCEDE_RAD_SRDS0_10G_PMA_TX_DISABLE	AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_IEEE_PERLANE_CFG + 4*0x9)
+#define TRANSCEDE_RAD_SRDS0_10G_PMA_RX_SIGDET	AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_IEEE_PERLANE_CFG + 4*0xA)
+#define TRANSCEDE_RAD_SRDS0_10G_PMA_EXT_ABILITY	AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_IEEE_PERLANE_CFG + 4*0xB)
+#define TRANSCEDE_RAD_SRDS0_PMA_PKGID_1			AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_IEEE_PERLANE_CFG + 4*0xE)
+#define TRANSCEDE_RAD_SRDS0_PMA_PKGID_2			AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_IEEE_PERLANE_CFG + 4*0xF)
+#define TRANSCEDE_RAD_SRDS0_10G_KR_PMD_CTRL		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_IEEE_PERLANE_CFG + 4*0x96)
+#define TRANSCEDE_RAD_SRDS0_10G_KR_PMD_STATUS	AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_IEEE_PERLANE_CFG + 4*0x97)
+#define TRANSCEDE_RAD_SRDS0_10G_KR_LP_COEFF_UPDT		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_IEEE_PERLANE_CFG + 4*0x98)
+#define TRANSCEDE_RAD_SRDS0_10G_KR_LP_STATUS	AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_IEEE_PERLANE_CFG + 4*0x99)
+#define TRANSCEDE_RAD_SRDS0_10G_KR_LD_COEFF_UPDT		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_IEEE_PERLANE_CFG + 4*0x9A)
+#define TRANSCEDE_RAD_SRDS0_10G_KR_LD_STATUS	AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_IEEE_PERLANE_CFG + 4*0x9B)
+#define TRANSCEDE_RAD_SRDS0_1000BASE_KX_CTRL		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_IEEE_PERLANE_CFG + 4*0xA0)
+#define TRANSCEDE_RAD_SRDS0_1000BASE_KX_STATUS	AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_IEEE_PERLANE_CFG + 4*0xA1)
+
+/* Table 6-3 PMA/PMD vendor-defined global configuration and control registers */
+#define TRANSCEDE_RAD_SRDS0_GLBL_PLL_CFG_0		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_CFG + 4*0x0)
+#define TRANSCEDE_RAD_SRDS0_GLBL_PLL_CFG_1		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_CFG + 4*0x1)
+#define TRANSCEDE_RAD_SRDS0_GLBL_DLL_CFG_0		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_CFG + 4*0x2)
+#define TRANSCEDE_RAD_SRDS0_GLBL_MISC_CONFIG		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_CFG + 4*0x3)
+#define TRANSCEDE_RAD_SRDS0_SLICE_CFG				AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_CFG + 4*0x4)
+#define TRANSCEDE_RAD_SRDS0_GLBL_RX_DETECT_LVL	AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_CFG + 4*0x5)
+#define TRANSCEDE_RAD_SRDS0_GLBL_AMON_SEL		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_CFG + 4*0x6)
+#define TRANSCEDE_RAD_SRDS0_LANE_AMON_SEL		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_CFG + 4*0x7)
+#define TRANSCEDE_RAD_SRDS0_GLBL_DMON_SEL		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_CFG + 4*0x8)
+#define TRANSCEDE_RAD_SRDS0_LANE_DMON_SEL		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_CFG + 4*0x9)
+#define TRANSCEDE_RAD_SRDS0_GLBL_NOISE_CTRL		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_CFG + 4*0xC)
+#define TRANSCEDE_RAD_SRDS0_GLBL_RD_SYNC_STATUS	AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_CFG + 4*0x10)
+#define TRANSCEDE_RAD_SRDS0_GLBL_PLL_CFG_2		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_CFG + 4*0x11)
+#define TRANSCEDE_RAD_SRDS0_RX_PWR_CTRL_P0		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_CFG + 4*0x14)
+#define TRANSCEDE_RAD_SRDS0_RX_PWR_CTRL_P0S		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_CFG + 4*0x15)
+#define TRANSCEDE_RAD_SRDS0_RX_PWR_CTRL_P1		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_CFG + 4*0x16)
+#define TRANSCEDE_RAD_SRDS0_RX_PWR_CTRL_P2		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_CFG + 4*0x17)
+#define TRANSCEDE_RAD_SRDS0_TX_PWR_CTRL_P0_P0S	AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_CFG + 4*0x18)
+#define TRANSCEDE_RAD_SRDS0_TX_PWR_CTRL_P1_P2	AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_CFG + 4*0x19)
+#define TRANSCEDE_RAD_SRDS0_GLBL_PWR_CTRL		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_CFG + 4*0x1A)
+#define TRANSCEDE_RAD_SRDS0_GLBL_PWR_MON		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_CFG + 4*0x1B)
+#define TRANSCEDE_RAD_SRDS0_PWRUP_DLY			AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_CFG + 4*0x1C)
+#define TRANSCEDE_RAD_SRDS0_GLBL_DLL_CFG_1		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_CFG + 4*0x1D)
+#define TRANSCEDE_RAD_SRDS0_RX_PI_CTRL_0			AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_CFG + 4*0x1E)
+#define TRANSCEDE_RAD_SRDS0_RX_PI_CTRL_1			AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_CFG + 4*0x1F)
+
+/* Table 6-4 PMA/PMD vendor-defined per-lane configuration and control registers */
+#define TRANSCEDE_RAD_SRDS0_RX_CFG_0		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0x0)
+#define TRANSCEDE_RAD_SRDS0_RX_CFG_1		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0x1)
+#define TRANSCEDE_RAD_SRDS0_RX_CFG_2		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0x2)
+#define TRANSCEDE_RAD_SRDS0_RX_AGC_CTRL		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0x3)
+#define TRANSCEDE_RAD_SRDS0_RX_LOOP_CTRL		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0x4)
+#define TRANSCEDE_RAD_SRDS0_RX_CDR_CTRL_0		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0x5)
+#define TRANSCEDE_RAD_SRDS0_RX_CDR_CTRL_1		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0x6)
+#define TRANSCEDE_RAD_SRDS0_RX_CDR_CTRL_2		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0x7)
+#define TRANSCEDE_RAD_SRDS0_RX_MVAL_0		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0x8)
+#define TRANSCEDE_RAD_SRDS0_RX_MVAL_1		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0x9)
+#define TRANSCEDE_RAD_SRDS0_RX_AEQ_VAL_0		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0xA)
+#define TRANSCEDE_RAD_SRDS0_RX_AEQ_VAL_1		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0xB)
+#define TRANSCEDE_RAD_SRDS0_RX_CTLE_CTRL		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0xC)
+#define TRANSCEDE_RAD_SRDS0_TX_CFG_0		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0xD)
+#define TRANSCEDE_RAD_SRDS0_TX_CFG_1		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0xE)
+#define TRANSCEDE_RAD_SRDS0_TX_CFG_2		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0xF)
+#define TRANSCEDE_RAD_SRDS0_TX_PREEMPH		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0x10)
+#define TRANSCEDE_RAD_SRDS0_TX_CLK_SEL_0		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0x11)
+#define TRANSCEDE_RAD_SRDS0_TX_CLK_SEL_1		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0x12)
+#define TRANSCEDE_RAD_SRDS0_TERM_CTRL		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0x13)
+#define TRANSCEDE_RAD_SRDS0_PMA_LOOPBACK_CTRL		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0x14)
+#define TRANSCEDE_RAD_SRDS0_PMA_DFE_TRAIN_CTRL		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0x15)
+#define TRANSCEDE_RAD_SRDS0_LANE_PWR_CTRL		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0x16)
+#define TRANSCEDE_RAD_SRDS0_RX_PWR_MON_0		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0x17)
+#define TRANSCEDE_RAD_SRDS0_RX_PWR_MON_1		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0x18)
+#define TRANSCEDE_RAD_SRDS0_TX_PWR_MON_0		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0x19)
+#define TRANSCEDE_RAD_SRDS0_TX_PWR_MON_1		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0x1A)
+#define TRANSCEDE_RAD_SRDS0_MACIFC_MON_0		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0x1B)
+#define TRANSCEDE_RAD_SRDS0_MACIFC_MON_1		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0x1C)
+#define TRANSCEDE_RAD_SRDS0_CTLIFC_CTRL_0		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0x1D)
+#define TRANSCEDE_RAD_SRDS0_CTLIFC_CTRL_1		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0x1E)
+#define TRANSCEDE_RAD_SRDS0_SDS_PIN_MON		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_CFG + 4*0x1F)
+
+
+/* Table 6-5 PMA/PMD vendor-defined global monitor registers */
+#define TRANSCEDE_RAD_SRDS0_GLBL_PLL_MONITOR		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_MON + 4*0x0)
+#define TRANSCEDE_RAD_SRDS0_GLBL_TERM_MON		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_MON + 4*0x1)
+#define TRANSCEDE_RAD_SRDS0_GLBL_SDS_PIN_MON_0		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_MON + 4*0x2)
+#define TRANSCEDE_RAD_SRDS0_GLBL_SDS_PIN_MON_1		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_GLOBAL_MON + 4*0x3)
+
+/* Table 6-6 PMA/PMD vendor-defined per-lane monitor registers */
+#define TRANSCEDE_RAD_SRDS0_RX_AEQ_MON_0		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_MON + 4*0x0)
+#define TRANSCEDE_RAD_SRDS0_RX_AEQ_MON_1		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_MON + 4*0x1)
+#define TRANSCEDE_RAD_SRDS0_RX_OS_MON_0		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_MON + 4*0x2)
+#define TRANSCEDE_RAD_SRDS0_RX_OS_MON_1		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_MON + 4*0x3)
+#define TRANSCEDE_RAD_SRDS0_RX_CDR_STATUS		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_MON + 4*0x4)
+#define TRANSCEDE_RAD_SRDS0_RX_MISC_STATUS		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_MON + 4*0x5)
+
+/* Table 6-7 PMA/PMD vendor-defined per-lane 10GBASE-KR registers */
+#define TRANSCEDE_RAD_SRDS0_LNK_TRN_CFG				AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_10GBASE_CFG + 4*0x0)
+#define TRANSCEDE_RAD_SRDS0_LNK_TRN_COEFF_REQ		AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_10GBASE_CFG + 4*0x1)
+#define TRANSCEDE_RAD_SRDS0_LNK_TRN_TX_COEFF_MON	AAB_XP_VADDR(TRANSCEDE_SERDES0_PMA_VENDOR_PERLANE_10GBASE_CFG + 4*0x2)
+
+/* Table 6-8 PCS IEEE-defined per-lane registers (45.2.3) */
+#define TRANSCEDE_RAD_SRDS0_PCS_CTRL_1				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_IEEE_PERLANE_CFG + 4*0x0)
+#define TRANSCEDE_RAD_SRDS0_PCS_STATUS_1				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_IEEE_PERLANE_CFG + 4*0x1)
+#define TRANSCEDE_RAD_SRDS0_PCS_DEVID_1				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_IEEE_PERLANE_CFG + 4*0x2)
+#define TRANSCEDE_RAD_SRDS0_PCS_DEVID_2				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_IEEE_PERLANE_CFG + 4*0x3)
+#define TRANSCEDE_RAD_SRDS0_PCS_SPEED_ABILITY				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_IEEE_PERLANE_CFG + 4*0x4)
+#define TRANSCEDE_RAD_SRDS0_PCS_DEV_IN_PKG_1				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_IEEE_PERLANE_CFG + 4*0x5)
+#define TRANSCEDE_RAD_SRDS0_PCS_DEV_IN_PKG_2				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_IEEE_PERLANE_CFG + 4*0x6)
+#define TRANSCEDE_RAD_SRDS0_PCS_CTRL_2				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_IEEE_PERLANE_CFG + 4*0x7)
+#define TRANSCEDE_RAD_SRDS0_PCS_STATUS_2				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_IEEE_PERLANE_CFG + 4*0x8)
+#define TRANSCEDE_RAD_SRDS0_PCS_PKGID_1				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_IEEE_PERLANE_CFG + 4*0xE)
+#define TRANSCEDE_RAD_SRDS0_PCS_PKGID_2				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_IEEE_PERLANE_CFG + 4*0xF)
+#define TRANSCEDE_RAD_SRDS0_PCS_10GBASEX_STATUS				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_IEEE_PERLANE_CFG + 4*0x18)
+#define TRANSCEDE_RAD_SRDS0_PCS_10GBASEX_TEST_CTRL				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_IEEE_PERLANE_CFG + 4*0x19)
+#define TRANSCEDE_RAD_SRDS0_PCS_10GBASE_R_STATUS_1				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_IEEE_PERLANE_CFG + 4*0x20)
+#define TRANSCEDE_RAD_SRDS0_PCS_10GBASE_R_STATUS_2				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_IEEE_PERLANE_CFG + 4*0x21)
+#define TRANSCEDE_RAD_SRDS0_PCS_BASE_R_SEED_A_0				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_IEEE_PERLANE_CFG + 4*0x22)
+#define TRANSCEDE_RAD_SRDS0_PCS_BASE_R_SEED_A_1				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_IEEE_PERLANE_CFG + 4*0x23)
+#define TRANSCEDE_RAD_SRDS0_PCS_BASE_R_SEED_A_2				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_IEEE_PERLANE_CFG + 4*0x24)
+#define TRANSCEDE_RAD_SRDS0_PCS_BASE_R_SEED_A_3				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_IEEE_PERLANE_CFG + 4*0x25)
+#define TRANSCEDE_RAD_SRDS0_PCS_BASE_R_SEED_B_0				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_IEEE_PERLANE_CFG + 4*0x26)
+#define TRANSCEDE_RAD_SRDS0_PCS_BASE_R_SEED_B_1				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_IEEE_PERLANE_CFG + 4*0x27)
+#define TRANSCEDE_RAD_SRDS0_PCS_BASE_R_SEED_B_2				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_IEEE_PERLANE_CFG + 4*0x28)
+#define TRANSCEDE_RAD_SRDS0_PCS_BASE_R_SEED_B_3				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_IEEE_PERLANE_CFG + 4*0x29)
+#define TRANSCEDE_RAD_SRDS0_PCS_BASE_R_TPAT_CTRL				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_IEEE_PERLANE_CFG + 4*0x2A)
+#define TRANSCEDE_RAD_SRDS0_PCS_BASE_R_TPAT_ERR_CNT				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_IEEE_PERLANE_CFG + 4*0x2B)
+
+/* Table 6-9 PCS common vendor-defined global registers */
+#define TRANSCEDE_RAD_SRDS0_PCS_BER_CONST_PAT_0				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_COMMON_VENDOR_GLOBAL_CFG + 4*0x0)
+#define TRANSCEDE_RAD_SRDS0_PCS_BER_CONST_PAT_1				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_COMMON_VENDOR_GLOBAL_CFG + 4*0x1)
+#define TRANSCEDE_RAD_SRDS0_PCS_RDET_TIME				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_COMMON_VENDOR_GLOBAL_CFG + 4*0x2)
+#define TRANSCEDE_RAD_SRDS0_PCS_LANE_LINK_CFG				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_COMMON_VENDOR_GLOBAL_CFG + 4*0x3)
+#define TRANSCEDE_RAD_SRDS0_PCS_LANE_LINK_MON				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_COMMON_VENDOR_GLOBAL_CFG + 4*0x4)
+#define TRANSCEDE_RAD_SRDS0_PCS_PPAT_0				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_COMMON_VENDOR_GLOBAL_CFG + 4*0x5)
+#define TRANSCEDE_RAD_SRDS0_PCS_PPAT_				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_COMMON_VENDOR_GLOBAL_CFG + 4*0x6)
+#define TRANSCEDE_RAD_SRDS0_PCS_PPAT_LEN				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_COMMON_VENDOR_GLOBAL_CFG + 4*0x7)
+
+/* Table 6-10 PCS common vendor-defined per-lane registers */
+#define TRANSCEDE_RAD_SRDS0_PCS_MODE				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_COMMON_VENDOR_PERLANE_CFG + 4*0x0)
+#define TRANSCEDE_RAD_SRDS0_PCS_MISC_CFG_0				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_COMMON_VENDOR_PERLANE_CFG + 4*0x1)
+#define TRANSCEDE_RAD_SRDS0_PCS_PRBS_ECNT_0				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_COMMON_VENDOR_PERLANE_CFG + 4*0x2)
+#define TRANSCEDE_RAD_SRDS0_PCS_PRBS_ECNT_1				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_COMMON_VENDOR_PERLANE_CFG + 4*0x3)
+#define TRANSCEDE_RAD_SRDS0_PCS_PRBS_TIMER_0				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_COMMON_VENDOR_PERLANE_CFG + 4*0x4)
+#define TRANSCEDE_RAD_SRDS0_PCS_PRBS_TIMER_1				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_COMMON_VENDOR_PERLANE_CFG + 4*0x5)
+#define TRANSCEDE_RAD_SRDS0_PCS_PRBS_TIMER_2				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_COMMON_VENDOR_PERLANE_CFG + 4*0x6)
+#define TRANSCEDE_RAD_SRDS0_PCS_BER_CFG_0				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_COMMON_VENDOR_PERLANE_CFG + 4*0x7)
+#define TRANSCEDE_RAD_SRDS0_PCS_BER_CFG_1				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_COMMON_VENDOR_PERLANE_CFG + 4*0x8)
+#define TRANSCEDE_RAD_SRDS0_PCS_RESET				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_COMMON_VENDOR_PERLANE_CFG + 4*0x9)
+#define TRANSCEDE_RAD_SRDS0_PCS_PPCHK_STATUS				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_COMMON_VENDOR_PERLANE_CFG + 4*0xA)
+#define TRANSCEDE_RAD_SRDS0_PCS_8B10B_STATE				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_COMMON_VENDOR_PERLANE_CFG + 4*0xB)
+#define TRANSCEDE_RAD_SRDS0_PCS_PRBS_CFG				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_COMMON_VENDOR_PERLANE_CFG + 4*0xC)
+#define TRANSCEDE_RAD_SRDS0_PCS_MISC_CFG_1				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_COMMON_VENDOR_PERLANE_CFG + 4*0xD)
+#define TRANSCEDE_RAD_SRDS0_PCS_RAW_SHIFT_MON				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_COMMON_VENDOR_PERLANE_CFG + 4*0xE)
+#define TRANSCEDE_RAD_SRDS0_PCS_RESET_1				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_COMMON_VENDOR_PERLANE_CFG + 4*0xF)
+
+/* Table 6-11 PCS XAUI vendor-defined quad registers */
+#define TRANSCEDE_RAD_SRDS0_PCS_XAUI_LANE_SEL				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_XAUI_VENDOR_PERLANE_CFG + 4*0x0)
+#define TRANSCEDE_RAD_SRDS0_PCS_XAUI_CFG						AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_XAUI_VENDOR_PERLANE_CFG + 4*0x1)
+#define TRANSCEDE_RAD_SRDS0_PCS_XAUI_FIFO_PTR				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_XAUI_VENDOR_PERLANE_CFG + 4*0x2)
+#define TRANSCEDE_RAD_SRDS0_PCS_XAUI_BASE_FIFO_PTR			AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_XAUI_VENDOR_PERLANE_CFG + 4*0x3)
+#define TRANSCEDE_RAD_SRDS0_PCS_XAUI_SM_STATUS				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_XAUI_VENDOR_PERLANE_CFG + 4*0x4)
+
+/* Table 6-12 PCS PCIE vendor-defined per-lane registers */
+#define TRANSCEDE_RAD_SRDS0_PCS_PCIE_MISC_CFG				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_PCIE_VENDOR_PERLANE_CFG + 4*0x0)
+#define TRANSCEDE_RAD_SRDS0_PCS_PCIE_SLIP_CFG				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_PCIE_VENDOR_PERLANE_CFG + 4*0x1)
+#define TRANSCEDE_RAD_SRDS0_PCS_PCIE_CCNT_0				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_PCIE_VENDOR_PERLANE_CFG + 4*0x2)
+#define TRANSCEDE_RAD_SRDS0_PCS_PCIE_CCNT_1				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_PCIE_VENDOR_PERLANE_CFG + 4*0x3)
+#define TRANSCEDE_RAD_SRDS0_PCS_PCIE_CCNT_2				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_PCIE_VENDOR_PERLANE_CFG + 4*0x4)
+#define TRANSCEDE_RAD_SRDS0_PCS_PCIE_RX_EIDLE				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_PCIE_VENDOR_PERLANE_CFG + 4*0x5)
+#define TRANSCEDE_RAD_SRDS0_PCS_PCIE_SMPL_PERIOD				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_PCIE_VENDOR_PERLANE_CFG + 4*0x6)
+#define TRANSCEDE_RAD_SRDS0_PCS_PCIE_SMPL_DIR				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_PCIE_VENDOR_PERLANE_CFG + 4*0x7)
+#define TRANSCEDE_RAD_SRDS0_PCS_PCIE_MON				AAB_XP_VADDR(TRANSCEDE_SERDES0_PCS_PCIE_VENDOR_PERLANE_CFG + 4*0x8)
+
+
+
+#define TRANSCEDE_SERDES_BASE_R(x)    (TRANSCEDE_SERDES0_4CH + (0x10000 *x))		/* SRDS(n) __baseaddr */
+
+#define TRANSCEDE_SERDES_PMA_IEEE_PERLANE_CFG(x)    				(TRANSCEDE_SERDES_BASE_R(x) + 0x0000)
+#define TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_CFG(x)    			(TRANSCEDE_SERDES_BASE_R(x) + 0xC000)
+#define TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x)    			(TRANSCEDE_SERDES_BASE_R(x) + 0x8000)
+#define TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_MON(x)    			(TRANSCEDE_SERDES_BASE_R(x) + 0xC020)
+#define TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_MON(x)    			(TRANSCEDE_SERDES_BASE_R(x) + 0x8020)
+#define TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_10GBASE_CFG(x)  	(TRANSCEDE_SERDES_BASE_R(x) + 0x8030)
+
+#define TRANSCEDE_SERDES_PCS_IEEE_PERLANE_CFG(x)    				(TRANSCEDE_SERDES_BASE_R(x) + 0x0000)
+#define TRANSCEDE_SERDES_PCS_COMMON_VENDOR_GLOBAL_CFG(x)    	(TRANSCEDE_SERDES_BASE_R(x) + 0xC000)
+#define TRANSCEDE_SERDES_PCS_COMMON_VENDOR_PERLANE_CFG(x)   	(TRANSCEDE_SERDES_BASE_R(x) + 0x8000)
+#define TRANSCEDE_SERDES_PCS_XAUI_VENDOR_PERLANE_CFG(x)    		(TRANSCEDE_SERDES_BASE_R(x) + 0x8060)
+#define TRANSCEDE_SERDES_PCS_PCIE_VENDOR_PERLANE_CFG(x)    		(TRANSCEDE_SERDES_BASE_R(x) + 0x80C0)
+#define TRANSCEDE_SERDES_PCS_10GBASEX_VENDOR_PERLANE_CFG(x)	(TRANSCEDE_SERDES_BASE_R(x) + 0x8040)
+#define TRANSCEDE_SERDES_PCS_10GBASER_VENDOR_PERLANE_CFG(x)	(TRANSCEDE_SERDES_BASE_R(x) + 0x8080)
+
+
+
+/* "SRDS.MMD = 0x1"  for  "PMA/PMD_x Registers Mapping" */
+
+/* Table 6-2 PMA/PMD IEEE-defined per-lane registers (45.2.1) */
+#define TRANSCEDE_RAD_SRDS_PMA_CTRL_1(x)			AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_IEEE_PERLANE_CFG(x) + 4*0x0)
+#define TRANSCEDE_RAD_SRDS_PMA_STATUS_1(x)  	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_IEEE_PERLANE_CFG(x) + 4*0x1)
+#define TRANSCEDE_RAD_SRDS_PMA_DEVID_1(x)  		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_IEEE_PERLANE_CFG(x) + 4*0x2)
+#define TRANSCEDE_RAD_SRDS_PMA_DEVID_2(x)			AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_IEEE_PERLANE_CFG(x) + 4*0x3)
+#define TRANSCEDE_RAD_SRDS_PMA_SPEED_ABILITY(x)	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_IEEE_PERLANE_CFG(x) + 4*0x4)
+#define TRANSCEDE_RAD_SRDS_PMA_DEV_IN_PKG_1(x)  AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_IEEE_PERLANE_CFG(x) + 4*0x5)
+#define TRANSCEDE_RAD_SRDS_PMA_DEV_IN_PKG_2(x)	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_IEEE_PERLANE_CFG(x) + 4*0x6)
+#define TRANSCEDE_RAD_SRDS_PMA_CTRL_2(x)			AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_IEEE_PERLANE_CFG(x) + 4*0x7)
+#define TRANSCEDE_RAD_SRDS_PMA_STATUS_2(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_IEEE_PERLANE_CFG(x) + 4*0x8)
+#define TRANSCEDE_RAD_SRDS_10G_PMA_TX_DISABLE(x)	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_IEEE_PERLANE_CFG(x) + 4*0x9)
+#define TRANSCEDE_RAD_SRDS_10G_PMA_RX_SIGDET(x)	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_IEEE_PERLANE_CFG(x) + 4*0xA)
+#define TRANSCEDE_RAD_SRDS_10G_PMA_EXT_ABILITY(x)	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_IEEE_PERLANE_CFG(x) + 4*0xB)
+#define TRANSCEDE_RAD_SRDS_PMA_PKGID_1(x)  		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_IEEE_PERLANE_CFG(x) + 4*0xE)
+#define TRANSCEDE_RAD_SRDS_PMA_PKGID_2(x)			AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_IEEE_PERLANE_CFG(x) + 4*0xF)
+#define TRANSCEDE_RAD_SRDS_10G_KR_PMD_CTRL(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_IEEE_PERLANE_CFG(x) + 4*0x96)
+#define TRANSCEDE_RAD_SRDS_10G_KR_PMD_STATUS(x)	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_IEEE_PERLANE_CFG(x) + 4*0x97)
+#define TRANSCEDE_RAD_SRDS_10G_KR_LP_COEFF_UPDT(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_IEEE_PERLANE_CFG(x) + 4*0x98)
+#define TRANSCEDE_RAD_SRDS_10G_KR_LP_STATUS(x)	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_IEEE_PERLANE_CFG(x) + 4*0x99)
+#define TRANSCEDE_RAD_SRDS_10G_KR_LD_COEFF_UPDT(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_IEEE_PERLANE_CFG(x) + 4*0x9A)
+#define TRANSCEDE_RAD_SRDS_10G_KR_LD_STATUS(x)	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_IEEE_PERLANE_CFG(x) + 4*0x9B)
+#define TRANSCEDE_RAD_SRDS_1000BASE_KX_CTRL(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_IEEE_PERLANE_CFG(x) + 4*0xA0)
+#define TRANSCEDE_RAD_SRDS_1000BASE_KX_STATUS(x)	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_IEEE_PERLANE_CFG(x) + 4*0xA1)
+
+/* Table 6-3 PMA/PMD vendor-defined global configuration and control registers */
+#define TRANSCEDE_RAD_SRDS_GLBL_PLL_CFG_0(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_CFG(x) + 4*0x0)
+#define TRANSCEDE_RAD_SRDS_GLBL_PLL_CFG_1(x)  	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_CFG(x) + 4*0x1)
+#define TRANSCEDE_RAD_SRDS_GLBL_DLL_CFG_0(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_CFG(x) + 4*0x2)
+#define TRANSCEDE_RAD_SRDS_GLBL_MISC_CONFIG(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_CFG(x) + 4*0x3)
+#define TRANSCEDE_RAD_SRDS_SLICE_CFG(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_CFG(x) + 4*0x4)
+#define TRANSCEDE_RAD_SRDS_GLBL_RX_DETECT_LVL(x)	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_CFG(x) + 4*0x5)
+#define TRANSCEDE_RAD_SRDS_GLBL_AMON_SEL(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_CFG(x) + 4*0x6)
+#define TRANSCEDE_RAD_SRDS_LANE_AMON_SEL(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_CFG(x) + 4*0x7)
+#define TRANSCEDE_RAD_SRDS_GLBL_DMON_SEL(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_CFG(x) + 4*0x8)
+#define TRANSCEDE_RAD_SRDS_LANE_DMON_SEL(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_CFG(x) + 4*0x9)
+#define TRANSCEDE_RAD_SRDS_GLBL_NOISE_CTRL(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_CFG(x) + 4*0xC)
+#define TRANSCEDE_RAD_SRDS_GLBL_RD_SYNC_STATUS(x)	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_CFG(x) + 4*0x10)
+#define TRANSCEDE_RAD_SRDS_GLBL_PLL_CFG_2(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_CFG(x) + 4*0x11)
+#define TRANSCEDE_RAD_SRDS_RX_PWR_CTRL_P0(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_CFG(x) + 4*0x14)
+#define TRANSCEDE_RAD_SRDS_RX_PWR_CTRL_P0S(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_CFG(x) + 4*0x15)
+#define TRANSCEDE_RAD_SRDS_RX_PWR_CTRL_P1(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_CFG(x) + 4*0x16)
+#define TRANSCEDE_RAD_SRDS_RX_PWR_CTRL_P2(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_CFG(x) + 4*0x17)
+#define TRANSCEDE_RAD_SRDS_TX_PWR_CTRL_P0_P0S(x)	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_CFG(x) + 4*0x18)
+#define TRANSCEDE_RAD_SRDS_TX_PWR_CTRL_P1_P2(x)	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_CFG(x) + 4*0x19)
+#define TRANSCEDE_RAD_SRDS_GLBL_PWR_CTRL(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_CFG(x) + 4*0x1A)
+#define TRANSCEDE_RAD_SRDS_GLBL_PWR_MON(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_CFG(x) + 4*0x1B)
+#define TRANSCEDE_RAD_SRDS_PWRUP_DLY(x)			AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_CFG(x) + 4*0x1C)
+#define TRANSCEDE_RAD_SRDS_GLBL_DLL_CFG_1(x)  	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_CFG(x) + 4*0x1D)
+#define TRANSCEDE_RAD_SRDS_RX_PI_CTRL_0(x)			AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_CFG(x) + 4*0x1E)
+#define TRANSCEDE_RAD_SRDS_RX_PI_CTRL_1(x)  		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_CFG(x) + 4*0x1F)
+
+/* Table 6-4 PMA/PMD vendor-defined per-lane configuration and control registers */
+#define TRANSCEDE_RAD_SRDS_RX_CFG_0(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0x0)
+#define TRANSCEDE_RAD_SRDS_RX_CFG_1(x)  	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0x1)
+#define TRANSCEDE_RAD_SRDS_RX_CFG_2(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0x2)
+#define TRANSCEDE_RAD_SRDS_RX_AGC_CTRL(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0x3)
+#define TRANSCEDE_RAD_SRDS_RX_LOOP_CTRL(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0x4)
+#define TRANSCEDE_RAD_SRDS_RX_CDR_CTRL_0(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0x5)
+#define TRANSCEDE_RAD_SRDS_RX_CDR_CTRL_1(x)  	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0x6)
+#define TRANSCEDE_RAD_SRDS_RX_CDR_CTRL_2(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0x7)
+#define TRANSCEDE_RAD_SRDS_RX_MVAL_0(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0x8)
+#define TRANSCEDE_RAD_SRDS_RX_MVAL_1(x)  	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0x9)
+#define TRANSCEDE_RAD_SRDS_RX_AEQ_VAL_0(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0xA)
+#define TRANSCEDE_RAD_SRDS_RX_AEQ_VAL_1(x)  	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0xB)
+#define TRANSCEDE_RAD_SRDS_RX_CTLE_CTRL(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0xC)
+#define TRANSCEDE_RAD_SRDS_TX_CFG_0(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0xD)
+#define TRANSCEDE_RAD_SRDS_TX_CFG_1(x)  	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0xE)
+#define TRANSCEDE_RAD_SRDS_TX_CFG_2(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0xF)
+#define TRANSCEDE_RAD_SRDS_TX_PREEMPH(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0x10)
+#define TRANSCEDE_RAD_SRDS_TX_CLK_SEL_0(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0x11)
+#define TRANSCEDE_RAD_SRDS_TX_CLK_SEL_1(x)  	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0x12)
+#define TRANSCEDE_RAD_SRDS_TERM_CTRL(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0x13)
+#define TRANSCEDE_RAD_SRDS_PMA_LOOPBACK_CTRL(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0x14)
+#define TRANSCEDE_RAD_SRDS_PMA_DFE_TRAIN_CTRL(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0x15)
+#define TRANSCEDE_RAD_SRDS_LANE_PWR_CTRL(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0x16)
+#define TRANSCEDE_RAD_SRDS_RX_PWR_MON_0(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0x17)
+#define TRANSCEDE_RAD_SRDS_RX_PWR_MON_1(x)  	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0x18)
+#define TRANSCEDE_RAD_SRDS_TX_PWR_MON_0(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0x19)
+#define TRANSCEDE_RAD_SRDS_TX_PWR_MON_1(x)  	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0x1A)
+#define TRANSCEDE_RAD_SRDS_MACIFC_MON_0(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0x1B)
+#define TRANSCEDE_RAD_SRDS_MACIFC_MON_1(x)  	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0x1C)
+#define TRANSCEDE_RAD_SRDS_CTLIFC_CTRL_0(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0x1D)
+#define TRANSCEDE_RAD_SRDS_CTLIFC_CTRL_1(x)  	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0x1E)
+#define TRANSCEDE_RAD_SRDS_SDS_PIN_MON(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_CFG(x) + 4*0x1F)
+
+
+/* Table 6-5 PMA/PMD vendor-defined global monitor registers */
+#define TRANSCEDE_RAD_SRDS_GLBL_PLL_MONITOR(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_MON(x) + 4*0x0)
+#define TRANSCEDE_RAD_SRDS_GLBL_TERM_MON(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_MON(x) + 4*0x1)
+#define TRANSCEDE_RAD_SRDS_GLBL_SDS_PIN_MON_0(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_MON(x) + 4*0x2)
+#define TRANSCEDE_RAD_SRDS_GLBL_SDS_PIN_MON_1(x)  	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_GLOBAL_MON(x) + 4*0x3)
+
+/* Table 6-6 PMA/PMD vendor-defined per-lane monitor registers */
+#define TRANSCEDE_RAD_SRDS_RX_AEQ_MON_0(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_MON(x) + 4*0x0)
+#define TRANSCEDE_RAD_SRDS_RX_AEQ_MON_1(x)  	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_MON(x) + 4*0x1)
+#define TRANSCEDE_RAD_SRDS_RX_OS_MON_0(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_MON(x) + 4*0x2)
+#define TRANSCEDE_RAD_SRDS_RX_OS_MON_1(x)  	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_MON(x) + 4*0x3)
+#define TRANSCEDE_RAD_SRDS_RX_CDR_STATUS(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_MON(x) + 4*0x4)
+#define TRANSCEDE_RAD_SRDS_RX_MISC_STATUS(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_MON(x) + 4*0x5)
+
+/* Table 6-7 PMA/PMD vendor-defined per-lane 10GBASE-KR registers */
+#define TRANSCEDE_RAD_SRDS_LNK_TRN_CFG(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_10GBASE_CFG(x) + 4*0x0)
+#define TRANSCEDE_RAD_SRDS_LNK_TRN_COEFF_REQ(x)		AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_10GBASE_CFG(x) + 4*0x1)
+#define TRANSCEDE_RAD_SRDS_LNK_TRN_TX_COEFF_MON(x)	AAB_XP_VADDR(TRANSCEDE_SERDES_PMA_VENDOR_PERLANE_10GBASE_CFG(x) + 4*0x2)
+
+/* Table 6-8 PCS IEEE-defined per-lane registers (45.2.3) */
+#define TRANSCEDE_RAD_SRDS_PCS_CTRL_1(x)  			AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_IEEE_PERLANE_CFG(x) + 4*0x0)
+#define TRANSCEDE_RAD_SRDS_PCS_STATUS_1(x)  			AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_IEEE_PERLANE_CFG(x) + 4*0x1)
+#define TRANSCEDE_RAD_SRDS_PCS_DEVID_1(x)  			AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_IEEE_PERLANE_CFG(x) + 4*0x2)
+#define TRANSCEDE_RAD_SRDS_PCS_DEVID_2(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_IEEE_PERLANE_CFG(x) + 4*0x3)
+#define TRANSCEDE_RAD_SRDS_PCS_SPEED_ABILITY(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_IEEE_PERLANE_CFG(x) + 4*0x4)
+#define TRANSCEDE_RAD_SRDS_PCS_DEV_IN_PKG_1(x)  			AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_IEEE_PERLANE_CFG(x) + 4*0x5)
+#define TRANSCEDE_RAD_SRDS_PCS_DEV_IN_PKG_2(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_IEEE_PERLANE_CFG(x) + 4*0x6)
+#define TRANSCEDE_RAD_SRDS_PCS_CTRL_2(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_IEEE_PERLANE_CFG(x) + 4*0x7)
+#define TRANSCEDE_RAD_SRDS_PCS_STATUS_2(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_IEEE_PERLANE_CFG(x) + 4*0x8)
+#define TRANSCEDE_RAD_SRDS_PCS_PKGID_1(x)  			AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_IEEE_PERLANE_CFG(x) + 4*0xE)
+#define TRANSCEDE_RAD_SRDS_PCS_PKGID_2(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_IEEE_PERLANE_CFG(x) + 4*0xF)
+#define TRANSCEDE_RAD_SRDS_PCS_10GBASEX_STATUS(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_IEEE_PERLANE_CFG(x) + 4*0x18)
+#define TRANSCEDE_RAD_SRDS_PCS_10GBASEX_TEST_CTRL(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_IEEE_PERLANE_CFG(x) + 4*0x19)
+#define TRANSCEDE_RAD_SRDS_PCS_10GBASE_R_STATUS_1(x)  			AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_IEEE_PERLANE_CFG(x) + 4*0x20)
+#define TRANSCEDE_RAD_SRDS_PCS_10GBASE_R_STATUS_2(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_IEEE_PERLANE_CFG(x) + 4*0x21)
+#define TRANSCEDE_RAD_SRDS_PCS_BASE_R_SEED_A_0(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_IEEE_PERLANE_CFG(x) + 4*0x22)
+#define TRANSCEDE_RAD_SRDS_PCS_BASE_R_SEED_A_1(x)  			AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_IEEE_PERLANE_CFG(x) + 4*0x23)
+#define TRANSCEDE_RAD_SRDS_PCS_BASE_R_SEED_A_2(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_IEEE_PERLANE_CFG(x) + 4*0x24)
+#define TRANSCEDE_RAD_SRDS_PCS_BASE_R_SEED_A_3(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_IEEE_PERLANE_CFG(x) + 4*0x25)
+#define TRANSCEDE_RAD_SRDS_PCS_BASE_R_SEED_B_0(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_IEEE_PERLANE_CFG(x) + 4*0x26)
+#define TRANSCEDE_RAD_SRDS_PCS_BASE_R_SEED_B_1(x)  			AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_IEEE_PERLANE_CFG(x) + 4*0x27)
+#define TRANSCEDE_RAD_SRDS_PCS_BASE_R_SEED_B_2(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_IEEE_PERLANE_CFG(x) + 4*0x28)
+#define TRANSCEDE_RAD_SRDS_PCS_BASE_R_SEED_B_3(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_IEEE_PERLANE_CFG(x) + 4*0x29)
+#define TRANSCEDE_RAD_SRDS_PCS_BASE_R_TPAT_CTRL(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_IEEE_PERLANE_CFG(x) + 4*0x2A)
+#define TRANSCEDE_RAD_SRDS_PCS_BASE_R_TPAT_ERR_CNT(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_IEEE_PERLANE_CFG(x) + 4*0x2B)
+
+/* Table 6-9 PCS common vendor-defined global registers */
+#define TRANSCEDE_RAD_SRDS_PCS_BER_CONST_PAT_0(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_COMMON_VENDOR_GLOBAL_CFG(x) + 4*0x0)
+#define TRANSCEDE_RAD_SRDS_PCS_BER_CONST_PAT_1(x)  			AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_COMMON_VENDOR_GLOBAL_CFG(x) + 4*0x1)
+#define TRANSCEDE_RAD_SRDS_PCS_RDET_TIME(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_COMMON_VENDOR_GLOBAL_CFG(x) + 4*0x2)
+#define TRANSCEDE_RAD_SRDS_PCS_LANE_LINK_CFG(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_COMMON_VENDOR_GLOBAL_CFG(x) + 4*0x3)
+#define TRANSCEDE_RAD_SRDS_PCS_LANE_LINK_MON(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_COMMON_VENDOR_GLOBAL_CFG(x) + 4*0x4)
+#define TRANSCEDE_RAD_SRDS_PCS_PPAT_0(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_COMMON_VENDOR_GLOBAL_CFG(x) + 4*0x5)
+#define TRANSCEDE_RAD_SRDS_PCS_PPAT_1(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_COMMON_VENDOR_GLOBAL_CFG(x) + 4*0x6)
+#define TRANSCEDE_RAD_SRDS_PCS_PPAT_LEN(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_COMMON_VENDOR_GLOBAL_CFG(x) + 4*0x7)
+
+/* Table 6-10 PCS common vendor-defined per-lane registers */
+#define TRANSCEDE_RAD_SRDS_PCS_MODE(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_COMMON_VENDOR_PERLANE_CFG(x) + 4*0x0)
+#define TRANSCEDE_RAD_SRDS_PCS_MISC_CFG_0(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_COMMON_VENDOR_PERLANE_CFG(x) + 4*0x1)
+#define TRANSCEDE_RAD_SRDS_PCS_PRBS_ECNT_0(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_COMMON_VENDOR_PERLANE_CFG(x) + 4*0x2)
+#define TRANSCEDE_RAD_SRDS_PCS_PRBS_ECNT_1(x)  			AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_COMMON_VENDOR_PERLANE_CFG(x) + 4*0x3)
+#define TRANSCEDE_RAD_SRDS_PCS_PRBS_TIMER_0(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_COMMON_VENDOR_PERLANE_CFG(x) + 4*0x4)
+#define TRANSCEDE_RAD_SRDS_PCS_PRBS_TIMER_1(x)  			AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_COMMON_VENDOR_PERLANE_CFG(x) + 4*0x5)
+#define TRANSCEDE_RAD_SRDS_PCS_PRBS_TIMER_2(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_COMMON_VENDOR_PERLANE_CFG(x) + 4*0x6)
+#define TRANSCEDE_RAD_SRDS_PCS_BER_CFG_0(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_COMMON_VENDOR_PERLANE_CFG(x) + 4*0x7)
+#define TRANSCEDE_RAD_SRDS_PCS_BER_CFG_1(x)  			AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_COMMON_VENDOR_PERLANE_CFG(x) + 4*0x8)
+#define TRANSCEDE_RAD_SRDS_PCS_RESET(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_COMMON_VENDOR_PERLANE_CFG(x) + 4*0x9)
+#define TRANSCEDE_RAD_SRDS_PCS_PPCHK_STATUS(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_COMMON_VENDOR_PERLANE_CFG(x) + 4*0xA)
+#define TRANSCEDE_RAD_SRDS_PCS_8B10B_STATE(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_COMMON_VENDOR_PERLANE_CFG(x) + 4*0xB)
+#define TRANSCEDE_RAD_SRDS_PCS_PRBS_CFG(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_COMMON_VENDOR_PERLANE_CFG(x) + 4*0xC)
+#define TRANSCEDE_RAD_SRDS_PCS_MISC_CFG_1(x)  			AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_COMMON_VENDOR_PERLANE_CFG(x) + 4*0xD)
+#define TRANSCEDE_RAD_SRDS_PCS_RAW_SHIFT_MON(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_COMMON_VENDOR_PERLANE_CFG(x) + 4*0xE)
+#define TRANSCEDE_RAD_SRDS_PCS_RESET_1(x)  			AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_COMMON_VENDOR_PERLANE_CFG(x) + 4*0xF)
+
+/* Table 6-11 PCS XAUI vendor-defined quad registers */
+#define TRANSCEDE_RAD_SRDS_PCS_XAUI_LANE_SEL(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_XAUI_VENDOR_PERLANE_CFG(x) + 4*0x0)
+#define TRANSCEDE_RAD_SRDS_PCS_XAUI_CFG(x)						AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_XAUI_VENDOR_PERLANE_CFG(x) + 4*0x1)
+#define TRANSCEDE_RAD_SRDS_PCS_XAUI_FIFO_PTR(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_XAUI_VENDOR_PERLANE_CFG(x) + 4*0x2)
+#define TRANSCEDE_RAD_SRDS_PCS_XAUI_BASE_FIFO_PTR(x)			AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_XAUI_VENDOR_PERLANE_CFG(x) + 4*0x3)
+#define TRANSCEDE_RAD_SRDS_PCS_XAUI_SM_STATUS(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_XAUI_VENDOR_PERLANE_CFG(x) + 4*0x4)
+
+/* Table 6-12 PCS PCIE vendor-defined per-lane registers */
+#define TRANSCEDE_RAD_SRDS_PCS_PCIE_MISC_CFG(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_PCIE_VENDOR_PERLANE_CFG(x) + 4*0x0)
+#define TRANSCEDE_RAD_SRDS_PCS_PCIE_SLIP_CFG(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_PCIE_VENDOR_PERLANE_CFG(x) + 4*0x1)
+#define TRANSCEDE_RAD_SRDS_PCS_PCIE_CCNT_0(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_PCIE_VENDOR_PERLANE_CFG(x) + 4*0x2)
+#define TRANSCEDE_RAD_SRDS_PCS_PCIE_CCNT_1(x)  			AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_PCIE_VENDOR_PERLANE_CFG(x) + 4*0x3)
+#define TRANSCEDE_RAD_SRDS_PCS_PCIE_CCNT_2(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_PCIE_VENDOR_PERLANE_CFG(x) + 4*0x4)
+#define TRANSCEDE_RAD_SRDS_PCS_PCIE_RX_EIDLE(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_PCIE_VENDOR_PERLANE_CFG(x) + 4*0x5)
+#define TRANSCEDE_RAD_SRDS_PCS_PCIE_SMPL_PERIOD(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_PCIE_VENDOR_PERLANE_CFG(x) + 4*0x6)
+#define TRANSCEDE_RAD_SRDS_PCS_PCIE_SMPL_DIR(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_PCIE_VENDOR_PERLANE_CFG(x) + 4*0x7)
+#define TRANSCEDE_RAD_SRDS_PCS_PCIE_MON(x)				AAB_XP_VADDR(TRANSCEDE_SERDES_PCS_PCIE_VENDOR_PERLANE_CFG(x) + 4*0x8)
+
+/* END of "SRDS.MMD = 0x3" for "PCS_x Registers Mapping" */
+
+
+#endif
diff --git a/arch/arm/mach-transcede/include/mach/sysheap.h b/arch/arm/mach-transcede/include/mach/sysheap.h
new file mode 100644
index 0000000..1ce9867
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/sysheap.h
@@ -0,0 +1,77 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+  *
+  * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+  *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+  *
+  * You should have received a copy of the GNU General Public License
+  * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+  */
+
+#ifndef __SYSHEAP_H__
+#define __SYSHEAP_H__
+
+typedef struct tMEMBLOCK
+{
+    unsigned Size;		// The size of memory block in bytes
+    struct tMEMBLOCK *NextFree;	// The pointer to the next free memory block
+} MEMBLOCK, *PMEMBLOCK;
+
+typedef struct tHEAPDESC
+{
+    void* HeapStorage;		// The pointer to the heap storage
+    PMEMBLOCK FreeBlock;	// The pointer to the first free memory block of heap (it can be null)
+    unsigned FreeSize;		// The total size of free space in bytes
+    unsigned Align;		// The data alignment in bytes
+    unsigned *SpinLock;
+    unsigned Options;
+} HEAPDESC, *PHEAPDESC;
+
+unsigned long cram_create_heap(unsigned pStorage, unsigned HeapSize, unsigned HeapAlign, unsigned SpinLockId);
+unsigned long iram_create_heap(unsigned pStorage, unsigned HeapSize, unsigned HeapAlign, unsigned SpinLockId);
+unsigned long ddr_create_heap(unsigned pStorage, unsigned HeapSize, unsigned HeapAlign, unsigned SpinLockId);
+unsigned long ddrcb_create_heap(unsigned pStorage, unsigned HeapSize, unsigned HeapAlign, unsigned SpinLockId);
+unsigned long icc_create_heap(unsigned pStorage, unsigned HeapSize, unsigned HeapAlign, unsigned SpinLockId);
+unsigned long amp_create_heap(unsigned cpuid, unsigned pStorage, unsigned HeapSize, unsigned HeapAlign, unsigned SpinLockId);
+
+void* cram_heap_alloc(unsigned nSize);
+void* iram_heap_alloc(unsigned nSize);
+void* ddr_heap_alloc(unsigned nSize);
+void* ddrcb_heap_alloc(unsigned nSize);
+void* icc_heap_alloc(unsigned nSize);
+void* amp_heap_alloc(unsigned nSize);
+
+unsigned cram_heap_free(void* pData);
+unsigned iram_heap_free(void* pData);
+unsigned ddr_heap_free(void* pData);
+unsigned ddrcb_heap_free(void* pData);
+void icc_heap_free(const void* pData);
+void amp_heap_free(const void* pData);
+
+unsigned cram_destroy_heap(void);
+unsigned iram_destroy_heap(void);
+unsigned ddr_destroy_heap(void);
+unsigned ddrcb_destroy_heap(void);
+unsigned icc_destroy_heap(void);
+
+unsigned long ddr_heap_virt_to_phys(unsigned long addr);
+unsigned long ddr_heap_phys_to_virt(unsigned long addr);
+
+unsigned long ddrcb_heap_virt_to_phys(unsigned long addr);
+unsigned long ddrcb_heap_phys_to_virt(unsigned long addr);
+
+unsigned long kheap_null_address_convert(unsigned long addr);
+
+#endif //__SYSHEAP_H__
diff --git a/arch/arm/mach-transcede/include/mach/syslib.h b/arch/arm/mach-transcede/include/mach/syslib.h
new file mode 100644
index 0000000..2a606b3
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/syslib.h
@@ -0,0 +1,92 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+#ifndef __SYSLIB_H__
+#define __SYSLIB_H__
+
+typedef unsigned char  U8;      /* unsigned 8-bit  integer */
+typedef unsigned short U16;     /* unsigned 16-bit integer */
+typedef unsigned long  U32;     /* unsigned 32-bit integer */
+typedef unsigned long long U64; /* unsigned 64-bit integer */
+
+typedef volatile unsigned char  V8;
+typedef volatile unsigned short V16;
+typedef volatile unsigned long  V32;
+
+typedef signed char S8;         /* 8-bit  signed integer */
+typedef signed short S16;       /* 16-bit signed integer */
+typedef signed long S32;        /* 32-bit signed integer */
+typedef signed long long S64;   /* 64 bit signed integer */
+
+#ifndef _PVOID_
+#define _PVOID_
+typedef void *PVOID;
+#endif
+
+
+#define K 			1024
+#define M			(K*K)
+#define KHZ         1000
+#define MHZ         (KHZ * KHZ)
+
+#ifndef TRUE
+#define TRUE (1)
+#endif
+
+#ifndef FALSE
+#define FALSE (0)
+#endif
+
+#ifndef NULL
+#define NULL ((PVOID)(0))
+#endif
+#define HANDLE	PVOID
+
+
+
+#define ISB()  asm volatile ("ISB")  /* instruction sync barrier */
+#define DSB()  asm volatile ("DSB")  /* data sync barrier */
+#define DMB()  asm volatile ("DMB")  /* data memory barrier */
+
+typedef struct tagFASTQUEUE {
+	PVOID *pStorage;
+	U32 BlockSize;
+	U32 sema;
+	V32 get;
+	V32 put;
+	U32 size;
+	U8	PoolID;
+} FASTQUEUE, *PFASTQUEUE;
+
+#define COUNT(some_array) ( sizeof(some_array)/sizeof((some_array)[0]) )
+
+void SFL_DefQueue(PFASTQUEUE pq, void *pStorage, int StorageSize);
+int	SFL_Enqueue(PFASTQUEUE pq, PVOID pData);
+int	SFL_Enqueue_NoSync(PFASTQUEUE pq, PVOID pData);
+PVOID SFL_Dequeue(PFASTQUEUE pq);
+PVOID SFL_Dequeue_NoSync(PFASTQUEUE pq);
+U32 SFL_Queue_BatchRead( PFASTQUEUE pq, U32 *pDestArr, U32 Count);
+U32 SFL_Queue_BatchWrite( PFASTQUEUE pq, U32 *pSrcArr, U32 Count);
+
+
+#endif // __SYSLIB_H__
+
+
diff --git a/arch/arm/mach-transcede/include/mach/system.h b/arch/arm/mach-transcede/include/mach/system.h
new file mode 100644
index 0000000..fdaa274
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/system.h
@@ -0,0 +1,68 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef __ASM_ARCH_SYSTEM_H
+#define __ASM_ARCH_SYSTEM_H
+
+#include <linux/io.h>
+#include <mach/hardware.h>
+
+#ifdef CONFIG_TRANSCEDE_MIPS_MONITOR
+#include <mach/mips_monitor.h>
+#endif
+
+#ifdef CONFIG_TRANSCEDE_MLOG_SEPARATE_IDLE
+extern u32	mlog_idle_start[NR_CPUS];
+#endif
+
+static inline void arch_idle(void)
+{
+	/*
+	 * This should do all the clock switching
+	 * and wait for interrupt tricks
+	 */
+#ifdef CONFIG_TRANSCEDE_MIPS_MONITOR
+{
+    int cpu = smp_processor_id();
+    mips_monitor_idle_start_update(cpu, *((volatile unsigned int*)TIMER0_CURR_COUNT));
+}
+#endif
+    
+#ifdef CONFIG_TRANSCEDE_MLOG_SEPARATE_IDLE
+	int cpu = smp_processor_id();
+	if (!mlog_idle_start[cpu]) {
+		mlog_idle_start[cpu] = *((volatile unsigned int*)TIMER0_CURR_COUNT);
+	}
+#endif
+
+	cpu_do_idle();
+}
+
+static inline void arch_reset(char mode, const char *cmd)
+{
+#if defined(CONFIG_MACH_M822XX)
+	__raw_writel(__raw_readl(AAB_XP_VADDR(PLLS_GLOBAL_CNTRL)) | 0xf, AAB_XP_VADDR(PLLS_GLOBAL_CNTRL));
+#endif
+	__raw_writel(__raw_readl(AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST)) | 1, AAB_XP_VADDR(TRANSCEDE_TOP_CLKRST));
+}
+
+#endif
diff --git a/arch/arm/mach-transcede/include/mach/tcb.h b/arch/arm/mach-transcede/include/mach/tcb.h
new file mode 100644
index 0000000..029bf57
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/tcb.h
@@ -0,0 +1,589 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifdef __cplusplus
+extern "C"
+{
+#endif
+
+#ifdef __CC_ARM     // ARM compiler
+#include "config.h"
+#endif
+
+#ifndef _TCB_H_
+#define _TCB_H_
+
+#define MAX_IOBUF_DESC      16
+#define REX_DEP_SIZE        17
+
+/*************************************************************************************************
+*           TCB::STATUS configuration and status bits                                            *
+**************************************************************************************************/
+#define TCB_CFG_STAT_ERROR_MASK     (0xFF)      // The error mask, used to get error code of operation(task processing)
+#define TCB_CFG_STAT_GET_ERROR(tcb)     ((tcb)->Status & TCB_CFG_STAT_ERROR_MASK)
+#define TCB_CFG_STAT_DIS_INT        (1<<16)     // To inform CEVA to prevent generating of interrupts when TCB is done
+#define TCB_CFG_STAT_MARK_DONE      (1<<17)     // CEVA marks this TCB(mainly used for SuperTCB) like done if TCB_CFG_STAT_DIS_INT was used
+
+
+typedef struct tIOBufDesc
+{
+    void *IOBufPtr;
+    unsigned int IOBufControl;
+} TIOBufDesc, *PTIOBufDesc;
+
+#define TCB_NEXT(tcb)        (tcb)->NextTcb
+#define TCB_CONTEXT(tcb)     (tcb)->ContextPtr
+#define TCB_CONTEXT_LEN(tcb) (tcb)->ContextLen
+#define TCB_CONTROL(tcb)     (tcb)->IOControlPtr
+#define TCB_CONTROL_LEN(tcb) (tcb)->IOControlLen
+#define TCB_INPUT(tcb)       (tcb)->InputDataPtr
+#define TCB_INPUT_LEN(tcb)   (tcb)->InputDataLen
+#define TCB_OUTPUT(tcb)      (tcb)->OutputDataPtr
+#define TCB_OUTPUT_LEN(tcb)  (tcb)->OutputDataLen
+#define TCB_TASK_ID(tcb)     (tcb)->TaskID
+#define TCB_RES_ID(tcb)      (tcb)->ResourceID
+#define TCB_STATUS(tcb)      (tcb)->Status
+#define TCB_CBDONE(tcb)      (tcb)->cbDone
+#define TCB_GROUP_ID(tcb)    (tcb)->GroupID
+#define TCB_RES_IDX(tcb)     (tcb)->ResourceIndex
+#define TCB_EXEC_TICKS(tcb)  (tcb)->ExecTicks
+#define TCB_SUP_TCB(tcb)     (tcb)->NextSupTcb
+#define TCB_HW(tcb)          (tcb)
+
+typedef struct _TCB_SUB_TASK_INFO_
+{
+    unsigned int        TaskID;
+    unsigned int        TaskMips;
+
+}TCB_SUB_TASK_INFO;
+
+#if (defined (CEVA_INTRINSICS) && !defined (EVM)) || defined (_WIN32)
+typedef struct tTCB
+{
+    //*****************<  HW BLOCK BEGIN  >*************************
+    // CACHELINE-1
+    struct tTCB *NextTcb;
+    unsigned int TaskID;
+    unsigned int ResourceID;
+    volatile unsigned int Status;
+    void (* cbDone)(struct tTCB *Tcb);
+    void *ContextPtr;
+    unsigned int ContextLen;
+    void *IOControlPtr;
+
+    // CACHELINE-2
+    unsigned int IOControlLen;
+    void *InputDataPtr;
+    unsigned int InputDataLen;
+    void *OutputDataPtr;
+    unsigned int OutputDataLen;
+    unsigned short ResourceIndex; //RIX is used to index which resource (ceva:0-9, MAP:0-9) are allocated to this tcb
+    unsigned short GroupID; //GID super Tcb group ID , FFFF is reserved,
+    void *DependencyList;
+
+    // CACHELINE-3
+    unsigned int ExecTicks;
+    struct tTCB *NextSupTcb; //THIS IS THE LAST FIELD OF TCB//pointer to linked list of tasks belonging to same superTCB group
+    //*****************<  HW BLOCK END  >*************************
+
+    //new.. below are needed for scheduler
+    unsigned int TaskPri; //task priority
+    unsigned int TaskMIPS; //task mips used in calculating a PU load factor.
+    struct tTCB *PrevTcb; //backward link for the task list
+    void *SeedList;
+    unsigned short isym; // symbol num to be used by new IFFT driver
+    unsigned short SubTaskTimer; //to be set by PHY and used by new FFT driver
+    unsigned int TaskControl; //bit field (TBD) that defines special handling for a tcb
+
+} TCB, *PTCB;
+
+#else // (defined (CEVA_INTRINSICS) && !defined (EVM)) || defined (_WIN32)
+
+typedef struct tTCB
+{
+    struct tTCB *           NextTcb;
+    unsigned int            TaskID;
+    unsigned int            ResourceID;
+    volatile unsigned int   Status;
+    void                    (*cbDone)(struct tTCB *Tcb);
+    void *                  ContextPtr;
+    unsigned int            ContextLen;
+    void *                  IOControlPtr;
+    unsigned int            IOControlLen;
+    void *                  InputDataPtr;
+    unsigned int            InputDataLen;
+    void *                  OutputDataPtr;
+    unsigned int            OutputDataLen;
+    unsigned short          ResourceIndex;  //RIX is used to index which resource (ceva:0-9, MAP:0-9) are allocated to this tcb
+    unsigned short          GroupID;        //GID super Tcb group ID , FFFF is reserved,
+    void *                  DependencyList;
+    unsigned int            ExecTicks;
+    struct tTCB *           NextSupTcb;     //THIS IS THE LAST FIELD OF TCB//pointer to linked list of tasks belonging to same superTCB group
+
+    unsigned short          isym;           // symbol num to be used by new IFFT driver
+    unsigned short          SubTaskTimer;   // to be set by PHY and used by new FFT driver
+    unsigned int            SubTaskNum;     // On input: the maximum number of elements that can be putted to the array pointed by SubTaskData
+                                            // On out:  the number of elements already putted by PHY code
+    TCB_SUB_TASK_INFO *     SubTaskinfo;    // The address of the storage where code may add sub-task information
+    void*                   RexDescr;       // The pointer to the REX-TCB descriptor
+
+}TCB, *PTCB;
+
+typedef struct _TCB_CEVA_CALL_
+{
+    unsigned int            ProcAddr;
+    unsigned int            TaskID;     //
+    unsigned int            Res1;       // ResourceID
+    volatile unsigned int   Status;     //
+
+    unsigned int            ParamNum;
+    unsigned int            Params[8];
+
+}TCBCEVACALL, *PTCBCEVACALL;
+
+#endif // (defined (CEVA_INTRINSICS) && !defined (EVM)) || defined (_WIN32)
+
+typedef struct tCEVAPROC
+{
+    void * 		 func;
+    unsigned int res[3];
+	unsigned int argnum;
+	unsigned int arg[6];
+} CEVAPROC, *PCEVAPROC;
+
+// the size of TCB block used to communicate with HW (like CEVA)
+#define HW_TCB_SIZE         (((UINT32)&((TCB*)NULL)->ExecTicks) + 8)
+
+#if (defined (CEVA_INTRINSICS) && !defined (EVM)) || defined (_WIN32)
+#define TCB_HW_SIZE         (96)
+#else  // (defined (CEVA_INTRINSICS) && !defined (EVM)) || defined (_WIN32)
+#define TCB_HW_SIZE         sizeof (TCB)
+#endif  // (defined (CEVA_INTRINSICS) && !defined (EVM)) || defined (_WIN32)
+
+#define IN_BUF              0x40000000
+#define OUT_BUF             0x80000000
+#define INOUT_BUF           0xC0000000
+#define BUF_SIZE_MASK       0x00FFFFFF
+#define IOCONTR_MASK        0xFF000000
+#define IO_MASK             0xc0000000
+
+#define MAX_CEVA_FUNCTION_PROFILE 16
+
+////////////////////////////////////////////
+// define TCB Resource TYPE
+////////////////////////////////////////////
+#define RESOURCE_RSRV       0
+#define RESOURCE_LOWERARM   1
+#define RESOURCE_LARM       1 //shorter
+#define RESOURCE_FLTP       2
+#define RESOURCE_MAP        2 //new name for FLTP
+#define RESOURCE_CEVA       3
+#define RESOURCE_FEC        4
+#define RESOURCE_HOST       5 //used for running tasks on HOST PC only
+#define RESOURCE_MDMA       6
+
+#define RESOURCE_FECUL      7
+#define RESOURCE_FECDL      8
+
+#define RESOURCE_XP_AXI     9
+#define RESOURCE_SYS_AXI    10
+#define RESOURCE_SPU_AXI    11
+#define RESOURCE_RAD_AXI    12
+
+#define RESOURCE_LARM0      1
+#define RESOURCE_LARM1      13
+#define RESOURCE_UARM0      14
+#define RESOURCE_UARM1      15
+#define RESOURCE_UARM2      16
+#define RESOURCE_UARM3      17
+
+
+
+
+
+////////////////////////////////////////////
+//define TCB Status types
+////////////////////////////////////////////
+#define STATUS_READY        0  //tcb is not scheduleed yet
+#define STATUS_QUED         3  //tcb is queued to run, should not be looked at it again
+#define STATUS_COMPLETE     1  //tcb has launched and has finished execution
+#define STATUS_RUNNING      2  //tcb has been launched but NOT done yet
+
+////////////////////////////////////////////
+//define TCB pri
+////////////////////////////////////////////
+#define TCB_DEFAULT_PRI    0x10000000   //pri goes from 0 the highest pri
+#define MAX_NUM_TCB        512         //4096
+#define MAX_NUM_RSRC       16
+
+////////////////////////////////////////////////////////////////////////////////////////
+// System Task IDs defined across CEVA code
+////////////////////////////////////////////////////////////////////////////////////////
+#define TASKID_CALL_CEVA_PROC                   0xF000
+
+////////////////////////////////////////////////////////////////////////////////////////
+// All Task IDs defined across PHY code
+////////////////////////////////////////////////////////////////////////////////////////
+
+
+////////////////////////////////////////////
+// 1. LTE BS Tx (DL) Tasks
+////////////////////////////////////////////
+#define TASKID_FEC_TX                            100
+#define TASKID_TX_DLCONTROL                      101
+#define TASKID_TX_SETUP_SYMB_BUFS                102
+#define TASKID_TX_MULTICHAN_MODULATION           103
+#define TASKID_TX_IFFT                           104
+#define TASKID_TX_PAPR_RCF                       105
+
+// Tasks for Wrapper Functions. Needed so Ceva will compile code in
+#define TASKID_TX_SETUP_SYMB_BUFS_WRAP           106
+#define TASKID_TX_SDU_PROC                       107
+#define TASKID_LTE_ENODEB_DL_START_ID            TASKID_FEC_TX
+#define TASKID_LTE_ENODEB_DL_END_ID              TASKID_TX_SDU_PROC
+
+
+////////////////////////////////////////////
+// 2. LTE BS Rx (UL) Tasks
+////////////////////////////////////////////
+//Common Task for PUSCH and PUCCH
+#define TASKID_RX_FFT                            130
+#define TASKID_RX_RUNNING_AVG                    131
+#define TASKID_RX_SNRSUBFRAME                    132
+
+//Tasks for PUSCH
+#define TASKID_RX_ULPILOT                        133
+#define TASKID_RX_CHANEST_P0                     134
+#define TASKID_RX_CHANEST_P1                     135
+#define TASKID_RX_MULTICHAN_DEMODULATION         136
+#define TASKID_RX_IDFT                           137
+#define TASKID_RX_DEMAPPER                       138
+#define TASKID_RX_FEC_MUX_ACK_CNTL               139
+#define TASKID_RX_FEC_MUX_RI_CQI_CNTL            140
+#define TASKID_FEC_RX                            141
+
+//Tasks for PUCCH
+#define TASKID_RX_ULPILOT_PUCCH                  142
+#define TASKID_RX_CHANEST_PUCCH_P0               143
+#define TASKID_RX_RUNNING_AVG_PUCCH              144
+#define TASKID_RX_CHANEST_PUCCH                  145
+#define TASKID_RX_MULTICHAN_PUCCH_DEMODULATION   146
+
+//Tasks for SRS
+#define TASKID_RX_SRS_PILOTS                     147
+#define TASKID_RX_SRS                            148
+#define TASKID_RX_SRS_SNR                        149
+
+//Tasks for PRACH
+#define TASKID_RX_PRACH                          150
+
+// Tasks for Wrapper Functions. Needed so Ceva will compile code in
+#define TASKID_RX_FEC_MUX_RI_CQI_CNTL_WRAP       151
+
+#define TASKID_RX_SDU_PRE_PROC                   152
+
+#define TASKID_LTE_ENODEB_UL_RX_START_ID         TASKID_RX_FFT
+#define TASKID_LTE_ENODEB_UL_RX_END_ID           TASKID_RX_SDU_PRE_PROC
+
+////////////////////////////////////////////
+// 3. Others Useful Tasks
+////////////////////////////////////////////
+#define TASKID_COPY_DATA                         200
+#define TASKID_COPYDATA                          201
+#define TASKID_CLEARBUFF                         202
+#define TASKID_FILEPRINT_TCB                     203
+#define TASKID_MALLOC                            204
+#define TASKID_MFREE                             205
+#define TASKID_CPY_FROM_EXT_MEM                  206
+#define TASKID_CPY_TO_EXT_MEM                    207
+#define TASKID_MAP_SETUP_DESCRIPTOR              208
+#define TASKID_UTIL_TASKS_START_ID               TASKID_COPY_DATA
+#define TASKID_UTIL_TASKS_END_ID                 TASKID_MAP_SETUP_DESCRIPTOR
+
+////////////////////////////////////////////
+// 3.1  run CEVA function
+////////////////////////////////////////////
+#define TASKID_RUN_CEVA_FUNC                     0xF000
+
+
+////////////////////////////////////////////
+// 3a. CEVA SUBTASK PROFILE IDs
+////////////////////////////////////////////
+
+////////////////////////////////////////////
+// 1. LTE BS Tx (DL) SubTasks
+////////////////////////////////////////////
+#define SUBTASKID_TX_DLCONTROL_EXTRACT                              304                                         // 0x0130
+#define SUBTASKID_TX_DLCONTROL_DMA_IN                               305
+#define SUBTASKID_TX_DLCONTROL_DMA_OUT                              306
+#define SUBTASKID_TX_DLCONTROL_SDU_ENC                              307
+#define SUBTASKID_TX_DLCONTROL_MAIN_CFI_PROC                        308
+#define SUBTASKID_TX_DLCONTROL_MAIN_PHICH_PROC                      309
+#define SUBTASKID_TX_DLCONTROL_MAIN_PDCCH_PROC                      310
+#define SUBTASKID_TX_DLCONTROL_MAIN_PILOT_PROC                      311
+#define SUBTASKID_TX_DLCONTROL_START                                SUBTASKID_TX_DLCONTROL_EXTRACT
+#define SUBTASKID_TX_DLCONTROL_END                                  SUBTASKID_TX_DLCONTROL_MAIN_PILOT_PROC
+
+
+#define SUBTASKID_TX_SETUP_SYMB_BUFS_EXTRACT                        320                                         // 0x0140
+#define SUBTASKID_TX_SETUP_SYMB_BUFS_DMA_IN                         321
+#define SUBTASKID_TX_SETUP_SYMB_BUFS_DMA_OUT                        322
+#define SUBTASKID_TX_SETUP_SYMB_BUFS_START                          SUBTASKID_TX_SETUP_SYMB_BUFS_EXTRACT
+#define SUBTASKID_TX_SETUP_SYMB_BUFS_END                            SUBTASKID_TX_SETUP_SYMB_BUFS_DMA_OUT
+
+
+#define SUBTASKID_TX_MULTICHAN_MODULATION_EXTRACT                   336                                         // 0x0150
+#define SUBTASKID_TX_MULTICHAN_MODULATION_DMA_IN                    337
+#define SUBTASKID_TX_MULTICHAN_MODULATION_MAPPER                    338
+#define SUBTASKID_TX_MULTICHAN_MODULATION_LAYERMAPPER               339
+#define SUBTASKID_TX_MULTICHAN_MODULATION_PRECODER                  340
+#define SUBTASKID_TX_MULTICHAN_MODULATION_DLPILOT                   341
+#define SUBTASKID_TX_MULTICHAN_MODULATION_MEMSET                    342
+#define SUBTASKID_TX_MULTICHAN_MODULATION_BSTXSYNCCH                343
+#define SUBTASKID_TX_MULTICHAN_MODULATION_DLRESELEMMAPPER           344
+#define SUBTASKID_TX_MULTICHAN_MODULATION_DMA_OUT                   345
+#define SUBTASKID_TX_MULTICHAN_MODULATION_START                     SUBTASKID_TX_MULTICHAN_MODULATION_EXTRACT
+#define SUBTASKID_TX_MULTICHAN_MODULATION_END                       SUBTASKID_TX_MULTICHAN_MODULATION_DMA_OUT
+
+
+
+////////////////////////////////////////////
+// 2. LTE BS Rx (UL) SubTasks
+////////////////////////////////////////////
+// Common SubTasks for PUSCH and PUCCH
+#define SUBTASKID_RX_RUNNING_AVG_EXTRACT                            352                                         // 0x0160
+#define SUBTASKID_RX_RUNNING_AVG_DMA_IN                             353
+#define SUBTASKID_RX_RUNNING_AVG_DMA_OUT                            354
+#define SUBTASKID_RX_RUNNING_AVG_START                              SUBTASKID_RX_RUNNING_AVG_EXTRACT
+#define SUBTASKID_RX_RUNNING_AVG_END                                SUBTASKID_RX_RUNNING_AVG_DMA_OUT
+
+
+#define SUBTASKID_RX_SNRSUBFRAME_EXTRACT                            368                                         // 0x0170
+#define SUBTASKID_RX_SNRSUBFRAME_DMA_IN                             369
+#define SUBTASKID_RX_SNRSUBFRAME_DMA_OUT                            370
+#define SUBTASKID_RX_SNRSUBFRAME_START                              SUBTASKID_RX_SNRSUBFRAME_EXTRACT
+#define SUBTASKID_RX_SNRSUBFRAME_END                                SUBTASKID_RX_SNRSUBFRAME_DMA_OUT
+
+
+
+// SubTasks for PUSCH
+#define SUBTASKID_RX_ULPILOT_EXTRACT                                384                                         // 0x0180
+#define SUBTASKID_RX_ULPILOT_DMA_IN                                 385
+#define SUBTASKID_RX_ULPILOT_DMA_OUT                                386
+#define SUBTASKID_RX_ULPILOT_CONFIG_PILOTS                          387
+#define SUBTASKID_RX_ULPILOT_MAIN                                   388
+#define SUBTASKID_RX_ULPILOT_DEMUX                                  389
+#define SUBTASKID_RX_ULPILOT_START                                  SUBTASKID_RX_ULPILOT_EXTRACT
+#define SUBTASKID_RX_ULPILOT_END                                    SUBTASKID_RX_ULPILOT_DEMUX
+
+
+#define SUBTASKID_RX_CHANEST_P0_EXTRACT                             400                                         // 0x0190
+#define SUBTASKID_RX_CHANEST_P0_DMA_IN                              401
+#define SUBTASKID_RX_CHANEST_P0_DMA_OUT                             402
+#define SUBTASKID_RX_CHANEST_P0_MAIN                                403
+#define SUBTASKID_RX_CHANEST_P0_START                               SUBTASKID_RX_CHANEST_P0_EXTRACT
+#define SUBTASKID_RX_CHANEST_P0_END                                 SUBTASKID_RX_CHANEST_P0_MAIN
+
+
+#define SUBTASKID_RX_CHANEST_P1_EXTRACT                             416                                         // 0x0200
+#define SUBTASKID_RX_CHANEST_P1_DMA_IN                              417
+#define SUBTASKID_RX_CHANEST_P1_DMA_OUT                             418
+#define SUBTASKID_RX_CHANEST_P1_START                               SUBTASKID_RX_CHANEST_P1_EXTRACT
+#define SUBTASKID_RX_CHANEST_P1_END                                 SUBTASKID_RX_CHANEST_P1_DMA_OUT
+
+
+#define SUBTASKID_RX_MULTICHAN_DEMODULATION_EXTRACT                 432                                         // 0x0210
+#define SUBTASKID_RX_MULTICHAN_DEMODULATION_DMA_IN                  433
+#define SUBTASKID_RX_MULTICHAN_DEMODULATION_CHANEST_P3              434
+#define SUBTASKID_RX_MULTICHAN_DEMODULATION_CHANEST_P4              435
+#define SUBTASKID_RX_MULTICHAN_DEMODULATION_EXP_EQ16                436
+#define SUBTASKID_RX_MULTICHAN_DEMODULATION_MRCOM                   437
+#define SUBTASKID_RX_MULTICHAN_DEMODULATION_FEQ                     438
+#define SUBTASKID_RX_MULTICHAN_DEMODULATION_DMA_OUT                 439
+#define SUBTASKID_RX_MULTICHAN_DEMODULATION_START                   SUBTASKID_RX_MULTICHAN_DEMODULATION_EXTRACT
+#define SUBTASKID_RX_MULTICHAN_DEMODULATION_END                     SUBTASKID_RX_MULTICHAN_DEMODULATION_DMA_OUT
+
+
+#define SUBTASKID_RX_DEMAPPER_EXTRACT                               448                                         // 0x0220
+#define SUBTASKID_RX_DEMAPPER_DMA_IN                                449
+#define SUBTASKID_RX_DEMAPPER_DMA_OUT                               450
+#define SUBTASKID_RX_DEMAPPER_MAIN                                  451
+#define SUBTASKID_RX_DEMAPPER_MUX_SOFTBITS                          452
+#define SUBTASKID_RX_DEMAPPER_START                                 SUBTASKID_RX_DEMAPPER_EXTRACT
+#define SUBTASKID_RX_DEMAPPER_END                                   SUBTASKID_RX_DEMAPPER_MUX_SOFTBITS
+
+
+#define SUBTASKID_RX_FEC_MUX_ACK_CNTL_EXTRACT                       464                                         // 0x0230
+#define SUBTASKID_RX_FEC_MUX_ACK_CNTL_DMA_IN                        465
+#define SUBTASKID_RX_FEC_MUX_ACK_CNTL_DMA_OUT                       466
+#define SUBTASKID_RX_FEC_MUX_ACK_CNTL_MAIN                          467
+#define SUBTASKID_RX_FEC_MUX_ACK_CNTL_START                         SUBTASKID_RX_FEC_MUX_ACK_CNTL_EXTRACT
+#define SUBTASKID_RX_FEC_MUX_ACK_CNTL_END                           SUBTASKID_RX_FEC_MUX_ACK_CNTL_MAIN
+
+
+#define SUBTASKID_RX_FEC_MUX_RI_CQI_CNTL_EXTRACT                    480                                         // 0x0240
+#define SUBTASKID_RX_FEC_MUX_RI_CQI_CNTL_DMA_IN                     481
+#define SUBTASKID_RX_FEC_MUX_RI_CQI_CNTL_CC_CHAN_CODING             482
+#define SUBTASKID_RX_FEC_MUX_RI_CQI_CNTL_DMA_OUT                    483
+#define SUBTASKID_RX_FEC_MUX_RI_CQI_SFCOMB                          484
+#define SUBTASKID_RX_FEC_MUX_RI_CQI_REED_MULLER_DMA_IN              485
+#define SUBTASKID_RX_FEC_MUX_RI_CQI_REED_MULLER                     486
+#define SUBTASKID_RX_FEC_MUX_RI_CQI_REED_MULLER_FHT_DMA_IN          487
+#define SUBTASKID_RX_FEC_MUX_RI_CQI_REED_MULLER_FHT                 488
+#define SUBTASKID_RX_FEC_MUX_RI_CQI_REED_RI_DEC                     489
+#define SUBTASKID_RX_FEC_MUX_RI_CQI_CNTL_START                      SUBTASKID_RX_FEC_MUX_RI_CQI_CNTL_EXTRACT
+#define SUBTASKID_RX_FEC_MUX_RI_CQI_CNTL_END                        SUBTASKID_RX_FEC_MUX_RI_CQI_REED_RI_DEC
+
+
+
+// SubTasks for PUCCH
+#define SUBTASKID_RX_ULPILOT_PUCCH_EXTRACT                          496                                         // 0x0250
+#define SUBTASKID_RX_ULPILOT_PUCCH_DMA_IN                           497
+#define SUBTASKID_RX_ULPILOT_PUCCH_DMA_OUT                          498
+#define SUBTASKID_RX_ULPILOT_PUCCH_START                            SUBTASKID_RX_ULPILOT_PUCCH_EXTRACT
+#define SUBTASKID_RX_ULPILOT_PUCCH_END                              SUBTASKID_RX_ULPILOT_PUCCH_DMA_OUT
+
+
+#define SUBTASKID_RX_CHANEST_PUCCH_EXTRACT                          512                                         // 0x0260
+#define SUBTASKID_RX_CHANEST_PUCCH_DMA_IN                           513
+#define SUBTASKID_RX_CHANEST_PUCCH_DMA_OUT                          514
+#define SUBTASKID_RX_CHANEST_PUCCH_START                            SUBTASKID_RX_CHANEST_PUCCH_EXTRACT
+#define SUBTASKID_RX_CHANEST_PUCCH_END                              SUBTASKID_RX_CHANEST_PUCCH_DMA_OUT
+
+
+#define SUBTASKID_RX_MULTICHAN_PUCCH_DEMODULATION_EXTRACT           528                                         // 0x0270
+#define SUBTASKID_RX_MULTICHAN_PUCCH_DEMODULATION_DMA_IN            529
+#define SUBTASKID_RX_MULTICHAN_PUCCH_DEMODULATION_DMA_OUT           530
+#define SUBTASKID_RX_MULTICHAN_PUCCH_DEMODULATION_START             SUBTASKID_RX_MULTICHAN_PUCCH_DEMODULATION_EXTRACT
+#define SUBTASKID_RX_MULTICHAN_PUCCH_DEMODULATION_END               SUBTASKID_RX_MULTICHAN_PUCCH_DEMODULATION_DMA_OUT
+
+
+
+// SubTasks for SRS
+#define SUBTASKID_RX_SRS_PILOTS_EXTRACT                             544                                         // 0x0280
+#define SUBTASKID_RX_SRS_PILOTS_DMA_IN                              545
+#define SUBTASKID_RX_SRS_PILOTS_DMA_OUT                             546
+#define SUBTASKID_RX_SRS_PILOTS_START                               SUBTASKID_RX_SRS_PILOTS_EXTRACT
+#define SUBTASKID_RX_SRS_PILOTS_END                                 SUBTASKID_RX_SRS_PILOTS_DMA_OUT
+
+
+#define SUBTASKID_RX_SRS_EXTRACT                                    560                                         // 0x0290
+#define SUBTASKID_RX_SRS_DMA_IN                                     561
+#define SUBTASKID_RX_SRS_DMA_OUT                                    562
+#define SUBTASKID_RX_SRS_START                                      SUBTASKID_RX_SRS_EXTRACT
+#define SUBTASKID_RX_SRS_END                                        SUBTASKID_RX_SRS_DMA_OUT
+
+
+#define SUBTASKID_RX_SRS_SNR_EXTRACT                                576                                         // 0x0300
+#define SUBTASKID_RX_SRS_SNR_DMA_IN                                 577
+#define SUBTASKID_RX_SRS_SNR_DMA_OUT                                578
+#define SUBTASKID_RX_SRS_SNR_START                                  SUBTASKID_RX_SRS_SNR_EXTRACT
+#define SUBTASKID_RX_SRS_SNR_END                                    SUBTASKID_RX_SRS_SNR_DMA_OUT
+
+
+
+// SubTasks for PRACH
+#define SUBTASKID_RX_PRACH_EXTRACT                                  592                                         // 0x0310
+#define SUBTASKID_RX_PRACH_DMA_IN                                   593
+#define SUBTASKID_RX_PRACH_DMA_OUT                                  594
+#define SUBTASKID_RX_PRACH_START                                    SUBTASKID_RX_PRACH_EXTRACT
+#define SUBTASKID_RX_PRACH_END                                      SUBTASKID_RX_PRACH_DMA_OUT
+
+#define TASKID_NMM_FIRST                                            650
+#define TASKID_NMM_DOWNSAMPLING                                     TASKID_NMM_FIRST
+#define TASKID_NMM_AGC                                              651
+#define TASKID_NMM_XCORR_LOOP                                       652
+#define TASKID_NMM_FIND_PSS_CAND                                    653
+#define TASKID_NMM_FFT64                                            654
+#define TASKID_NMM_SSYNC_CHANEST                                    655
+#define TASKID_NMM_DECODE_SSS                                       656
+#define TASKID_NMM_GET_RSRP                                         657
+#define TASKID_NMM_PSEUDO_RANDOM_SEQ                                658
+#define TASKID_NMM_EXTRACT_PBCH                                     659
+#define TASKID_NMM_DEMOD_PBCH                                       660
+#define TASKID_NMM_EXTRACT_PCFI                                     661
+#define TASKID_NMM_EXTRACT_PDCCH                                    662
+#define TASKID_NMM_PDCCH_DEMOD                                      663
+#define TASKID_NMM_EXTRACT_PDSCH                                    664
+#define TASKID_NMM_CHANNEL_EST                                      665
+#define TASKID_NMM_PSS_FOE                                          666
+#define TASKID_NMM_FOE_COMP                                         667
+#define TASKID_NMM_LAST                                             (TASKID_NMM_FIRST + 50)
+
+#define TIME_NMM_FIRST                                              5000
+#define TIME_NMM_DOWNSAMPLING                                       5000
+#define TIME_NMM_AGC                                                5000
+#define TIME_NMM_XCORR_LOOP                                         5000
+#define TIME_NMM_FIND_PSS_CAND                                      5000
+#define TIME_NMM_FFT64                                              5000
+#define TIME_NMM_SSYNC_CHANEST                                      5000
+#define TIME_NMM_DECODE_SSS                                         5000
+#define TIME_NMM_GET_RSRP                                           5000
+#define TIME_NMM_PSEUDO_RANDOM_SEQ                                  5000
+#define TIME_NMM_EXTRACT_PBCH                                       5000
+#define TIME_NMM_DEMOD_PBCH                                         5000
+#define TIME_NMM_EXTRACT_PCFI                                       5000
+#define TIME_NMM_EXTRACT_PDCCH                                      5000
+#define TIME_NMM_PDCCH_DEMOD                                        5000
+#define TIME_NMM_EXTRACT_PDSCH                                      5000
+#define TIME_NMM_CHANNEL_EST                                        5000
+#define TIME_NMM_PSS_FOE                                            5000
+#define TIME_NMM_FOE_COMP                                           5000
+#define TIME_NMM_LAST                                               (TIME_NMM_FIRST + 50)
+
+
+#define IS_NMM_TASKID(task_id) (((task_id) >= TASKID_NMM_FIRST) && ((task_id) <= TASKID_NMM_LAST))
+
+////////////////////////////////////////////
+// 1. LTE BS Tx (DL) Ceva Tasks time limit
+////////////////////////////////////////////
+#define TIME_TX_DLCONTROL                                           500
+#define TIME_TX_SETUP_SYMB_BUFS                                     400
+#define TIME_TX_MULTICHAN_MODULATION                                450
+
+////////////////////////////////////////////
+// 2. LTE BS Rx (UL) Ceva Tasks time limit
+////////////////////////////////////////////
+#define TIME_RX_RUNNING_AVG                                         200
+#define TIME_RX_SNRSUBFRAME                                         200
+#define TIME_RX_ULPILOT                                             250
+#define TIME_RX_CHANEST_P0                                          100
+#define TIME_RX_CHANEST_P1                                          200
+#define TIME_RX_MULTICHAN_DEMODULATION                              350
+#define TIME_RX_DEMAPPER                                            200
+#define TIME_RX_FEC_MUX_ACK_CNTL                                    100
+#define TIME_RX_FEC_MUX_RI_CQI_CNTL                                 1000
+#define TIME_RX_ULPILOT_PUCCH                                       300
+#define TIME_RX_CHANEST_PUCCH_P0                                    300
+#define TIME_RX_RUNNING_AVG_PUCCH                                   300
+#define TIME_RX_CHANEST_PUCCH                                       500
+#define TIME_RX_MULTICHAN_PUCCH_DEMODULATION                        300
+#define TIME_RX_SRS_PILOTS                                          300
+#define TIME_RX_SRS                                                 800
+#define TIME_RX_SRS_SNR                                             200
+#define TIME_RX_PRACH                                               2500
+
+#endif
+
+#ifdef __cplusplus
+}
+#endif
diff --git a/arch/arm/mach-transcede/include/mach/timex.h b/arch/arm/mach-transcede/include/mach/timex.h
new file mode 100644
index 0000000..3fb62e0
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/timex.h
@@ -0,0 +1,34 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef __ASM_ARCH_TIMEX_H
+#define __ASM_ARCH_TIMEX_H
+
+#include <mach/hardware.h>
+
+#ifdef CONFIG_MACH_M822XX
+# define CLOCK_TICK_RATE		(TRANSCEDE_ARMPCLK_HZ)
+#else
+# define CLOCK_TICK_RATE		(TRANSCEDE_ARMCLK_HZ / 2)
+#endif
+
+#endif
diff --git a/arch/arm/mach-transcede/include/mach/transcede-2200.h b/arch/arm/mach-transcede/include/mach/transcede-2200.h
new file mode 100644
index 0000000..c1b07f5
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/transcede-2200.h
@@ -0,0 +1,949 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef __ASM_ARCH_TRANSCEDE_2200_H
+#define __ASM_ARCH_TRANSCEDE_2200_H
+
+/*
+  * Ceva Group Slaves - CevaXC, INTC, Timer, Semaphore Blocks
+ */
+
+#define CEVA0_SLAVE                     0xF0000000
+#define CEVA1_SLAVE                     0xF0100000
+#define CEVA_SLAVE(n)                   (CEVA0_SLAVE+(n)*0x00100000)
+
+#define CEVA_RESET                      0xF0D00000
+#define CEVA_GRP_CLK_RST_REG            0xF0D00004
+#define CEVA_BOOT_ADDR                  0xF0D00010
+#define CEVA_JTAG_CTRL_REG              0xF0D00014
+
+#define SET_INT0_REG                    0xF0D20000
+#define CLEAR_INT0_REG                  0xF0D20004
+#define AUTO_CLEAR_INT0                 0xF0D20008
+#define ARM_IRQ_EN                      0xF0D2000C
+
+#define SET_INT1_REG                    0xF0D20010
+#define CLEAR_INT1_REG                  0xF0D20014
+#define AUTO_CLEAR_INT1                 0xF0D20018
+#define MESSAGE_IRQ_EN                  0xF0D2001C
+
+#define SET_INT2_REG                    0xF0D20020
+#define CLEAR_INT2_REG                  0xF0D20024
+#define AUTO_CLEAR_INT2                 0xF0D20028
+#define SNOOP_TIMER_IRQ_EN              0xF0D2002C
+
+#define SET_NMI_REG                     0xF0D20030
+#define CLEAR_NMI_REG                   0xF0D20034
+#define AUTO_CLEAR_NMI                  0xF0D20038
+#define VIOLATION_IRQ_EN                0xF0D2003C
+
+#define SET_BP1_REG                     0xF0D20040
+#define BP1_STATUS_REG                  0xF0D20040
+#define CLEAR_BP1_REG                   0xF0D20044
+#define AUTO_CLEAR_BP1                  0xF0D20048
+#define EXT_BP1_IRQ_EN                  0xF0D2004C
+
+#define SET_BP2_REG                     0xF0D20050
+#define BP2_STATUS_REG                  0xF0D20050
+#define CLEAR_BP2_REG                   0xF0D20054
+#define AUTO_CLEAR_BP2                  0xF0D20058
+#define EXT_BP2_IRQ_EN                  0xF0D2005C
+
+#define CEVA_VINT_BASE                  SET_INT0_REG+0x100
+
+#define VINT_SET_STAT(CEVA_ID)          (CEVA_VINT_BASE+(CEVA_ID)*0x100+0x0)
+#define VINT_CLEAR_EXT_STAT(CEVA_ID)    (CEVA_VINT_BASE+(CEVA_ID)*0x100+0x4)
+#define VINT_ENABLE(CEVA_ID)            (CEVA_VINT_BASE+(CEVA_ID)*0x100+0x8)
+#define VINT_RR_ARBITER(CEVA_ID)        (CEVA_VINT_BASE+(CEVA_ID)*0x100+0xC)
+
+#define VINT_IRQ0_VECTOR(CEVA_ID)       (CEVA_VINT_BASE+(CEVA_ID)*0x100+0x80)
+#define VINT_IRQ1_VECTOR(CEVA_ID)       (CEVA_VINT_BASE+(CEVA_ID)*0x100+0x84)
+#define VINT_IRQ2_VECTOR(CEVA_ID)       (CEVA_VINT_BASE+(CEVA_ID)*0x100+0x88)
+#define VINT_IRQ3_VECTOR(CEVA_ID)       (CEVA_VINT_BASE+(CEVA_ID)*0x100+0x8C)
+#define VINT_IRQ4_VECTOR(CEVA_ID)       (CEVA_VINT_BASE+(CEVA_ID)*0x100+0x90)
+#define VINT_IRQ5_VECTOR(CEVA_ID)       (CEVA_VINT_BASE+(CEVA_ID)*0x100+0x94)
+#define VINT_IRQ6_VECTOR(CEVA_ID)       (CEVA_VINT_BASE+(CEVA_ID)*0x100+0x98)
+#define VINT_IRQ7_VECTOR(CEVA_ID)       (CEVA_VINT_BASE+(CEVA_ID)*0x100+0x9C)
+
+#define TIMER_CEVA                      0xF0D30000
+#define BPROC_BASEADDR                  0xF0D80000
+
+#define SEMACEVA_BASEADDR               0xF0F00000 /**< CEVA Hardware semaphores base address */
+#define SEMACEVACTRL(semaid)            (SEMACEVA_BASEADDR+semaid*4) /**< Macro to get address based on semaphore ID */
+#define SEMACEVANUM                     32
+
+/*
+ * Memory layout
+ */
+#define TRANSCEDE_SDRAM_BASE	0x00000000	/* SDRAM (3G max) */
+#define TRANSCEDE_SDRAM_SIZE	0xC0000000
+#define TRANSCEDE_DDR_BASE  (0x00000000) /* DDR0 */
+#define TRANSCEDE_DDR_SIZE  (0x80000000)
+#define TRANSCEDE_DDR_END   (TRANSCEDE_DDR_BASE + TRANSCEDE_DDR_SIZE - 1)
+#define TRANSCEDE_ICC_VIRT	(transcede_icc_virt)
+#define TRANSCEDE_ICC_AMP_VIRT	(transcede_icc_amp_virt)
+#define TRANSCEDE_ICC_MAX_SIZE	0x20000000
+#define TRANSCEDE_ICC_SIZE	(transcede_icc_heap_size \
+	 + transcede_icc_local_part_size \
+	 + transcede_icc_remote_part_size)
+#define TRANSCEDE_ICC_LOCAL_PART_DEF_SIZE (SZ_128M)
+#define TRANSCEDE_ICC_REMOTE_PART_DEF_SIZE (TRANSCEDE_ICC_LOCAL_PART_DEF_SIZE)
+#define TRANSCEDE_ICC_LOCAL_PART_SIZE (transcede_icc_local_part_size)
+#define TRANSCEDE_ICC_REMOTE_PART_SIZE (transcede_icc_remote_part_size)
+#define TRANSCEDE_JRAM_BASE	0xF5000000	/* JRAM 32K  	*/
+#define TRANSCEDE_JRAM_SIZE	0x00008000
+#define TRANSCEDE_IRAM_BASE	0xF4000000	/* IRAM 256K  	*/
+#define TRANSCEDE_IRAM_SIZE	0x00040000
+#define TRANSCEDE_CRAM_BASE	0xF3000000	/* CRAM 768K 	*/
+#define TRANSCEDE_CRAM_SIZE	0x000C0000
+
+#define TRANSCEDE_PUBLIC_VIRT (TRANSCEDE_ICC_VIRT)
+#define TRANSCEDE_PUBLIC_SIZE (TRANSCEDE_ICC_SIZE)
+
+#define TRANSCEDE_AAB_XP_BASE	0xFE000000
+#define TRANSCEDE_AAB_XP_SIZE	0x01000000
+#define TRANSCEDE_HIMEM_BASE	0xFFFF0000	/* High vectors */
+#define TRANSCEDE_HIMEM_SIZE	0x00010000
+
+/*
+ * Heaps in CRAM, IRAM, DDR
+ */
+#ifndef __ASSEMBLY__
+
+#ifdef CONFIG_GLOBAL_POLLING
+#include "periodic_task.h"
+#endif	/* CONFIG_GLOBAL_POLLING */
+
+extern unsigned long transcede_cram_heap_offset;
+extern unsigned long transcede_ddr_common_base;
+extern unsigned long transcede_ddr_heap_base;
+extern unsigned long transcede_ddr_heap_size;
+extern unsigned long transcede_ddrcb_heap_base;
+extern unsigned long transcede_ddrcb_heap_size;
+extern unsigned long transcede_ddr_ceva_shared_size;
+extern unsigned long transcede_icx_base;
+extern unsigned long transcede_icc_amp_base;
+#ifdef CONFIG_AMP_STACK
+extern unsigned long transcede_amp_base;
+#endif	/* CONFIG_AMP_STACK */
+extern unsigned long transcede_icc_virt;
+extern unsigned long transcede_icc_amp_virt;
+extern unsigned long transcede_icc_amp_heap_size;
+extern unsigned long transcede_icc_heap_base;
+extern unsigned long transcede_icc_heap_size;
+extern unsigned long transcede_icc_local_part_size;
+extern unsigned long transcede_icc_remote_part_size;
+extern unsigned long transcede_reserved_size;
+#endif	/* __ASSEMBLY__ */
+
+#define TRANSCEDE_DDR_CEVA_SHARED_BASE (TRANSCEDE_SDRAM_BASE + 0x03000000)
+#define TRANSCEDE_DDR_CEVA_SHARED_DEF_SIZE (SZ_4M)
+#define TRANSCEDE_DDR_CEVA_SHARED_MAX_SIZE (SZ_128M)
+#define TRANSCEDE_DDR_CEVA_SHARED_SIZE (transcede_ddr_ceva_shared_size)
+
+#define TRANSCEDE_CRAMHEAP_DEF_OFFSET 	0x40000
+#define TRANSCEDE_CRAMHEAP_OFFSET 	transcede_cram_heap_offset
+#define TRANSCEDE_CRAMHEAP 		(TRANSCEDE_CRAM_BASE + TRANSCEDE_CRAMHEAP_OFFSET)
+#define TRANSCEDE_CRAMHEAP_SIZE		(TRANSCEDE_CRAM_SIZE - TRANSCEDE_CRAMHEAP_OFFSET)
+#define TRANSCEDE_CRAMHEAP_ALIGN	128
+
+
+#define TRANSCEDE_ETH_DESC_SIZE		0x6000	/* 2k RX and 1k TX descriptors */
+#define TRANSCEDE_RESERVED_SEC_SIZE		SZ_32K	/* reserved for custom usage in Secure World */
+#define TRANSCEDE_RESERVED_NORM_SIZE	SZ_128K	/* reserved for custom usage in Normal World */
+#define TRANSCEDE_RESERVED_SIZE		(transcede_reserved_size)
+#define TRANSCEDE_RESERVED_BASE		(TRANSCEDE_IRAM_BASE + TRANSCEDE_IRAM_SIZE - TRANSCEDE_RESERVED_SIZE)
+#define TRANSCEDE_IRAM_ETH_DESC_BASE	(TRANSCEDE_IRAM_BASE + TRANSCEDE_IRAM_SIZE - TRANSCEDE_RESERVED_SIZE - TRANSCEDE_ETH_DESC_SIZE)
+#define TRANSCEDE_IRAMHEAP_OFFSET 	0x100
+#define TRANSCEDE_IRAMHEAP 		(TRANSCEDE_IRAM_BASE + TRANSCEDE_IRAMHEAP_OFFSET)
+#define TRANSCEDE_IRAMHEAP_SIZE		(TRANSCEDE_IRAM_SIZE - TRANSCEDE_IRAMHEAP_OFFSET - TRANSCEDE_ETH_DESC_SIZE - TRANSCEDE_RESERVED_SIZE)
+#define TRANSCEDE_IRAMHEAP_ALIGN	64
+
+#define TRANSCEDE_DDRHEAP_DEF_SIZE	(SZ_256M)
+#define TRANSCEDE_DDRHEAP_OFFSET 	0
+#define TRANSCEDE_DDRHEAP_BASE 		(transcede_ddr_heap_base + TRANSCEDE_DDRHEAP_OFFSET)
+#define TRANSCEDE_DDRHEAP_SIZE		(transcede_ddr_heap_size)
+#define TRANSCEDE_DDRHEAP_ALIGN		256
+
+#define TRANSCEDE_DDRCBHEAP_DEF_SIZE	(SZ_32M)
+#define TRANSCEDE_DDRCBHEAP_OFFSET 	0
+#define TRANSCEDE_DDRCBHEAP_BASE 		(transcede_ddrcb_heap_base + TRANSCEDE_DDRCBHEAP_OFFSET)
+#define TRANSCEDE_DDRCBHEAP_SIZE		(transcede_ddrcb_heap_size)
+#define TRANSCEDE_DDRCBHEAP_ALIGN		128
+
+#define TRANSCEDE_ICX_LL_APP_NUM		4
+#define TRANSCEDE_ICX_LL_PART_BLK_SIZE		SZ_2K
+#define TRANSCEDE_ICX_LL_PART_BLK_NUM		SZ_1K
+#define TRANSCEDE_ICX_OFFSET		0
+#define TRANSCEDE_ICX_BASE		(transcede_icx_base + TRANSCEDE_ICX_OFFSET)
+#define TRANSCEDE_ICX_SIZE		(TRANSCEDE_ICX_LL_APP_NUM * TRANSCEDE_ICX_LL_PART_BLK_SIZE * TRANSCEDE_ICX_LL_PART_BLK_NUM)
+#define TRANSCEDE_ICX_ALIGN		128
+
+#define TRANSCEDE_ICC_AMP_DEF_SIZE	(SZ_16M)
+#define TRANSCEDE_ICC_AMP_SIZE	(transcede_icc_amp_heap_size)
+#define TRANSCEDE_ICC_AMP_BASE	(transcede_icc_amp_base)
+#define TRANSCEDE_ICC_AMP_ALIGN		128
+
+#ifdef CONFIG_AMP_STACK
+#define TRANSCEDE_AMP_DEF_SIZE	(SZ_2M)
+#define TRANSCEDE_AMP_SIZE	(TRANSCEDE_AMP_DEF_SIZE)
+#define TRANSCEDE_AMP_BASE	(transcede_amp_base)
+#define TRANSCEDE_AMP_ALIGN		SZ_8K
+#endif	/* CONFIG_AMP_STACK */
+
+#define TRANSCEDE_ICCHEAP_DEF_SIZE	(SZ_16M)
+#define TRANSCEDE_ICCHEAP_OFFSET	0
+#define TRANSCEDE_ICCHEAP_BASE		(transcede_icc_heap_base + TRANSCEDE_ICCHEAP_OFFSET)
+#define TRANSCEDE_ICC_BASE		TRANSCEDE_ICCHEAP_BASE
+#define TRANSCEDE_ICCHEAP_SIZE		(transcede_icc_heap_size)
+#define TRANSCEDE_ICCHEAP_ALIGN		128
+
+/*
+ * SCU registers
+ */
+#define TRANSCEDE_SCU_BASE		0xFFF00000
+#define TRANSCEDE_SNSAC_BASE		(TRANSCEDE_SCU_BASE + 0x054)
+#define TRANSCEDE_GIC_CPU_BASE		(TRANSCEDE_SCU_BASE + 0x100)
+#define TRANSCEDE_GLOBAL_TIMER_BASE	(TRANSCEDE_SCU_BASE + 0x200)
+#define TRANSCEDE_TWD_BASE		(TRANSCEDE_SCU_BASE + 0x600)
+#define TRANSCEDE_TWD_PERCPU_BASE	(TRANSCEDE_SCU_BASE + 0x700)
+#define TRANSCEDE_TWD_SIZE		0x00000100
+#define TRANSCEDE_GIC_DIST_BASE		(TRANSCEDE_SCU_BASE + 0x1000)
+#define TRANSCEDE_L310_BASE		(TRANSCEDE_SCU_BASE + 0x10000)
+
+/*
+ * Global timer
+ */
+#define TRANSCEDE_LOW_TIMER_COUNTER	(TRANSCEDE_GLOBAL_TIMER_BASE + 0x00)
+#define TRANSCEDE_UP_TIMER_COUNTER	(TRANSCEDE_GLOBAL_TIMER_BASE + 0x04)
+#define TRANSCEDE_TIMER_CTRL		(TRANSCEDE_GLOBAL_TIMER_BASE + 0x08)
+#define TRANSCEDE_TIMER_STATUS		(TRANSCEDE_GLOBAL_TIMER_BASE + 0x0C)
+#define TRANSCEDE_LOW_COMPARATOR	(TRANSCEDE_GLOBAL_TIMER_BASE + 0x10)
+#define TRANSCEDE_UP_COMPARATOR		(TRANSCEDE_GLOBAL_TIMER_BASE + 0x14)
+#define TRANSCEDE_AUTO_INCREMENT	(TRANSCEDE_GLOBAL_TIMER_BASE + 0x18)
+
+#define GLOBAL_TIMER_AUTO_INC_ENA   (0x08)
+#define GLOBAL_TIMER_IRQ_ENA        (0x04)
+#define GLOBAL_TIMER_COMPARATOR_ENA (0x02)
+#define GLOBAL_TIMER_ENA            (0x01)
+
+/*
+ * Semaphores
+ */
+#define TRANSCEDE_SEMAARM_BASEADDR	0xF4F00000					/**< Internal Hardware semaphores base address */
+#define TRANSCEDE_SEMAARMCTRL(semaid)	(TRANSCEDE_SEMAARM_BASEADDR + semaid * 4)	/**< Macro to get address based on semaphore ID */
+#define TRANSCEDE_SEMAARMNUM		32
+
+#define TRANSCEDE_SYSLOCKID_IRAMHEAP          0
+#define TRANSCEDE_SYSLOCKID_CRAMHEAP          1
+#define TRANSCEDE_SYSLOCKID_DDRL1HEAP         2
+#define TRANSCEDE_SYSLOCKID_DDRL2HEAP         3
+#define TRANSCEDE_SYSLOCKID_TXUART            4
+#define TRANSCEDE_SYSLOCKID_ICCHEAP           5
+#define TRANSCEDE_SYSLOCKID_DSP0PRINT         6
+#define TRANSCEDE_SYSLOCKID_DSP1PRINT         7
+#define TRANSCEDE_SYSLOCKID_SHM_PHY2MAC       8
+#define TRANSCEDE_SYSLOCKID_GPIO              9
+#define TRANSCEDE_SYSLOCKID_SPI0             10
+#define TRANSCEDE_SYSLOCKID_SPI1             11
+#define TRANSCEDE_SYSLOCKID_SPI(x)           (TRANSCEDE_SYSLOCKID_SPI0 + (x))
+#define TRANSCEDE_SYSLOCKID_AMPHEAP0         12
+#define TRANSCEDE_SYSLOCKID_AMPHEAP1         13
+#define TRANSCEDE_SYSLOCKID_AMPHEAP(x)       (TRANSCEDE_SYSLOCKID_AMPHEAP0 + (x))
+
+
+#define TRANSCEDE_SYSLOCK_SPI_TIMEOUT		10000000
+
+/*
+ * Expansion Cluster Blocks
+ */
+#define TRANSCEDE_TDM		(TRANSCEDE_AAB_XP_BASE + 0x00000000)
+#define TRANSCEDE_TSB		(TRANSCEDE_AAB_XP_BASE + 0x00008000)
+#define TRANSCEDE_HI		(TRANSCEDE_AAB_XP_BASE + 0x00010000)
+#define TRANSCEDE_TDMA		(TRANSCEDE_AAB_XP_BASE + 0x00020000)
+#define TRANSCEDE_TIMER		(TRANSCEDE_AAB_XP_BASE + 0x00050000)
+#define TRANSCEDE_GPIO		(TRANSCEDE_AAB_XP_BASE + 0x00070000)
+#define TRANSCEDE_UART1		(TRANSCEDE_AAB_XP_BASE + 0x00090000)
+#define TRANSCEDE_UART0		(TRANSCEDE_AAB_XP_BASE + 0x00800000)
+#define TRANSCEDE_SPI0		(TRANSCEDE_AAB_XP_BASE + 0x00804000)
+#define TRANSCEDE_SPI1		(TRANSCEDE_AAB_XP_BASE + 0x00098000)
+#define TRANSCEDE_SPI		TRANSCEDE_SPI1
+#define TRANSCEDE_I2C		(TRANSCEDE_AAB_XP_BASE + 0x0009C000)
+#define TRANSCEDE_MBIST		(TRANSCEDE_AAB_XP_BASE + 0x000B0000)
+#define TRANSCEDE_USIM		(TRANSCEDE_AAB_XP_BASE + 0x000E0000)
+
+#define TRANSCEDE_EXP_BUS	(TRANSCEDE_AAB_XP_BASE + 0x00100000)
+#define TRANSCEDE_GEM1		(TRANSCEDE_AAB_XP_BASE + 0x00190000)	// TODO: confirm
+#define TRANSCEDE_USB		(TRANSCEDE_AAB_XP_BASE + 0x00820000)	// TODO: confirm
+
+/*
+ * Timer control registers
+*/
+#define TIMER0_CNTR_REG                 (TRANSCEDE_TIMER + 0x00)
+#define TIMER0_CURR_COUNT               (TRANSCEDE_TIMER + 0x04)
+#define TIMER1_CNTR_REG                 (TRANSCEDE_TIMER + 0x08)
+#define TIMER1_CURR_COUNT               (TRANSCEDE_TIMER + 0x0C)
+#define TIMER2_LBOUND_REG               (TRANSCEDE_TIMER + 0x10)
+#define TIMER2_HBOUND_REG               (TRANSCEDE_TIMER + 0x14)
+#define TIMER2_CNTR_REG                 (TRANSCEDE_TIMER + 0x18)
+#define TIMER2_CURR_COUNT               (TRANSCEDE_TIMER + 0x1C)
+#define TIMER3_LBOUND_REG               (TRANSCEDE_TIMER + 0x20)
+#define TIMER3_HBOUND_REG               (TRANSCEDE_TIMER + 0x24)
+#define TIMER3_CNTR_REG                 (TRANSCEDE_TIMER + 0x28)
+#define TIMER3_CURR_COUNT               (TRANSCEDE_TIMER + 0x2C)
+
+#define get_tick() (*(volatile unsigned long *)(TIMER0_CURR_COUNT))
+
+/*
+ * SPU Partition 0xF3xxxxxx - FFT_TOP, CRP, DSP ROM Blocks
+ */
+#define SPU_CONFIG_BASEADDR             0xF3D00000  /**< SPU Configuration base address */
+
+#define SYNCNET_BASEADDR                0xF3D10000 /**SYNCNET SPU base address */
+
+#define MAP_BASEADDR                    0xF3D40000  /**< MAP (FFT) base address*/
+
+#define MDMACRP0_BASEADDR               0xF3D80000  /**< CRP 8 Channel MDMA 0*/
+#define MDMACRP1_BASEADDR               0xF3D84000  /**< CRP 8 Channel MDMA 1*/
+#define MDMACRP_BASEADDR(id)            (MDMACRP0_BASEADDR + (id)*0x4000)
+
+/* TODO: VIVA cleanup with one defined in syscfg.h */
+/*
+ * System blocks
+ */
+#define SYS_CFG_BASEADDR	0xF4C00000
+#define SYS_SEC_CFG_BASEADDR	0xF4C08000
+#define SYS_SEC_CFG_OFFSET	(SYS_SEC_CFG_BASEADDR - SYS_CFG_BASEADDR)
+
+#define SYS_SEC_CFG_BCR         (SYS_SEC_CFG_BASEADDR + 0x00)   /**< Strap Pin And Boot Control Register */
+#define SYS_SEC_CFG_GPR         (SYS_SEC_CFG_BASEADDR + 0x04)   /**< General Purpose Register */
+#define SYS_SEC_CFG_SBSCR       (SYS_SEC_CFG_BASEADDR + 0x10)   /**< Slave Block Secure Mode Enable Register */
+#define SYS_SEC_CFG_MBSCR       (SYS_SEC_CFG_BASEADDR + 0x14)   /**< Master Block Secure Mode Enable Register */
+#define SYS_SEC_CFG_IRAM_SMWR   (SYS_SEC_CFG_BASEADDR + 0x20)   /**< IRAM Secure Mode Window Register */
+#define SYS_SEC_CFG_EXP_SMWR    (SYS_SEC_CFG_BASEADDR + 0x24)   /**< Expansion Bus Secure Mode Window Register */
+#define SYS_SEC_CFG_TM0_CTL_REG (SYS_SEC_CFG_BASEADDR + 0x40)   /**< Secure Timer 0 Control Register */
+#define SYS_SEC_CFG_TM0_CNT_REG (SYS_SEC_CFG_BASEADDR + 0x44)   /**< Secure Timer 0 Count Register */
+
+#define SYS_CFG_CA9_CLK_RST_REG (SYS_CFG_BASEADDR + 0x00)       /**< Dual ARM Clock Reset Register */
+#define SYS_CFG_DDR_WND         (SYS_CFG_BASEADDR + 0x10)       /**< DDR Address Space Window */
+#define SYS_CFG_CEVAGRP_ACP_WND (SYS_CFG_BASEADDR + 0x30)       /**< ARM ACP Snoop Window For Ceva GP */
+#define SYS_CFG_GEMBM_ACP_WND   (SYS_CFG_BASEADDR + 0x40)       /**< ARM ACP Snoop Window For GEM BM */
+#define SYS_CFG_PCIEBM_ACP_WND  (SYS_CFG_BASEADDR + 0x44)       /**< ARM ACP Snoop Window For PCIe BM */
+#define SYS_CFG_BM_ARB_PRI_REG1 (SYS_CFG_BASEADDR + 0x80)       /**< SYS BM Arbitration Priority 1 */
+#define SYS_CFG_BM_ARB_PRI_REG2 (SYS_CFG_BASEADDR + 0x84)       /**< SYS BM Arbitration Priority 2 */
+#define SYS_CFG_BM_ARB_PRI_REG3 (SYS_CFG_BASEADDR + 0x88)       /**< SYS BM Arbitration Priority 3 */
+#define SYS_CFG_BM_BUS_SEL      (SYS_CFG_BASEADDR + 0x98)       /**< SYS BM Bus Select */
+#define SYS_CFG_TRIG_SRC_ENA    (SYS_CFG_BASEADDR + 0x100)      /**< Cross Trigger Source Enable Register */
+#define SYS_CFG_TRIG_RCV_ENA    (SYS_CFG_BASEADDR + 0x104)      /**< Cross Trigger Source Enable Register */
+#define SYS_CFG_PCIE_C2C_INT    (SYS_CFG_BASEADDR + 0x110)      /**< PCIe Chip to Chip Interrupt Register */
+#define SYS_CFG_ARM_REQ_SET_STAT (SYS_CFG_BASEADDR + 0x120)      /**< ARM Request Set/Status Register */
+#define SYS_CFG_ARM_REQ_CLEAR   (SYS_CFG_BASEADDR + 0x124)      /**< ARM Request Clear Register */
+#define SYS_CFG_ARM_IRQ_ACK_SET (SYS_CFG_BASEADDR + 0x128)      /**< IRQ_ARM_ACK Set Register */
+#define SYS_CFG_RAM_MARGIN_REG1 (SYS_CFG_BASEADDR + 0x130)      /**< RAM Memory Margin Register 1 */
+#define SYS_CFG_RAM_MARGIN_REG2 (SYS_CFG_BASEADDR + 0x134)      /**< RAM Memory Margin Register 2 */
+#define SYS_CFG_RAM_MARGIN_REG3 (SYS_CFG_BASEADDR + 0x138)      /**< RAM Memory Margin Register 3 */
+#define SYS_CFG_RAM_MARGIN_REG4 (SYS_CFG_BASEADDR + 0x140)      /**< RAM Memory Margin Register 4 */
+#define SYS_CFG_GP_REG0         (SYS_CFG_BASEADDR + 0x144)      /**< General Purpose Register 0 */
+#define SYS_CFG_GP_REG1         (SYS_CFG_BASEADDR + 0x148)      /**< General Purpose Register 1 */
+#define SYS_CFG_GP_REG2         (SYS_CFG_BASEADDR + 0x14C)      /**< General Purpose Register 2 */
+
+/*! @brief Internal DDR3 DRAM controller base address */
+#define DDR0_CONTROLLER_BASE    (0xF4C20000)
+
+/** some defines in order not to rewrite higher level soft **/
+#define TRANSCEDE_ARM_IRQ_SET		AAB_XP_VADDR(SYS_SEC_CFG_OFFSET + SYS_CFG_ARM_REQ_SET_STAT)
+#define TRANSCEDE_ARM_IRQ_CLR		AAB_XP_VADDR(SYS_SEC_CFG_OFFSET + SYS_CFG_ARM_REQ_CLEAR)
+#define TRANSCEDE_SYS_A9DP_RESET_0	0			/* fake define for armldr, which is used for t4k as well */
+#define TRANSCEDE_SYS_A9QP_RESET_0	0			/* fake define for armldr, which is used for t4k as well */
+
+#define MDMASYS0_BASEADDR		0xF4C80000
+#define MDMASYS1_BASEADDR		0xF4C90000
+#define MDMASYS_BASEADDR(id)		(MDMASYS0_BASEADDR + (id)*0x10000)
+
+/*
+ * Top Level Clock and Reset Control Registers
+ */
+#define DEVICE_RST_CNTRL                0xF4CF0000  /**< Device Reset Control Register */
+#define TRANSCEDE_TOP_CLKRST		DEVICE_RST_CNTRL
+#define SERDES_RST_CNTRL                0xF4CF0004  /**< Serdes Reset Control Register */
+#define PCIe_RST_CNTRL                  0xF4CF0008  /**< PCIe Reset Control Register */
+#define USB_RST_CNTRL                   0xF4CF000C  /**< USB Reset Control Register */
+#define CA9_MC_PWR_STAT                 0xF4CF0028  /**< ARM9 DP Power Status Register */
+#define CA9_MC_PWR_CNTRL                0xF4CF002C  /**< ARM9 DP Power Control Register */
+#define PLLS_GLOBAL_CNTRL               0xF4CF0038  /**< PLLs Global Control Register */
+#define AXI_CLK_CNTRL_0                 0xF4CF0040  /**< AXI Clock Control Register 0 */
+#define AXI_CLK_CNTRL_1                 0xF4CF0044  /**< AXI Clock Control Register 1 */
+#define AXI_CLK_CNTRL_2                 0xF4CF0048  /**<AXI Clock Control Register 2 */
+#define AXI_CLK_DIV_CNTRL               0xF4CF004C  /**< AXI Clock Divider Control Register */
+#define AXI_RESET_0                     0xF4CF0050  /**< AXI Reset Control Register 0 */
+#define AXI_RESET_1                     0xF4CF0054  /**< AXI Reset Control Register 1 */
+#define AXI_RESET_2                     0xF4CF0058  /**< AXI Reset Control Register 2 */
+#define CA9_MC_MPU_CLK_CNTRL            0xF4CF0068  /**< CA9_MC MP Units Clock Control Register */
+#define CA9_MC_MPU_CLK_DIV_CNTRL        0xF4CF006C  /**< CA9_MC MP Units Clock Divider Control Register */
+#define CA9_MC_MPU_RESET                0xF4CF0070  /**< CA9_MC MP Units Reset Register */
+#define CA9_MC_CPU_CLK_CNTRL            0xF4CF0074  /**< CA9_MC CPU Clock Control Register */
+#define CA9_MC_CPU_RESET                0xF4CF0078  /**< CA9_MC CPU Reset Register */
+#define CA9_MC_CLK_CNTRL                0xF4CF0080  /**< CA9_MC Clock Control Register */
+#define CA9_MC_CLK_DIV_CNTRL            0xF4CF0084  /**< CA9_MC Clock Divider Control Register */
+#define CA9_MC_RESET                    0xF4CF0088  /**< CA9_MC Reset Register */
+
+#define L2CC_CLK_CNTRL                  0xF4CF0090  /**< L2CC Clock Control Register */
+#define L2CC_CLK_DIV_CNTRL              0xF4CF0094  /**< L2CC Clock Divider Control Register */
+#define L2CC_RESET                      0xF4CF0098  /**< L2CC Clock Domain Reset Register */
+
+#define TPI_CLK_CNTRL                   0xF4CF00A0  /**< TPI Clock Control Register */
+#define TPI_CLK_DIV_CNTRL               0xF4CF00A4  /**< TPI Clock Divider Control Register */
+#define TPI_RESET                       0xF4CF00A8  /**< TPI Reset Register */
+
+#define CSYS_CLK_CNTRL                  0xF4CF00B0  /**< CORESIGHT Clock Control Register */
+#define CSYS_CLK_DIV_CNTRL              0xF4CF00B4  /**< CORESIGHT Clock Divider Control Register */
+#define CSYS_RESET                      0xF4CF00B8  /**< CORESIGHT Reset Register */
+
+#define EXTPHY0_CLK_CNTRL               0xF4CF00C0  /**< External PHY0 Clock Control Register */
+#define EXTPHY0_CLK_DIV_CNTRL           0xF4CF00C4  /**< External PHY0 Clock Divider Control Register */
+#define EXTPHY0_RESET                   0xF4CF00C8  /**< External PHY0 Reset Register */
+
+#define EXTPHY1_CLK_CNTRL               0xF4CF00D0  /**< External PHY1 Clock Control Register */
+#define EXTPHY1_CLK_DIV_CNTRL           0xF4CF00D4  /**< External PHY1 Clock Divider Control Register */
+#define EXTPHY1_RESET                   0xF4CF00D8  /**< External PHY1 Reset Register */
+
+#define FEC_UL_CLK_CNTRL                0xF4CF00E0  /**< FEC Uplink Clock Control Register */
+#define FEC_UL_CLK_DIV_CNTRL            0xF4CF00E4  /**< FEC Uplink Clock Divider Control Register */
+#define FEC_UL_RESET                    0xF4CF00E8  /**< FEC Uplink Reset Register */
+
+#define FEC_DL_CLK_CNTRL                0xF4CF00F0  /**< FEC Downlink Clock Control Register */
+#define FEC_DL_CLK_DIV_CNTRL            0xF4CF00F4  /**< FEC Downlink Clock Divider Control Register */
+#define FEC_DL_RESET                    0xF4CF00F8  /**< FEC Downlink Reset Register */
+
+#define FFT_CLK_CNTRL                   0xF4CF0100  /**< FFT Top Clock Control Register */
+#define FFT_CLK_DIV_CNTRL               0xF4CF0104  /**< FFT Top Clock Divider Control Register */
+#define FFT_RESET                       0xF4CF0108  /**< FFT Top Reset Register */
+
+#define IPSEC_CLK_CNTRL                 0xF4CF0110  /**< IPSEC Clock Control Register */
+#define IPSEC_CLK_DIV_CNTRL             0xF4CF0114  /**< IPSEC Clock Divider Control Register */
+#define IPSEC_RESET                     0xF4CF0118  /**< IPSEC Reset Register */
+
+#define DDR3_CLK_CNTRL                  0xF4CF0120  /**< DDR3 PHY Clock Control Register */
+#define DDR3_CLK_DIV_CNTRL              0xF4CF0124  /**< DDR3 PHY Clock Divider Control Register */
+#define DDR3_RESET                      0xF4CF0128  /**< DDR3 PHY Reset Register */
+
+#define GEMTX_CLK_CNTRL                 0xF4CF0130  /**< GEM TX Clock Control Register */
+#define GEMTX_CLK_DIV_CNTRL             0xF4CF0134  /**< GEM TX Clock Divider Control Register */
+#define GEMTX_RESET                     0xF4CF0138  /**< GEM TX Reset Register */
+
+#define TDMNTG_REF_CLK_CNTRL            0xF4CF0140  /**< TDM NTG Clock Control Register */
+#define TDMNTG_REF_CLK_DIV_CNTRL        0xF4CF0144  /**< TDM NTG Clock Divider Control Register */
+#define TDMNTG_RESET                    0xF4CF0148  /**< TDM NTG Reset Register */
+
+#define TSUNTG_REF_CLK_CNTRL            0xF4CF0150  /**< TSU NTG Clock Control Register */
+#define TSUNTG_REF_CLK_DIV_CNTRL        0xF4CF0154  /**< TSU NTG Clock Divider Control Register */
+#define TSUNTG_RESET                    0xF4CF0158  /**< TSU NTG Reset Register */
+
+#define CRP_CLK_CNTRL                   0xF4CF0170  /**< CRP Clock Control Register */
+#define CRP_CLK_DIV_CNTRL               0xF4CF0174  /**< CRP Clock Divider Control Register */
+#define CRP_CLK_RESET                   0xF4CF0178  /**< CRP Reset Register */
+
+#define CEVA_CLK_CNTRL                  0xF4CF0180  /**< CEVA Clock Control Register */
+#define CEVA_CLK_DIV_CNTRL              0xF4CF0184  /**< CEVA Clock Divider Control Register */
+#define CEVA_CLK_RESET                  0xF4CF0188  /**< CEVA Reset Register */
+
+#define SPACC_CLK_CNTRL                 0xF4CF0190  /**< SPACC Clock Control Register */
+#define SPACC_CLK_DIV_CNTRL             0xF4CF0194  /**< SPACC Clock Divider Control Register */
+#define SPACC_RESET                     0xF4CF0198  /**< SPACC Reset Register */
+
+#define SASPA_CLK_CNTRL                 0xF4CF01A0  /**< SASPA Clock Control Register */
+#define SASPA_CLK_DIV_CNTRL             0xF4CF01A4  /**< SASPA Clock Divider Control Register */
+#define SASPA_RESET                     0xF4CF01A8  /**< SASPA Reset Register */
+
+#define CRP_CLK_CNTRL                   0xF4CF0170  /**< CRP Clock Control Register */
+#define CRP_CLK_DIV_CNTRL               0xF4CF0174  /**< CRP Clock Divider Control Register */
+#define CRP_CLK_RESET                   0xF4CF0178  /**< CRP Reset Register */
+
+#define CEVA_BM_CLK_CNTRL               0xF4CF0300  /**< CEVA Clock Control Register */
+#define CEVA_BM_CLK_DIV_CNTRL           0xF4CF0304  /**< CEVA Clock Divider Control Register */
+#define CRP_BM_CLK_CNTRL                0xF4CF0310  /**< CRP Clock Control Register */
+#define CRP_BM_CLK_DIV_CNTRL            0xF4CF0314  /**< CRP Clock Divider Control Register */
+#define FFT_BM_CLK_CNTRL                0xF4CF0320  /**< FFT Clock Control Register */
+#define FFT_BM_CLK_DIV_CNTRL            0xF4CF0324  /**< FFT Clock Divider Control Register */
+
+#define FEC_DL_DIV2_CLK_CNTRL           0xF4CF0330  /**< FEC Clock Control Register */
+#define FEC_DL_DIV2_CLK_DIV_CNTRL       0xF4CF0334  /**< FEC Clock Divider Control Register */
+#define FEC_DL_DIV2_UNDOCUMENTED        0xF4CF0338  /**< Unknown Divider Control Register */
+
+#define DEVICE_CLK_RST_CTRL_BASE        0xF4CF0000  /**< Device CLK and CTRL  base for all registers in clock and reset directory treee including CLK and NTG */
+
+#define TSUNTG_REF_CLK_CNTRL_OFFSET            (0x150)  /**< TSU NTG Clock Control Register */
+#define TSUNTG_REF_CLK_DIV_CNTRL_OFFSET        (0x154)   /**< TSU NTG Clock Divider Control Register */
+#define TSUNTG_RESET_OFFSET_OFFSET             (0x158)  /**< TSU NTG Reset Register */
+
+/* Time Division Multiplexing (TDM) Network Timing Generator (NTG) module register */
+#define TDMNTG_FREQ_SET_INT_OFFSET             (0x400)
+#define TDMNTG_FREQ_SET_FRA_OFFSET             (0x404)
+#define TDMNTG_PHASE_ADJ_FREQ_INT_OFFSET       (0x410)
+#define TDMNTG_PHASE_ADJ_FREQ_FRA_OFFSET       (0x414)
+#define TDMNTG_PHASE_ADJ_DUR_OFFSET            (0x418)
+#define TDMNTG_PHASE_ADJ_START_OFFSET          (0x41C)
+#define TDMNTG_PULSE_WIDTH_OFFSET              (0x430)
+#define TDMNTG_FRAME_LENGTH_OFFSET             (0x434)
+#define TDMNTG_PHASE_FRAME_OFFSET              (0x438)
+#define TDMNTG_FRAME_COUNT_OFFSET              (0x43C)
+
+/*   Gigabit Ethernet MAC (GEM) Network Timing Generator (NTG) module registers     */
+#define GEMNTG_FREQ_SET_INT_OFFSET             (0x500)
+#define GEMNTG_FREQ_SET_FRA_OFFSET             (0x504)
+#define GEMNTG_PHASE_ADJ_FREQ_INT_OFFSET       (0x510)
+#define GEMNTG_PHASE_ADJ_FREQ_FRA_OFFSET       (0x514)
+#define GEMNTG_PHASE_ADJ_DUR_OFFSET            (0x518)
+#define GEMNTG_PHASE_ADJ_START_OFFSET          (0x51C)
+#define GEMNTG_CONTROL_OFFSET                  (0x520)
+#define GEMNTG_PULSE_WIDTH_OFFSET              (0x530)
+#define GEMNTG_FRAME_LENGTH_OFFSET             (0x534)
+#define GEMNTG_PHASE_FRAME_OFFSET              (0x538)
+#define GEMNTG_FRAME_COUNT_OFFSET              (0x53C)
+
+#define PLL_M_LSB(n)			(0xF4CF01C0 + 0x20 * n + 0x00)
+#define PLL_M_MSB(n)			(0xF4CF01C0 + 0x20 * n + 0x04)
+#define PLL_P(n)			(0xF4CF01C0 + 0x20 * n + 0x08)
+#define PLL_S(n)			(0xF4CF01C0 + 0x20 * n + 0x0C)
+#define PLL_CNTRL(n)			(0xF4CF01C0 + 0x20 * n + 0x10)
+#define PLL_STATUS(n)			(0xF4CF01C0 + 0x20 * n + 0x1C)
+
+#define PLL_SOURCE(pll)			(((pll) << 1) | 1)
+
+#define	FEC_DL_BASEADDR                 0xF4D00000
+#define	FEC_UL_BASEADDR                 0xF4D10000
+
+/*
+ * RAD Group Slaves
+ */
+
+#define JDMA0_BASEADDR		0xF5B80000
+#define JESD0_BASEADDR		0xF5B88000
+#define JDMA1_BASEADDR		0xF5B90000
+#define JESD1_BASEADDR		0xF5B98000
+#define JDMA_BASEADDR(id)	(0xF5B80000 + id * 0x10000)
+#define JESD_BASEADDR(id)	(0xF5B88000 + id * 0x10000)
+
+#define CPRI0_BASEADDR		0xF5B00000
+#define CPRI1_BASEADDR		0xF5B10000
+#define CPRI2_BASEADDR		0xF5B20000
+#define CPRI3_BASEADDR		0xF5B30000
+#define CPRI_BASEADDR(id)	(0xF5B30000 + id * 0x10000)
+
+#define CPDMA0_BASEADDR		0xF5C00000
+#define CPDMA1_BASEADDR		0xF5C10000
+#define CPDMA2_BASEADDR		0xF5C20000
+#define CPDMA3_BASEADDR		0xF5C30000
+#define CPDMA_BASEADDR(id)	(0xF5C00000+id*0x10000)
+
+#define TRANSCEDE_PCIE_CFG	(0xF5D00000)	// 64K
+#define TRANSCEDE_RAD_CFG	(0xF5E00000)	// 64K
+#define TRANSCEDE_RAD_TIMER	(0xF5E30000)	// 64K
+#define TRANSCEDE_GEM0		(0xF5E60000)	// 64K
+#define TRANSCEDE_SERDES0	(0xF5E80000)	// 64K
+#define TRANSCEDE_SERDES1	(0xF5E90000)	// 64K
+#define TRANSCEDE_PCIESlv_CPRISlv (0xF6000000)	// 8M
+
+#define TRANSCEDE_PCIe0_BASE (0xF5D00000)
+#define TRANSCEDE_PCIe0_SLAVE (0xF6000000)
+#define TRANSCEDE_PCIe1_BASE (0xFA000000)
+#define TRANSCEDE_PCIe1_SLAVE (0xFA200000)
+
+/*
+ *  RAD cluster configuration registers
+ */
+#define RAD_CFG_SRDS1_CFG0          (TRANSCEDE_RAD_CFG + 0x80)
+
+#define RAD_CFG_PCIE_X4_CFG0        (TRANSCEDE_RAD_CFG + 0x200)
+#define RAD_CFG_PCIE_X4_CFG1        (TRANSCEDE_RAD_CFG + 0x204)
+#define RAD_CFG_PCIE_X4_CFG2        (TRANSCEDE_RAD_CFG + 0x208)
+#define RAD_CFG_PCIE_X4_CFG3        (TRANSCEDE_RAD_CFG + 0x20C)
+#define RAD_CFG_PCIE_X4_CFG4        (TRANSCEDE_RAD_CFG + 0x210)
+#define RAD_CFG_PCIE_X4_CFG5        (TRANSCEDE_RAD_CFG + 0x214)
+#define RAD_CFG_PCIE_X4_CFG6        (TRANSCEDE_RAD_CFG + 0x218)
+#define RAD_CFG_PCIE_X4_CFG7        (TRANSCEDE_RAD_CFG + 0x21C)
+
+#define RAD_CFG_PCIE_X4_STAT0       (TRANSCEDE_RAD_CFG + 0x224)
+#define RAD_CFG_PCIE_X4_STAT1       (TRANSCEDE_RAD_CFG + 0x228)
+#define RAD_CFG_PCIE_X4_STAT2       (TRANSCEDE_RAD_CFG + 0x22C)
+#define RAD_CFG_PCIE_X4_STAT3       (TRANSCEDE_RAD_CFG + 0x230)
+#define RAD_CFG_PCIE_X4_STAT4       (TRANSCEDE_RAD_CFG + 0x234)
+#define RAD_CFG_PCIE_X4_STAT5       (TRANSCEDE_RAD_CFG + 0x238)
+#define RAD_CFG_PCIE_X4_STAT6       (TRANSCEDE_RAD_CFG + 0x23C)
+
+#define RAD_CFG_PCIE_X1_CFG0        (TRANSCEDE_RAD_CFG + 0x078)
+#define RAD_CFG_PCIE_X1_CFG1        (TRANSCEDE_RAD_CFG + 0x09C)
+
+#define RAD_CFG_PCIE_X4_DBI         (TRANSCEDE_RAD_CFG + 0x0D8)  /* DBI Address Bus Configuration */
+
+#define RAD_CFG_PCIE_X1_STAT0       (TRANSCEDE_RAD_CFG + 0x100)
+#define RAD_CFG_PCIE_X1_STAT1       (TRANSCEDE_RAD_CFG + 0x104)
+#define RAD_CFG_PCIE_X1_STAT2       (TRANSCEDE_RAD_CFG + 0x110)
+#define RAD_CFG_PCIE_X1_STAT5       (TRANSCEDE_RAD_CFG + 0x114)
+#define RAD_CFG_PCIE_X1_STAT6       (TRANSCEDE_RAD_CFG + 0x118)
+
+
+
+/*
+ * Virtual address mapping
+ */
+
+#define JRAM_BASE_VADDR		TRANSCEDE_JRAM_BASE	/* 1-1 mapping for JRAM	*/
+#define IRAM_BASE_VADDR		TRANSCEDE_IRAM_BASE	/* 1-1 mapping for ARAM */
+#define AAB_XP_BASE_VADDR	TRANSCEDE_AAB_XP_BASE	/* 1-1 of IO on APB bus  */
+#define AAB_XP_VADDR(x)		((x) - TRANSCEDE_AAB_XP_BASE + AAB_XP_BASE_VADDR)
+#define IO_ADDRESS(x)		AAB_XP_VADDR(x)
+#define __io_address(n)		__io(IO_ADDRESS(n))
+
+#define virt_to_jram(v)		( ((unsigned long)v - JRAM_BASE_VADDR) + TRANSCEDE_JRAM_BASE)
+#define virt_to_iram(v)		( ((unsigned long)v - IRAM_BASE_VADDR) + TRANSCEDE_IRAM_BASE)
+
+/*
+ * Clocks
+ */
+#ifndef __ASSEMBLY__
+extern unsigned long transcede_axi_clk_hz;
+extern unsigned long transcede_arm_clk_hz;
+extern unsigned long transcede_arm_peripheral_clk_hz;
+#endif  /* __ASSEMBLY__ */
+
+#define TRANSCEDE_AXICLK_HZ	transcede_axi_clk_hz
+#define TRANSCEDE_AHBCLK_HZ	TRANSCEDE_AXICLK_HZ
+#define TRANSCEDE_ARMPCLK_HZ	250000000	// TODO: used for ticks, check if need to rely on this value
+#define TRANSCEDE_ARMCLK_HZ	transcede_arm_clk_hz
+/*
+ * Macros for making sure memory mapped registers are accessed
+ * using c "volatile" keyword to force non-cached reads and writes
+ */
+#define REG32(regaddr)	                (*(volatile unsigned int *)(regaddr))
+#define REG16(regaddr)	                (*(volatile unsigned short *)(regaddr))
+#define REG8(regaddr)	                (*(volatile unsigned char *)(regaddr))
+
+#define REG32CLR(addr, clr_mask)		( REG32(addr) = REG32(addr) & (~(clr_mask)) )
+#define REG32SET(addr, set_mask)		( REG32(addr) = REG32(addr) | (set_mask) )
+#define REG32UPD(addr, clr_mask, set_mask)	( REG32(addr) = (REG32(addr) & (~(clr_mask))) | (set_mask) )
+
+#include <mach/gpio.h>
+#include <mach/exp-bus.h>
+
+#include <asm/types.h>
+
+#ifndef __ASSEMBLY__
+
+#define CONFIG_TRANSCEDE_GEMAC          1
+
+#define CONFIG_TRANSCEDE_USE_MII        1
+#define CONFIG_TRANSCEDE_USE_RMII       2
+#define CONFIG_TRANSCEDE_USE_GMII       4
+#define CONFIG_TRANSCEDE_USE_RGMII      8
+
+#define GEMAC_SW_CONF                   ((1 << 8) | (1 << 11))  // GEMAC configured by SW
+#define GEMAC_PHY_CONF                  0                       // GEMAC configured by phy lines (not for MII/GMII)
+#define GEMAC_SW_FULL_DUPLEX            (1 << 9)
+#define GEMAC_SW_SPEED_10M              (0 << 12)
+#define GEMAC_SW_SPEED_100M             (1 << 12)
+#define GEMAC_SW_SPEED_1G               (2 << 12)
+
+#define GEMAC_PHY_1000                  1                       // set if a GIg phy available
+#define GEMAC_NO_PHY                    2                       // set if no phy connected to MAC (ex ethernet switch). In this case use MAC fixed configuration
+#define GEMAC_PHY_RGMII_ADD_DELAY       4
+
+
+struct transcede_eth_platform_data {
+        /* device specific information */
+        unsigned int device_flags;
+        char name[16];
+
+
+        /* board specific information */
+        unsigned int phy_flags;
+        unsigned int gem_id;
+        char bus_id[4];
+        unsigned int phy_id;
+        unsigned char *mac_addr;
+};
+
+struct transcede_mdio_data {
+        int irq[32];
+        unsigned int phy_mask;
+        int mdc_div;
+};
+
+
+#define MAX_DMA_BDESC           8
+
+typedef struct tDMABDESC
+{
+    volatile unsigned int       BPtr;
+    volatile unsigned int       BCtrl;
+} DMABDESC, *PDMABDESC;
+
+typedef void (*DMACOMPCB)(void);
+
+typedef struct tDMAFDESC
+{
+    volatile struct tDMAFDESC   *NextFDesc;
+    volatile unsigned int       FControl;
+    DMACOMPCB                   DmaCompCb; // this field is not used by HW
+    volatile unsigned int       FStatus;
+    volatile DMABDESC           InOutBDesc[MAX_DMA_BDESC];
+} DMAFDESC, *PDMAFDESC;
+
+typedef struct tDMACTRLREGS
+{
+    volatile unsigned int       Mem2DmaCtrl;
+    volatile unsigned int       Mem2DmaHeadPtr;
+    volatile unsigned int       Mem2DmaBurstLen;
+    volatile unsigned int       Res0;
+    volatile unsigned int       Mem2DmaIrqEna;
+    volatile unsigned int       Mem2DmaIrqStatClear;
+    volatile unsigned int       Res1;
+    volatile unsigned int       Res2;
+    volatile unsigned int       Mem2DmaSoftReset;
+    volatile unsigned int       Res3[55];
+    volatile unsigned int       Dma2MemCtrl;
+    volatile unsigned int       Dma2MemHeadPtr;
+    volatile unsigned int       Dma2MemBurstLen;
+    volatile unsigned int       Res4;
+    volatile unsigned int       Dma2MemIrqEna;
+    volatile unsigned int       Dma2MemIrqStatClear;
+    volatile unsigned int       Res5;
+    volatile unsigned int       Res6;
+    volatile unsigned int       Dma2MemSoftReset;
+    volatile unsigned int       Res7[55];
+    volatile unsigned int       FComCtrl;
+    volatile unsigned int       FComHeadPtr;
+    volatile unsigned int       FComBurstLen;
+    volatile unsigned int       Res8;
+    volatile unsigned int       FComIrqEna;
+    volatile unsigned int       FComIrqStatClear;
+    volatile unsigned int       Res9;
+    volatile unsigned int       Res10;
+    volatile unsigned int       FComSoftReset;
+    volatile unsigned int       Res11[55];
+    volatile unsigned int       Mem2DmaFifoThreshold;
+    volatile unsigned int       Dma2MemFifoThreshold;
+    volatile unsigned int       AxiOutReadsAllowed;
+} DMACTRLREGS, *PDMACTRLREGS;
+
+#define FSTAT_FDONE             ((unsigned int)1<<31)
+#define FSYNC                   ((unsigned int)1 << 1)
+
+#define IRQFDONE                ((unsigned int)1 << 0)
+#define IRQFREADY               ((unsigned int)1 << 1)
+#define IRQFLAST                ((unsigned int)1 << 2)
+
+typedef struct tMAPAPBREGS
+{
+    volatile unsigned int       Ctrl;           // 0x00 offset
+    volatile unsigned int       IAddr;          // 0x04 offset
+    volatile unsigned int       reserved_0;     // 0x08 offset
+    volatile unsigned int       reserved_1;     // 0x0C offset
+    volatile unsigned int       TReqEna;        // 0x10 offset
+    volatile unsigned int       FPCom;          // 0x14 offset
+    volatile unsigned int       FPStat;         // 0x18 offset
+    volatile unsigned int       reserved_2;     // 0x1C offset
+    volatile unsigned int       DbgRun;         // 0x20 offset
+    volatile unsigned int       DbgPcTrig;      // 0x24 offset
+    volatile unsigned int       DbgCntTrig;     // 0x28 offset
+    volatile unsigned int       reserved_3;     // 0x2C offset
+    volatile unsigned int       Min_31_0;       // 0x30 offset
+    volatile unsigned int       Min_63_32;      // 0x34 offset
+    volatile unsigned int       Mout_31_0;      // 0x38 offset
+    volatile unsigned int       Mout_63_32;     // 0x3C offset
+    volatile unsigned int       reserved_4[48]; // 0x40-0xFC offsets
+    volatile unsigned int       MAcc_31_0;      // 0x100 offset
+    volatile unsigned int       MAcc_63_32;     // 0x104 offset
+    volatile unsigned int       MAcc_95_64;     // 0x108 offset
+    volatile unsigned int       MAcc_127_96;    // 0x10C offset
+    volatile unsigned int       MAcc_159_128;   // 0x110 offset
+
+} MAPAPBREGS, *PMAPAPBREGS;
+
+typedef struct tagMAPDMACTRL
+{
+    volatile unsigned int       Control;            //0x0 - offset, Control
+    volatile unsigned int       HeadFDesc;          //0x4 - offset, Head ptr.
+    volatile unsigned int       Lock;               //0x8 - offset, Lock (to setup burst length and so on).
+    volatile unsigned int       Status;             //0xC - offset, Status (read only).
+    volatile unsigned int       DmaIrqEnable;       //0x10 - offset, IRQ enable
+    volatile unsigned int       IrqStatusClear;     //0x14 - offset, IRQ Status and Clear
+    volatile unsigned int       Reserved[2];        //0x18..0x1C offsets
+    volatile unsigned int       SoftReset;          //0x20 - offset, Soft reset
+    volatile unsigned int       DiagRegs[55];       //0x24..0xFC offsets, reserved
+} MAPDMACTRL, *PMAPDMACTRL;
+
+typedef struct tMAPDMAREGS
+{
+    MAPDMACTRL                  DMAInCtrl;
+    MAPDMACTRL                  DMAOutCtrl;
+} MAPDMAREGS, *PMAPDMAREGS;
+
+typedef struct tMAPMASTERCTRLREGS
+{
+    volatile unsigned int       Ctrl1;
+    volatile unsigned int       Ctrl2;
+    volatile unsigned int       Ctrl3;
+    volatile unsigned int       BCastMask;
+    volatile unsigned int       IrqStatAck;
+    volatile unsigned int       IrqEna;
+    volatile unsigned int       Grp0OutTrLen;
+    volatile unsigned int       Grp1OutTrLen;
+    volatile unsigned int       Grp0OutLoad0Ctrl0;
+    volatile unsigned int       Grp0OutLoad0Ctrl1;
+    volatile unsigned int       Grp0OutLoad1Ctrl0;
+    volatile unsigned int       Grp0OutLoad1Ctrl1;
+    volatile unsigned int       Grp1OutLoad0Ctrl0;
+    volatile unsigned int       Grp1OutLoad0Ctrl1;
+    volatile unsigned int       Grp1OutLoad1Ctrl0;
+    volatile unsigned int       Grp1OutLoad1Ctrl1;
+    volatile unsigned int       InterFpAddrPhase;
+} MAPMASTERCTRLREGS, *PMAPMASTERCTRLREGS;
+
+typedef struct tMAPEXPDMAREGS
+{
+    volatile unsigned int       Enable;
+    volatile unsigned int       Priority;
+    volatile unsigned int       Status;
+    volatile unsigned int       RRobinCtrl;
+    volatile unsigned int       FHeadIn[5];
+    volatile unsigned int       FHeadSharedIn;
+    volatile unsigned int       Reserved[2];
+    volatile unsigned int       FHeadOut[5];
+    volatile unsigned int       FHeadSharedOut;
+} MAPEXPDMAREGS, *PMAPEXPDMAREGS;
+
+#ifndef _VUINT64_
+#define _VUINT64_
+typedef volatile unsigned long long VUINT64, *PVUINT64;
+#endif /*_UINT64_*/
+
+typedef union tMAPDMATRCTRL
+{
+    VUINT64 Reg;
+    struct {
+        VUINT64  FpId:5;
+        VUINT64  LoadType:2;
+        VUINT64  LoadCont:1;
+        VUINT64  StartBank:8;
+        VUINT64  SegStart:1;
+        VUINT64  BusSize:2;
+        VUINT64  DataType:1;
+        VUINT64  TrInt:3;
+        VUINT64  StartAddr:11;
+        VUINT64  SegCount:11;
+        VUINT64  TrId:6;
+        VUINT64  Res:13;
+    } B;
+} MAPDMATRCTRL, *PMAPDMATRCTRL;
+
+#define MAPAPB_CTRL_REG_OFFSET          0x00000000
+#define MAPAPB_IADDR_REG_OFFSET         0x00000004
+#define MAPAPB_IDATA0_REG_OFFSET        0x00000100
+#define MAPAPB_IDATA1_REG_OFFSET        0x00000104
+#define MAPAPB_IDATA2_REG_OFFSET        0x00000108
+#define MAPAPB_IDATA3_REG_OFFSET        0x0000010c
+#define MAPAPB_IDATA4_REG_OFFSET        0x00000110
+
+#define MAPAPB_CTRL_PMEM_ACCESS_ENABLE  (1<<0)
+#define MAPAPB_CTRL_RESET               (1<<1)
+#define MAPAPB_CTRL_RESET_IO            (1<<2)
+#define MAPAPB_CTRL_DEBUG_MODE          (1<<3)
+#define MAPAPB_CTRL_MEM_ACCESS_ENABLE   (1<<4)
+
+#define MAPDMA_REGS_OFFSET              0x0000E000
+#define MAPMASTER_REGS_OFFSET           0x0000F000
+#define MAPAPB_BCST_REGS_OFFSET         0x0000F800
+
+#define MAP_SEGMENT_SIZE                 2048
+#define MAP_SEGMENT_COUNT                16
+#define MAP_BANK_COUNT                   MAP_SEGMENT_COUNT/2
+
+#define MAP_24BIT_BANK_SIZE              (MAP_SEGMENT_SIZE*2*(sizeof(short)+sizeof(char)))
+#define MAP_16BIT_BANK_SIZE              (MAP_SEGMENT_SIZE*2*(sizeof(short)))
+//half banks are 16bit word size and other half are 24bit
+#define MAP_DRAM_SIZE                   (MAP_BANK_COUNT/2*(MAP_24BIT_BANK_SIZE+MAP_16BIT_BANK_SIZE))
+#define MAP_PRAM_SIZE                   (10*1024)
+
+#define MAP_SEGMENT_MODE                 0x00
+#define MAP_BANK_MODE                    0x10
+
+#define SYS_REF_CLK                     25
+#define PLL_CTRL_RESET                  (1<<0)
+#define PLL_CTRL_BYPASS                 (1<<4)
+#define PLL_CTRL_LOCK_EN                (1<<5)
+#define PLL_CTRL_VSEL                   (1<<6)
+#define PLL_STAT_LOCK                   (1<<0)
+
+
+#define _TRANSCEDE_SHOW(_name) transcede_show_##_name
+#define _TRANSCEDE_SET(_name) transcede_set_##_name
+
+#define TRANSCEDE_CREATE_FILE(_name)	  \
+	(device_create_file(dev, &dev_attr_##_name))
+#define TRANSCEDE_REMOVE_FILE(_name)      \
+	(device_remove_file(dev, &dev_attr_##_name))
+#define TRANSCEDE_ACT_SHOW(_name)	  \
+	static ssize_t _TRANSCEDE_SHOW(_name)( \
+		struct device * dev, \
+		struct device_attribute * attr, \
+		char * buf)
+#define TRANSCEDE_ACT_SET(_name)	\
+	static ssize_t _TRANSCEDE_SET(_name)( \
+		struct device * dev, \
+		struct device_attribute * attr, \
+		const char * buf, \
+		size_t count)
+#define TRANSCEDE_ATTR_SHOW(_name)	  \
+	TRANSCEDE_ACT_SHOW(_name); \
+	static DEVICE_ATTR(_name, 0444, _TRANSCEDE_SHOW(_name), NULL)
+#define TRANSCEDE_ATTR_SET(_name)	  \
+	TRANSCEDE_ACT_SET(_name); \
+	static DEVICE_ATTR(_name, 0200, NULL, _TRANSCEDE_SET(_name))
+#define TRANSCEDE_ATTR(_name)	  \
+	TRANSCEDE_ACT_SET(_name); \
+	TRANSCEDE_ACT_SHOW(_name); \
+	static DEVICE_ATTR(_name, 0644, _TRANSCEDE_SHOW(_name), _TRANSCEDE_SET(_name))
+
+#ifdef CONFIG_PCI
+#include <mach/pcie.h>
+
+#define PCIBIOS_MIN_MEM (TRANSCEDE_PCIe0_SLAVE)
+#define PCIBIOS_MIN_IO (iATU_GET_IO_BASE(TRANSCEDE_PCIe0_SLAVE, 0))
+
+static inline int pcibios_assign_all_busses(void)
+{
+       return 1;
+}
+#endif	/* CONFIG_PCI */
+
+extern unsigned t3300_slave;
+
+extern int t2k_reg_xfrm_ipv4(void * pxfrm, uint d_ip, uint s_ip);
+extern int t2k_unreg_xfrm(void* pxfrm);
+extern void* t2k_find_xfrm_by_ipv4(uint d_ip, uint s_ip);
+extern void* t2k_get_xfrm_by_idx(uint idx);
+extern uint t2k_get_reg_xfrm_num(void);
+
+#endif //__ASSEMBLY__
+
+#endif
diff --git a/arch/arm/mach-transcede/include/mach/transcede-4000.h b/arch/arm/mach-transcede/include/mach/transcede-4000.h
new file mode 100644
index 0000000..cb4395e
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/transcede-4000.h
@@ -0,0 +1,317 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef __ASM_ARCH_TRANSCEDE_4000_H
+#define __ASM_ARCH_TRANSCEDE_4000_H
+
+/*
+ * Memory layout
+ */
+#define TRANSCEDE_SDRAM_BASE	0x00000000	/* SDRAM (3G max) */
+#define TRANSCEDE_SDRAM_SIZE	0xC0000000
+#define TRANSCEDE_DDR0_BASE	0x00000000	/* DDR0  	*/
+#define TRANSCEDE_DDR0_SIZE	0x40000000ULL
+#define TRANSCEDE_DDR1_BASE	0x40000000ULL	/* DDR1  	*/
+#define TRANSCEDE_DDR1_SIZE	0x40000000ULL
+#define TRANSCEDE_JRAM_BASE	0xFA000000	/* JRAM 16K  	*/
+#define TRANSCEDE_JRAM_SIZE	0x00004000
+#define TRANSCEDE_PCIE_BASE	0xFA800000	/* PCIe_0 */
+#define TRANSCEDE_PCIE_SIZE	0x00400000
+#define TRANSCEDE_CPRI_BASE	0xFAC00000
+#define TRANSCEDE_CPRI_SIZE	0x00100000
+#define TRANSCEDE_SRIO1_BASE	0xFAE00000
+#define TRANSCEDE_SRIO1_SIZE	0x00100000
+#define TRANSCEDE_SRIO0_BASE	0xFAF00000
+#define TRANSCEDE_SRIO0_SIZE	0x00100000
+#define TRANSCEDE_IRAM_BASE	0xFB000000	/* IRAM 256K  	*/
+#define TRANSCEDE_IRAM_SIZE	0x00040000
+#define TRANSCEDE_DDR0_CFG	0xFBE00000	/* DDR Controllers */
+#define TRANSCEDE_DDR1_CFG	0xFBE80000
+#define TRANSCEDE_SEMA_BASE	0xFBF00000	/* HW semaphores*/
+#define TRANSCEDE_SEMA_SIZE	0x00040000
+#define TRANSCEDE_CRAM_BASE	0xFC000000	/* CRAM 3M 	*/
+#define TRANSCEDE_CRAM_SIZE	0x00300000
+#define TRANSCEDE_AAB_XP_BASE	0xFE000000
+#define TRANSCEDE_AAB_XP_SIZE	(16*1024*1024)
+#define TRANSCEDE_HIMEM_BASE	0xFFFF0000	/* High vectors */
+#define TRANSCEDE_HIMEM_SIZE	0x00010000
+
+#define TRANSCEDE_PUBLIC_VIRT (transcede_public_virt)
+#define TRANSCEDE_PUBLIC_SIZE (transcede_public_size)
+
+/*
+ * SCU registers
+ */
+#define TRANSCEDE_SCU_BASE		0xFFF00000
+#define TRANSCEDE_GIC_CPU_BASE		(TRANSCEDE_SCU_BASE + 0x100)
+#define TRANSCEDE_GLOBAL_TIMER_BASE	(TRANSCEDE_SCU_BASE + 0x200)
+#define TRANSCEDE_TWD_BASE		(TRANSCEDE_SCU_BASE + 0x600)
+#define TRANSCEDE_TWD_SIZE		0x00000100
+#define TRANSCEDE_GIC_DIST_BASE		(TRANSCEDE_SCU_BASE + 0x1000)
+#define TRANSCEDE_L310_BASE		(TRANSCEDE_SCU_BASE + 0x10000)
+
+/*
+ * Global timer
+ */
+#define TRANSCEDE_LOW_TIMER_COUNTER	(TRANSCEDE_GLOBAL_TIMER_BASE + 0x00)
+#define TRANSCEDE_UP_TIMER_COUNTER	(TRANSCEDE_GLOBAL_TIMER_BASE + 0x04)
+#define TRANSCEDE_TIMER_CTRL		(TRANSCEDE_GLOBAL_TIMER_BASE + 0x08)
+#define TRANSCEDE_TIMER_STATUS		(TRANSCEDE_GLOBAL_TIMER_BASE + 0x0C)
+#define TRANSCEDE_LOW_COMPARATOR	(TRANSCEDE_GLOBAL_TIMER_BASE + 0x10)
+#define TRANSCEDE_UP_COMPARATOR		(TRANSCEDE_GLOBAL_TIMER_BASE + 0x14)
+#define TRANSCEDE_AUTO_INCREMENT	(TRANSCEDE_GLOBAL_TIMER_BASE + 0x18)
+
+#define GLOBAL_TIMER_AUTO_INC_ENA   (0x08)
+#define GLOBAL_TIMER_IRQ_ENA        (0x04)
+#define GLOBAL_TIMER_COMPARATOR_ENA (0x02)
+#define GLOBAL_TIMER_ENA            (0x01)
+
+/*
+ * Expansion Cluster Blocks
+ */
+#define TRANSCEDE_TDM		(TRANSCEDE_AAB_XP_BASE + 0x00000000)
+#define TRANSCEDE_TSB		(TRANSCEDE_AAB_XP_BASE + 0x00004000)
+#define TRANSCEDE_HI		(TRANSCEDE_AAB_XP_BASE + 0x00010000)
+#define TRANSCEDE_TDMA		(TRANSCEDE_AAB_XP_BASE + 0x00020000)
+#define TRANSCEDE_TDMA_EXT	(TRANSCEDE_AAB_XP_BASE + 0x00030000)
+#define TRANSCEDE_TIMER		(TRANSCEDE_AAB_XP_BASE + 0x00050000)
+#define TRANSCEDE_GPIO		(TRANSCEDE_AAB_XP_BASE + 0x00070000)
+#define TRANSCEDE_UART0		(TRANSCEDE_AAB_XP_BASE + 0x00090000)
+#define TRANSCEDE_UART1		(TRANSCEDE_AAB_XP_BASE + 0x00092000)
+#define TRANSCEDE_UART2		(TRANSCEDE_AAB_XP_BASE + 0x00094000)
+#define TRANSCEDE_SPI		(TRANSCEDE_AAB_XP_BASE + 0x00098000)
+#define TRANSCEDE_I2C		(TRANSCEDE_AAB_XP_BASE + 0x0009C000)
+#define TRANSCEDE_TOP_CLKRST	(TRANSCEDE_AAB_XP_BASE + 0x000B0000)
+#define TRANSCEDE_EXP_CLKRST	(TRANSCEDE_AAB_XP_BASE + 0x000C0000)
+#define TRANSCEDE_NTG_TDM	(TRANSCEDE_AAB_XP_BASE + 0x000C8000)
+#define TRANSCEDE_NTG_TSU	(TRANSCEDE_AAB_XP_BASE + 0x000CC000)
+#define TRANSCEDE_GEM0		(TRANSCEDE_AAB_XP_BASE + 0x000D0000)
+#define TRANSCEDE_EXP_BUS	(TRANSCEDE_AAB_XP_BASE + 0x00100000)
+#define TRANSCEDE_GEM1		(TRANSCEDE_AAB_XP_BASE + 0x00170000)
+#define TRANSCEDE_SERDES_GEM	(TRANSCEDE_AAB_XP_BASE + 0x001A0000)
+#define TRANSCEDE_AAB_SYS	(TRANSCEDE_AAB_XP_BASE + 0x00400000)
+#define TRANSCEDE_AAB_RAD	(TRANSCEDE_AAB_XP_BASE + 0x00800000)
+#define TRANSCEDE_IBR		(TRANSCEDE_AAB_XP_BASE + 0x01000000)
+
+/* These definitions have not been verified to match the hardware.
+ * They've been just copy-pasted from T2200.
+ * Committed only to make some other code compilable (kheap).
+ */
+#if 1 /* start of block of untested definitions */
+/*
+ * Timer control registers
+ */
+#define TIMER0_CNTR_REG                 (TRANSCEDE_TIMER + 0x00)
+#define TIMER0_CURR_COUNT               (TRANSCEDE_TIMER + 0x04)
+#define TIMER1_CNTR_REG                 (TRANSCEDE_TIMER + 0x08)
+#define TIMER1_CURR_COUNT               (TRANSCEDE_TIMER + 0x0C)
+#define TIMER2_LBOUND_REG               (TRANSCEDE_TIMER + 0x10)
+#define TIMER2_HBOUND_REG               (TRANSCEDE_TIMER + 0x14)
+#define TIMER2_CNTR_REG                 (TRANSCEDE_TIMER + 0x18)
+#define TIMER2_CURR_COUNT               (TRANSCEDE_TIMER + 0x1C)
+#define TIMER3_LBOUND_REG               (TRANSCEDE_TIMER + 0x20)
+#define TIMER3_HBOUND_REG               (TRANSCEDE_TIMER + 0x24)
+#define TIMER3_CNTR_REG                 (TRANSCEDE_TIMER + 0x28)
+#define TIMER3_CURR_COUNT               (TRANSCEDE_TIMER + 0x2C)
+
+#define get_tick() (*(volatile unsigned long *)(TIMER0_CURR_COUNT))
+#endif /* 1 */
+
+/*
+ * System cluster blocks
+ */
+#define TRANSCEDE_MDMA_SYS	(TRANSCEDE_AAB_SYS + 0x00000000)
+#define TRANSCEDE_FEC		(TRANSCEDE_AAB_SYS + 0x00010000)
+#define TRANSCEDE_ARM_SYS	(TRANSCEDE_AAB_SYS + 0x00020000)
+#define TRANSCEDE_CFG_SYS	(TRANSCEDE_AAB_SYS + 0x00030000)
+#define TRANSCEDE_CLKRST_SYS	(TRANSCEDE_AAB_SYS + 0x00040000)
+#define TRANSCEDE_DDRPHY0	(TRANSCEDE_AAB_SYS + 0x00050000)
+#define TRANSCEDE_DDRPHY1	(TRANSCEDE_AAB_SYS + 0x00060000)
+#define TRANSCEDE_SYS_MBIST	(TRANSCEDE_AAB_SYS + 0x00070000)
+#define TRANSCEDE_ARM_CORESIGHT	(TRANSCEDE_AAB_SYS + 0x00200000)
+
+/*
+ * Radio cluster blocks
+ */
+#define TRANSCEDE_RAD_CLKRST	(TRANSCEDE_AAB_RAD + 0x00000000)
+#define TRANSCEDE_CONFIG_RAD	(TRANSCEDE_AAB_RAD + 0x00010000)	/* 0xFE81_0000 */
+#define TRANSCEDE_SRIO0		(TRANSCEDE_AAB_RAD + 0x00020000)
+#define TRANSCEDE_SRIO1		(TRANSCEDE_AAB_RAD + 0x00030000)
+#define TRANSCEDE_SRIO_PFWD	(TRANSCEDE_AAB_RAD + 0x00040000)
+#define TRANSCEDE_CPRI_SYS	(TRANSCEDE_AAB_RAD + 0x00090000)
+#define TRANSCEDE_CPDMA		(TRANSCEDE_AAB_RAD + 0x000A0000)
+#define TRANSCEDE_PCIE_SYS	(TRANSCEDE_AAB_RAD + 0x000B0000)	/* 0xFE8B_0000 */
+#define TRANSCEDE_MDMA		(TRANSCEDE_AAB_RAD + 0x000C0000)
+#define TRANSCEDE_SERDES0_4CH	(TRANSCEDE_AAB_RAD + 0x000F0000)	/* 0xFE8F_0000 */
+#define TRANSCEDE_SERDES1_2CH	(TRANSCEDE_AAB_RAD + 0x00100000)	/* 0xFE90_0000 */
+#define TRANSCEDE_SERDES2_4CH	(TRANSCEDE_AAB_RAD + 0x00110000)	/* 0xFE91_0000 */
+
+/*
+ * SPU cluster blocks
+ */
+#define TRANSCEDE_AAB_SPU	0xFD000000
+#define TRANSCEDE_MDMA_SPU	(TRANSCEDE_AAB_SPU + 0x00010000)
+#define TRANSCEDE_FFT		(TRANSCEDE_AAB_SPU + 0x00020000)
+#define TRANSCEDE_SPU_CLKRST	(TRANSCEDE_AAB_SPU + 0x00030000)
+#define TRANSCEDE_INTC_SPU	(TRANSCEDE_AAB_SPU + 0x00040000)
+#define TRANSCEDE_MISC_SPU	(TRANSCEDE_AAB_SPU + 0x00050000)
+#define TRANSCEDE_CRAM01	(TRANSCEDE_AAB_SPU + 0x00060000)
+#define TRANSCEDE_CRAM23	(TRANSCEDE_AAB_SPU + 0x00070000)
+#define TRANSCEDE_CRAM45	(TRANSCEDE_AAB_SPU + 0x00080000)
+#define TRANSCEDE_MBIST		(TRANSCEDE_AAB_SPU + 0x00090000)
+
+/*
+ * Virtual address mapping
+ */
+#define PCIE_BASE_VADDR		0xFA800000		/* 1-1 mapping for PCIE */
+#define JRAM_BASE_VADDR		TRANSCEDE_JRAM_BASE	/* 1-1 mapping for JRAM	*/
+#define IRAM_BASE_VADDR		TRANSCEDE_IRAM_BASE	/* 1-1 mapping for ARAM */
+#define AAB_XP_BASE_VADDR	TRANSCEDE_AAB_XP_BASE	/* 1-1 of IO on APB bus  */
+#define AAB_XP_VADDR(x)		((x) - TRANSCEDE_AAB_XP_BASE + AAB_XP_BASE_VADDR)
+#define IO_ADDRESS(x)		AAB_XP_VADDR(x)
+#define __io_address(n)		__io(IO_ADDRESS(n))
+
+#define virt_to_jram(v)		( ((unsigned long)v - JRAM_BASE_VADDR) + TRANSCEDE_JRAM_BASE)
+#define virt_to_iram(v)		( ((unsigned long)v - IRAM_BASE_VADDR) + TRANSCEDE_IRAM_BASE)
+
+/*
+ * Clocks
+ */
+#define TRANSCEDE_OSC_HZ	 25000000
+#define TRANSCEDE_EXPCLK_HZ	 30000000
+#define TRANSCEDE_AHBCLK_HZ	150000000
+#define TRANSCEDE_AXICLK_HZ	300000000
+#define TRANSCEDE_ARMCLK_HZ	600000000
+
+#include <mach/clk-rst.h>
+#include <mach/gpio.h>
+#include <mach/exp-bus.h>
+#include <mach/syscfg.h>
+
+#include <asm/types.h>
+
+/*
+ * PCIe IP Physical & Virtual Address
+ */
+#define TRANSCEDE_PCIE_MEM_SIZE		SZ_2M	/* mem space	= 1M */
+#define TRANSCEDE_PCIE_IO_SIZE		SZ_1M	/* io space	= 1M */
+#define TRANSCEDE_PCIE_CFG_SIZE		SZ_1M	/* cfg space	= 1M */
+
+#define TRANSCEDE_PCIE_MEM_BASE		(TRANSCEDE_PCIE_BASE)					/* PCIe Phy Memory Space: 0xFA80_0000 ~ 0xFA9F_FFFF */
+#define TRANSCEDE_PCIE_IO_BASE		(TRANSCEDE_PCIE_MEM_BASE + TRANSCEDE_PCIE_MEM_SIZE)	/* PCIe Phy I/O Space: 0xFAA0_0000 ~ 0xFAAF_FFFF */
+#define TRANSCEDE_PCIE_CFG_BASE		(TRANSCEDE_PCIE_IO_BASE + TRANSCEDE_PCIE_IO_SIZE)	/* PCIe Phy Config Space: 0xFAB0_0000 ~ 0xFABF_FFFF */
+
+#define TRANSCEDE_PCIE_MEM_VADDR	(PCIE_BASE_VADDR)					/* PCIe Virtual Memory Space */
+#define TRANSCEDE_PCIE_IO_VADDR		(TRANSCEDE_PCIE_MEM_BASE_VIRT + TRANSCEDE_PCIE_MEM_SIZE)/* PCIe Virtual IO Space */
+#define TRANSCEDE_PCIE_CFG_VADDR	(TRANSCEDE_PCIE_IO_BASE_VIRT + TRANSCEDE_PCIE_IO_SIZE)	/* PCIe Virtual Config Space */
+
+#define PCIBIOS_MIN_MEM			TRANSCEDE_PCIE_MEM_BASE
+#define PCIBIOS_MIN_IO			0
+
+#define pcibios_assign_all_busses()	1
+
+
+#define SPI_RAD0			1	/* SPI CS for radio module 0 */
+#define SPI_RAD1			2
+#define SPI_DEJITTER_CS			4	/* SPI CS for dejitter */
+#define SPI_EPROM_CS			8
+
+
+#ifndef __ASSEMBLY__
+
+#ifdef CONFIG_GLOBAL_POLLING
+#include "periodic_task.h"
+#endif	/* CONFIG_GLOBAL_POLLING */
+
+extern unsigned long transcede_ddr_common_base;
+extern unsigned long transcede_public_virt;
+extern unsigned long transcede_public_size;
+
+#define CONFIG_TRANSCEDE_GEMAC		1
+
+#define CONFIG_TRANSCEDE_USE_MII	1
+#define CONFIG_TRANSCEDE_USE_RMII	2
+#define CONFIG_TRANSCEDE_USE_GMII	4
+#define CONFIG_TRANSCEDE_USE_RGMII	8
+
+#define GEMAC_SW_CONF			((1 << 8) | (1 << 11))	// GEMAC configured by SW
+#define GEMAC_PHY_CONF			0			// GEMAC configured by phy lines (not for MII/GMII)
+#define GEMAC_SW_FULL_DUPLEX		(1 << 9)
+#define GEMAC_SW_SPEED_10M		(0 << 12)
+#define GEMAC_SW_SPEED_100M		(1 << 12)
+#define GEMAC_SW_SPEED_1G		(2 << 12)
+
+#define GEMAC_PHY_1000			1			// set if a GIg phy available
+#define GEMAC_NO_PHY			2			// set if no phy connected to MAC (ex ethernet switch). In this case use MAC fixed configuration
+#define GEMAC_PHY_RGMII_ADD_DELAY	4
+
+
+struct transcede_eth_platform_data {
+	/* device specific information */
+	u32 device_flags;
+	char name[16];
+
+
+	/* board specific information */
+	u32 phy_flags;
+	u32 gem_id;
+	char bus_id[4];
+	u32 phy_id;
+	u8 *mac_addr;
+};
+
+struct transcede_mdio_data {
+	int irq[32];
+	u32 phy_mask;
+	int mdc_div;
+};
+
+#define _TRANSCEDE_SHOW(_name) transcede_show_##_name
+#define _TRANSCEDE_SET(_name) transcede_set_##_name
+
+#define TRANSCEDE_CREATE_FILE(_name)	  \
+	(device_create_file(dev, &dev_attr_##_name))
+#define TRANSCEDE_ACT_SHOW(_name)	  \
+	static ssize_t _TRANSCEDE_SHOW(_name)( \
+		struct device * dev, \
+		struct device_attribute * attr, \
+		char * buf)
+#define TRANSCEDE_ACT_SET(_name)	  \
+	static ssize_t _TRANSCEDE_SET(_name)( \
+		struct device * dev, \
+		struct device_attribute * attr, \
+		const char * buf, \
+		size_t count)
+#define TRANSCEDE_ATTR_SHOW(_name)	  \
+	TRANSCEDE_ACT_SHOW(_name); \
+	static DEVICE_ATTR(_name, 0444, _TRANSCEDE_SHOW(_name), NULL)
+#define TRANSCEDE_ATTR_SET(_name)	  \
+	TRANSCEDE_ACT_SET(_name); \
+	static DEVICE_ATTR(_name, 0200, NULL, _TRANSCEDE_SET(_name))
+#define TRANSCEDE_ATTR(_name)	  \
+	TRANSCEDE_ACT_SET(_name); \
+	TRANSCEDE_ACT_SHOW(_name); \
+	static DEVICE_ATTR(_name, 0644, _TRANSCEDE_SHOW(_name), _TRANSCEDE_SET(_name))
+
+#endif //__ASSEMBLY__
+
+#endif
diff --git a/arch/arm/mach-transcede/include/mach/uncompress.h b/arch/arm/mach-transcede/include/mach/uncompress.h
new file mode 100644
index 0000000..ac1a745
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/uncompress.h
@@ -0,0 +1,68 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef __ASM_ARCH_UNCOMPRESS_H
+#define __ASM_ARCH_UNCOMPRESS_H
+
+#include <linux/serial_reg.h>
+#include <asm/io.h>
+
+#ifdef CONFIG_TRANSCEDE_UART0_SUPPORT
+
+#define UART_BASE	TRANSCEDE_UART0
+
+#elif defined(CONFIG_TRANSCEDE_UART1_SUPPORT)
+
+#define UART_BASE	TRANSCEDE_UART1
+
+#elif defined(CONFIG_TRANSCEDE_UART2_SUPPORT)
+
+#define UART_BASE	TRANSCEDE_UART2
+
+#endif
+
+#ifdef UART_BASE
+static inline void putc(int c)
+{
+	while (!(__raw_readb(UART_BASE + UART_LSR*4) & UART_LSR_THRE))
+		barrier();
+
+	__raw_writeb(c, UART_BASE);
+}
+#else
+static inline void putc(int c)
+{
+}
+#endif
+
+static void flush(void)
+{
+}
+
+/*
+ * nothing to do
+ */
+#define arch_decomp_setup()
+
+#define arch_decomp_wdog()
+
+#endif
diff --git a/arch/arm/mach-transcede/include/mach/usb_mmap.h b/arch/arm/mach-transcede/include/mach/usb_mmap.h
new file mode 100644
index 0000000..e795217
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/usb_mmap.h
@@ -0,0 +1,1018 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+#ifndef USB_MEMORY_MAP_H
+#define USB_MEMORY_MAP_H
+
+/*  Transcede Memory Map for the USB OTG Controller IP  */
+
+/*! @brief Base Addresses of the T2200/T3300 USB OTG Controller 0 Instance's Configuration Space */
+#define USB0_CFG_BASE 0xFE820000
+
+/*! @brief Reserved Base Addresses for possible additional USB OTG Controller Instance's Configuration Space */
+#define USB1_CFG_BASE 0xFE870000
+
+/*! @brief T2200/T3300 USB Reset control register address */
+#define USB_RST_CNTRL 0xF4CF000C
+
+/* Verilog code excerpts:
+
+for reading registers:
+            14'h0000: reg_hrdata <= phy_ctrl_reg0 & reg_ahb_mask_d;
+            14'h0001: reg_hrdata <= phy_ctrl_reg1 & reg_ahb_mask_d;
+            14'h0002: reg_hrdata <= phy_ctrl_reg2 & reg_ahb_mask_d;
+            14'h0003: reg_hrdata <= {29'b0, reg_phy_status_sync2} & reg_ahb_mask_d;
+            14'h0004: reg_hrdata <= {ctrlr_config_reg[31:22], emaw_int, ctrlr_config_reg[19], ema_int, ctrlr_config_reg[15:0]} & reg_ahb_mask_d;
+            14'h0005: reg_hrdata <= internal_probes_p[31:0] & reg_ahb_mask_d;
+            14'h0006: reg_hrdata <= {1'b0, internal_probes_p[34:32], 16'h0000, internal_probes} & reg_ahb_mask_d;
+            14'h0007: reg_hrdata <= ctrlr_status & reg_ahb_mask_d;
+
+Register bit field mapping: (Reset value 0x00210000)
+
+  assign txbitstuffen       = phy_ctrl_reg0[0];   // 0
+  assign txbitstuffenh      = phy_ctrl_reg0[1];   // 0
+  assign vbusvldextsel      = phy_ctrl_reg0[2];   // 0
+  assign vbusvldext         = phy_ctrl_reg0[3];   // 0
+  assign otgdisable         = phy_ctrl_reg0[4];   // 0
+  assign vatestenb          = phy_ctrl_reg0[5];   // 0
+  assign siddq              = phy_ctrl_reg0[6];   // 0
+  assign loopbackenb        = phy_ctrl_reg0[7];   // 0
+  // Reset and Power saving
+  assign portreset          = phy_ctrl_reg0[8];   // 0
+  // Oscillator and PLL
+  assign refclkdiv          = phy_ctrl_reg0[17:16];  // 01
+  assign refclksel          = phy_ctrl_reg0[21:20];  // 10
+  assign commonn            = phy_ctrl_reg0[24];  // 0
+
+ // Analog Block   (Reset value: 0x30331441)
+ assign biastune           = phy_ctrl_reg1[0];    // 1
+ assign compdistune        = phy_ctrl_reg1[6:4];  // 100
+ assign otgtune            = phy_ctrl_reg1[10:8]; // 100
+ assign plltune            = phy_ctrl_reg1[12];   // 1
+ assign sqrxtune           = phy_ctrl_reg1[18:16];  // 011
+ assign txfslstune         = phy_ctrl_reg1[23:20];  // 0011
+ assign txpreemphasistune  = phy_ctrl_reg1[25:24];  // 00
+ assign txpreemppulsetune  = phy_ctrl_reg1[26];   // 0
+ assign txrisetune         = phy_ctrl_reg1[27];   // 0
+ assign txvreftune         = phy_ctrl_reg1[31:28];  // 0011
+
+ // Analog & Full-Speed Blocks (Reset value: 0x00000023)
+ assign txhsxvtune         = phy_ctrl_reg2[1:0];  // 11
+ assign vregtune           = phy_ctrl_reg2[2];   // 0
+ assign fsdataext          = phy_ctrl_reg2[3];   // 0
+ assign fsse0ext           = phy_ctrl_reg2[4];   // 0
+ assign txenablen          = phy_ctrl_reg2[5];   // 1
+
+ // PHY Status Signals
+ assign phy_status         = {fslsrcv, fsvplus, fsvminus};
+
+ //// Controller AHB Register ////
+ // Specifying the default value
+ // ctrlr_config_reg == 32'h0001_0000 ---> MASK = 32'h00FF_0003
+
+ // Controller Scale-down Mode
+ assign ss_scaledown_mode  = ctrlr_config_reg[1:0];  // USB Scaledown Mode = 0 (Disabled) by default!
+
+ // Digital ID Value Indication for Controller
+ assign usb_iddig_sel  = ctrlr_config_reg[8]; // Select Line to Multiplexer that selects between the PHY ID pin's digital output & sideband register value
+                                              // usb_iddig_sel = 0, by default
+ assign usb_iddig_reg  = ctrlr_config_reg[9]; // Sideband Register value to bypass the ID value detected by the PHY
+                                              // usb_iddig_reg = 0 (Controller is Host), by default
+ assign usb_utmi_iddig = usb_iddig_sel ? usb_iddig_reg : utmiotg_iddig; // Logic for the multiplexer
+                                                                        // The USB PHY's digital ID output (utmiotg_iddig) is selected by default
+
+ // Memory (Controller's DFIFO) EMA Ports
+ assign ema  = ema_int ; // Memory's EMA = 3'b001 & EMAW = 2'b00 by default!
+ assign emaw = emaw_int;
+
+ assign ema_int  = (scan_mode || bist_mode) ? 3'b001 : ctrlr_config_reg[18:16]; // Memory's EMA = 3'b001 & EMAW = 2'b00 by default!
+ assign emaw_int = (scan_mode || bist_mode) ? 2'b00  : ctrlr_config_reg[21:20];
+
+ // Controller Status Signals
+ assign ctrlr_status = {8'h00, 3'b000, dev_hird_vld_tgl, dev_hird_rcvd, 2'b00, sof_sent_rcvd_tgl, sof_toggle_out, 2'b00, int_dma_done, int_dma_req, 3'b000, chep_last_trans, chep_number};
+
+
+defaults set:
+      phy_ctrl_reg0 <= 32'h0021_0000;
+      phy_ctrl_reg1 <= 32'h3033_1441;
+      phy_ctrl_reg2 <= 32'h0000_0023;
+      ctrlr_config_reg <= 32'h0001_0000; // USB Scaledown Mode = 0, usb_iddig_sel = 0, usb_iddig_reg = 0, EMA = 3'b001 & EMAW = 2'b00, set by default
+      reg_hrdata <= 32'h00000000;
+ */
+
+/*! @brief T2200/T3300 USB controller #0 PHY control register address */
+#define USB0_MISC_REG_BASE        0xFE860000
+#define USB0_PHY_CTRL_REG0        (USB0_MISC_REG_BASE  + 0x00)
+#define USB0_PHY_CTRL_REG1        (USB0_MISC_REG_BASE  + 0x04)
+#define USB0_PHY_CTRL_REG2        (USB0_MISC_REG_BASE  + 0x08)
+#define USB0_PHY_STATUS_REG       (USB0_MISC_REG_BASE  + 0x0C)
+#define USB0_CTRLR_CONFIG_REG     (USB0_MISC_REG_BASE  + 0x10)
+#define USB0_INTERNAL_PROBES_REG0 (USB0_MISC_REG_BASE  + 0x14)
+#define USB0_INTERNAL_PROBES_REG1 (USB0_MISC_REG_BASE  + 0x18)
+#define USB0_CTRLR_STATUS_REG     (USB0_MISC_REG_BASE  + 0x1C)
+
+/*! @brief USB control register 0: Low-Byte Transmit Bit-Stuffing Enable
+
+Function: This controller signal controls bit stuffing on DATAIN[7:0] when
+OPMODE[1:0] = 2'b11.
+  - 1: Bit stuffing is enabled.
+  - 0: Bit stuffing is disabled.
+
+Reset Value:0
+*/
+#define USB_CTRL_TXBITSTUFFEN  (1<< 0)
+
+/*! @brief USB PHY control register 0: High-Byte Transmit Bit-Stuffing Enable
+
+Function: This controller signal controls bit stuffing on DATAINH[7:0] when
+OPMODE[1:0] = 2'b11.
+  - 1: Bit stuffing is enabled.
+  - 0: Bit stuffing is disabled.
+
+Reset Value:0
+*/
+#define USB_CTRL_TXBITSTUFFENH (1<< 1)
+
+/*! @brief USB PHY control register 0: External VBUS Valid Select
+
+Function: This signal selects the VBUSVLDEXT input or the internal Session
+Valid comparator to indicate when the VBUS signal on the USB cable is valid.
+  - 1: The VBUSVLDEXT input is used.
+  - 0: The internal Session Valid comparator is used.
+
+This signal is a strapping option that must be set prior to a power-on reset and
+remain static during normal operation.
+
+Reset Value:0
+*/
+#define USB_CTRL_VBUSVLDEXTSEL (1<< 2)
+
+/*! @brief USB PHY control register 0: External VBUS Valid Indicator
+
+Function: This signal is valid in Device mode and only when the VBUSVLDEXTSEL
+signal is set to 1'b1. VBUSVLDEXT indicates whether the VBUS signal on the USB
+cable is valid. In addition, VBUSVLDEXT enables the pull-up resistor on the D+ line.
+  - 1: The VBUS signal is valid, and the pull-up resistor on D+ is enabled.
+  - 0: The VBUS signal is not valid, and the pull-up resistor on D+ is disabled.
+
+In Host mode, this input is not used and can be tied to 1'b0.
+
+Reset Value:0
+*/
+#define USB_CTRL_VBUSVLDEXT    (1<< 3)
+
+/*! @brief USB PHY control register 0: OTG Block Disable
+
+Function: This controller signal powers down the OTG block, which disables the
+VBUS Valid and Session End comparators. The Session Valid comparator (the
+output of which is used to enable the pull-up resistor on DP in Device mode) is
+always on irrespective of the state of OTGDISABLE. If the application does not
+use the OTG function, this input can be set high to save power.
+  - 1: The OTG block is powered down.
+  - 0: The OTG block is powered up.
+
+Reset Value:0
+*/
+#define USB_CTRL_OTGDISABLE    (1<< 4)
+
+/*! @brief USB PHY control register 0: ANALOGTEST Pin Enable
+
+Function: This test signal controls the input/output of analog test signals on the
+ANALOGTEST pin.
+  - 1: The ANALOGTEST pin is enabled for the input and output of applicable analog test
+       signals.
+  - 0: The ANALOGTEST pin is disabled.
+
+Reset Value:0
+*/
+#define USB_CTRL_VATESTENB     (1<< 5)
+
+/*! @brief USB PHY control register 0: IDDQ Test Enable
+
+Function: This test signal enables you to perform IDDQ testing by powering down all
+analog blocks.
+  - 1: The analog blocks are powered down.
+  - 0: The analog blocks are powered up.
+
+Reset Value:0
+*/
+#define USB_CTRL_SIDDQ         (1<< 6)
+
+/*! @brief USB PHY control register 0: Loopback Test Enable
+
+Function: This controller signal places the USB 2.0 nanoPHY in Loopback mode, which
+enables the receive and transmit logic concurrently.
+  - 1: During data transmission, the receive logic is enabled.
+  - 0: During data transmission, the receive logic is disabled.
+NOTE: Loopback mode is for test purposes only; it cannot be used for normal operation.
+
+Reset Value:0
+*/
+#define USB_CTRL_LOOPBACKENB   (1<< 7)
+
+/*! @brief USB PHY control register 0: Per-Port Reset
+
+Function: When asserted, this customer-specific signal resets the corresponding port's
+transmit and receive logic without disabling the clocks within the USB 2.0 nanoPHY.
+  - 1: The transmit and receive finite state machines (FSMs) are reset, and the line_state
+       logic combinatorially reflects the state of the single-ended receivers.
+  - 0: The transmit and receive FSMs are operational, and the line_state logic becomes
+       sequential after 11 PHYCLOCK cycles.
+
+Asserting PORTRESET does not override any USB 2.0 nanoPHY inputs that normally
+control the USB state, nor does it cause any transient, illegal USB states. Within 100ns
+of asserting PORTRESET, the controller must set the inputs that control the USB to
+values that cause a safe state. A safe state for Host and Device modes is defined as
+follows:
+  - Host mode: Non-driving (OPMODE = 2'b01) with the 15k? pull-down resistors
+    enabled (DPPULLDOWN and DMPULLDOWN = 1'b1)
+  - Device mode: Non-driving (OPMODE = 2'b01) with the 1.5k? pull-up resistor
+    disabled.
+
+Reset Value:0
+*/
+#define USB_CTRL_PORTRESET     (1<< 8)
+
+/*! @brief USB PHY control register 0: Reference Clock Frequency Select
+
+Function: This bus selects the USB 2.0 nanoPHY reference clock frequency.
+  - 11: 19.2 MHz or Reserved (depending on version of the PHY)
+  - 10: 48 MHz
+  - 01: 24 MHz
+  - 00: 12 MHz
+
+This bus is a strapping option that must be set prior to a power-on reset and remain
+static during normal operation.
+
+NOTE: When running external crystal option, only 12 MHz is supported, other frequencies are not.
+
+Reset Value: binaty 01 (24 MHz)
+*/
+#define USB_CTRL_REFCLKDIV(x)  (x<<16)
+
+/*! @brief USB PHY control register 0: Reference Clock Select for PLL Block
+
+Function: This signal selects the reference clock source for the PLL block.
+  - 11: The PLL uses CLKCORE as reference.
+  - 10: The PLL uses CLKCORE as reference.
+  - 01: The XO block uses an external, 2.5 V clock supplied on the XO pin.
+  - 00: The XO block uses the clock from a crystal.
+
+This bus is a strapping option that must be set prior to a power-on reset and remain
+static during normal operation.
+
+NOTE: When running external crystal option, only 12 MHz is supported, other frequencies are not.
+
+Reset Value: binary 10 (The PLL uses CLKCORE as reference).
+
+NOTE: In Transcede, CLKCORE is from the Internal System Timing frequency.
+On EVMs, this is currently 25 MHz, so on EVMs as built
+by default, using CLKCORE is not supported for USB unless the
+external oscillator is changed to one of the USB supported frequencies.)
+*/
+#define USB_CTRL_REFCLKSEL(x)  (x<<20)
+
+/*! @brief USB PHY control register 0: Common Block Power-Down Control
+
+Function: This signal controls the power-down signals in the XO, Bias, and PLL
+blocks when the USB 2.0 nanoPHY is in Suspend or Sleep mode.
+  - 1: In Suspend mode, the XO, Bias, and PLL blocks are powered down. In Sleep
+       mode, the Bias and PLL blocks are powered down.
+  - 0: In Suspend mode, the XO, Bias, and PLL blocks remain powered in Suspend
+       mode. In Sleep mode, if the reference clock is a crystal, the XO block remains
+       powered.
+
+This signal is a strapping option that must be set prior to a power-on reset and remain
+static during normal operation. Strapping options are not critical for STA, and any
+other timings or loading limits for the pin are specified in the .lib timing model included
+in the product deliverables.
+
+NOTES:
+  - If COMMONONN is set low, CLK48MOHCI and CLK480M remain available in
+    Suspend mode.
+  - If the reference clock source is a crystal, CLK12MOHCI remains available in
+    Suspend mode, only if COMMONONN is set to 1'b0.
+  - If the reference clock source is either an external clock (connected to XO) or
+    CLKCORE, CLK12MOHCI will continue to pulse in Suspend mode, even when
+    COMMONONN is set to 1'b1.
+  - In Sleep mode, CLK48MOHCI and CLK12MOHCI are always available, irrespective
+    of COMMONONN.
+*/
+#define USB_CTRL_COMMONONN     (1<<24)
+
+
+/*
+ // Analog Block   (Reset value: 0x30331441)
+
+ TUNING PINS
+ BIASTUNE I n/a Tie to High
+ PLLTUNE  I n/a Tie to Low
+
+ assign biastune           = phy_ctrl_reg1[0];     // 1  <== OK
+ assign compdistune        = phy_ctrl_reg1[ 6:4 ]; // 100
+ assign otgtune            = phy_ctrl_reg1[10:8 ]; // 100
+ assign plltune            = phy_ctrl_reg1[12];    // 1  <== DEFAULT SHOULD BE LOW, NOT HIGH
+ assign sqrxtune           = phy_ctrl_reg1[18:16]; // 011
+ assign txfslstune         = phy_ctrl_reg1[23:20]; // 0011
+ assign txpreemphasistune  = phy_ctrl_reg1[25:24]; // 00
+ assign txpreemppulsetune  = phy_ctrl_reg1[26];    // 0
+ assign txrisetune         = phy_ctrl_reg1[27];    // 0
+ assign txvreftune         = phy_ctrl_reg1[31:28]; // 0011
+ */
+
+/*! @brief Parameter override: BIASTUNE, Input, Bandgap Circuit Adjustment
+
+Function: Reserved
+Tie this signal to the digital power supply (DVDD).
+*/
+#define USB_CTRL_BIASTUNE               (1 <<  0)
+
+/*! @brief Parameter override: COMPDISTUNE[2:0] Input, Disconnect Threshold Adjustment
+Function: This bus adjusts the voltage level for the threshold used to
+detect a disconnect event at the host.
+  -  111: + 4.5%
+  -  110: + 3%
+  -  101: + 1.5%
+  -  100: Design default
+  -  011:  1.5%
+  -  010:  3%
+  -  001:  4.5%
+  -  000:  6%
+If this bus is not used, leave it at the default setting.
+*/
+#define USB_CTRL_COMPDISTUNE(x)         (x <<  4)
+
+/*! @brief Parameter override: OTGTUNE[2:0], Input, VBUS Valid Threshold Adjustment
+
+Function: This bus adjusts the voltage level for the VBUS Valid threshold.
+  -  111: + 9%
+  -  110: + 6%
+  -  101: + 3%
+  -  100: Design default
+  -  011:  3%
+  -  010:  6%
+  -  001:  9%
+  -  000:  12%
+If this bus is not used, leave it at the default setting.
+For information about using this bus, see OTGTUNE on page 102.
+*/
+#define USB_CTRL_OTGTUNE(x)             (x <<  8)
+
+/*! @brief Parameter override: PLLTUNE I PLL Voltage Reference Select
+
+Function: Reserved
+Tie this signal to the ground supply (VSSA).
+*/
+#define USB_CTRL_PLLTUNE                (1 << 12)
+
+/*! @brief Parameter override: SQRXTUNE[2:0], Input, Squelch Threshold Adjustment
+Function: This bus adjusts the voltage level for the threshold used to
+detect valid high-speed data.
+  -  111:  20%
+  -  110:  15%
+  -  101:  10%
+  -  100:  5%
+  -  011: Design default
+  -  010: + 5%
+  -  001: + 10%
+  -  000: + 15%
+*/
+#define USB_CTRL_SQRXTUNE(x)            (x << 16)
+
+/*! @brief Parameter override: TXFSLSTUNE[3:0] Input, FS/LS Source Impedance Adjustment
+
+Function: This bus adjusts the low- and full-speed single-ended source
+impedance while driving high. The following adjustment values are based
+on nominal process, voltage, and temperature.
+  -  1111:  5%
+  -  0111:  2.5%
+  -  0011: Design default
+  -  0001: + 2.5%
+  -  0000: + 5%
+  - All other bit settings are reserved.
+*/
+#define USB_CTRL_TXFSLTUNE(x)           (x << 20)
+
+/*! @brief Parameter override: TXPREEMPHASISTUNE[1:0], Input HS Transmitter Pre-Emphasis Current Control
+
+Function: This signal controls the amount of current sourced to DP and
+DM after a J-to-K or K-to-J transition. The HS Transmitter pre-emphasis
+current is defined in terms of unit amounts. One unit amount is
+approximately 1.4 mA and is defined as 1X pre-emphasis current.
+  -  11: HS Transmitter pre-emphasis circuit sources 3X pre-emphasis
+current.
+  -  10: HS Transmitter pre-emphasis circuit sources 2X pre-emphasis
+current.
+  -  01: HS Transmitter pre-emphasis circuit sources 1X pre-emphasis
+current.
+  -  00 (design default): HS Transmitter pre-emphasis is disabled.
+
+If these signals are not used, set them to 2b00.
+*/
+#define USB_CTRL_TXPREEMPHASISTUNE(x)   (x << 24)
+
+/*! @brief Parameter override: TXPREEMPPULSETUNE, Input HS Transmitter Pre-Emphasis Duration Control
+
+Function: This signal controls the duration for which the HS pre-emphasis
+current is sourced onto DP or DM. This signal is valid only if either
+TXPREEMPHASISTUNE[1] or TXPREEMPHASISTUNE[0] is set to 1'b1.
+  -  1: Short pre-emphasis current duration
+  -  0 (design default): Long pre-emphasis current duration
+
+If TXPREEMPPULSETUNE is not used, set it to 1b0.
+*/
+#define USB_CTRL_TXPREEMPPULSETUNE      (1 << 26)
+
+/*! @brief Parameter override: TXRISETUNE, Input, HS Transmitter Rise/Fall Time Adjustment
+
+Function: This bus adjusts the rise/fall times of the high-speed waveform.
+  -  1:  8%
+  -  0: Design default
+*/
+#define USB_CTRL_RXRISETIME             (1 << 27)
+
+/*! @brief Parameter override: TXVREFTUNE[3:0], Input, HS DC Voltage Level Adjustment
+
+Function: This bus adjusts the high-speed DC level voltage.
+  -  1111: + 24%
+  -  1110: + 22%
+  -  1101: + 20%
+  -  1100: + 18%
+  -  1011: + 16%
+  -  1010: + 14%
+  -  1001: + 12%
+  -  1000: + 10%
+  -  0111: + 8%
+  -  0110: + 6%
+  -  0101: + 4%
+  -  0100: + 2%
+  -  0011: Design default
+  -  0010:  2%
+  -  0001:  4%
+  -  0000:  6%
+*/
+#define USB_CTRL_TXVREFTUNE(x)          (x << 28)
+
+
+/*
+ // Analog & Full-Speed Blocks (Reset value: 0x00000023)
+ assign txhsxvtune         = phy_ctrl_reg2[1:0]; // 11
+ assign vregtune           = phy_ctrl_reg2[2];   // 0  <== OK
+
+ FULL SPEED pins, not used, not supported, set them to
+ Inactive:
+ FSDATAEXT - 0
+ FSSE0EXT  - 0
+ TXENABLEN - 1
+
+ assign fsdataext          = phy_ctrl_reg2[3];   // 0
+ assign fsse0ext           = phy_ctrl_reg2[4];   // 0
+ assign txenablen          = phy_ctrl_reg2[5];   // 1
+ */
+/*! @brief Parameter override: TXHSXVTUNE[1:0], Input, Transmitter High-Speed Crossover Adjustment
+
+Function: This bus adjusts the voltage at which the DP and DM signals
+cross while transmitting in HS mode.
+  -  11: Default setting
+  -  10: + 15 mV
+  -  01:  15 mV
+  -  00: Reserved
+*/
+#define USB_CTRL_HSXVTUNE(x)    (x << 0)
+
+/*! @brief Parameter override: VREGTUNE, Input 1.8-V Voltage Regulator HS Boost Adjustment
+
+Function: Reserved
+Tie this signal to the ground supply (VSSA).
+*/
+#define USB_CTRL_VREGTUNE       (1 << 2)
+
+/*! @brief FS Serial interface signal: FSDATAEXT, Input, USB 1.1 Transmit Data
+
+Function: This controller signal sets the USB to either a J or K state. This signal is
+valid only if FSXCVROWNER is set to 1b1, TXENABLEN is set to 1b0, and
+FSSE0EXT is set to 1b0.
+  -  1: The D+ and D lines are driven to a Differential 1.
+  -  0: The D+ and D lines are driven to a Differential 0.
+
+FSDATAEXT is not used, set it to 1b0.
+*/
+#define USB_CTRL_FSDATAEXT      (1 << 3)
+
+/*! @brief FS Serial interface: FSSE0EXT, Input, USB 1.1 SE0 Generation
+
+Function: This controller signal sets the USB to an SE0 state. This signal is valid
+only if FSXCVROWNER is set to 1b1 and TXENABLEN is set to 1b0.
+  -  1: The D+ and D lines are driven to an SE0 state.
+  -  0: The D+ and D line states are determined by FSDATAEXT.
+FSSE0EXT is not used, set it to 1'b0.
+*/
+#define USB_CTRL_FSSE0EXT       (1 << 4)
+
+/*! @brief FS Serial interface: TXENABLEN, Input USB 1.1 Data Enable
+
+Function: This controller signal enables the FSDATAEXT and FSSE0EXT inputs.
+TXENABLEN is valid only when the FSXCVROWNER signal is set to 1b1.
+  -  1: FSDATAEXT and FSSE0EXT are disabled.
+  -  0: FSDATAEXT and FSSE0EXT are enabled.
+
+TXENABLEN is not used, set it to 1'b1.
+*/
+#define USB_CTRL_TXENABLEN      (1 << 5)
+
+/*! @brief T2200/T3300 USB controller Misc. control including scaledown value */
+#define USB_PHY_SCALEDOWN_ADDR   USB0_CTRLR_CONFIG_REG
+
+/*! @brief T2200/T3300 AXI bus reset control used for USB AXI bus reset control (among other bits) */
+#define USB_AXI_RST_CTRL             0xF4CF0058
+
+/*! @brief T2200/T3300 AXI bus clock control used for USB AXI bus reset control (among other bits) */
+#define USB_AXI_CLK_CTRL             0xF4CF0048
+
+
+// Define the Addresses within the Configuration Space
+
+/*! @brief Offset 0x000: Global OTG control register offset */
+#define GOTGCTL_Offset    0x000
+
+/*! @brief Offset 0x004: Global OTG interrupt register offset */
+#define GOTGINT_Offset    0x004
+
+/*! @brief Offset 0x008: Global AHB bus configuration register offset
+ *
+ * NOTE: In T2200/T3300, AHB is mapped/bridged to internal AXI bus
+ */
+#define GAHBCFG_Offset    0x008
+
+/*! @brief Offset 0x00C: Global USB configuration register */
+#define GUSBCFG_Offset    0x00C
+
+/*! @brief Offset 0x014: Global Interrupt Status register */
+#define GINTSTS_Offset    0x014
+
+/*! @brief Offset 0x018: Global Interrupt Mask register offset */
+#define GINTMSK_Offset    0x018
+
+/*! @brief Offset 0x024: Global Receive Frame Size register */
+#define GRXFSIZ_Offset    0x024
+
+/*! @brief Offset 0x028: Global Transmit Frame Size register */
+#define GNPTXFSIZ_Offset  0x028
+
+/*! @brief Offset 0x03C: User ID register */
+#define GUID_Offset       0x03C
+
+/*! @brief Offset 0x040: Synposys ID register */
+#define GSNPSID_Offset    0x040
+
+/*! @brief Offset 0x044 Global Hardware configuration register #1 */
+#define GHWCFG1_Offset    0x044
+
+/*! @brief Offset 0x048 Global Hardware configuration register #2 */
+#define GHWCFG2_Offset    0x048
+
+/*! @brief Offset 0x04C Global Hardware configuration register #3 */
+#define GHWCFG3_Offset    0x04C
+
+/*! @brief Offset 0x044 Global Hardware configuration register #4 */
+#define GHWCFG4_Offset    0x050
+
+/*! @brief Offset 0x100: Bi-directional Host port 0 Transmit Frame Size register */
+#define HPTXFSIZ_Offset   0x100
+
+/*! @brief Offset 0x104: Device IN Endpoint 1 Transmit FIFO Size Register */
+#define DIEPTXF1_Offset   0x104
+
+/*! @brief Offset 0x108: Device IN Endpoint 2 Transmit FIFO Size Register */
+#define DIEPTXF2_Offset   0x108
+
+/*! @brief Offset 0x10C: Device IN Endpoint 3 Transmit FIFO Size Register */
+#define DIEPTXF3_Offset   0x10C
+
+/*! @brief Offset 0x110: Device IN Endpoint 4 Transmit FIFO Size Register */
+#define DIEPTXF4_Offset   0x110
+
+/*! @brief Offset 0x114: Device IN Endpoint 5 Transmit FIFO Size Register */
+#define DIEPTXF5_Offset   0x114
+
+/*! @brief Offset 0x118: Device IN Endpoint 6 Transmit FIFO Size Register */
+#define DIEPTXF6_Offset   0x118
+
+/*! @brief Offset 0x400: Host configuration register */
+#define HCFG_Offset       0x400
+
+/*! @brief Offset 0x404: Host Frame Interval Register (HFIR) */
+#define HFIR_Offset       0x404
+
+/*! @brief Offset 0x408: Host Frame Number (HFNUM) register */
+#define HFNUM_Offset      0x408
+
+/*! @brief Offset 0x418: Host All Interrupts Mask (HAINTMSK) Register */
+#define HAINTMSK_Offset   0x418
+
+/*! @brief Offset 0x440: Host Port Control & Status Register (HPRT) */
+#define HPRT_Offset       0x440
+
+/*! @brief Offset 0x500: Host Channel 0 Characteristics Register (HCCHAR0) */
+#define HCCHAR0_Offset    0x500
+
+/*! @brief Offset 0x508: Host Channel 0 Interrupt Register (HCINT0)*/
+#define HCINT0_Offset     0x508
+
+/*! @brief Offset 0x50C: Host Channel 0 Characteristics Register (HCCHAR0) */
+#define HCINTMSK0_Offset  0x50C
+
+/*! @brief Offset 0x510: Host Channel-n Transfer Size Register (HCTSIZ0) */
+#define HCTSIZ0_Offset    0x510
+
+/*! @brief Offset 0x513: */
+#define HCDMA0_Offset     0x514
+
+/*! @brief Host Channel-n Characteristics Register (HCCHARn) (Datasheet section 5.3.4.8)
+ *
+ * Channel_number: 0 <= n <= 15
+ * Offset: 0x500 + (Channel_number * 0x20)
+ */
+#define HCCHARn_Offset(Channel_number)  (HCCHAR0_Offset + (ChannelNumber * 0x20))
+
+/*! @brief Host Channel-n Interrupt Register (HCINTn) (Datasheet section 5.3.4.10)
+ *
+ * Channel_number: 0 <= n <= 15
+ *
+ * Offset: 0x508 + (Channel_number * 0x20)
+ */
+#define HCNINTn_Offset(Channel_number)  (HCINT0_Offset + (ChannelNumber * 0x20))
+
+/*! @brief Host Channel-n Interrupt Mask Register (HCINTMSKn) (Datasheet section 5.3.4.11)
+ *
+ * Channel_number: 0 <= n <= 15
+ *
+ * Offset: 0x50C + (Channel_number * 0x20)
+ */
+#define HCNINTMSKn_Offset(Channel_number)  (HCINTMSK0_Offset + (ChannelNumber * 0x20))
+
+/*! @brief Host Channel-n Transfer Size Register (HCTSIZn) (Datasheet section 5.3.4.12)
+ *
+ * Channel_number: 0 <= n <=  15
+ *
+ * Offset: 0x510 + (Channel_number * 0x20)
+ */
+#define HCTSIZn_Offset(Channel_number)  (HCTSIZ0_Offset + (ChannelNumber * 0x20))
+
+/*! @brief Host Channel-n DMA Address Register (HCDMAn) (Datasheet section 5.3.4.13)
+ *
+ * Channel_number: 0 <= n <= 15
+ *
+ * Offset: 0x514 + (Channel_number * 0x20)
+ *
+ * This register is used by the OTG host in the internal DMA mode to maintain the current buffer pointer for
+ * IN/OUT transactions. The starting DMA address must be DWORD-aligned.
+ */
+#define HCDMA_Offset(Channel_number)  (HCDMA0_Offset + (ChannelNumber * 0x20))
+
+
+/*! @brief Offset 0x520: Host Channel 1 Characteristics Register (HCCHAR1) */
+#define HCCHAR1_Offset    0x520
+#define HCINT1_Offset     0x528
+#define HCINTMSK1_Offset  0x52C
+#define HCTSIZ1_Offset    0x530
+#define HCDMA1_Offset     0x534
+
+/*! @brief Offset 0x540: Host Channel 2 Characteristics Register (HCCHAR2) */
+#define HCCHAR2_Offset    0x540
+#define HCINT2_Offset     0x548
+#define HCINTMSK2_Offset  0x54C
+#define HCTSIZ2_Offset    0x550
+#define HCDMA2_Offset     0x554
+
+/*! @brief Offset 0x560: Host Channel 3 Characteristics Register (HCCHAR3) */
+#define HCCHAR3_Offset    0x560
+#define HCINT3_Offset     0x568
+#define HCINTMSK3_Offset  0x56C
+#define HCTSIZ3_Offset    0x570
+#define HCDMA3_Offset     0x574
+
+/*! @brief Offset 0x580: Host Channel 4 Characteristics Register (HCCHAR4) */
+#define HCCHAR4_Offset    0x580
+#define HCINT4_Offset     0x588
+#define HCINTMSK4_Offset  0x58C
+#define HCTSIZ4_Offset    0x590
+#define HCDMA4_Offset     0x594
+
+/*! @brief Offset 0x5A0: Host Channel 5 Characteristics Register (HCCHAR5) */
+#define HCCHAR5_Offset    0x5A0
+#define HCINT5_Offset     0x5A8
+#define HCINTMSK5_Offset  0x5AC
+#define HCTSIZ5_Offset    0x5B0
+#define HCDMA5_Offset     0x5B4
+
+/*! @brief Offset 0x5C0: Host Channel 6 Characteristics Register (HCCHAR6) */
+#define HCCHAR6_Offset    0x5C0
+#define HCINT6_Offset     0x5C8
+#define HCINTMSK6_Offset  0x5CC
+#define HCTSIZ6_Offset    0x5D0
+#define HCDMA6_Offset     0x5D4
+
+/*! @brief Offset 0x5E0: Host Channel 7 Characteristics Register (HCCHAR7) */
+#define HCCHAR7_Offset    0x5E0
+#define HCINT7_Offset     0x5E8
+#define HCINTMSK7_Offset  0x5EC
+#define HCTSIZ7_Offset    0x5F0
+#define HCDMA7_Offset     0x5F4
+
+/*! @brief Offset 0x600: Host Channel 8 Characteristics Register (HCCHAR8) */
+#define HCCHAR8_Offset    0x600
+#define HCINT8_Offset     0x608
+#define HCINTMSK8_Offset  0x60C
+#define HCTSIZ8_Offset    0x610
+#define HCDMA8_Offset     0x614
+
+/*! @brief Offset 0x620: Host Channel 9 Characteristics Register (HCCHAR9) */
+#define HCCHAR9_Offset    0x620
+#define HCINT9_Offset     0x628
+#define HCINTMSK9_Offset  0x62C
+#define HCTSIZ9_Offset    0x630
+#define HCDMA9_Offset     0x634
+
+/*! @brief Offset 0x640: Host Channel 10 Characteristics Register (HCCHAR10) */
+#define HCCHAR10_Offset   0x640
+#define HCINT10_Offset    0x648
+#define HCINTMSK10_Offset 0x64C
+#define HCTSIZ10_Offset   0x650
+#define HCDMA10_Offset    0x654
+
+/*! @brief Offset 0x660: Host Channel 11 Characteristics Register (HCCHAR11) */
+#define HCCHAR11_Offset   0x660
+#define HCINT11_Offset    0x668
+#define HCINTMSK11_Offset 0x66C
+#define HCTSIZ11_Offset   0x670
+#define HCDMA11_Offset    0x674
+
+/*! @brief Offset 0x680: Host Channel 12 Characteristics Register (HCCHAR12) */
+#define HCCHAR12_Offset   0x680
+#define HCINT12_Offset    0x688
+#define HCINTMSK12_Offset 0x68C
+#define HCTSIZ12_Offset   0x690
+#define HCDMA12_Offset    0x694
+
+/*! @brief Offset 0x6A0: Host Channel 13 Characteristics Register (HCCHAR13) */
+#define HCCHAR13_Offset   0x6A0
+#define HCINT13_Offset    0x6A8
+#define HCINTMSK13_Offset 0x6AC
+#define HCTSIZ13_Offset   0x6B0
+#define HCDMA13_Offset    0x6B4
+
+/*! @brief Offset 0x800: Device configuration register */
+#define DCFG_Offset       0x800
+
+/*! @brief Offset 0x804: Device control register */
+#define DCTL_Offset       0x804
+
+/*! @brief Offset 0x804: Device status register */
+#define DSTS_Offset       0x808
+
+/*! @brief Offset 0x810: Device Input Endpoint mask register */
+#define DIEPMSK_Offset    0x810
+
+/*! @brief Offset 0x814: Device Output Endpoint mask register */
+#define DOEPMSK_Offset    0x814
+
+/*! @brief Offset 0x818: Device All Endpoints Interrupt Register (DAINT) */
+#define DAINT_Offset      0x818
+
+/*! @brief Offset 0x81C: Device All Endpoints Interrupt Mask Register (DAINTMSK) */
+#define DAINTMSK_Offset   0x81C
+
+
+#define DIEPCTL0_Offset   0x900
+#define DIEPINT0_Offset   0x908
+#define DIEPTSIZ0_Offset  0x910
+#define DIEPDMA0_Offset   0x914
+#define DTXFSTS0_Offset   0x918
+
+/*! @brief Device Endpoint-n (1 through 15) Control Register (DIEPCTLn/DOEPCTLn) Datasheet section 5.3.5.22
+ *
+ * Endpoint_number: 1 <= n <= 15
+ *
+ * Offset for IN endpoints: 0x900 + (Endpoint_number * 0x20)
+ *
+ * Offset for OUT endpoints: B00h + (Endpoint_number * 20h)
+ *
+ * The application uses this register to control the behavior of each logical endpoint other than endpoint 0.
+ */
+#define DIEPCTLn_Offset(Endpoint_number) (DIEPCTL0_Offset + (Endpoint_number * 0x20))
+
+/*! @brief Device Endpoint-n Interrupt Register (DIEPINTn/DOEPINTn) Datasheet section 5.3.5.23
+ *
+ * Endpoint_number: 0 <= n <= 15
+ *
+ * Offset for IN endpoints: 0x908 + (Endpoint_number * 0x20)
+ *
+ * Offset for OUT endpoints: 0xB08 + (Endpoint_number * 0x20)
+ *
+ * This register indicates the status of an endpoint with respect to USB- and AHB-related events.
+ * The application must read this register when the OUT Endpoints Interrupt bit or IN Endpoints
+ * Interrupt bit of the Core Interrupt register (GINTSTS.OEPInt or GINTSTS.IEPInt, respectively) is set. Before
+ * the application can read this register, it must first read the Device All Endpoints Interrupt (DAINT) register
+ * to get the exact endpoint number for the Device Endpoint-n Interrupt register. The application must clear
+ * the appropriate bit in this register to clear the corresponding bits in the DAINT and GINTSTS registers.
+ */
+#define DIEPINTn_Offset(Endpoint_number) (DIEPINT0_Offset + (Endpoint_number * 0x20))
+
+/*! @brief 5.3.5.25Device Endpoint-n Transfer Size Register (DIEPTSIZn/DOEPTSIZn)
+ *
+ * Endpoint_number: 1 <= n <=  15
+ *
+ * Offset for IN endpoints: 0x910 + (Endpoint_number * 0x20)
+ *
+ * Offset for OUT endpoints: 0xB10 + (Endpoint_number * 0x20)
+ *
+ * The application must modify this register before enabling the endpoint. Once the endpoint is enabled using
+ * Endpoint Enable bit of the Device Endpoint-n Control registers (DIEPCTLn.EPEna/DOEPCTLn.EPEna), the
+ * core modifies this register. The application can only read this register once the core has cleared the Endpoint
+ * Enable bit.
+ */
+#define DIEPTSIZn_Offset(Endpoint_number) (DIEPTSIZ0_Offset + (Endpoint_number * 0x20))
+
+/*! @brief Device Endpoint-n DMA Address Register (DIEPDMAn/DOEPDMAn) (Datasheet section 5.3.5.26)
+ *
+ * Endpoint_number: 0 <= n <=  15
+ * Offset for IN endpoints: 0x914 + (Endpoint_number * 0x20)
+ *
+ * Offset for OUT endpoints: 0xB14h + (Endpoint_number * 0x20)
+ *
+ * This register holds the start address of the external memory for storing or fetching endpoint
+ * data.
+ *
+ * Note: For control endpoints, this field stores control OUT data packets as well as
+ * SETUP transaction data packets. When more than three SETUP packets are
+ * received back-to-back, the SETUP data packet in the memory is overwritten.
+ *
+ * This register is incremented on every AHB transaction. The application can give
+ * only a DWORD-aligned address.
+ *   - When Scatter/Gather DMA mode is not enabled, the application programs the
+ *     start address value in this field.
+ *   - When Scatter/Gather DMA mode is enabled, this field indicates the base
+ *     pointer for the descriptor list.
+ */
+#define DIEPDMAn_Offset(Endpoint_number) (DIEPDMA0_Offset + (Endpoint_number * 0x20))
+
+#define DIEPCTL1_Offset   0x920
+#define DIEPINT1_Offset   0x928
+#define DIEPTSIZ1_Offset  0x930
+#define DIEPDMA1_Offset   0x934
+#define DTXFSTS1_Offset   0x938
+
+#define DIEPCTL2_Offset   0x940
+#define DIEPINT2_Offset   0x948
+#define DIEPTSIZ2_Offset  0x950
+#define DIEPDMA2_Offset   0x954
+#define DTXFSTS2_Offset   0x958
+
+#define DIEPCTL3_Offset   0x960
+#define DIEPINT3_Offset   0x968
+#define DIEPTSIZ3_Offset  0x970
+#define DIEPDMA3_Offset   0x974
+#define DTXFSTS3_Offset   0x978
+
+#define DIEPCTL4_Offset   0x980
+#define DIEPINT4_Offset   0x988
+#define DIEPTSIZ4_Offset  0x990
+#define DIEPDMA4_Offset   0x994
+#define DTXFSTS4_Offset   0x998
+
+#define DIEPCTL5_Offset   0x9A0
+#define DIEPINT5_Offset   0x908
+#define DIEPTSIZ5_Offset  0x9B0
+#define DIEPDMA5_Offset   0x9B4
+#define DTXFSTS5_Offset   0x9B8
+
+#define DIEPCTL6_Offset   0x9C0
+#define DIEPINT6_Offset   0x9C8
+#define DIEPTSIZ6_Offset  0x9D0
+#define DIEPDMA6_Offset   0x9D4
+#define DTXFSTS6_Offset   0x9D8
+
+#define DOEPCTL0_Offset   0xB00
+#define DOEPINT0_Offset   0xB08
+#define DOEPTSIZ0_Offset  0xB10
+#define DOEPDMA0_Offset   0xB14
+
+/*! @brief Device Endpoint-n (1 through 15) Control Register (DIEPCTLn/DOEPCTLn) (Datasheet section 5.3.5.22)
+ *
+ * Endpoint_number: 1 <= n <= 15
+ *
+ * Offset for IN endpoints: 0x900 + (Endpoint_number * 0x20)
+ *
+ * Offset for OUT endpoints: B00h + (Endpoint_number * 20h)
+ *
+ * The application uses this register to control the behavior of each logical endpoint other than endpoint 0.
+ */
+#define DOEPCTLn_Offset(Endpoint_number) (DOEPCTL0_Offset + (Endpoint_number * 0x20))
+
+/*! @brief Device Endpoint-n Interrupt Register (DIEPINTn/DOEPINTn) (Datasheet section 5.3.5.23)
+ *
+ * Endpoint_number: 0 <= n <=  15
+ *
+ * Offset for IN endpoints: 0x908 + (Endpoint_number * 0x20)
+ *
+ * Offset for OUT endpoints: 0xB08 + (Endpoint_number * 0x20)
+ *
+ * This register indicates the status of an endpoint with respect to USB- and AHB-related events.
+ * The application must read this register when the OUT Endpoints Interrupt bit or IN Endpoints
+ * Interrupt bit of the Core Interrupt register (GINTSTS.OEPInt or GINTSTS.IEPInt, respectively) is set. Before
+ * the application can read this register, it must first read the Device All Endpoints Interrupt (DAINT) register
+ * to get the exact endpoint number for the Device Endpoint-n Interrupt register. The application must clear
+ * the appropriate bit in this register to clear the corresponding bits in the DAINT and GINTSTS registers.
+ */
+#define DOEPINTn_Offset(Endpoint_number) (DOEPINT0_Offset + (Endpoint_number * 0x20))
+
+/*! @brief Device Endpoint-n Transfer Size Register (DIEPTSIZn/DOEPTSIZn) (Datasheet section 5.3.5.25)
+ *
+ * Endpoint_number: 1 <= n <=  15
+ *
+ * Offset for IN endpoints: 0x910 + (Endpoint_number * 0x20)
+ *
+ * Offset for OUT endpoints: 0xB10 + (Endpoint_number * 0x20)
+ *
+ * The application must modify this register before enabling the endpoint. Once the endpoint is enabled using
+ * Endpoint Enable bit of the Device Endpoint-n Control registers (DIEPCTLn.EPEna/DOEPCTLn.EPEna), the
+ * core modifies this register. The application can only read this register once the core has cleared the Endpoint
+ * Enable bit.
+ */
+#define DOEPTSIZn_Offset(Endpoint_number) (DOEPTSIZ0_Offset + (Endpoint_number * 0x20))
+
+/*! @brief Device Endpoint-n DMA Address Register (DIEPDMAn/DOEPDMAn) (Datasheet section 5.3.5.26)
+ *
+ * Endpoint_number: 0 <= n <=  15
+ * Offset for IN endpoints: 0x914 + (Endpoint_number * 0x20)
+ *
+ * Offset for OUT endpoints: 0xB14h + (Endpoint_number * 0x20)
+ *
+ * This register holds the start address of the external memory for storing or fetching endpoint
+ * data.
+ *
+ * Note: For control endpoints, this field stores control OUT data packets as well as
+ * SETUP transaction data packets. When more than three SETUP packets are
+ * received back-to-back, the SETUP data packet in the memory is overwritten.
+ *
+ * This register is incremented on every AHB transaction. The application can give
+ * only a DWORD-aligned address.
+ *   - When Scatter/Gather DMA mode is not enabled, the application programs the
+ *     start address value in this field.
+ *   - When Scatter/Gather DMA mode is enabled, this field indicates the base
+ *     pointer for the descriptor list.
+ */
+#define DOEPDMAn_Offset(Endpoint_number) (DOEPDMA0_Offset + (Endpoint_number * 0x20))
+
+#define DOEPCTL1_Offset   0xB20
+#define DOEPINT1_Offset   0xB28
+#define DOEPTSIZ1_Offset  0xB30
+#define DOEPDMA1_Offset   0xB34
+
+#define DOEPCTL2_Offset   0xB40
+#define DOEPINT2_Offset   0xB48
+#define DOEPTSIZ2_Offset  0xB50
+#define DOEPDMA2_Offset   0xB54
+
+#define DOEPCTL3_Offset   0xB60
+#define DOEPINT3_Offset   0xB68
+#define DOEPTSIZ3_Offset  0xB70
+#define DOEPDMA3_Offset   0xB74
+
+#define DOEPCTL4_Offset   0xB80
+#define DOEPINT4_Offset   0xB88
+#define DOEPTSIZ4_Offset  0xB90
+#define DOEPDMA4_Offset   0xB94
+
+#define DOEPCTL5_Offset   0xBA0
+#define DOEPINT5_Offset   0xBA8
+#define DOEPTSIZ5_Offset  0xBB0
+#define DOEPDMA5_Offset   0xBB4
+
+#define DOEPCTL6_Offset   0xBC0
+#define DOEPINT6_Offset   0xBC8
+#define DOEPTSIZ6_Offset  0xBD0
+#define DOEPDMA6_Offset   0xBD4
+#define PCGCCTL_Offset    0xE00
+
+#endif //#ifndef USB_MEMORY_MAP_H
+
diff --git a/arch/arm/mach-transcede/include/mach/vmalloc.h b/arch/arm/mach-transcede/include/mach/vmalloc.h
new file mode 100644
index 0000000..e4bc406
--- /dev/null
+++ b/arch/arm/mach-transcede/include/mach/vmalloc.h
@@ -0,0 +1,35 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#if defined(CONFIG_MACH_M84XXX)
+
+#define VMALLOC_END       (0xFA000000)
+
+#elif defined(CONFIG_MACH_M822XX)
+
+#define VMALLOC_END       (0xF0000000)
+
+#else
+
+#error "Unsupported CPU"
+
+#endif
diff --git a/arch/arm/mach-transcede/l-arm.c b/arch/arm/mach-transcede/l-arm.c
new file mode 100644
index 0000000..54b7b8b
--- /dev/null
+++ b/arch/arm/mach-transcede/l-arm.c
@@ -0,0 +1,293 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#include <linux/cdev.h>
+#include <linux/device.h>
+#include <linux/interrupt.h>
+#include <linux/fs.h>
+#include <linux/kernel.h>
+#include <linux/poll.h>
+#include <linux/sched.h>
+#include <asm/delay.h>
+#include <asm/io.h>
+
+
+static struct class	*larm_class;
+static dev_t		larm_dev;
+static int		larm_major;
+
+
+static struct device	*icpu2_dev;
+static struct cdev	icpu2_cdev;
+static atomic_t		icpu2_refs;
+static int		icpu2_irq=-1;
+static u32		icpu2_irq_count;
+static u32		icpu2_context_phys;
+static struct icpu2_queue {
+	u32	ptr_phys;
+	u32	put;
+	u32	get;
+	u32	num;		/* number of elements in the queue */
+	u32	stat_put;	/* put counter */
+	u32	stat_get;	/* get counter */
+} *icpu2_rxq, *icpu2_rxq_priority;
+static struct icpu2_context {
+	u32	version;	/* version of I-CPU */
+	u32	status;		/* bit 0: 1 == the I-CPU is initialized */
+	u32	rx_irq;		/* inter-ARM ID (0..7) of interrupt to notify remote side */
+	u32	tx_irq;		/* inter-ARM ID (0..7) of interrupt to notify local side */
+	u32	rx_storage;	/* pointer to the array where TX blocks are located, physical address */
+	u32	tx_storage;	/* pointer to the array where RX blocks are located, physical address */
+	u32	rx_blocks_num;	/* number of blocks in TX array */
+	u32	rx_block_size;	/* size of block in bytes of TX array */
+	u32	tx_blocks_num;	/* number of blocks in RX array */
+	u32	tx_block_size;	/* size of block in bytes of RX array */
+	u32	rx_queue_num;	/* number of TX queues */
+	u32	tx_queue_num;	/* number of RX queues */
+	struct icpu2_queue	queues[6];
+} *icpu2_context;
+static DECLARE_WAIT_QUEUE_HEAD(icpu2_wait_queue);
+
+
+#define ICPU2_IOCTL_WAIT	_IO('I', 1)
+
+
+int icpu2_start(u32 context)
+{
+	int i;
+
+	if (icpu2_context)
+		iounmap(icpu2_context);
+
+	icpu2_context = ioremap_nocache(context, sizeof(*icpu2_context));
+	if (!icpu2_context)
+		return -ENXIO;
+
+	for (i = 0; i < 5*1000; i++) {
+		if (icpu2_context->status & 1)
+			break;
+		udelay(1000);
+	}
+
+	if ((icpu2_context->status & 1) == 0) {
+		iounmap(icpu2_context);
+		icpu2_context = NULL;
+		printk(KERN_ERR "ICPU2: status bit not set, aborting\n");
+		return -ENODEV;
+	}
+
+	icpu2_context_phys = context;
+
+	printk(KERN_ERR "ICPU2: v%u.%u, context at 0x%08X\n", icpu2_context->version>>8, icpu2_context->version&255, context);
+
+	icpu2_irq = icpu2_context->rx_irq;
+	icpu2_irq_count = 0;
+	icpu2_rxq_priority = &icpu2_context->queues[1];
+	icpu2_rxq = &icpu2_context->queues[2];
+
+	return 0;
+}
+EXPORT_SYMBOL(icpu2_start);
+
+int icpu2_stop(void)
+{
+	if (icpu2_context)
+		iounmap(icpu2_context);
+
+	return 0;
+}
+EXPORT_SYMBOL(icpu2_stop);
+
+static irqreturn_t icpu2_irq_handler(int irq, void *arg)
+{
+	wake_up_interruptible(&icpu2_wait_queue);
+
+/* TODO: VIVA check this for t2200 */
+#if defined(CONFIG_MACH_M84XXX)
+	writel(readl(TRANSCEDE_ARM_IRQ_CLR) | (1 << (irq - 32)), TRANSCEDE_ARM_IRQ_CLR);
+#endif
+
+	icpu2_irq_count++;
+
+	return IRQ_HANDLED;
+}
+
+static int icpu2_open(struct inode *inode, struct file *file)
+{
+	if (!icpu2_context)
+		return -ENODEV;
+
+	if (atomic_inc_return(&icpu2_refs) == 1) {
+		if (request_irq(icpu2_irq, icpu2_irq_handler, IRQF_DISABLED, "icpu2", icpu2_dev)) {
+			atomic_dec(&icpu2_refs);
+			return -ENXIO;
+		}
+	}
+
+	return 0;
+}
+
+static int icpu2_close(struct inode *inode, struct file *file)
+{
+	if (atomic_dec_return(&icpu2_refs) == 0) {
+		if (icpu2_context)
+			free_irq(icpu2_irq, icpu2_dev);
+	}
+
+	return 0;
+}
+
+static long icpu2_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	if (cmd == ICPU2_IOCTL_WAIT) {
+		if (icpu2_rxq->get == icpu2_rxq->put && icpu2_rxq_priority->get == icpu2_rxq_priority->put) {
+			if (wait_event_interruptible(icpu2_wait_queue, icpu2_rxq->get != icpu2_rxq->put || icpu2_rxq_priority->get != icpu2_rxq_priority->put))
+				return -ERESTARTSYS;
+		}
+
+		return 0;
+	}
+
+	return -EOPNOTSUPP;
+}
+
+static unsigned int icpu2_poll(struct file *file, struct poll_table_struct *wait)
+{
+	poll_wait(file, &icpu2_wait_queue, wait);
+
+	if (icpu2_rxq->get != icpu2_rxq->put || icpu2_rxq_priority->get != icpu2_rxq_priority->put)
+		return (POLLIN | POLLRDNORM);
+
+	return 0;
+}
+
+static u32 icpu2_queue_used(struct icpu2_queue *q)
+{
+	u32 free;
+
+	if (q->get > q->put)
+		free = q->get - q->put;
+	else
+		free = q->num - q->put + q->get;
+
+	return q->num - free;
+}
+
+static ssize_t icpu2_info_show(struct device *dev, struct device_attribute *attr, char *buf)
+{
+	int n = 0, i;
+
+	if (!icpu2_context)
+		return sprintf(buf, "Not available\n");
+
+	n = sprintf(buf,
+			"Version: %u.%u\n"
+			"Context: 0x%08X\n"
+			"TX/RX block sizes: %u/%u\n"
+			"TX/RX IRQ:         %u/%u, count: %u\n"
+			"TX/RX storage:     0x%08X/0x%08X\n"
+			"===\n"
+			"Queue # put get num used\tstats put/get\n",
+			icpu2_context->version>>8, icpu2_context->version&255,
+			icpu2_context_phys,
+			icpu2_context->tx_block_size, icpu2_context->rx_block_size,
+			icpu2_context->tx_irq, icpu2_context->rx_irq, icpu2_irq_count,
+			icpu2_context->tx_storage, icpu2_context->rx_storage
+	);
+
+	for (i = 0; i < ARRAY_SIZE(icpu2_context->queues); i++)
+		n += sprintf(buf+n,
+			"%d\t%3u %3u %3u %3u\t\t%10u %10u\n",
+			i,
+			icpu2_context->queues[i].put, icpu2_context->queues[i].get,
+			icpu2_context->queues[i].num,
+			icpu2_queue_used(&icpu2_context->queues[i]),
+			icpu2_context->queues[i].stat_put, icpu2_context->queues[i].stat_get);
+
+	return n;
+}
+
+struct file_operations icpu2_fops = {
+	.open		= icpu2_open,
+	.release	= icpu2_close,
+	.unlocked_ioctl	= icpu2_ioctl,
+	.poll		= icpu2_poll,
+};
+
+struct device_attribute icpu2_info = {
+	.attr = {
+		.name = "info",
+		.mode = S_IRUGO,
+	},
+	.show   = icpu2_info_show,
+	.store  = NULL,
+};
+
+static int __init larm_init(void)
+{
+	int err;
+
+	larm_class = class_create(THIS_MODULE, "l-arm");
+	if (IS_ERR(larm_class))
+		return PTR_ERR(larm_class);
+
+	err = alloc_chrdev_region(&larm_dev, 0, 1, "l-arm");
+	if (err)
+		goto err0;
+
+	larm_major = MAJOR(larm_dev);
+
+	cdev_init(&icpu2_cdev, &icpu2_fops);
+	if (cdev_add(&icpu2_cdev, larm_dev, 1))
+		goto err1;
+
+	icpu2_dev = device_create(larm_class, NULL, MKDEV(larm_major, 0), NULL, "icpu2");
+	if (IS_ERR(icpu2_dev))
+		goto err2;
+
+	if (device_create_file(icpu2_dev, &icpu2_info))
+		goto err3;
+
+	return 0;
+
+err3:
+	device_destroy(larm_class, MKDEV(larm_major, 0));
+err2:
+	unregister_chrdev_region(larm_dev, 1);
+err1:
+	cdev_del(&icpu2_cdev);
+err0:
+	class_destroy(larm_class);
+	return -1;
+}
+
+static void __exit larm_exit(void)
+{
+	device_remove_file(icpu2_dev, &icpu2_info);
+	device_destroy(larm_class, MKDEV(larm_major, 0));
+	cdev_del(&icpu2_cdev);
+	unregister_chrdev_region(larm_dev, 1);
+	class_destroy(larm_class);
+}
+
+module_init(larm_init);
+module_exit(larm_exit);
diff --git a/arch/arm/mach-transcede/mips_monitor.c b/arch/arm/mach-transcede/mips_monitor.c
new file mode 100644
index 0000000..95f1883
--- /dev/null
+++ b/arch/arm/mach-transcede/mips_monitor.c
@@ -0,0 +1,366 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+
+//-------------------------------------------------------------------------------------------
+
+#include <linux/module.h>
+#include <linux/version.h>
+#include <linux/kernel.h>
+#include <linux/hrtimer.h>
+#include <linux/device.h>
+#include <linux/fs.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+
+#include <mach/transcede-2200.h>
+#include <mach/mips_monitor.h>
+#include <mach/sysheap.h>
+
+#ifdef CONFIG_TRANSCEDE_MLOG
+#include <mach/mlog.h>
+#include <mach/tcb.h>
+#endif
+
+#define DEVICE_NAME		"mmonitor"
+#define MMONITOR_IOCTL_INTERVAL	_IOW( 'r', 1, unsigned int)
+
+#define DEFAULT_INTERVAL_OF_MEASURMENTS 3000
+
+#if defined(CONFIG_MACH_M822XX)
+#define mmAlloc(size)	iram_heap_alloc(size)
+#define mmFree(ptr)	    iram_heap_free(ptr)
+#else
+#define mmAlloc(size)   kmalloc(size, GFP_KERNEL)
+#define mmFree(ptr)	    kfree(ptr)
+#endif
+
+TRANSCEDE_ATTR_SHOW(mips_stat);
+
+static idle_mips_monitor_t *mips_monitor_ctx = NULL;
+
+static int mips_monitor_major;
+static struct class *mips_monitor_class;
+static struct device *mips_monitor_device;
+
+static int     init_sysfs(struct device *dev, const char *name);
+static long    mmonitor_ioctl(struct file *file, unsigned int cmd, unsigned long arg);
+static ssize_t mmonitor_bin_read(struct file *file, struct kobject *, struct bin_attribute *, char *buf, loff_t pos, size_t size);
+static ssize_t mmonitor_read(struct file *file, const char __user *buf, size_t count, loff_t *offp);
+static ssize_t mmonitor_write(struct file *file, const char __user *buf, size_t count, loff_t *offp);
+
+static struct file_operations mips_monitor_fops[] = {
+	{
+		.owner = THIS_MODULE,
+		.read = mmonitor_read,
+		.write = mmonitor_write,
+		.unlocked_ioctl = mmonitor_ioctl,
+	}
+};
+
+struct bin_attribute mips_monitor_bin_attr[] = {
+	{
+		.attr = {
+			.name = "mmonitor.bin",
+			.mode = S_IRUGO,
+		},
+		.size = 0,
+		.read = mmonitor_bin_read,
+	}
+};
+
+unsigned int mmonitor_set_ctx(idle_mips_monitor_t* ptr)
+{
+    mips_monitor_ctx = ptr;
+    return 0;
+}
+
+idle_mips_monitor_t* mmonitor_get_ctx(void)
+{
+    if (mips_monitor_ctx)
+       if (mips_monitor_ctx->meas_interval)
+           return mips_monitor_ctx;
+
+    return NULL;
+}
+
+unsigned int mips_monitor_idle_update(unsigned int start, unsigned int endtime, unsigned int cpu)
+{
+    idle_mips_monitor_t* mmonitor = mmonitor_get_ctx();
+
+    if (mmonitor){
+        mmonitor->accum_idle_time_ticks[cpu] += MMTicksDiff(endtime, start);
+    }
+
+	return 0;
+}
+EXPORT_SYMBOL(mips_monitor_idle_update);
+
+
+unsigned int mips_monitor_tick_timer_update(unsigned int time, unsigned int cpu)
+{
+    idle_mips_monitor_t* mmonitor = mmonitor_get_ctx();
+
+    if (mmonitor){
+        mmonitor->tick_period[cpu] = MMTicksDiff(time, mmonitor->tick_start[cpu]);
+        mmonitor->tick_start[cpu] = time;
+
+        mmonitor->total_time_ticks[cpu] += mmonitor->tick_period[cpu];
+
+        if (--mmonitor->meas_counter[cpu] == 0) {
+            if (mmonitor->currnt_idle_start[cpu]){
+                mmonitor->accum_idle_time_ticks[cpu] += MMTicksDiff(time,mmonitor->currnt_idle_start[cpu]);
+                mmonitor->currnt_idle_start[cpu] = time;
+            }
+
+            mmonitor->last_ratio[cpu] = (100 * (mmonitor->accum_idle_time_ticks[cpu] >> 6)) / (mmonitor->total_time_ticks[cpu] >> 6);
+            mmonitor->last_avg_ratio[cpu] = (mmonitor->last_avg_ratio[cpu] >> 1) + (mmonitor->last_ratio[cpu] >> 1);
+            mmonitor->last_total_time[cpu] = mmonitor->total_time_ticks[cpu];
+
+#ifdef CONFIG_TRANSCEDE_MLOG
+{
+            unsigned int mlogVars[10], mlogVarsCnt = 0;
+            unsigned long axi_divisor = TRANSCEDE_AXICLK_HZ / 1000000UL;
+            if (MLogIsEnabled() && cpu == 1) {
+                mlogVars[mlogVarsCnt++] = MLOG_VAR_MIPS;
+                mlogVars[mlogVarsCnt++] = cpu;
+                mlogVars[mlogVarsCnt++] = mmonitor->last_avg_ratio[cpu];
+                mlogVars[mlogVarsCnt++] = mmonitor->last_total_time[cpu] / axi_divisor;
+                MLogAddVariables(mlogVarsCnt, mlogVars, time);
+            }
+}
+#endif
+            /* update for next tick */
+            mmonitor->accum_idle_time_ticks[cpu] = 0;
+            mmonitor->total_time_ticks[cpu] = 0;
+            mmonitor->meas_counter[cpu] = mmonitor->meas_interval;
+        }
+    }
+
+	return 0;
+}
+EXPORT_SYMBOL(mips_monitor_tick_timer_update);
+
+
+static long mmonitor_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	long ret = 0;
+    int __user *p_user = (int __user*) arg;
+    unsigned int val;
+
+	switch (cmd) {
+	case MMONITOR_IOCTL_INTERVAL:
+        get_user(val, p_user);
+        if (mips_monitor_ctx)
+        {
+            memset(mips_monitor_ctx, 0, sizeof(idle_mips_monitor_t));
+            mips_monitor_ctx->meas_counter[0] = val;
+            mips_monitor_ctx->meas_counter[1] = val;
+            mips_monitor_ctx->meas_interval   = val;
+        }
+		break;
+	default:
+        printk("MMONITOR: IOCTL error %u\n", cmd);
+		break;
+	}
+
+	return ret;
+}
+
+static ssize_t mmonitor_bin_read(struct file *file, struct kobject *kobj, struct bin_attribute *bin_attr, char *buf, loff_t pos, size_t size)
+{
+    ssize_t ret = 0;
+
+    if (mips_monitor_ctx)
+    {
+        if (pos >= sizeof(idle_mips_monitor_t)) {
+            printk("MMONITOR: f_pos(%d) > file size\n", (int)pos);
+            goto out;   /* return 0 - EOF */
+        }
+
+        if (pos + size >= sizeof(idle_mips_monitor_t))
+            size = sizeof(idle_mips_monitor_t) - pos;
+
+
+        memcpy(buf, (char*)mips_monitor_ctx + pos, size);
+
+        pos += size;
+        ret = size;
+    }
+
+    out:
+	return ret;
+}
+
+static ssize_t mmonitor_read(struct file *file, const char __user *buf, size_t size, loff_t *pos)
+{
+    ssize_t ret = 0;
+
+    if (mips_monitor_ctx)
+    {
+        if (*pos >= sizeof(idle_mips_monitor_t)) {
+            printk("MMONITOR: bin read:f_pos = %d  size = %d\n\n", (int)*pos, size);
+            goto out;   /* return 0 - EOF */
+        }
+
+        if (*pos + size >= sizeof(idle_mips_monitor_t))
+            size = sizeof(idle_mips_monitor_t) - *pos;
+
+        if (copy_to_user((void *)buf, (char*)mips_monitor_ctx + *pos, size)) {
+            ret = -EFAULT;
+            goto out;
+        }
+
+        *pos += size;
+        ret = size;
+    }
+    out:
+	return ret;
+}
+
+static ssize_t mmonitor_write(struct file *file, const char __user *buf, size_t count, loff_t *f_pos)
+{
+    idle_mips_monitor_t* mmonitor = mmonitor_get_ctx();
+    char *tmp = kmalloc(count + 1, GFP_KERNEL);
+    unsigned int val;
+
+	if (copy_from_user(tmp, buf, count)) {
+		printk(KERN_ERR "copy failed: 0x%08lx\n", (unsigned long)buf);
+		kfree(tmp);
+		return 0;
+	}
+
+    tmp[count] = '\0';
+
+    val = simple_strtoul(tmp, NULL, 10);
+
+    if (mmonitor)
+        mmonitor->meas_interval = val;
+    else // monitor is off due to 'interval' was set to '0'
+        if (mips_monitor_ctx)
+        {
+            memset(mips_monitor_ctx, 0, sizeof(idle_mips_monitor_t));
+
+            mips_monitor_ctx->meas_counter[0] = val;
+            mips_monitor_ctx->meas_counter[1] = val;
+            mips_monitor_ctx->meas_interval   = val;
+        }
+
+    kfree(tmp);
+
+	return count;
+}
+
+TRANSCEDE_ACT_SHOW(mips_stat)
+{
+    idle_mips_monitor_t* mmonitor = mmonitor_get_ctx();
+    int len = 0;
+
+    if (mmonitor){
+        len += sprintf(buf + len, "MIPS: %d %d \n", mmonitor->last_avg_ratio[0], mmonitor->last_avg_ratio[1]);
+        len += sprintf(buf + len, "interval %2u x 1ms = %d ms\n", mmonitor->meas_interval,  mmonitor->meas_interval * 1);
+    }
+    else
+        len += sprintf(buf + len, "MIPS Monitor is OFF [interval == 0]\n");
+
+    return len;
+}
+
+static int init_sysfs(struct device *dev, const char *name)
+{
+	if (TRANSCEDE_CREATE_FILE(mips_stat)) {
+		printk(KERN_ERR "failed to create sysfs for %s\n", name);
+		return -1;
+	}
+
+	return 0;
+}
+
+static int __init mips_monitor_init(void)
+{
+   idle_mips_monitor_t *pMmonitor = NULL;
+
+    pMmonitor = mmAlloc(sizeof(idle_mips_monitor_t));
+
+    if (pMmonitor == NULL) {
+        printk("memory Allocation error\n");
+        return -1;
+    }
+
+    memset(pMmonitor, 0, sizeof(idle_mips_monitor_t));
+    pMmonitor->meas_interval   = DEFAULT_INTERVAL_OF_MEASURMENTS;
+    pMmonitor->meas_counter[0] = DEFAULT_INTERVAL_OF_MEASURMENTS;
+    pMmonitor->meas_counter[1] = DEFAULT_INTERVAL_OF_MEASURMENTS;
+    mmonitor_set_ctx(pMmonitor);
+
+    printk("MIPS monitor 0x%08x %d bytes [interval %d of HZ tick]\n", (unsigned) pMmonitor, sizeof(idle_mips_monitor_t), pMmonitor->meas_interval);
+
+    mips_monitor_class = class_create(THIS_MODULE, DEVICE_NAME);
+	if (IS_ERR(mips_monitor_class)) {
+		return PTR_ERR(mips_monitor_class);
+	}
+
+	mips_monitor_major = register_chrdev(0, DEVICE_NAME, mips_monitor_fops);
+
+	if (mips_monitor_major < 0) {
+		printk ("Registering %s device failed with %d\n", DEVICE_NAME, mips_monitor_major);
+		return mips_monitor_major;
+	}
+
+	mips_monitor_device = device_create(mips_monitor_class, NULL, MKDEV(mips_monitor_major, 0), NULL, DEVICE_NAME);
+	if (IS_ERR(mips_monitor_device)) {
+		class_destroy(mips_monitor_class);
+		unregister_chrdev(mips_monitor_major, DEVICE_NAME);
+		return PTR_ERR(mips_monitor_device);
+	}
+
+    if (init_sysfs(mips_monitor_device, DEVICE_NAME)) {
+        printk(KERN_ERR "sysfs failed for %s\n", DEVICE_NAME);
+    }
+
+	if (sysfs_create_bin_file(&mips_monitor_device->kobj, mips_monitor_bin_attr)) {
+		printk("Error while creating binary file in sysfs occured.\n");
+	}
+
+	return 0;
+}
+
+static void __exit mips_monitor_exit(void)
+{
+	sysfs_remove_bin_file(&mips_monitor_device->kobj, mips_monitor_bin_attr);
+	device_destroy(mips_monitor_class, MKDEV(mips_monitor_major, 0));
+	/* Unregister the device */
+	unregister_chrdev(mips_monitor_major, DEVICE_NAME);
+
+	class_destroy(mips_monitor_class);
+    mmFree(mips_monitor_ctx);
+
+	return;
+}
+
+module_init(mips_monitor_init);
+module_exit(mips_monitor_exit);
+
+MODULE_AUTHOR("Intel");
+MODULE_DESCRIPTION("MIPS Monitor");
+MODULE_LICENSE("GPL");
+MODULE_VERSION("0.1");
diff --git a/arch/arm/mach-transcede/mlog.c b/arch/arm/mach-transcede/mlog.c
new file mode 100644
index 0000000..a605238
--- /dev/null
+++ b/arch/arm/mach-transcede/mlog.c
@@ -0,0 +1,2584 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+
+//-------------------------------------------------------------------------------------------
+
+#include <linux/version.h>
+#include <mach/tcb.h>
+#include <mach/mlog.h>
+#include <linux/kernel.h>
+#include <linux/hrtimer.h>
+#include <linux/kthread.h>
+#include <linux/delay.h>
+#include <linux/mutex.h>
+
+#if !defined(__KERNEL__)
+#include <stdio.h>
+#include <time.h>
+#include <string.h>
+#endif
+
+#include <linux/jiffies.h>
+#include <linux/smp.h>
+#include <linux/vmalloc.h>
+#include <linux/device.h>
+#include <linux/fs.h>
+#include <linux/errno.h>
+#include <linux/slab.h>
+#include <mach/hardware.h>
+#include <mach/sysheap.h>
+#include <asm/uaccess.h>
+
+static int mlog_major;      /* Major number assigned to our device driver */
+static struct class *mlog_class;
+static struct device *mlog_device;
+
+#define UINT32 		uint32_t
+#define UINT8 		uint8_t
+typedef void*		LPVOID;
+#define MxGetCpuID()	smp_processor_id()
+#define MxGetARMID()	(1)
+
+#define uart_printf 	printk
+#define _ASSERT_PTR(x)	BUG_ON(x)
+#define MLOG_BUF_SIZE	(1024*1024)  //bytes             Moved this to Config.h for EVM
+#define MLOG_LOCK	raw_spin_lock_irqsave(&mlog_spin, irq_flags[MxGetCpuID()])
+#define MLOG_RELEASE	raw_spin_unlock_irqrestore(&mlog_spin, irq_flags[MxGetCpuID()])
+
+#define MLOG_GETTICK()  *((unsigned int*)TIMER0_CURR_COUNT)
+
+#if defined(CONFIG_MACH_M822XX)
+#define MLogAlloc(size)	ddr_heap_alloc(size) //vmalloc(size)
+#define MLogFree(ptr)	ddr_heap_free(ptr) //vfree(ptr)
+#define MLogVirt2Phys(ptr) ddr_heap_virt_to_phys((unsigned long)ptr)
+#else
+#define MLogAlloc(size)	kmalloc(size, GFP_KERNEL)
+#define MLogFree(ptr)	kfree(ptr)
+#define MLogVirt2Phys(ptr) virt_to_phys(ptr)
+#endif
+static unsigned long irq_flags[CPU_NUM];
+
+/* To protect race conditions when mlog is accessed from kernel thread, timer, userspace, IRQ. */
+static DEFINE_RAW_SPINLOCK(mlog_spin);
+
+/* To protect race conditions when control happens from various userspace contexts simultaneously.
+ * For example, from ioctl and 'echo'.
+ */
+static DEFINE_MUTEX(control_lock); 
+
+//#define MLOG_DEBUG
+
+#ifndef MLOG_DEBUG
+#define MLOG_DEBUG_PRINT(...)
+#else
+#if defined(__KERNEL__)
+#define MLOG_DEBUG_PRINT(...) printk (__VA_ARGS__)
+#elif defined(_WIN32) || defined(__linux__)
+#define MLOG_DEBUG_PRINT(...) printf (__VA_ARGS__)
+#else
+#define MLOG_DEBUG_PRINT(...) uart_printf (__VA_ARGS__)
+#endif
+#endif
+
+
+static MLOGCTX mlog_ctx = {0};
+static unsigned int MLogWriteRecord(unsigned char *rec, unsigned int size);
+static unsigned int MLogWriteRecordCpu(unsigned int cpu, unsigned char *rec, unsigned int size);
+static unsigned int MLogWriteStaticRecord(unsigned char *rec, unsigned int size);
+static unsigned int MLogCreateStorage(MLOG_STORAGE_CTX *ctx, LPVOID mem);
+static unsigned int MLogCreatePartition(unsigned int cpu);
+static unsigned int MLogWriteFirstTime(unsigned int cpu, unsigned char *rec, unsigned int size);
+static unsigned int MLogWriteRecordEx(unsigned int cpu, unsigned char *rec, unsigned int size);
+static unsigned int MLogWriteStaticRecordEx(unsigned int cpu, unsigned char *rec, unsigned int size);
+static unsigned int MLogWriteStaticRecordEx2(unsigned int cpu, unsigned char *rec, unsigned int size);
+static unsigned int MLogFinish(void);
+static void MLogOnTTIDev(void);
+static void MLogFREQ(unsigned int resourceid, unsigned long freqvalue);
+static int MLogOpen(void);
+static int MLogClose(void);
+static int _MLogSetMask (MLOG_DWORD nMask);
+static void _MLogExTime(unsigned int resourceId, unsigned int type, unsigned int time_ms, unsigned int event, unsigned int ticks);
+
+static MLOGCTX * MLogGetCtx (void);
+static unsigned int MLogSetMaskProg (MLOG_DWORD nMask);
+static unsigned int MLogActivateMaskProg (void);
+static MLOG_DWORD MLogGetMask (void);
+static unsigned int * MLogGetFileLocation(void);
+static unsigned int MLogGetFileSize(void);
+
+static void MLogTCB(void* ptcb);
+static void MLogEXEStart(void* ptcb,int resourcenum,unsigned int thisTicks);
+static void MLogEXEFinish(void* ptcb);
+static void MLogEXEFinish2(void* ptcb,int resourcenum);
+
+static void MLogEnableRC(unsigned int bVal);
+
+static void MLogDevInfo(unsigned devInfo);
+static void MLogSubTask(void* ptcb_parent, unsigned int subtaskid, unsigned int ticks);
+
+static void MLogMark(unsigned int markid,  unsigned int ticks);
+
+#ifdef MLOG_ADD_DEPENDENCY
+static void MLogDepends(int mode,unsigned int param1,unsigned int param2);
+#else
+#define MLogDepends(a, b, c)
+#endif
+
+static void MLogMarkTcbControl(void* ptcb, unsigned int markid);
+
+
+static void MLogTCBList(unsigned int listid);
+static void MLogTCBListStart(unsigned int listid);
+static void MLogTCBListStartEx(unsigned int listid, unsigned int cpu);
+static void MLogTCBListStop(unsigned int listid);
+static void MLogTCBAddToList(void* ptcb,unsigned int listid);
+
+static unsigned int MLogPrint(void);
+static unsigned int MLogFlush(void);
+
+#ifdef MLOG_IRQ_SUP_ENABLE
+static void MLogResourceCondition(unsigned int code,unsigned int resid,unsigned int resindex);
+#else
+#define MLogResourceCondition(a, b, c)
+#endif
+
+static void MLogRegisterFrameSubframe(unsigned int frameNum, unsigned int subFrameNum);
+static void MLogCacheMipsStats(unsigned int ticks);
+static void MLogCacheMipsStatsSingle(unsigned int ticks);
+static void MLogCacheMipsStatsSingleEx(unsigned int ticks);
+
+static MLOG_FRAME_BLOCK_HEADER * MLogDevGetCurBlock(unsigned int devid);
+
+#if defined(_WIN32) || defined(__linux__) | defined (__KERNEL__)
+static void MxiDefSysPart(SYSFPART * pPart, LPVOID pStorage, UINT32 nBlkSize, UINT32 nBlkCnt);
+static LPVOID MxiAllocSysPart(SYSFPART * pPart);
+static int MxiGetBlockIndex (SYSFPART * pPart, LPVOID pBlock);
+static void MxiFreeSysPart(SYSFPART * pPart, LPVOID pBlk);
+#endif
+
+#define NAME_LEN_MAX (32)
+
+static unsigned int mlog_counter;
+static unsigned int is_watched;
+static uint8_t auto_stop = 0;
+
+static struct hrtimer frameborder_timer;
+static struct task_struct* sync_threads[NR_CPUS];
+
+#define MLOG_MARK_FRAMEBORDER_PERIOD_MS 1
+
+static enum hrtimer_restart frameborder_timer_fn(struct hrtimer *t)
+{
+	ktime_t ktime = ktime_set(0, 1000000ULL * MLOG_MARK_FRAMEBORDER_PERIOD_MS);
+
+	MLOG_LOCK;
+
+	MLogMark(MLOG_MARK_FRAMEBORDER, get_tick());
+
+	MLOG_RELEASE;
+
+	hrtimer_forward(t, ktime_get(), ktime);
+
+	return HRTIMER_RESTART;
+}
+
+#define MLOG_SYNC_PERIOD_MS 100
+static DECLARE_COMPLETION(first_sync_completion);
+
+static int sync_thread_fn(void *data)
+{
+	printk(KERN_INFO "mlog_thread: %d\n", current->pid);
+
+	while (!kthread_should_stop()) {
+		struct timespec now = ktime_to_timespec(ktime_get());
+
+//		MLogExTime(MxGetCpuID() + MLOG_RESOURCE_KERNEL_MASK, MLOG_EX_TIME_TYPE_MSEC, now.tv_sec * 1000 + now.tv_nsec / 1000000UL, MLOG_MARK_KERNEL_5SEC, get_tick());
+		complete(&first_sync_completion);
+
+		MLOG_LOCK;
+		if (is_watched && auto_stop) {
+			if (mlog_counter >= 1) {
+				_MLogSetMask(0);
+				is_watched = 0;
+			}
+
+			mlog_counter++;
+		}
+		MLOG_RELEASE;
+
+		msleep(MLOG_SYNC_PERIOD_MS);
+	}
+
+	return 0;
+}
+
+void clear_mlog_counter(void)
+{
+	MLOG_LOCK;
+
+	if (MLogIsEnabled()) {
+		mlog_counter = 0;
+	}
+
+	MLOG_RELEASE;
+}
+EXPORT_SYMBOL(clear_mlog_counter);
+
+void init_mlog_counter(void)
+{
+	MLOG_LOCK;
+
+	if (MLogIsEnabled()) {
+		is_watched = 1;
+	}
+
+	MLOG_RELEASE;
+}
+EXPORT_SYMBOL(init_mlog_counter);
+
+/* Kernel threads write periodical synchronization marks to mlog file.
+ * They log marks on behalf of each CPU.
+ * Per-CPU threads are needed to log marks as long CPU is alive.
+ * We assume that even if all other CPUs are stuck, the alive CPU will still log marks, as long as its thread is alive.
+ * And therefore it would be possible to debug such a case in TTBox.
+ */
+static int sync_threads_start(void)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(sync_threads); i++) {
+		sync_threads[i] = kthread_create(sync_thread_fn,
+						 NULL,
+						 "mlog_quick/%d", i);
+
+		if (IS_ERR(sync_threads[i])) {
+			printk(KERN_ERR "failed create MLOG thread%d\n", i);
+			return -1;
+		}
+
+		kthread_bind(sync_threads[i], i);
+		wake_up_process(sync_threads[i]);
+
+		/* Wait for 1st sync to happen on behalf of a thread.
+		 * This is needed to ensure, that when this function exits, at least one sync mark have been written to mlog file on behalf of each CPU.
+		 * Otherwise, if we return immediately, MLogStart() will enable logging for other tasks and any other task may log BEFORE 1st sync mark.
+		 * This will result weird mlog picture in TTBox when there might be several tasks logged at the beggining, then a HUGE gap and then all other tasks.
+		 * This happens because sync mark contains first 4 bytes of timestamp and task log contains last 4 bytes.
+		 *
+		 * Correct sequence is:
+		 * 1) write sync mark - TTBox reads first 4 bytes of timestamp
+		 * 2) write task log  - TTBox reads last 4 bytes of timestamp and reassembles full timestamp (sync + current timestamp)
+		 * 3) write task log
+		 * 4) write task log
+		 * 5) and so on
+		 *
+		 * If this is mixed up - you'll get weird pictures in TTBox.
+		 * For example, wrong sequence:
+		 * 1) write task log  - TTBox reads last 4 bytes of timestamp and reassembles full timestamp (sync (0) + current timestamp)
+		 * 2) write sync mark - TTBox reads first 4 bytes of timestamp
+		 * 3) write task log  - TTBox reads last 4 bytes of timestamp and reassembles full timestamp (sync + current timestamp) 
+		 * 4) write task log
+		 * 5) and so on
+		 * As you may see, due to i.1 and i.2 are mixed up, first entry is logged with wrong timestamp and this is the reason of a huge gap to appear.
+		 */
+		wait_for_completion(&first_sync_completion);
+
+		udelay(555);
+	}
+
+	printk(KERN_INFO "Started\n");
+
+	return 0;
+}
+
+static void sync_threads_stop(void)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(sync_threads); i++)
+		kthread_stop(sync_threads[i]);
+
+	printk(KERN_INFO "Stopped\n");
+}
+
+static void frameborder_timer_start(void)
+{
+	ktime_t ktime = ktime_set(1, 0);
+	hrtimer_init(&frameborder_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	frameborder_timer.function = frameborder_timer_fn;
+	hrtimer_start(&frameborder_timer, ktime, HRTIMER_MODE_REL);
+}
+
+static void frameborder_timer_stop(void)
+{
+	hrtimer_cancel(&frameborder_timer);
+}
+
+////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
+
+// Local Functions
+//--------------
+
+
+int MLogIsEnabled()
+{
+    return (mlog_ctx.Started && mlog_ctx.StoragePtr);
+}
+
+
+static int MLogCheckMask(UINT32 DevId)
+{
+    if (mlog_ctx.Started && (mlog_ctx.EnabledMask & (1<<DevId)))
+        return MLOG_TRUE;
+    else
+        return MLOG_FALSE;
+}
+
+//-------------------------------------------------------------------------------------------
+/** @brief DOX_INTERNAL DOXYGEN_TO_DO
+ *
+ *
+ *  @param  code
+ *  @param  listid
+ *
+ *  @return none
+ *
+ *  \ingroup group_lte_mlog
+ *
+**/
+//-------------------------------------------------------------------------------------------
+static MLOG_INLINE void MLogTCBListPut(unsigned int code,unsigned int listid)
+{
+    MLOGLISTREC listrec;
+
+    if (MLogIsEnabled() == MLOG_FALSE)
+        return;
+
+    listrec.code = code;
+    listrec.listid = listid;
+	listrec.ticks = MLOG_GETTICK();
+
+    if(code != MLOG_ID_LIST)
+        MLogWriteRecord((unsigned char *)&listrec, sizeof(MLOGLISTREC));
+    else
+        MLogWriteStaticRecord((unsigned char *)&listrec, sizeof(MLOGLISTREC));
+
+}
+
+static MLOG_INLINE void MLogTCBListPutEx(unsigned int code,unsigned int listid, unsigned int cpu)
+{
+    MLOGLISTREC listrec;
+
+    if (MLogIsEnabled() == MLOG_FALSE)
+        return;
+
+    listrec.code = code;
+    listrec.listid = listid;
+	listrec.ticks = MLOG_GETTICK();
+
+    MLogWriteRecordCpu(cpu, (unsigned char *)&listrec, sizeof(MLOGLISTREC));
+}
+
+
+#ifdef MLOG_ENABLED
+
+////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
+
+// MLog APIs
+//-----------
+
+
+//-------------------------------------------------------------------------------------------
+/** @brief This returns the Mlog Context Location
+ *
+ *  @param  none
+ *
+ *  @return MLOGCTX Pointer to MLOG Context
+ *
+ *  \ingroup group_lte_mlog
+ *
+**/
+//-------------------------------------------------------------------------------------------
+static MLOGCTX * MLogGetCtx (void)
+{
+    return (MLOGCTX *)&mlog_ctx;
+}
+
+static int _MLogSetMask (MLOG_DWORD nMask)
+{
+    mlog_ctx.EnabledMask = nMask;
+
+    if (nMask != 0) {
+        mlog_ctx.Started = MLOG_TRUE;
+    } else {
+	mlog_ctx.Started = MLOG_FALSE;
+    }
+
+    return 0;
+}
+
+//-------------------------------------------------------------------------------------------
+/** @brief This sets the MLOG Logging Mask
+ *
+ *  @param  Masks to enable
+ *
+ *  @return MX_OK
+ *
+ *  \ingroup group_lte_mlog
+ *
+**/
+//-------------------------------------------------------------------------------------------
+unsigned int MLogSetMask (MLOG_DWORD nMask)
+{
+    int res;
+
+    MLOG_LOCK;
+
+    res = _MLogSetMask(nMask);
+
+    MLOG_RELEASE;
+
+    return res;
+}
+EXPORT_SYMBOL(MLogSetMask);
+
+//-------------------------------------------------------------------------------------------
+/** @brief This presets the MLOG Logging Mask
+ *
+ *  @param  Masks to enable
+ *
+ *  @return MX_OK
+ *
+ *  \ingroup group_lte_mlog
+ *
+**/
+//-------------------------------------------------------------------------------------------
+static unsigned int MLogSetMaskProg (MLOG_DWORD nMask)
+{
+    mlog_ctx.EnabledMaskProg = nMask;
+    return 0;
+}
+
+//-------------------------------------------------------------------------------------------
+/** @brief This activates preset MLOG Logging Mask
+ *
+ *  @param  none
+ *
+ *  @return MX_OK
+ *
+ *  \ingroup group_lte_mlog
+ *
+**/
+//-------------------------------------------------------------------------------------------
+static unsigned int MLogActivateMaskProg (void)
+{
+    _MLogSetMask(mlog_ctx.EnabledMaskProg);
+    return 0;
+}
+
+
+//-------------------------------------------------------------------------------------------
+/** @brief This gets the MLOG Logging Mask
+ *
+ *  @param  none
+ *
+ *  @return MLOG Enable Mask
+ *
+ *  \ingroup group_lte_mlog
+ *
+**/
+//-------------------------------------------------------------------------------------------
+static MLOG_DWORD MLogGetMask (void)
+{
+    return mlog_ctx.EnabledMask;
+}
+
+//-------------------------------------------------------------------------------------------
+/** @brief This function is used to get the current size of the MLOG logged in. By default
+ *         it gives the allocated size of the Log (done in MlogOpen()). Once the Log is
+ *         finalized by calling MlogFinish(), it gives the correct size of the log
+ *
+ *
+ *  @param   none
+ *
+ *  @return  none
+ *
+ *  \ingroup group_lte_mlog
+ *
+**/
+//-------------------------------------------------------------------------------------------
+static unsigned int MLogGetFileSize(void)
+{
+    return mlog_ctx.StorageSize;
+}
+
+//-------------------------------------------------------------------------------------------
+/** @brief This function is used to get the location in DDR to the start of the Logged output
+ *         It is inialized in MlogOpen() function and deallocated in the MlogClose() function
+ *
+ *
+ *  @param   none
+ *
+ *  @return  none
+ *
+ *  \ingroup group_lte_mlog
+ *
+**/
+//-------------------------------------------------------------------------------------------
+static unsigned int * MLogGetFileLocation(void)
+{
+#if defined(_WIN32) || defined(__linux__) || defined (__KERNEL__)
+    return  (unsigned int *)mlog_ctx.StoragePtr->ptr;
+#else
+    return  (mlog_ctx.StoragePtr) ? (unsigned int *)mlog_ctx.StoragePtr->ptr : (unsigned int *)gMlogBuf;
+#endif
+}
+
+static int MLogCreate(void)
+{
+    if(mlog_ctx.StoragePtr) {
+	MLOG_DEBUG_PRINT("\r\n MLOG DOUBLE CREATE!!! \r\n");
+	return -1;
+    }
+
+    MLOG_DEBUG_PRINT("Mlog: alloc storage ptr\r\n");
+    mlog_ctx.StoragePtr = MLogAlloc(sizeof(MLOG_STORAGE_CTX));
+    if(!mlog_ctx.StoragePtr) {
+	printk(KERN_ERR "Mlog: error allocating StoragePtr of size %d bytes!\r\n", sizeof(MLOG_STORAGE_CTX));
+	return -1;
+    }
+
+//    don't erase memory, this is needed to read Mlog after reboot
+//    memset(mlog_ctx.StoragePtr, 0, sizeof(MLOG_STORAGE_CTX));
+
+    mlog_ctx.StorageSize = MLOG_STORAGE_SIZE;
+    mlog_ctx.StoragePtr->ptr = MLogAlloc(mlog_ctx.StorageSize);
+    if(!mlog_ctx.StoragePtr->ptr) {
+	printk(KERN_ERR "Mlog: error allocating StoragePtr->ptr of size %d bytes! Increasing system memory might help (for ex. increase 'mem' parameter in bootargs)\r\n", mlog_ctx.StorageSize);
+	return -1;
+    }
+
+    uart_printf("Mlog: allocated storage @ 0x%08lx (%d)\r\n", MLogVirt2Phys(mlog_ctx.StoragePtr->ptr), mlog_ctx.StorageSize);
+
+    return 0;
+}
+
+static void MLogDestroy(void)
+{
+    MLogFree(mlog_ctx.StoragePtr->ptr);
+    MLogFree(mlog_ctx.StoragePtr);
+}
+
+//-------------------------------------------------------------------------------------------
+/** @brief This allocates memory in Cacheable DDR space for the Logger and initializes the
+ *         logger's internal variables. This is the first function that needs to be called to
+ *         start the Mlogger. Thisfunction is automatically called by the TrEx during
+ *         system Init
+ *
+ *  @param  none
+ *
+ *  @return none
+ *
+ *  \ingroup group_lte_mlog
+ *
+**/
+//-------------------------------------------------------------------------------------------
+static int MLogOpen()
+{
+    unsigned int ret;
+
+    MLOG_DEBUG_PRINT("MLOG Open. \r\n");
+
+    if (mlog_ctx.Opened == MLOG_TRUE)
+    {
+#if defined(_WIN32) || defined(__linux__) || defined(__KERNEL__)
+        uart_printf("MLOG already Opened. Please close it first!!!\r\nGoing to Infinite Loop\r\n");
+#else
+        MLOG_DEBUG_PRINT("MLOG already Opened. Please close it first!!!\r\nGoing to Infinite Loop\r\n");
+        MxDelay(50000);
+#endif
+        while(1);
+    }
+
+    // Initializations for Context
+    mlog_ctx.Opened = MLOG_TRUE;
+    mlog_ctx.Finalized = MLOG_FALSE;
+    mlog_ctx.Started = MLOG_TRUE;
+    mlog_ctx.Mlogenablerc = 0;
+    mlog_ctx.Mlogtaskkeycounter = 1;
+
+    if (!mlog_ctx.StoragePtr || !mlog_ctx.StoragePtr->ptr) {
+        printk(KERN_ERR "MLog storage hasn't been allocated!\n");
+        return MLOG_FALSE;
+    }
+
+    ret = MLogCreateStorage(mlog_ctx.StoragePtr, (LPVOID)mlog_ctx.StoragePtr->ptr);
+
+    if(ret != MX_OK)
+    {
+        MLOG_DEBUG_PRINT("Creating error %d \r\n", ret);
+        return MLOG_FALSE;
+    }
+
+    MLOG_DEBUG_PRINT("Mlog: create operation done\r\n");
+
+    printk("MLOG: TRANSCEDE_ARMCLK_HZ = %u, TRANSCEDE_AXICLK_HZ = %u\n", TRANSCEDE_ARMCLK_HZ, TRANSCEDE_AXICLK_HZ);
+    MLogFREQ(1, TRANSCEDE_ARMCLK_HZ);
+    MLogFREQ(RESOURCE_XP_AXI, TRANSCEDE_AXICLK_HZ);
+
+    frameborder_timer_start();
+
+    return sync_threads_start() == 0 ? MLOG_TRUE : MLOG_FALSE;
+}
+
+
+//-------------------------------------------------------------------------------------------
+/** @brief This function un-initializes the allocated memory for the logger and clearly all
+ *         other internally used variables. This function is called automatically by the
+ *         TrEx during System sutdown
+ *
+ *  @param  none
+ *
+ *  @return none
+ *
+ *  \ingroup group_lte_mlog
+ *
+**/
+//-------------------------------------------------------------------------------------------
+static int MLogClose()
+{
+    int j;
+
+    MLOG_DEBUG_PRINT("\r\n MLOG Close. \r\n");
+
+    sync_threads_stop();
+
+    frameborder_timer_stop();
+
+    if (mlog_ctx.Opened == MLOG_FALSE)
+    {
+#if defined(_WIN32)
+        uart_printf("MLOG not Opened. Please open it first before closing!!!\r\nGoing to Infinite Loop\r\n");
+#else
+        MLOG_DEBUG_PRINT("MLOG not Opened. Please open it first before closing!!!\r\nGoing to Infinite Loop\r\n");
+#endif
+        while(1);
+    }
+
+    mlog_ctx.Opened = MLOG_FALSE;
+    mlog_ctx.Started = MLOG_FALSE;
+    mlog_ctx.EnabledMask = 0;
+
+    if (mlog_ctx.StoragePtr == 0)
+        return MLOG_FALSE;
+
+#if !defined(_WIN32) && !defined(__linux__) && !defined(__KERNEL__)
+    MLogFlush();
+#endif
+
+    for(j = 0; j < (CPU_NUM + MLOG_DEVICE_COUNT); j++)
+    {
+        if (mlog_ctx.StoragePtr->storage[j]!= 0)
+        {
+            MLogFree(mlog_ctx.StoragePtr->storage[j]);
+            mlog_ctx.StoragePtr->storage[j] = NULL;
+        }
+    }
+
+    mlog_ctx.StoragePtr = NULL;
+
+#if defined(_WIN32)
+    DeleteCriticalSection(&_criticalsection);
+#endif
+
+#if defined(__linux__) && !defined(__KERNEL__)
+    if(timers_base)
+	munmap(timers_base, MMAP_SIZE);
+#endif
+
+    return MLOG_TRUE;
+}
+
+
+//-------------------------------------------------------------------------------------------
+/** @brief This function logs a TCB into its memory.Variables that are stored are TCB ID, Resource ID, Task ID,
+ *         This is usually called during the creation of the TCB. It is automatically called when the TrEx
+ *         when a new TCB is created using its API
+ *
+ *  @param  ptcb  This is the pointer to the TCB that needs to be logged
+ *
+ *  @return none
+ *
+ *  \ingroup group_lte_mlog
+ *
+**/
+//-------------------------------------------------------------------------------------------
+static void MLogTCB(void* ptcb)
+{
+    MLOGTCBREC tcbrec;
+
+    if (MLogIsEnabled() == MLOG_FALSE)
+        return;
+
+    if (MLogCheckMask(((PTCB)ptcb)->ResourceID) == MLOG_FALSE)
+        return;
+
+    tcbrec.code = MLOG_ID_NEWTASK;
+    tcbrec.ptcb = (MLOG_DWORD)ptcb;
+    tcbrec.taskID = ((PTCB)ptcb)->TaskID;
+    tcbrec.resID = ((PTCB)ptcb)->ResourceID;
+
+    MLogWriteStaticRecord((unsigned char *)&tcbrec, sizeof(MLOGTCBREC));
+}
+
+//-------------------------------------------------------------------------------------------
+/** @brief DOX_INTERNAL DOXYGEN_TO_DO
+ *
+ *
+ *  @param  ptcb
+ *  @param  resourcenum
+ *  @param  isstart
+ *  @param  ticks
+ *
+ *  @return none
+ *
+ *  \ingroup group_lte_mlog
+ *
+**/
+//-------------------------------------------------------------------------------------------
+static void MLogEXE(void* ptcb,int resourcenum,int isstart,unsigned int ticks)
+{
+    MLOGEXEREC exerec;
+
+    if (MLogIsEnabled() == MLOG_FALSE)
+        return;
+
+    if (MLogCheckMask(((PTCB)ptcb)->ResourceID)== MLOG_FALSE)
+        return;
+
+    exerec.code     = isstart ? MLOG_ID_EXESTART : MLOG_ID_EXEFINISH;
+    exerec.ptcb     = (MLOG_DWORD)ptcb;
+    exerec.ticks    = (ticks != 0) ? (ticks) : (MLOG_GETTICK());
+    exerec.resID    = resourcenum;
+    exerec.errcode  = 0;
+
+    if (isstart == MLOG_FALSE)
+    {
+        exerec.execticks = ((PTCB)ptcb)->ExecTicks;
+        MLogWriteRecord((unsigned char *)&exerec, sizeof(MLOGEXEREC));
+    }
+    else
+    {
+        MLogWriteRecord((unsigned char *)&exerec, sizeof(MLOGEXEREC) - (2*MLOG_UINTSIZE));
+    }
+}
+
+//-------------------------------------------------------------------------------------------
+/** @brief This function logs the start of execution of the TCB. It is automatically called by
+ *         the TrEx when it dispatches a TCB
+ *
+ *
+ *  @param  ptcb          Pointer to the TCB that needs to be logged
+ *  @param  resourcenum   Resource Index of the Resource being used. If Ceva 5 is used, then 5
+ *  @param  thisTicks     Clock State at this point of time. This is found by reading the internal
+ *                        clock register
+ *
+ *  @return none
+ *
+ *  \ingroup group_lte_mlog
+ *
+**/
+//-------------------------------------------------------------------------------------------
+static void MLogEXEStart(void* ptcb,int resourcenum,unsigned int thisTicks)
+{
+    MLogEXE(ptcb,resourcenum,MLOG_TRUE,thisTicks);
+}
+
+
+
+//-------------------------------------------------------------------------------------------
+/** @brief This function logs the end of execution of the TCB. It is automatically called by
+ *         the TrEx when it get interrupted by the resource when TCB completed
+ *
+ *
+ *  @param   ptcb Pointer to the TCB that needs to be logged
+ *
+ *  @return  none
+ *
+ *  \ingroup group_lte_mlog
+ *
+**/
+//-------------------------------------------------------------------------------------------
+static void MLogEXEFinish(void* ptcb)
+{
+    MLogEXE(ptcb,(((PTCB) ptcb)->ResourceIndex),MLOG_FALSE,0); //pass 0 for ticks
+}
+
+//-------------------------------------------------------------------------------------------
+/** @brief This function logs the end of execution of the TCB. It is automatically called by
+ *         the scheduler when it dispatches a TCB. This is used to log in the end of a TCB that
+ *         is part of a SuperTCB. This is a special kind of TCB. It is automitically called by the
+ *         TrEx when it get interrupted by the resource when TCB completed
+ *
+ *  @param   ptcb          Pointer to the TCB that needs to be logged
+ *  @param   resourcenum   Resource Index of the Resource being used. If Ceva 5 is used, then 5
+ *
+ *  @return  none
+ *
+ *  \ingroup group_lte_mlog
+ *
+**/
+//-------------------------------------------------------------------------------------------
+static void MLogEXEFinish2(void* ptcb,int resourcenum)
+{
+    MLogEXE(ptcb,resourcenum,MLOG_FALSE,0); //pass 0 for ticks
+}
+
+//-------------------------------------------------------------------------------------------
+/** @brief This functions helps in logging tasks that are within a TCB inside a resource.
+ *         One can profile subtasks within a TCB using this approach to see how many cycles that
+ *         a smaller task within a TCB took and optimize this function if needed. It is automatically
+ *         called by the TrEx if the TCB has subtasks
+ *
+ *
+ *  @param   ptcb_parent    Pointer to the TCB that needs to be logged
+ *  @param   subtaskid      ID for the Subtask that needs to be profiled
+ *  @param   ticks          Time of the task logged within the resource
+ *
+ *  @return  none
+ *
+ *  \ingroup group_lte_mlog
+ *
+**/
+//-------------------------------------------------------------------------------------------
+static void MLogSubTask(void* ptcb_parent, unsigned int subtaskid, unsigned int ticks)
+{
+    MLOGSUBTASKREC strec;
+
+    if (MLogIsEnabled() == MLOG_FALSE)
+        return;
+
+    if (MLogCheckMask(((PTCB)ptcb_parent)->ResourceID)== MLOG_FALSE)
+        return;
+
+    strec.code = MLOG_ID_SUBTASK;
+    strec.ptcb = (MLOG_DWORD)ptcb_parent;
+    strec.subID = subtaskid;
+    strec.ticks = ticks;
+    MLogWriteRecord((unsigned char *)&strec, sizeof(MLOGSUBTASKREC));
+}
+
+
+
+//-------------------------------------------------------------------------------------------
+/** @brief This function helps in logging tasks that are not dispatched by the TrEx in one's
+ *         solution. This might include things like FEC / MDMA / other hardware accelrator paths
+ *         that may be out of the path of the TrEx
+ *
+ *
+ *  @param   taskid     ID for the Subtask that needs to be profiled
+ *  @param   resourceid Can be a prefined ID for a Resource that the TTBox Tool recognizes.
+ *                      A list of available values are shown in table below
+ *  @param   ticksstart Start time of the task logged within the resource
+ *  @param   ticksstop  End time of the task logged within the resoruce
+ *
+ *  @return  unsigned int   This is the unique ID assigned by the MLogger to identify this task.
+ *                          This ID can be used at a later stage to add dependencies to other tasks
+ *                          so that the TTBox tool can display the dependecies to this task
+ *
+ * \anchor MLOG_RESOURCE_TABLE
+ * <BR/>
+ *  TABLE_START
+ *  ---------------------------------------
+ *  |Resource ID   |Resource Type         |
+ *  ---------------------------------------
+ *  |0             |RESOURCE_RSRV         |
+ *  ---------------------------------------
+ *  |1             |RESOURCE_LARM0        |
+ *  ---------------------------------------
+ *  |2             |RESOURCE_MAP          |
+ *  ---------------------------------------
+ *  |3             |RESOURCE_CEVA         |
+ *  ---------------------------------------
+ *  |4             |RESOURCE_FEC          |
+ *  ---------------------------------------
+ *  |5             |RESOURCE_HOST         |
+ *  ---------------------------------------
+ *  |6             |RESOURCE_MDMA         |
+ *  ---------------------------------------
+ *  |7             |RESOURCE_FECUL        |
+ *  ---------------------------------------
+ *  |8             |RESOURCE_FECDL        |
+ *  ---------------------------------------
+ *  |9             |RESOURCE_XP_AXI       |
+ *  ---------------------------------------
+ *  |10            |RESOURCE_SYS_AXI      |
+ *  ---------------------------------------
+ *  |11            |RESOURCE_SPU_AXI      |
+ *  ---------------------------------------
+ *  |12            |RESOURCE_RAD_AXI      |
+ *  ---------------------------------------
+ *  |13            |RESOURCE_LARM1        |
+ *  ---------------------------------------
+ *  |14            |RESOURCE_UARM0        |
+ *  ---------------------------------------
+ *  |15            |RESOURCE_UARM1        |
+ *  ---------------------------------------
+ *  |16            |RESOURCE_UARM2        |
+ *  ---------------------------------------
+ *  |17            |RESOURCE_UARM3        |
+ *  ---------------------------------------
+ *  TABLE_END
+ * <BR/><BR/>
+ *
+ *  \ingroup group_lte_mlog
+ *
+**/
+//-------------------------------------------------------------------------------------------
+
+unsigned int MLogTask(unsigned int taskid, unsigned int resourceid , unsigned int ticksstart, unsigned int ticksstop)
+{
+    unsigned int key=0;
+    MLOGTASKREC taskrec;
+    int cpu = MxGetCpuID();
+
+    MLOG_LOCK;
+
+    if (MLogIsEnabled() == MLOG_FALSE) {
+	MLOG_RELEASE;
+        return 0;
+    }
+
+    if (MLogCheckMask(resourceid)== MLOG_FALSE) {
+	MLOG_RELEASE;
+        return 0;
+    }
+
+    taskrec.code = MLOG_ID_TASK;
+    taskrec.taskID = taskid;
+    taskrec.resID = MLOG_RESOURCE_KERNEL_MASK + cpu;
+    taskrec.tickstart = ticksstart;
+    taskrec.tickstop = ticksstop;
+    taskrec.key= mlog_ctx.Mlogtaskkeycounter++;
+
+    MLogWriteRecord((unsigned char *)&taskrec, sizeof(MLOGTASKREC));
+
+    MLOG_RELEASE;
+
+    return key;
+}
+#ifdef __KERNEL__
+EXPORT_SYMBOL(MLogTask);
+#endif
+
+//-------------------------------------------------------------------------------------------
+/** @brief This Function helps in logging the currently set frequncies of each of the different
+ *         modules within the device. This is used to calculate the exact resource cycle count
+ *         for the task as all logged times are based on a global clock within the system. This
+ *         function is called for all resources by TrEx automatically during System Init
+ *
+ *
+ *  @param   resourceid  This is the resource ID shown in the \ref MLOG_RESOURCE_TABLE
+ *  @param   freqvalue   The frequency value of the PLL register that has been attached
+ *                       to this resource
+ *
+ *  @return  none
+ *
+ *  \ingroup group_lte_mlog
+ *
+**/
+//-------------------------------------------------------------------------------------------
+static void MLogFREQ(unsigned int resourceid, unsigned long freqvalue)
+{
+    MLOGFREQREC freqreg;
+
+    if (MLogIsEnabled() == MLOG_FALSE)
+        return;
+
+    freqreg.code = MLOG_ID_FREQ;
+    freqreg.resID = resourceid;
+    freqreg.freqID = *((MLOG_DWORD*)&freqvalue);
+    freqreg.freqval = *((MLOG_DWORD*)(((char*)&freqvalue)+MLOG_UINTSIZE));
+
+    MLogWriteStaticRecord((unsigned char *)&freqreg, sizeof(MLOGFREQREC));
+}
+
+
+
+//-------------------------------------------------------------------------------------------
+/** @brief This function is used to mark important instances in exucution path. This is currently
+ *         used in instances like the completion of a subframe of processing. These markers are
+ *         read by the offline TTBox tool to mark lines / delimeters in the graph when plotting
+ *         the output of the MLogger
+ *
+ *
+ *  @param   markid  The options avilable are mentioned in table below
+ *  @param   ticks   State of the global clock at this instance
+ *
+ *  @return  none
+ *
+ * \anchor MLOG_MARKID_TABLE
+ * <BR/>
+ *  TABLE_START
+ *  ---------------------------------------
+ *  |MARK ID       |Type of MARK in TTBox |
+ *  ---------------------------------------
+ *  |0             |MLOG_MARK_NOTHING     |
+ *  ---------------------------------------
+ *  |1             |MLOG_MARK_FRAMEBORDER |
+ *  ---------------------------------------
+ *  TABLE_END
+ * <BR/><BR/>
+ *
+ *  \ingroup group_lte_mlog
+ *
+**/
+//-------------------------------------------------------------------------------------------
+static void MLogMark(unsigned int markid,  unsigned int ticks)
+{
+    MLOGMARKREC markrec;
+
+    //MLOG_DEBUG_PRINT("MLog mark! \r\n");
+
+    if (MLogIsEnabled() == MLOG_FALSE)
+        return;
+
+    if (markid == MLOG_MARK_FRAMEBORDER)
+    {
+        mlog_ctx.StoragePtr->tti_counter++;
+
+#if !defined(_WIN32) && !defined(__linux__) && !defined(__KERNEL__)
+        if(MLOG_DEVICE_COUNT)
+            MLogOnTTIDev();
+#endif
+    }
+
+    markrec.code = MLOG_ID_MARK;
+    markrec.markID = markid;
+    markrec.ticks = ticks;
+    MLogWriteRecord((unsigned char *)&markrec, sizeof(MLOGMARKREC));
+}
+
+static void _MLogExTime(unsigned int resourceId, unsigned int type, unsigned int time_ms, unsigned int event, unsigned int ticks)
+{
+    MLOGTIMEEXREC markrec;
+
+    if (MLogIsEnabled() == MLOG_FALSE) {
+        return;
+    }
+
+    markrec.code = MLOG_ID_EXTIME;
+    markrec.type = type | ((resourceId) << 8) | (event << 16);
+    markrec.time_ms = time_ms;
+    markrec.ticks = ticks;
+
+    MLogWriteRecord((unsigned char *)&markrec, sizeof(markrec));
+}
+
+void MLogExTime(unsigned int resourceId, unsigned int type, unsigned int time_ms, unsigned int event, unsigned int ticks)
+{
+    MLOG_LOCK;
+
+    _MLogExTime(resourceId, type, time_ms, event, ticks);
+
+    MLOG_RELEASE;
+}
+EXPORT_SYMBOL(MLogExTime);
+
+#ifdef MLOG_ADD_DEPENDENCY
+//-------------------------------------------------------------------------------------------
+/** @brief This function attaches dependencies to Tasks / TCBs / Lists that were created. These are
+ *         are displayed in the TTBox tool as lines and are used to anylze where the critical path
+ *         is to recommend better way to scehdule the tasks. It is automatically called by TrEx when
+ *         Dependencies are added to TCBs. It can be called by the user for Tasks / Lists
+ *         that were logged in using the Mlogger APIs
+ *
+ *
+ *  @param   mode    The Mode value is defined in table below
+ *  @param   param1  Pointer to TCB or Task value returned by the MlogTask function
+ *  @param   param2  Pointer to TCB or Task value returned by the MlogTask function
+ *
+ *  @return  none
+ *
+ * \anchor MLOG_DEPENDENCY_MODE_TABLE
+ * <BR/>
+ *  TABLE_START
+ *  -----------------------------------------------------------------
+ *  |MODE ID                  |Dependency Type                      |
+ *  -----------------------------------------------------------------
+ *  |MLOG_DEPENDS_TCB_TCB     |Dependency between 2 TCBs            |
+ *  -----------------------------------------------------------------
+ *  |MLOG_DEPENDS_KEY_KEY     |Dependency between 2 Tasks           |
+ *  -----------------------------------------------------------------
+ *  |MLOG_DEPENDS_LISTID_KEY  |Dependency between a Task and a List |
+ *  -----------------------------------------------------------------
+ *  |MLOG_DEPENDS_KEY_LISTID  |Dependency between a List and a Task |
+ *  -----------------------------------------------------------------
+ *  |MLOG_DEPENDS_LISTID_TCB  |Dependency between a List and a TCB  |
+ *  -----------------------------------------------------------------
+ *  |MLOG_DEPENDS_TCB_LISTID  |Dependency between a TCB and a List  |
+ *  -----------------------------------------------------------------
+ *  TABLE_END
+ * <BR/><BR/>
+ *
+ *  \ingroup group_lte_mlog
+ *
+**/
+//-------------------------------------------------------------------------------------------
+static void MLogDepends(int mode,unsigned int param1,unsigned int param2)
+{
+    MLOGDEPREC deprec;
+
+    if (MLogIsEnabled() == MLOG_FALSE)
+        return;
+
+    deprec.mode = mode;
+    deprec.param1 = param1;
+    deprec.param2 = param2;
+
+    MLogWriteRecord((unsigned char *)&deprec, sizeof(MLOGDEPREC));
+}
+#endif
+
+//-------------------------------------------------------------------------------------------
+/** @brief This function marks a TCB as BYPASSED. The bypassed TCB is not going to be processed
+ *
+ *  @param  ptcb  This is the pointer to the TCB that needs to be logged
+ *  @param  markid  The ID of the mark.
+ *
+ *  @return none
+ *
+ *  \ingroup group_lte_mlog
+ *
+**/
+//-------------------------------------------------------------------------------------------
+static void MLogMarkTcbControl(void* ptcb, unsigned int markid)
+{
+    MLOGTCBCNTRREC tcbctrlrec;
+
+    if (MLogIsEnabled() == MLOG_FALSE)
+        return;
+
+    if (MLogCheckMask(((PTCB)ptcb)->ResourceID) == MLOG_FALSE) return;
+
+    tcbctrlrec.code = MLOG_ID_MARK_TCB_CTRL;
+    tcbctrlrec.ptcb = (MLOG_DWORD)ptcb;
+    tcbctrlrec.markid = markid;
+
+    MLogWriteRecord((unsigned char *)&tcbctrlrec, sizeof(MLOGTCBCNTRREC));
+}
+
+
+#ifdef MLOG_IRQ_SUP_ENABLE
+
+
+//-------------------------------------------------------------------------------------------
+/** @brief This function is automatically called by the 4GMX OS when the ARM switches states.
+ *         It is used later on to analyze what portion of the ARM was handling interrupts and
+ *         what portion of the ARM was handling dispatching / other house cleaning work
+ *
+ *
+ *  @param  code      ID for the state of ARM. For all defined states of the ARM, please look at table below
+ *  @param  resid     ARM ID.
+ *  @param  resindex  This is a reserved files for future. Currently always set to 0
+ *
+ *  @return none
+ *
+ * \anchor MLOG_ARM_STATE_TABLE
+ * <BR/>
+ *  TABLE_START
+ *  ----------------------------------------------
+ *  |CODE                   |ARM State           |
+ *  ----------------------------------------------
+ *  |MLOG_MODE_UNKNOWN      |Unknown state       |
+ *  ----------------------------------------------
+ *  |MLOG_MODE_IRQ          |Servicing Interrups |
+ *  ----------------------------------------------
+ *  |MLOG_MODE_SUPERVISOR   |Normal Master State |
+ *  ----------------------------------------------
+ *  |MLOG_MODE_IDLE         |Idle State          |
+ *  ----------------------------------------------
+ *  TABLE_END
+ * <BR/><BR/>
+ *
+ *  \ingroup group_lte_mlog
+ *
+**/
+//-------------------------------------------------------------------------------------------
+static void MLogResourceCondition(unsigned int code,unsigned int resid,unsigned int resindex)
+{
+    MLOGRESCONDREC rescondrec;
+
+    if (MLogIsEnabled() == MLOG_FALSE)
+        return;
+
+    if (mlog_ctx.Mlogenablerc == 0)
+        return;
+
+    if (MLogCheckMask(resid)== MLOG_FALSE) return;
+
+    rescondrec.id = MLOG_ID_RESCONDITION;
+    rescondrec.code = code;
+    rescondrec.resid_index = ( resid << 16 ) | resindex;
+    rescondrec.ticks = MLOG_GETTICK();
+    MLogWriteRecord((unsigned char *)&rescondrec, sizeof(MLOGRESCONDREC));
+}
+#endif
+
+
+
+//-------------------------------------------------------------------------------------------
+/** @brief This function is used to enable the logging of the state of the ARM. There are a few
+ *         modes that the 4GMX OS recognizes the ARM to be under. This is automatically called
+ *         by the OS during bootup to enable this feature
+ *
+ *
+ *  @param   bVal  0 = Disable, 1 = Enable
+ *
+ *  @return  none
+ *
+ *  \ingroup group_lte_mlog
+ *
+**/
+//-------------------------------------------------------------------------------------------
+static void MLogEnableRC(unsigned int bVal)
+{
+    mlog_ctx.Mlogenablerc = bVal;
+
+}
+
+
+
+//-------------------------------------------------------------------------------------------
+/** @brief This function is used to register the creation of a TaskList. It is automatically
+ *         by the TrEx when a new list is created
+ *
+ *
+ *  @param   listid  ID of the List being created
+ *
+ *  @return  none
+ *
+ *  \ingroup group_lte_mlog
+ *
+**/
+//-------------------------------------------------------------------------------------------
+static void MLogTCBList(unsigned int listid)
+{
+    MLogTCBListPut(MLOG_ID_LIST, listid);
+}
+
+
+
+
+//-------------------------------------------------------------------------------------------
+/** @brief This function is used to log in the time when a task list was started to execute.
+ *         It is automatically called by the TrEx when a List start execution
+ *
+ *
+ *  @param   listid  ID of the List being Executed
+ *
+ *  @return  none
+ *
+ *  \ingroup group_lte_mlog
+ *
+**/
+//-------------------------------------------------------------------------------------------
+static void MLogTCBListStart(unsigned int listid)
+{
+    MLogTCBListPut(MLOG_ID_LISTSTART, listid);
+}
+
+static void MLogTCBListStartEx(unsigned int listid, unsigned int cpu)
+{
+    MLogTCBListPutEx(MLOG_ID_LISTSTART, listid, cpu);
+}
+
+
+//-------------------------------------------------------------------------------------------
+/** @brief This function is used to log in the time when a task list was started to execute.
+ *         It is automatically called by the TrEx when a List finishes execution
+ *
+ *
+ *  @param   listid  ID of the List that finished Execution
+ *
+ *  @return  none
+ *
+ *  \ingroup group_lte_mlog
+ *
+**/
+//-------------------------------------------------------------------------------------------
+static void MLogTCBListStop(unsigned int listid)
+{
+    MLogTCBListPut(MLOG_ID_LISTSTOP, listid);
+}
+
+
+
+
+
+
+//-------------------------------------------------------------------------------------------
+/** @brief This function is to register a TCB with a certain Task List. It is automatically
+ *         called by the TrEx when a TCB is added to a List
+ *
+ *
+ *  @param   ptcb    Pointer to the TCB that needs to be logged
+ *  @param   listid  ID of the List that the TCB is being added to
+ *
+ *  @return  none
+ *
+ *  \ingroup group_lte_mlog
+ *
+**/
+//-------------------------------------------------------------------------------------------
+static void MLogTCBAddToList(void* ptcb,unsigned int listid)
+{
+    MLOGADDLISTREC addlistrec;
+
+    if (MLogIsEnabled() == MLOG_FALSE)
+        return;
+
+    if (MLogCheckMask(((PTCB)ptcb)->ResourceID)== MLOG_FALSE)
+        return;
+
+    addlistrec.code = MLOG_ID_ADDTOLIST;
+    addlistrec.ptcb = (MLOG_DWORD)ptcb;
+    addlistrec.listid = listid;
+
+    MLogWriteRecord((unsigned char *)&addlistrec, sizeof(MLOGADDLISTREC));
+}
+
+
+
+#if !defined(_WIN32)
+static void MLogDevInfo(unsigned devInfo)
+{
+    MOGDEVINFOREC devinforec;
+
+    if (MLogIsEnabled() == MLOG_FALSE)
+        return;
+
+    devinforec.code = MLOG_ID_DEVICE_INFO;
+    devinforec.devinfo = devInfo;
+
+    MLogWriteStaticRecord((unsigned char *)&devinforec, sizeof(MOGDEVINFOREC));
+}
+
+static void MLogRegisterFrameSubframe(unsigned int frameNum, unsigned int subFrameNum)
+{
+    MLOGFRAMESFREC farmesfrec;
+
+    if (MLogIsEnabled() == MLOG_FALSE)
+        return;
+
+    farmesfrec.code = MLOG_ID_FR_SUBFR;
+    farmesfrec.framenum = frameNum;
+    farmesfrec.subframenum = subFrameNum;
+
+    MLogWriteRecord((unsigned char *)&farmesfrec, sizeof(MLOGFRAMESFREC));
+}
+
+void MLogAddVariables(unsigned int numVar, unsigned int *variables, unsigned int ticks)
+{
+    unsigned int buff[16];
+    unsigned int numBytes = (numVar + 1) * sizeof(unsigned int);
+
+    if((numBytes + sizeof(unsigned int)) > (sizeof(buff) * sizeof(unsigned int)))
+        return;
+
+    MLOG_LOCK;
+
+    if (MLogIsEnabled() == MLOG_FALSE) {
+	MLOG_RELEASE;
+        return;
+    }
+
+    buff[0] = MLOG_ID_ADD_VARIABLES | numBytes;
+    buff[1] = ticks;
+
+    _memcpy(buff + 2, variables, numVar * sizeof(unsigned int));
+
+    MLogWriteRecord((unsigned char *)&buff, numBytes + sizeof(unsigned int));
+
+    MLOG_RELEASE;
+}
+
+
+static void MLogCacheMipsStats(unsigned int ticks)
+{
+#ifdef PM_ENABLED
+    MLOGCACHEMIPSHEADREC headrec;
+    MLOGCACHEMIPSREC cmrec;
+    unsigned int i;
+
+    if (MLogIsEnabled() == MLOG_FALSE)
+        return;
+
+    headrec.code = MLOG_ID_CACHE_MIPS_STATS;
+    headrec.ticks = ticks;
+    headrec.buffcount = CPU_NUM;
+
+    MLogWriteStaticRecord((unsigned char *)&headrec, sizeof(MLOGCACHEMIPSHEADREC));
+
+    for (i = 0; i < CPU_NUM; i++)
+    {
+        PMStopCalcDataCacheMissEx(i);
+        PMStopCalcDataCacheHitEx(i);
+        PMStopCalcCycleEx(i);
+
+        cmrec.buffnum = i;
+        cmrec.cachemiss = PMGetDataCacheMissNumberEx(i);    // Cache Miss
+        cmrec.cachehit = PMGetDataCacheHitNumberEx(i);     // Cache Hit
+        cmrec.irqcycles = PMGetCyclesForArmModeEx(i, PM_ARM_MODE_IRQ);  // Arm IRQ Cycles
+        cmrec.idlecycles = PMGetCyclesForArmModeEx(i, PM_ARM_MODE_IDLE); // Arm IDLE Cycles
+        cmrec.supervisiorcycles = PMGetCyclesForArmModeEx(i, PM_ARM_MODE_SVSR); // Arm SUPERVISOR Cycles
+        cmrec.irqcount = PMGetARMProcIrqNumEx(i);                      // Number of IRQs
+
+        //MLogWriteStaticRecord((unsigned char *)&cmrec, sizeof(MLOGCACHEMIPSREC));
+        MLogWriteRecord((unsigned char *)&cmrec, sizeof(MLOGCACHEMIPSREC));
+
+        PMStartCalcDataCacheMissEx(i);
+        PMStartCalcDataCacheHitEx(i);
+        PMStartCalcCycleEx(i);
+    }
+#else
+    if (MLogIsEnabled() == MLOG_FALSE)
+        return;
+#endif
+
+}
+
+
+static void MLogCacheMipsStatsSingle(unsigned int ticks)
+{
+#ifdef PM_ENABLED
+
+    MLOGCACHEMIPSHEADREC headrec;
+    MLOGCACHEMIPSSREC cmrec;
+    unsigned int armId = MxGetCpuID();
+
+    if (MLogIsEnabled() == MLOG_FALSE)
+        return;
+
+    headrec.code = MLOG_ID_CACHE_MIPS_STATS_SINGLE;
+    headrec.ticks = ticks;
+    headrec.buffcount = 8;
+
+    cmrec.buffnum = armId;
+    cmrec.cachemiss = PMGetCacheMissStat();                             // Cache Miss
+    cmrec.cachehit = PMGetCacheHitStat();                              // Cache Hit
+    cmrec.cachemissinst = PMGetInstrCacheMissStat();                        // Instruction Cache Miss
+    cmrec.irqcycles = PMGetCyclesForArmModeEx(armId, PM_ARM_MODE_IRQ);  // Arm IRQ Cycles
+    cmrec.idlecycles = PMGetCyclesForArmModeEx(armId, PM_ARM_MODE_IDLE); // Arm IDLE Cycles
+    cmrec.supervisiorcycles = PMGetCyclesForArmModeEx(armId, PM_ARM_MODE_SVSR); // Arm SUPERVISOR Cycles
+    cmrec.irqcount = PMGetARMProcIrqNumEx(armId);                      // Number of IRQs
+
+    //MLogWriteStaticRecord((unsigned char *)&headrec, sizeof(MLOGCACHEMIPSHEADREC));
+    //MLogWriteStaticRecord((unsigned char *)&cmrec, sizeof(MLOGCACHEMIPSSREC));
+
+    MLogWriteRecord((unsigned char *)&headrec, sizeof(MLOGCACHEMIPSHEADREC));
+    MLogWriteRecord((unsigned char *)&cmrec, sizeof(MLOGCACHEMIPSSREC));
+
+
+#else
+    if (MLogIsEnabled() == MLOG_FALSE)
+        return;
+#endif
+}
+
+static void MLogCacheMipsStatsSingleEx(unsigned int ticks)
+{
+#ifdef PM_ENABLED
+    unsigned int armId = MxGetCpuID();
+    MLOGCACHEMIPSHEADREC headrec;
+    MLOGCACHEMIPSREC cmrec;
+
+    if (MLogIsEnabled() == MLOG_FALSE)
+        return;
+
+    headrec.code = MLOG_ID_CACHE_MIPS_STATS_SINGLE;
+    headrec.ticks = ticks;
+    headrec.buffcount = 1;
+
+    PMStopCalcDataCacheMissEx(armId);
+    PMStopCalcDataCacheHitEx(armId);
+    PMStopCalcCycleEx(armId);
+
+    cmrec.buffnum = armId;
+    cmrec.cachemiss = PMGetCacheMissStat();                             // Cache Miss
+    cmrec.cachehit = PMGetCacheHitStat();                              // Cache Hit
+    cmrec.irqcycles = PMGetCyclesForArmModeEx(armId, PM_ARM_MODE_IRQ);  // Arm IRQ Cycles
+    cmrec.idlecycles = PMGetCyclesForArmModeEx(armId, PM_ARM_MODE_IDLE); // Arm IDLE Cycles
+    cmrec.supervisiorcycles = PMGetCyclesForArmModeEx(armId, PM_ARM_MODE_SVSR); // Arm SUPERVISOR Cycles
+    cmrec.irqcount = PMGetARMProcIrqNumEx(armId);                      // Number of IRQs
+
+    MLogWriteStaticRecord((unsigned char *)&headrec, sizeof(MLOGCACHEMIPSHEADREC));
+    MLogWriteStaticRecord((unsigned char *)&cmrec, sizeof(MLOGCACHEMIPSREC));
+
+    PMStartCalcDataCacheMissEx(armId);
+    PMStartCalcDataCacheHitEx(armId);
+    PMStartCalcCycleEx(armId);
+#else
+    if (MLogIsEnabled() == MLOG_FALSE)
+       return;
+#endif
+
+}
+
+
+#endif
+
+
+
+//-------------------------------------------------------------------------------------------
+/** @brief DOX_INTERNAL DOXYGEN_TO_DO
+ *
+ *  @param  none
+ *
+ *
+ *  @return none
+ *
+ *  \ingroup group_lte_mlog
+ *
+**/
+//-------------------------------------------------------------------------------------------
+static unsigned int MLogFinish()
+{
+    unsigned int j, size, shift;
+    unsigned char *copy_to;
+
+#if defined(_WIN32)
+    printf("\nCleaning up MLOG!!!\n");
+#else
+    MLOG_DEBUG_PRINT("\nCleaning up MLOG!!!\n");
+#endif
+
+    if (mlog_ctx.Finalized == MLOG_TRUE)
+        return MLOG_TRUE;
+
+    mlog_ctx.StorageSize = sizeof(MLOGFILEHEADER) + sizeof(MLOGFHPART) * mlog_ctx.StoragePtr->hdr->PartNum;
+    copy_to = mlog_ctx.StoragePtr->ptr + mlog_ctx.StorageSize;
+
+#ifdef MLOG_DEVICE_COUNT
+    for(j = CPU_NUM; j < (CPU_NUM + MLOG_DEVICE_COUNT); j++)
+    {
+#if !defined(_WIN32) && !defined(__linux__) && !defined(__KERNEL__)
+        MLOG_STORAGE_HEADER *storage = mlog_ctx.StoragePtr->storage[j];
+	if(storage->mem_part.storage)
+	MxCacheInvalidate(storage->ptr, storage->mempart_size);
+#endif
+    }
+
+#endif
+
+    for(j = 0; j < mlog_ctx.StoragePtr->hdr->PartNum ; j++)
+    {
+        MLOG_STORAGE_HEADER *storage = mlog_ctx.StoragePtr->storage[j];
+        MLOG_FRAME_BLOCK_HEADER *block;
+
+		if(!storage->mem_part.storage)
+        {
+			continue;
+        }
+
+        size = storage->block_num * MLOG_FRAME_SIZE;
+        mlog_ctx.StorageSize += size;
+        storage->hdr->Size = size;
+        if(!j)
+        {
+            copy_to += size;
+        }
+        else  if (j && storage->ptr)
+        {
+            memcpy(copy_to, storage->ptr, size);
+            shift = storage->ptr - copy_to;
+
+            copy_to += size;
+
+            storage->current_block -= shift;
+
+            if (storage->static_block != NULL)
+            {
+                storage->static_block -= shift;
+                storage->hdr->StaticData -= shift;
+                block = (MLOG_FRAME_BLOCK_HEADER *)(storage->hdr->StaticData);
+            }
+            else
+            {
+                block = NULL;
+            }
+
+            if (storage->first_block != NULL)
+            {
+                storage->first_block -= shift;
+            }
+
+            storage->hdr->DynamicData -= shift;
+
+            while(block)
+            {
+                if(block->next)
+                    block->next = (MLOG_FRAME_BLOCK_HEADER *)((UINT32)(block->next) - shift);
+                block = block->next;
+            }
+
+            block = (MLOG_FRAME_BLOCK_HEADER *)(storage->hdr->DynamicData);
+            while(block)
+            {
+                if(block->next)
+                    block->next = (MLOG_FRAME_BLOCK_HEADER *)((UINT32)(block->next) - shift);
+                block = block->next;
+            }
+        }
+    }
+
+    mlog_ctx.Finalized = MLOG_TRUE;
+
+    return MLOG_TRUE;
+}
+
+
+//-------------------------------------------------------------------------------------------
+/** @brief This function is used to finalize the Mlog Logged in and print out the location
+ *         and length of the Mlog stored in DDR. It is automatically called when the PHY_STOP
+ *         or PHY_SHUTDOWN API are initiated from the MAC side
+ *
+ *
+ *  @param  none
+ *
+ *  @return none
+ *
+ *  \ingroup group_lte_mlog
+ *
+**/
+//-------------------------------------------------------------------------------------------
+#if defined(__KERNEL__)
+static unsigned int MLogPrint()
+{
+	MLogFinish();
+
+	uart_printf("Location of MLog: 0x%08lx %d size\n", MLogVirt2Phys(MLogGetFileLocation()), MLogGetFileSize());
+
+	return MLOG_TRUE;
+}
+#else
+static unsigned int MLogPrint()
+{
+#if defined(_WIN32) || defined(__linux__)
+    FILE* file;
+#endif
+
+    if (mlog_ctx.Opened == MLOG_FALSE)
+        return MLOG_FALSE;
+
+    if (mlog_ctx.Finalized == MLOG_TRUE)
+    {
+#if defined(_WIN32)
+        printf("MLOG already Finalized!!!\r\n");
+#else
+        MLOG_DEBUG_PRINT("MLOG already Finalized!!!\r\n");
+#endif
+    return MLOG_TRUE;
+    }
+
+#if !defined(_WIN32) && !defined(__linux__)
+    MxDelayTicks(150000);
+#endif
+
+#if !defined(_WIN32) && !defined(__linux__) && !defined(__KERNEL__)
+    if (MLogFlush() == MLOG_FALSE)
+        return MLOG_FALSE;
+#endif
+
+#if defined(_WIN32) || defined(__linux__) && !defined(__KERNEL__)
+    file = fopen (MLOG_FILENAME, "wb");
+
+    if (file == NULL)
+        return MLOG_FALSE;
+
+    fwrite(MLogGetFileLocation(), 1, MLogGetFileSize(), file);
+    fclose(file);
+
+#endif
+
+    return MLOG_TRUE;
+
+}
+#endif
+
+#if !defined(_WIN32) && !defined(__linux__) && !defined(__KERNEL__)
+
+static unsigned int MLogFlush (void)
+{
+    unsigned int *pMLogArray;
+    unsigned int MLogLength;
+
+    MLogFinish();
+
+    MLogLength = MLogGetFileSize();
+    pMLogArray = MLogGetFileLocation();
+
+    if (pMLogArray != NULL)
+    {
+	uart_printf("Location of MLog: 0x%x %d size\n", MLogVirt2Phys(pMLogArray), MLogLength);
+
+        uart_printf("MLog(ver %02d) location: 0x%x\r\n", MLOG_VERSION, pMLogArray);
+        uart_printf("MLog storage length: %d (bytes)\r\n", MLogLength);
+        uart_printf("[rdmem 0x%x %d -o mlog.bin]\r\n", pMLogArray, MLogLength);
+
+        MxCacheClean(pMLogArray, MLogLength);
+    }
+    else
+    {
+        uart_printf("The storage is NULL!\r\n");
+        return MLOG_FALSE;
+    }
+
+    mlog_ctx.Started = MLOG_FALSE;
+
+    return MLOG_TRUE;
+}
+#endif
+
+static INLINE unsigned int MLogWriteRecord(unsigned char *rec, unsigned int size)
+{
+    return mlog_ctx.StoragePtr->wr_dyn[MxGetCpuID()](MxGetCpuID(), rec, size);
+}
+
+static INLINE unsigned int MLogWriteRecordCpu(unsigned int cpu, unsigned char *rec, unsigned int size)
+{
+    return mlog_ctx.StoragePtr->wr_dyn[cpu](cpu, rec, size);
+}
+
+
+static INLINE unsigned int MLogWriteStaticRecord(unsigned char *rec, unsigned int size)
+{
+    return mlog_ctx.StoragePtr->wr_stat[MxGetCpuID()](MxGetCpuID(), rec, size);
+}
+
+static unsigned int MLogCreateStorage(MLOG_STORAGE_CTX *ctx, LPVOID mem)
+{
+    int i;
+    MLOGFILEHEADER  *hdr;
+
+    if(!ctx)
+        return RC_MLOG_CTX_ERROR;
+
+    hdr = (MLOGFILEHEADER *)mem;
+    hdr->Magic = MLOG_MAGIC;
+    hdr->Version = MLOG_VERSION;
+    hdr->BasePointer = (MLOG_DWORD)mem;
+    hdr->PartNum = (CPU_NUM + MLOG_DEVICE_COUNT);
+    ctx->hdr = hdr;
+
+    for(i = 0; i < CPU_NUM + MLOG_DEVICE_COUNT; i++)
+    {
+        ctx->storage[i] = MLogAlloc(sizeof(MLOG_STORAGE_HEADER));
+
+#if !defined(_WIN32) && !defined(__linux__) && !defined(__KERNEL__)
+        _ASSERT_PTR(ctx->storage[i]);
+#endif
+
+        memset(ctx->storage[i], 0, sizeof(MLOG_STORAGE_HEADER));
+
+        ctx->storage[i]->hdr = (MLOGFHPART *)((unsigned char *)ctx->hdr + sizeof(MLOGFILEHEADER) + i * sizeof(MLOGFHPART));
+        ctx->storage[i]->hdr->Index = i;
+        ctx->storage[i]->hdr->Size = MLOG_FRAME_LIMIT * MLOG_FRAME_SIZE;
+        ctx->storage[i]->hdr->DynamicData = ctx->storage[i]->hdr->StaticData = 0;
+
+        if(i < CPU_NUM)
+        {
+            ctx->wr_dyn[i] = ctx->wr_stat[i] = MLogWriteFirstTime;
+        }
+        else
+        {
+            // ???
+        }
+    }
+
+    return MX_OK;
+}
+
+static unsigned int MLogWriteFirstTime(unsigned int cpu, unsigned char *rec, unsigned int size)
+{
+	UINT32 rc;
+	MLOG_STORAGE_HEADER * storage;
+
+	if((rc = MLogCreatePartition(cpu)) != MX_OK)
+		return rc;
+
+	storage = mlog_ctx.StoragePtr->storage[cpu];
+
+    _memcpy(storage->static_block->cur_data, rec, size);
+    storage->static_block->cur_data += size;
+    storage->static_block->payload += size;
+    mlog_ctx.StoragePtr->wr_dyn[cpu] = mlog_ctx.StoragePtr->wr_stat[cpu] = MLogWriteStaticRecordEx;
+
+    return MX_OK;
+}
+
+static unsigned int MLogCreatePartition(unsigned int cpu)
+{
+    unsigned char * membgn = NULL;
+    MLOG_STORAGE_HEADER * storage = mlog_ctx.StoragePtr->storage[cpu];
+
+#if !defined(_WIN32) && !defined(__linux__) && !defined(__KERNEL__)
+        _ASSERT_PTR(storage);
+#endif
+
+    membgn = ((unsigned char *)mlog_ctx.StoragePtr->hdr + sizeof(MLOGFILEHEADER) + (CPU_NUM + MLOG_DEVICE_COUNT) * sizeof(MLOGFHPART));
+
+    if (cpu < CPU_NUM)
+    {
+        storage->ptr = membgn+ cpu*(MLOG_FRAME_LIMIT * MLOG_FRAME_SIZE);
+    }
+    else
+    {
+        storage->ptr = mlog_ctx.DevStoragePtr + (cpu-CPU_NUM) * (MLOG_FRAME_LIMIT * MLOG_FRAME_SIZE);
+    }
+
+    MxiDefSysPart(&(storage->mem_part),(unsigned char *)storage->ptr, MLOG_FRAME_SIZE, MLOG_FRAME_LIMIT);
+
+    storage->mempart_size = MLOG_FRAME_LIMIT * MLOG_FRAME_SIZE;
+    storage->static_block = (MLOG_FRAME_BLOCK_HEADER *) MxiAllocSysPart(&storage->mem_part);
+    storage->first_block = storage->current_block = (MLOG_FRAME_BLOCK_HEADER *) MxiAllocSysPart(&storage->mem_part);
+
+    if(!storage->current_block || !storage->static_block)
+        return RC_MLOG_ALLOC_ERROR;
+
+    storage->block_num += 2;
+
+    memset(storage->static_block, 0, sizeof(MLOG_FRAME_BLOCK_HEADER));
+
+    storage->static_block->cur_data = ((unsigned char *)storage->static_block) + sizeof(MLOG_FRAME_BLOCK_HEADER);
+    storage->static_block->size = MLOG_FRAME_SIZE - sizeof(MLOG_FRAME_BLOCK_HEADER);
+
+    storage->hdr->StaticData = (MLOG_DWORD)storage->static_block;
+
+    //MLOG_DEBUG_PRINT("Create storage (%d) mem ptr %X, static block ptr %X, cur block ptr %X\r\n", cpu, membgn, storage->static_block, storage->current_block);
+
+    memset(storage->current_block, 0, sizeof(MLOG_FRAME_BLOCK_HEADER));
+
+    storage->current_block->cur_data = ((unsigned char *)storage->current_block) + sizeof(MLOG_FRAME_BLOCK_HEADER);
+    storage->current_block->size = MLOG_FRAME_SIZE - sizeof(MLOG_FRAME_BLOCK_HEADER);
+    storage->hdr->DynamicData = (MLOG_DWORD)storage->current_block;
+
+    return MX_OK;
+}
+
+static unsigned int MLogWriteRecordEx(unsigned int cpu, unsigned char *rec, unsigned int size)
+{
+    MLOG_STORAGE_HEADER * storage;
+    MLOG_FRAME_BLOCK_HEADER * curr;
+
+#if !defined(_WIN32) && !defined(__linux__) && !defined(__KERNEL__)
+    _ASSERT_PTR(mlog_ctx.StoragePtr);
+#endif
+
+    storage = mlog_ctx.StoragePtr->storage[cpu];
+#if !defined(_WIN32) && !defined(__linux__) && !defined(__KERNEL__)
+    _ASSERT_PTR(storage);
+#endif
+
+    //MLOG_DEBUG_PRINT("dynamic: storage (%d): DYN tti %d, global tti %d\r\n", cpu, storage->tti_counter, mlog_ctx.StoragePtr->tti_counter);
+
+    curr = storage->current_block;
+
+    if(storage->tti_counter < mlog_ctx.StoragePtr->tti_counter)
+    {
+        //MLOG_DEBUG_PRINT("storage (%d): DYN block num %d\r\n", cpu, storage->block_num);
+
+        while(storage->block_num >= MLOG_FRAME_LIMIT)
+        {
+            if(storage->first_block)
+            {
+                MLOG_FRAME_BLOCK_HEADER * first = storage->first_block->next;
+
+                MxiFreeSysPart(&storage->mem_part,storage->first_block);
+
+                storage->first_block = first;
+                storage->block_num--;
+                storage->hdr->DynamicData = (MLOG_DWORD)first;
+            }
+            else
+            {
+                MLOG_DEBUG_PRINT("MLOG: First block not present!!!\r\n");
+                return RC_MLOG_ALLOC_ERROR;
+            }
+        }
+
+        //MLOG_DEBUG_PRINT("dynamic storage (%d): add next tti block\r\n");
+        curr->next = (MLOG_FRAME_BLOCK_HEADER *) MxiAllocSysPart(&storage->mem_part);
+
+#if !defined(_WIN32) && !defined(__linux__) && !defined(__KERNEL__)
+        _ASSERT_PTR(curr->next);
+#endif
+
+        if(curr->next)
+        {
+            curr = curr->next;
+            storage->current_block = curr;
+
+            curr->next = NULL;
+            curr->payload = 0;
+            curr->cur_data = ((unsigned char *)curr) + sizeof(MLOG_FRAME_BLOCK_HEADER);
+            curr->size = MLOG_FRAME_SIZE - sizeof(MLOG_FRAME_BLOCK_HEADER);
+            storage->block_num++;
+        }
+        else
+        {
+            MLOG_DEBUG_PRINT("MLogWriteRecordEx: no memory\r\n");
+            return 1;
+        }
+
+        storage->tti_counter = mlog_ctx.StoragePtr->tti_counter;
+    }
+    //MLOG_DEBUG_PRINT("dynamic storage (%d): payload %d, size %d, block size %d\r\n", cpu, storage->current_block->payload, size,  storage->current_block->size);
+
+    if(curr->payload + size > curr->size)
+    {
+        //MLOG_DEBUG_PRINT(" DYN Add block \r\n");
+
+        while(storage->block_num >= MLOG_FRAME_LIMIT)
+        {
+            if(storage->first_block)
+            {
+                MLOG_FRAME_BLOCK_HEADER * first = storage->first_block->next;
+
+                MxiFreeSysPart(&storage->mem_part,storage->first_block);
+
+                storage->first_block = first;
+                storage->block_num--;
+                storage->hdr->DynamicData = (MLOG_DWORD)first;
+            }
+            else
+            {
+                MLOG_DEBUG_PRINT("MLOG: First block not present 2!!!\r\n");
+                return RC_MLOG_ALLOC_ERROR;
+            }
+        }
+
+        curr->next = (MLOG_FRAME_BLOCK_HEADER *) MxiAllocSysPart(&storage->mem_part);
+
+#if !defined(_WIN32) && !defined(__linux__) && !defined(__KERNEL__)
+        _ASSERT_PTR(curr->next);
+#endif
+
+        if(curr->next)
+        {
+            curr = curr->next;
+            storage->current_block = curr;
+
+            curr->next = NULL;
+            curr->payload = 0;
+            curr->cur_data = ((unsigned char *)curr) + sizeof(MLOG_FRAME_BLOCK_HEADER);
+            curr->size = MLOG_FRAME_SIZE - sizeof(MLOG_FRAME_BLOCK_HEADER);
+            storage->block_num++;
+        }
+        else
+        {
+            MLOG_DEBUG_PRINT("MLogWriteRecordEx: no memory2\r\n");
+            return 1;
+        }
+    }
+
+    _memcpy(curr->cur_data, rec, size);
+    curr->cur_data += size;
+    curr->payload += size;
+
+    return MX_OK;
+}
+
+
+
+static unsigned int MLogWriteStaticRecordEx(unsigned int cpu, unsigned char *rec, unsigned int size)
+{
+    MLOG_STORAGE_HEADER * storage = mlog_ctx.StoragePtr->storage[cpu];
+    MLOG_FRAME_BLOCK_HEADER * curr;
+
+#if !defined(_WIN32) && !defined(__linux__) && !defined(__KERNEL__)
+    _ASSERT_PTR(storage);
+#endif
+
+    //MLOG_DEBUG_PRINT("stat storage (%d): payload %d, size %d, block size %d\r\n", cpu, storage->static_block->payload, size,  storage->static_block->size);
+
+    if(mlog_ctx.StoragePtr->tti_counter)
+    {
+        mlog_ctx.StoragePtr->wr_dyn[cpu] = MLogWriteRecordEx;
+        mlog_ctx.StoragePtr->wr_stat[cpu] = MLogWriteStaticRecordEx2;
+        MLogWriteRecordEx(cpu, rec, size);
+        return MX_OK;
+    }
+
+    curr = storage->static_block;
+
+    if(curr->payload + size > curr->size)
+    {
+        //MLOG_DEBUG_PRINT("STAT Add block \r\n");
+        curr->next = (MLOG_FRAME_BLOCK_HEADER *) MxiAllocSysPart(&storage->mem_part);
+#if !defined(_WIN32) && !defined(__linux__) && !defined(__KERNEL__)
+        _ASSERT_PTR(storage->static_block->next);
+#endif
+
+        if(curr->next)
+        {
+            curr = curr->next;
+            storage->static_block = curr;
+
+            curr->next = NULL;
+            curr->payload = 0;
+            curr->cur_data = ((unsigned char *)curr) + sizeof(MLOG_FRAME_BLOCK_HEADER);
+            curr->size = MLOG_FRAME_SIZE - sizeof(MLOG_FRAME_BLOCK_HEADER);
+            storage->block_num++;
+        }
+        else
+        {
+            MLOG_DEBUG_PRINT("MLogWriteStaticRecordEx: no memory\r\n");
+            return 1;
+        }
+    }
+
+    _memcpy(curr->cur_data, rec, size);
+    curr->cur_data += size;
+    curr->payload += size;
+
+    return MX_OK;
+}
+
+static unsigned int MLogWriteStaticRecordEx2(unsigned int cpu, unsigned char *rec, unsigned int size)
+{
+    MLOG_STORAGE_HEADER * storage = mlog_ctx.StoragePtr->storage[cpu];
+    MLOG_FRAME_BLOCK_HEADER * curr;
+
+#if !defined(_WIN32) && !defined(__linux__) && !defined(__KERNEL__)
+    _ASSERT_PTR(storage);
+#endif
+
+    curr = storage->static_block;
+
+    if(curr->payload + size > curr->size)
+    {
+        //MLOG_DEBUG_PRINT("Stat 2 Add block \r\n");
+        curr->next = (MLOG_FRAME_BLOCK_HEADER *) MxiAllocSysPart(&storage->mem_part);
+#if !defined(_WIN32) && !defined(__linux__) && !defined(__KERNEL__)
+        _ASSERT_PTR(curr->next);
+#endif
+
+        if(curr->next)
+        {
+            curr = curr->next;
+            storage->static_block = curr;
+
+            curr->next = NULL;
+            curr->payload = 0;
+            curr->cur_data = ((unsigned char *)curr) + sizeof(MLOG_FRAME_BLOCK_HEADER);
+            curr->size = MLOG_FRAME_SIZE - sizeof(MLOG_FRAME_BLOCK_HEADER);
+            storage->block_num++;
+        }
+        else
+        {
+            MLOG_DEBUG_PRINT("MLogWriteStaticRecordEx2: no memory\r\n");
+            return 1;
+        }
+    }
+
+    _memcpy(curr->cur_data, rec, size);
+    curr->cur_data += size;
+    curr->payload += size;
+
+    return MX_OK;
+}
+
+#if defined(_WIN32) || defined(__linux__) || defined(__KERNEL__)
+
+static void MxiDefSysPart(SYSFPART * pPart, LPVOID pStorage, UINT32 nBlkSize,
+                   UINT32 nBlkCnt)
+{
+    UINT32 *p;
+#if !defined(_WIN32) && !defined(__linux__) && !defined(__KERNEL__)
+    _ASSERT(pPart != NULL);
+    _ASSERT(pStorage != NULL);
+#endif
+    pPart->storage = (UINT32 *) pStorage;
+    pPart->freeblk = (UINT32 *) pStorage;
+    pPart->blkcnt = nBlkCnt;
+    pPart->blksize = nBlkSize;
+
+    p = pPart->storage;
+
+    while (--nBlkCnt)
+    {
+        *p = (UINT32)(((UINT8 *) p) + nBlkSize);
+        p = (UINT32 *) * p;
+    }
+
+    *p = (UINT32) NULL;
+}
+
+
+static LPVOID MxiAllocSysPart(SYSFPART * pPart)
+{
+    UINT32 *p;
+
+#if !defined(_WIN32) && !defined(__linux__) && !defined(__KERNEL__)
+    _ASSERT(pPart != NULL);
+#endif
+
+    p = pPart->freeblk;
+
+    if (p)
+    {
+        pPart->freeblk = (UINT32 *) * p;
+        pPart->AllocCnt++;
+    }
+
+    return p;
+}
+
+static int MxiGetBlockIndex (SYSFPART * pPart, LPVOID pBlock)
+{
+#if !defined(_WIN32) && !defined(__linux__) && !defined(__KERNEL__)
+    _ASSERT_PTR(pPart);
+    _ASSERT_PTR(pBlock);
+#endif
+    return ((UINT32)pBlock - (UINT32)pPart->storage) / pPart->blksize;
+}
+
+static void MxiFreeSysPart(SYSFPART * pPart, LPVOID pBlk)
+{
+#if !defined(_WIN32) && !defined(__linux__) && !defined(__KERNEL__)
+    _ASSERT(pPart != NULL);
+    _ASSERT(pBlk != NULL);
+#endif
+    *(UINT32 *) pBlk = (UINT32) pPart->freeblk;
+    pPart->freeblk = (UINT32 *) pBlk;
+
+    pPart->AllocCnt--;
+}
+
+
+#endif
+
+#if !defined(_WIN32) && !defined(__linux__) && !defined(__KERNEL__)
+
+static MLOG_FRAME_BLOCK_HEADER * MLogDevGetCurBlock(unsigned int devid)
+{
+	MLOG_STORAGE_HEADER * storage = NULL;
+
+    if(devid < (CPU_NUM + MLOG_DEVICE_COUNT))
+        storage = mlog_ctx.StoragePtr->storage[devid];
+    else
+        return NULL;
+
+    _ASSERT_PTR(storage);
+
+	if(!storage->mem_part.storage)
+    {
+		if(MLogCreatePartition(devid) != MX_OK)
+			return NULL;
+    }
+
+    // we need to put data to the DDR (CEVA needs to read correct data)
+    // and also to invalidate the same memory to allow ARM read data from DDR
+    // the data prepared by CEVA
+
+    if (storage->current_block != NULL)
+    {
+        MxCacheFlush(storage->current_block, sizeof (MLOG_FRAME_BLOCK_HEADER));
+    }
+
+    return storage->current_block;
+}
+
+static MLOG_STORAGE_HEADER * MlogDevGetCurStorage(unsigned int devid)
+{
+    if(devid < (CPU_NUM + MLOG_DEVICE_COUNT))
+        return mlog_ctx.StoragePtr->storage[devid];
+    else
+        return NULL;
+}
+
+#if 0
+static unsigned int MLogDevWriteRecord(MLOG_STORAGE_HEADER *storage, MLOG_FRAME_BLOCK_HEADER * current_block, unsigned char *rec, unsigned int size)
+{
+    if(storage->current_block->payload + size <= storage->current_block->size)
+    {
+        _memcpy(current_block->cur_data, rec, size);
+        current_block->cur_data += size;
+        current_block->payload += size;
+    }
+    else
+        return (unsigned int)-1;
+
+    return MX_OK;
+}
+#endif
+
+static void MLogOnTTIDev(void)
+{
+    unsigned int j;
+    MLOG_STORAGE_HEADER *storage;
+    MLOG_STORAGE_CTX *ctx = mlog_ctx.StoragePtr;
+
+#if 0
+    unsigned char * membgn = NULL;
+#endif
+
+    for(j = CPU_NUM; j < (CPU_NUM + MLOG_DEVICE_COUNT); j++)
+    {
+        storage = ctx->storage[j];
+
+#if !defined(_WIN32) && !defined(__linux__) && !defined(__KERNEL__)
+        _ASSERT_PTR(storage);
+#endif
+
+        if(! storage->block_num)
+        {
+#if 1
+            continue;
+#else
+            membgn = ((unsigned char *)mlog_ctx.StoragePtr->hdr + sizeof(MLOGFILEHEADER) + (CPU_NUM + MLOG_DEVICE_COUNT) * sizeof(MLOGFHPART));
+            storage->ptr = membgn + j*(MLOG_FRAME_LIMIT * MLOG_FRAME_SIZE);
+            MxiDefSysPart(&(storage->mem_part),(unsigned char *)storage->ptr, MLOG_FRAME_SIZE, MLOG_FRAME_LIMIT);
+
+            storage->mempart_size = MLOG_FRAME_LIMIT * MLOG_FRAME_SIZE;
+            storage->first_block = storage->current_block = (MLOG_FRAME_BLOCK_HEADER *) MxiAllocSysPart(&storage->mem_part);
+
+
+
+            if(!storage->current_block)
+                return;
+
+            storage->block_num += 1;
+
+            //MLOG_DEBUG_PRINT("Create storage (%d) mem ptr %X, static block ptr %X, cur block ptr %X\r\n", cpu, membgn, storage->static_block, storage->current_block);
+
+            memset(storage->current_block, 0, sizeof(MLOG_FRAME_BLOCK_HEADER));
+
+            storage->current_block->cur_data = ((unsigned char *)storage->current_block) + sizeof(MLOG_FRAME_BLOCK_HEADER);
+            storage->current_block->size = MLOG_FRAME_SIZE - sizeof(MLOG_FRAME_BLOCK_HEADER);
+            storage->hdr->DynamicData = (MLOG_DWORD)storage->current_block;
+#endif
+        }
+
+        if(storage->block_num == MLOG_FRAME_LIMIT)
+        {
+            MLOG_FRAME_BLOCK_HEADER * first;
+            if(storage->first_block)
+            {
+                first = storage->first_block->next;
+                MxiFreeSysPart(&storage->mem_part,storage->first_block);
+
+                storage->first_block = first;
+                storage->block_num--;
+                storage->hdr->DynamicData = (MLOG_DWORD)first;
+            }
+            else
+            {
+                MLOG_DEBUG_PRINT("MLOG: First block not present!!!\r\n");
+                return;
+            }
+        }
+
+
+        //MLOG_DEBUG_PRINT("dynamic storage (%d): add next tti block\r\n");
+        storage->current_block->next = (MLOG_FRAME_BLOCK_HEADER *) MxiAllocSysPart(&storage->mem_part);
+
+#if !defined(_WIN32) && !defined(__linux__) && !defined(__KERNEL__)
+        _ASSERT_PTR(storage->current_block->next);
+#endif
+        storage->current_block = storage->current_block->next;
+
+        storage->current_block->next = NULL;
+        storage->current_block->payload = 0;
+        storage->current_block->cur_data = ((unsigned char *)storage->current_block) + sizeof(MLOG_FRAME_BLOCK_HEADER);
+        storage->current_block->size = MLOG_FRAME_SIZE - sizeof(MLOG_FRAME_BLOCK_HEADER);
+        storage->block_num++;
+
+    }
+}
+
+#endif
+
+#endif
+
+#if defined(__KERNEL__)
+#define DEVICE_NAME		"mlogdrv"
+#define MLOG_IOCTL_START	_IO('d', 1)
+#define MLOG_IOCTL_STOP 	_IO('d', 2)
+#define MLOG_IOCTL_CLEAR 	_IO('d', 3)
+
+static long mlog_ioctl(struct file *file, unsigned int cmd, unsigned long arg);
+static ssize_t mlog_bin_read(struct file *file, struct kobject *, struct bin_attribute *, char *buf, loff_t pos, size_t size);
+static ssize_t mlog_read(struct file *file, const char __user *buf, size_t count, loff_t *offp);
+static ssize_t mlog_write(struct file *file, const char __user *buf, size_t count, loff_t *offp);
+
+static struct file_operations mlog_fops[] = {
+	{
+		.owner = THIS_MODULE,
+		.read = mlog_read,
+		.write = mlog_write,
+		.unlocked_ioctl = mlog_ioctl,
+	}
+};
+
+struct bin_attribute mlog_bin_attr[] = {
+	{
+		.attr = {
+			.name = "mlog.bin",
+			.mode = S_IRUGO,
+		},
+		.size = 0,
+		.read = mlog_bin_read,
+	}
+};
+
+static long mlog_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	long rc = 0;
+
+	mutex_lock(&control_lock);
+
+	switch (cmd) {
+	case  MLOG_IOCTL_START:
+		if (mlog_ctx.Opened == MLOG_FALSE)
+			MLogOpen();
+
+		MLogSetMask(0xFFFFFFFF);
+		break;
+	case  MLOG_IOCTL_STOP:
+		MLogSetMask(0x0);
+		MLogPrint();
+		break;
+	case MLOG_IOCTL_CLEAR:
+		// Restart MLog
+		MLogSetMask(0x0);
+		MLogClose();
+		MLogOpen();
+		break;
+	default:
+		break;
+	}
+
+	mutex_unlock(&control_lock);
+
+	return rc;
+}
+
+static ssize_t mlog_bin_read(struct file *file, struct kobject *kobj, struct bin_attribute *bin_attr, char *buf, loff_t pos, size_t size)
+{
+	ssize_t ret = 0;
+
+	mutex_lock(&control_lock);
+
+	/* Stop mlog if not stopped already.
+	 * To prevent copying of buffer in the middle of changing.
+	 */
+	MLogSetMask(0x0);
+
+	MLOG_DEBUG_PRINT("MLog bin read:f_pos = %p, size = %d\n", (void *)pos, size);
+	if (pos >= MLogGetFileSize()) {
+		uart_printf("MLog: f_pos(%p) > file size\n", (void *)pos);
+		goto out;	/* return 0 - EOF */
+	}
+
+	if (pos + size >= MLogGetFileSize())
+		size = MLogGetFileSize() - pos;
+
+
+	memcpy(buf, (char*)MLogGetFileLocation() + pos, size);
+
+	pos += size;
+	ret = size;
+
+out:
+	mutex_unlock(&control_lock);
+
+	return ret;
+}
+
+// Duplicates mlog_bin, but might be used to retrieve pid gathered data
+static ssize_t mlog_read(struct file *file, const char __user *buf, size_t size, loff_t *pos)
+{
+	ssize_t ret = 0;
+
+	mutex_lock(&control_lock);
+
+	/* Stop mlog if not stopped already.
+	 * To prevent copying of buffer in the middle of changing.
+	 */
+	MLogSetMask(0x0);
+
+	MLOG_DEBUG_PRINT("MLog bin read:f_pos = %p, size = %d\n", (void *)*pos, size);
+	if (*pos >= MLogGetFileSize()) {
+		uart_printf("MLog: f_pos(%p) > file size\n", (void *)*pos);
+		goto out;	/* return 0 - EOF */
+	}
+
+	if (*pos + size >= MLogGetFileSize())
+		size = MLogGetFileSize() - *pos;
+
+
+	if (copy_to_user(buf, (char*)MLogGetFileLocation() + *pos, size)) {
+		ret = -EFAULT;
+		goto out;
+	}
+
+	*pos += size;
+	ret = size;
+
+out:
+	mutex_unlock(&control_lock);
+	return ret;
+}
+
+static ssize_t mlog_write(struct file *file, const char __user *buf, size_t count, loff_t *f_pos)
+{
+	ssize_t ret = 0;
+	char c;
+
+	if (get_user(c, buf)) {
+		printk(KERN_ERR "MLog: unable to copy from user\n");
+		return -EIO;
+	}
+
+	mutex_lock(&control_lock);
+
+	switch (c) {
+	case 'i':
+		/* MLog Init.
+		 * This separated stage is needed to init MLog beforehand.
+		 * On initialization MLog may run threads, wait for completions, allocate buffer, etc which are the time-consuming operations and can cause CPU usage overhead.
+		 * There may be cases when it's needed to start/stop MLog quickly, possibly many times, without extra overhead.
+		 * For this case user may first init Mlog separately, and then, when needed start/stop it.
+		 * We don't integrate MLog init stage into module init sequence in order to support feature of dumping MLog after board reboot.
+		 */
+		MLogOpen();
+		ret = count;
+		break;
+	case '0':
+		// Stop logging
+		MLogSetMask(0x0);
+		MLogPrint();
+		ret = count;
+		break;
+	case '1':
+		// Start logging
+		if (mlog_ctx.Opened == MLOG_FALSE)
+			MLogOpen();
+
+		MLogSetMask(0xFFFFFFFF);
+		ret = count;
+		break;
+	case 'r':
+		// Restart MLog
+		MLogSetMask(0x0);
+		MLogClose();
+		MLogOpen();
+		break;
+	case 's':
+		// Enable Auto Stop
+		MLOG_LOCK;
+		auto_stop = 1;
+		MLOG_RELEASE;
+		ret = count;
+		break;
+	case 'c':
+		// Disable Auto Stop
+		MLOG_LOCK;
+		auto_stop = 0;
+		MLOG_RELEASE;
+		ret = count;
+		break;
+	default:
+		break;
+	}
+
+	mutex_unlock(&control_lock);
+
+	*f_pos += count;
+
+	return ret;
+}
+
+static int __init mlog_init(void)
+{
+	/* Alloc static storage address.
+	 * We assume that:
+	 * - on Linux initialization memory will be intact after previous board boot up.
+	 * - Linux will give the same addresses when allocating storage in __init function.
+	 * This gives up theoretical possibility to get Mlog file after board reload.
+	 *
+	 * Create storage before registering at Linux.
+	 * Otherwise someone might call registered interface while we're not yet ready.
+	 */
+	MLogCreate();
+
+	mlog_class = class_create(THIS_MODULE, DEVICE_NAME);
+	if (IS_ERR(mlog_class)) {
+		return PTR_ERR(mlog_class);
+	}
+
+	mlog_major = register_chrdev(0, DEVICE_NAME, mlog_fops);
+
+	if (mlog_major < 0) {
+		printk ("Registering %s device failed with %d\n", DEVICE_NAME, mlog_major);
+		return mlog_major;
+	}
+
+	mlog_device = device_create(mlog_class, NULL, MKDEV(mlog_major, 0), NULL, DEVICE_NAME);
+	if (IS_ERR(mlog_device)) {
+		class_destroy(mlog_class);
+		unregister_chrdev(mlog_major, DEVICE_NAME);
+		return PTR_ERR(mlog_device);
+	}
+
+	if (sysfs_create_bin_file(&mlog_device->kobj, mlog_bin_attr)) {
+		printk("Error while creating binary file in sysfs occured.\n");
+	}
+
+	return 0;
+}
+
+static void __exit mlog_exit(void)
+{
+	sysfs_remove_bin_file(&mlog_device->kobj, mlog_bin_attr);
+	device_destroy(mlog_class, MKDEV(mlog_major, 0));
+	/* Unregister the device */
+	unregister_chrdev(mlog_major, DEVICE_NAME);
+	class_destroy(mlog_class);
+
+	MLogClose();
+	MLogDestroy();
+}
+
+module_init(mlog_init);
+module_exit(mlog_exit);
+
+MODULE_AUTHOR("Intel Corporation");
+MODULE_DESCRIPTION("MLog Driver");
+MODULE_LICENSE("GPL");
+MODULE_VERSION("0.2");
+#endif
diff --git a/arch/arm/mach-transcede/mmu_protect.c b/arch/arm/mach-transcede/mmu_protect.c
new file mode 100644
index 0000000..e75d6d3
--- /dev/null
+++ b/arch/arm/mach-transcede/mmu_protect.c
@@ -0,0 +1,749 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+#include <linux/io.h>
+#include <linux/proc_fs.h>
+#include <linux/kthread.h>
+#include <linux/delay.h>
+#include <linux/cpuset.h>
+
+#include <asm/uaccess.h>
+
+#include <mach/mmu_protect.h>
+#include <mach/mmu_protect_v7.h>
+
+static unsigned int mmu_test;
+
+static struct proc_dir_entry *mmu_proc;
+static struct proc_dir_entry *test_proc;
+
+static void amp_setup(void);
+static int amp_thread_fn(void *data);
+
+struct mmu_mem amp_mem;
+
+static DEFINE_SPINLOCK(amp_stack_lock);
+static DEFINE_SPINLOCK(amp_thread_lock);
+static LIST_HEAD(amp_thread_list);
+static wait_queue_head_t wait_q;
+static atomic_t wakeup_flag;
+
+struct amp_kthread {
+	int (*threadfn)(void *);
+	void *data;
+	unsigned long cpu;
+	const char *namefmt;
+
+	struct task_struct *result;
+	struct completion done;
+
+	struct list_head list;
+};
+
+static struct task_struct *amp_thread_master;
+static int kthreadd_amp(void *unused);
+
+struct amp_queue {
+	void *storage;
+	uint32_t size;
+	void *queue;
+	uint32_t *get;
+	uint32_t *put;
+};
+
+static struct amp_queue amp_queue;
+
+static int amp_stack_setup(void);
+
+/* /proc */
+static int mmu_proc_read_test(char *page, char **start, off_t off,
+                              int count, int *eof, void *data);
+static int mmu_proc_write_test(struct file *file, const char __user *buffer,
+                               unsigned long count, void *data);
+
+/* control */
+static int mmu_control_mem(struct mmu_mem *mem, enum mem_control action,
+                           enum cache_type cache);
+static int __mmu_control_mem(unsigned long *p, unsigned long count,
+                             enum mem_control action, enum cache_type cache);
+
+inline static int mmu_lock_mem(struct mmu_mem *mem)
+{
+	return mmu_control_mem(mem, MEM_LOCK, REGION_IGNORE);
+}
+
+inline static int mmu_unlock_mem(struct mmu_mem *mem)
+{
+	return mmu_control_mem(mem, MEM_UNLOCK, REGION_IGNORE);
+}
+
+inline static int mmu_unlock_l1_mem(struct mmu_mem *mem)
+{
+	return mmu_control_mem(mem, MEM_UNLOCK, REGION_WB_WA);
+}
+
+inline static int mmu_rdonly_mem(struct mmu_mem *mem)
+{
+	return mmu_control_mem(mem, MEM_RDONLY, REGION_IGNORE);
+}
+
+inline static int mmu_unlock_amp_l1_mem(struct mmu_mem *mem)
+{
+	return mmu_control_mem(mem, MEM_UNLOCK_AMP, REGION_WB_WA);
+}
+
+static void amp_setup(void)
+{
+	amp_mem.phys = TRANSCEDE_AMP_BASE;
+	amp_mem.size = TRANSCEDE_AMP_SIZE;
+	amp_mem.virt = (unsigned long)ioremap_nocache(amp_mem.phys, amp_mem.size);
+
+	MMU_INFO("%s %s", __DATE__, __TIME__);
+	MMU_DEBUG("AMP phys: 0x%08lx", amp_mem.phys);
+	MMU_DEBUG("AMP size: 0x%08lx", amp_mem.size);
+	MMU_DEBUG("AMP virt: 0x%08lx", amp_mem.virt);
+
+	if ((void *)amp_mem.virt == NULL) {
+		MMU_ERROR("no virt memory for 0x%08lx", amp_mem.phys);
+		return;
+	}
+
+	memset((void *)amp_mem.virt, 0x00, amp_mem.size);
+
+	asm volatile ("dsb");
+	asm volatile ("dmb");
+
+	mmu_unlock_amp_l1_mem(&amp_mem);
+
+	asm volatile ("dsb");
+	asm volatile ("dmb");
+
+	if (!is_mmu_shared(amp_mem.virt + amp_mem.size / 2)) {
+		MMU_INFO("Check AMP: OK");
+	} else {
+		MMU_ERROR("Check AMP: Failed: 0x%08lx is not recognized", amp_mem.virt + amp_mem.size / 2);
+	}
+
+	if (is_mmu_shared((unsigned long)&amp_mem.virt)) {
+		MMU_INFO("Check SMP: OK");
+	} else {
+		MMU_ERROR("Check SMP: Failed: 0x%p is recognized wrong", &amp_mem.virt);
+	}
+
+	amp_stack_setup();
+
+	amp_thread_master = kthread_create(kthreadd_amp, NULL, "kthreadd_amp");
+	kthread_bind(amp_thread_master, 1);
+	wake_up_process(amp_thread_master);
+}
+
+static int kthreadd_amp(void *unused)
+{
+
+	struct task_struct *tsk = current;
+
+	/* Setup a clean context for our children to inherit. */
+	/* set_task_comm(tsk, "kthreadd_amp"); */
+	ignore_signals(tsk);
+	/* set_cpus_allowed_ptr(tsk, cpu_all_mask); */
+	set_mems_allowed(node_states[N_HIGH_MEMORY]);
+
+	current->flags |= PF_NOFREEZE | PF_FREEZER_NOSIG;
+
+	for (;;) {
+		MMU_DEBUG("Wait");
+
+		set_current_state(TASK_INTERRUPTIBLE);
+
+		if (list_empty(&amp_thread_list)) {
+			schedule();
+		}
+
+		__set_current_state(TASK_RUNNING);
+
+		spin_lock(&amp_thread_lock);
+
+		while (!list_empty(&amp_thread_list)) {
+			struct amp_kthread *amp;
+
+			amp = list_entry(amp_thread_list.next,
+			                 struct amp_kthread, list);
+
+			list_del_init(&amp->list);
+			spin_unlock(&amp_thread_lock);
+
+			MMU_DEBUG("Creating '%s' on CPU%lu", amp->namefmt, amp->cpu);
+
+			amp->result = _kthread_amp_create(amp->cpu, amp->threadfn, amp->data, amp->namefmt);
+			complete(&amp->done);
+
+			/* kfree(amp); */
+
+			spin_lock(&amp_thread_lock);
+		}
+
+		spin_unlock(&amp_thread_lock);
+	}
+
+	do_exit(0);
+}
+
+static int amp_thread_fn(void *data)
+{
+	struct thread_info *p = current_thread_info();
+	register unsigned long sp asm ("sp");
+	struct thread_info *myp = (struct thread_info *)(sp & ~(THREAD_SIZE - 1));
+	struct cpu_context_save *cpu_context = &p->cpu_context;
+	int count = 0;
+
+	MMU_INFO("##################### pid: %d", current->pid);
+	MMU_INFO("current:           0x%lx", (unsigned long)current);
+	MMU_INFO("thread_info->stack_smp: 0x%lx", (unsigned long)(p->stack_smp));
+	MMU_INFO("thread_info->task: 0x%lx", (unsigned long)(p->task));
+	MMU_INFO("mythread_info->task: 0x%lx", (unsigned long)(myp->task));
+	MMU_INFO("current->stack: 0x%lx", (unsigned long)(current->stack));
+	MMU_INFO("current->stack_amp: 0x%lx", (unsigned long)(current->amp_stack));
+	MMU_INFO("sp: 0x%lx", (unsigned long)(sp));
+	MMU_INFO("myp: 0x%lx", (unsigned long)(myp));
+	MMU_INFO("context: 0x%lx", (unsigned long)cpu_context);
+	MMU_INFO("     sp: 0x%lx", (unsigned long)cpu_context->sp);
+	MMU_INFO("     pc: 0x%lx", (unsigned long)cpu_context->pc);
+
+	for (;;) {
+		const int sz = SZ_1M;
+		unsigned long tic, toc;
+		void *mem = NULL;
+		int i = 5;
+
+		tic = get_tick();
+
+		while (i--) {
+			mem = kmalloc(sz, GFP_KERNEL);
+
+			if (mem == NULL) {
+				printk(KERN_ERR "err: no memory\n");
+			} else {
+				memset(mem, 0x0a, sz);
+				kfree(mem);
+			}
+		}
+
+		toc = get_tick();
+
+		if (count++ == 50) {
+			MMU_DEBUG("%d: tick: %lu, msec: %lu", current->pid, (toc - tic),
+			          (toc - tic) * 1000UL / TRANSCEDE_AXICLK_HZ);
+		}
+
+		atomic_set(&wakeup_flag, 0);
+		wait_event_interruptible(wait_q, atomic_read(&wakeup_flag));
+		MMU_DEBUG("Awaken, count = %d", count);
+	}
+
+	do_exit(0);
+}
+
+static int mmu_proc_read_test(char *page, char **start, off_t off,
+                              int count, int *eof, void *data)
+{
+	int len = 0;
+
+	if (off > 0) {
+		return 0;
+	}
+
+	len += sprintf(page + len, "%d\n", mmu_test);
+
+	*start = NULL;
+
+	if (count <= len) {
+		*eof = 1;
+	}
+
+	return len;
+}
+
+static int mmu_proc_write_test(struct file *file, const char __user *buffer,
+                               unsigned long count, void *data)
+{
+	char *tmp = kmalloc(count + 1, GFP_KERNEL);
+	struct task_struct *tsk;
+
+	if (copy_from_user(tmp, buffer, count)) {
+		printk(KERN_ERR "copy failed: 0x%08lx\n", (unsigned long)buffer);
+		kfree(tmp);
+		return 0;
+	}
+
+	tmp[count] = '\0';
+
+	mmu_test = simple_strtoul(tmp, NULL, 10);
+
+	kfree(tmp);
+
+	/* run MMU stuff */
+
+	if (mmu_test < 2) {
+		MMU_DEBUG("Create");
+		atomic_set(&wakeup_flag, 0);
+		init_waitqueue_head(&wait_q);
+		tsk = kthread_amp_create(mmu_test, amp_thread_fn, NULL, "amp_test");
+		wake_up_process(tsk);
+	} else {
+		MMU_DEBUG("Waking up");
+		atomic_set(&wakeup_flag, 1);
+		wake_up_interruptible(&wait_q);
+	}
+
+	return count;
+}
+
+/**
+ * @brief  Setup /proc MMU protect
+ */
+int mmu_protect_set_proc(void)
+{
+	mmu_proc = proc_mkdir("mmu_protect", NULL);
+	test_proc = create_proc_entry("test", 0600, mmu_proc);
+
+	test_proc->read_proc = mmu_proc_read_test;
+	test_proc->write_proc = mmu_proc_write_test;
+	test_proc->data = NULL;
+
+	amp_setup();
+
+	return 0;
+}
+
+static int mmu_control_mem(struct mmu_mem *mem, enum mem_control action,
+                           enum cache_type cache)
+{
+	unsigned long reg_sctlr = 0;
+	unsigned long reg_ttbcr = 0;
+	unsigned long reg_ttbr0 = 0;
+	unsigned long reg_ttbr1 = 0;
+
+	unsigned long tt_phys;
+	unsigned long tt0_virt, tt1_virt;
+
+	unsigned long *block_mem;
+	unsigned long block_count;
+
+	if (action == MEM_UNLOCK_AMP) {
+		MMU_DEBUG("MEM_UNLOCK_AMP: virt: 0x%08lx, phys: 0x%08lx, size: %lu",
+		          mem->virt, mem->phys, mem->size);
+	}
+
+	RD15_SCTLR(reg_sctlr);
+	RD15_TTBCR(reg_ttbcr);
+	RD15_TTBR0(reg_ttbr0);
+	RD15_TTBR1(reg_ttbr1);
+
+	MMU_DEBUG("reg SCTLR: 0x%08lx", reg_sctlr);
+	MMU_DEBUG("reg TTBR0: 0x%08lx", reg_ttbr0);
+	MMU_DEBUG("reg TTBR1: 0x%08lx", reg_ttbr1);
+
+	if (reg_sctlr & SCTLR_AFE) {
+		MMU_ERROR("AFE=1 is not supported");
+		return -1;
+	}
+
+	if (reg_sctlr & SCTLR_TRE) {
+		MMU_INFO("TEX remap enabled");
+	} else {
+		MMU_INFO("TEX remap disabled");
+		MMU_ERROR("TRE=0 mode is not supported");
+		return -1;
+	}
+
+	tt_phys = reg_ttbr0 & ~((1UL << (TTB0_SHIFT - (reg_ttbcr & TTBCR_N))) - 1);
+	tt0_virt = (unsigned long)phys_to_virt(tt_phys);
+
+	MMU_DEBUG("table0 phys: 0x%08lx", tt_phys);
+	MMU_DEBUG("table0 virt: 0x%08lx", tt0_virt);
+
+	tt_phys = reg_ttbr1 & ~((1UL << TTB1_SHIFT) - 1);
+	tt1_virt = (unsigned long)phys_to_virt(tt_phys);
+
+	MMU_DEBUG("table1 phys: 0x%08lx", tt_phys);
+	MMU_DEBUG("table1 virt: 0x%08lx", tt1_virt);
+
+	block_count = mem->size >> SZ_1M_SHIFT;
+
+	MMU_INFO("table entry count: %lu", block_count);
+	MMU_INFO("shared memory virt: 0x%08lx", mem->virt);
+
+	block_mem = (unsigned long *)tt0_virt + (mem->virt >> SZ_1M_SHIFT);
+
+	__mmu_control_mem(&block_mem[0], block_count, action, cache);
+
+	block_mem = (unsigned long *)tt1_virt + (mem->virt >> SZ_1M_SHIFT);
+
+	__mmu_control_mem(&block_mem[0], block_count, action, cache);
+
+	return 0;
+}
+
+static int __mmu_control_mem(unsigned long *p, unsigned long count,
+                             enum mem_control action, enum cache_type cache)
+{
+	unsigned long reg_prrr = 0;
+	unsigned long reg_nrrr = 0;
+	int i, k;
+
+	RD15_PRRR(reg_prrr);
+	RD15_NRRR(reg_nrrr);
+
+	MMU_DEBUG("reg PRRR: 0x%08lx", reg_prrr);
+	MMU_DEBUG("reg NRRR: 0x%08lx", reg_nrrr);
+
+	for (i = 0; i < count; i++) {
+		unsigned long entry;
+		unsigned long type;
+
+		entry = p[i];
+
+		if (entry == 0) {
+			MMU_ERROR("ignore entry [%d]", i);
+			continue;
+		}
+
+		/* MMU_DEBUG("[%03lx] 0x%08lx", (mem->virt >> SZ_1M_SHIFT) + i, entry); */
+
+		type = entry & TT_MASK;
+
+		/* MMU_DEBUG("\ttype: %02lx", type); */
+
+		switch (type) {
+			case TT_PAGE: {
+				unsigned long *page;
+				page = phys_to_virt(entry & TT_PAGE_BASE_MASK);
+
+				for (k = 0; k < (SZ_1M >> PAGE_SHIFT); k++) {
+					switch (page[k] & TT_MASK) {
+						case TT_SMALL: {
+							unsigned long attr = 0;
+							unsigned long attr_tex = 0;
+							unsigned long attr_b = 0;
+							unsigned long attr_c = 0;
+							unsigned long tex_n = 0;
+							unsigned long tr_off = 0;
+							unsigned long ir_off = 0;
+							unsigned long or_off = 0;
+
+							attr = page[k] & ~TT_SMALL_ATR_AP(AP_MASK);
+
+							if ((action == MEM_UNLOCK) || (action == MEM_UNLOCK_AMP)) {
+								attr |= TT_SMALL_ATR_AP(AP_FULL_ACCESS);
+								attr &= ~TT_SMALL_ATR_AP2;
+							} else if (action == MEM_LOCK) {
+								attr |= TT_SMALL_ATR_AP(AP_PL0_FAULT);
+								attr &= ~TT_SMALL_ATR_AP2;
+							} else if (action == MEM_RDONLY) {
+								attr |= TT_SMALL_ATR_AP(AP_PL0_RDONLY);
+								attr &= ~TT_SMALL_ATR_AP2;
+							} else {
+								MMU_ERROR("control action is not supported: %d", action);
+								/* TODO: add forloop exit */
+								break;
+							}
+
+							/* Prepare shared mode: set S-bit */
+
+							if (action == MEM_UNLOCK_AMP) {
+								attr &= ~TT_SMALL_ATR_S;
+							} else {
+								attr |= TT_SMALL_ATR_S;
+							}
+
+							if (cache == REGION_WB_WA) {
+								/*
+								 * SCTLR_TRE==1, TEX remap enabled, TEX[2:1] are ignored
+								 */
+
+								/* 101 = TEX[0],C,B */
+
+								attr &= ~(TT_SMALL_ATR_TEX(TEX_MASK) | TT_SMALL_ATR_B | TT_SMALL_ATR_C);
+								attr |= TT_SMALL_ATR_TEX(1) | TT_SMALL_ATR_B;
+
+								attr_tex = (attr & (TT_SMALL_ATR_TEX(TEX_MASK))) >> 6; /*TEX[0] */
+								attr_b = (attr & TT_SMALL_ATR_B) >> 2; /*B-bit */
+								attr_c = (attr & TT_SMALL_ATR_C) >> 3; /*C-bit */
+
+								if ((k == i) && ((i & 0xfff) == 0)) {
+									MMU_DEBUG("\tattr: 0x%08lx, TEX=0x%01lx, B=0x%01lx, C=0x%01lx",
+									          attr, attr_tex, attr_b, attr_c);
+								}
+
+								tex_n = ((attr_tex & 1) << 2) | (attr_c << 1) | (attr_b << 0);
+								tr_off = tex_n *2;
+								ir_off = tex_n *2;
+								or_off = tex_n *2 + 16;
+
+								if ( ((reg_prrr & (TEX_MEM_MASK << tr_off))
+								      != (TEX_MEM_NORMAL << tr_off)) )
+								{
+									MMU_ERROR("Wrong map: should be Normal");
+									MMU_DEBUG("reg PRRR: 0x%08lx", reg_prrr);
+									MMU_DEBUG("reg NRRR: 0x%08lx", reg_nrrr);
+									MMU_DEBUG("\tattr: 0x%08lx, TEX=0x%01lx, C=0x%01lx, B=0x%01lx",
+									          attr, attr_tex, attr_c, attr_b);
+								}
+
+								if ( ((reg_nrrr & (TEX_REGION_MASK << ir_off))
+								      != (TEX_REGION_WB_WA << ir_off)) )
+								{
+									MMU_ERROR("Wrong inner cache-attr: should be write-back, write-allocate");
+									MMU_DEBUG("reg PRRR: 0x%08lx", reg_prrr);
+									MMU_DEBUG("reg NRRR: 0x%08lx", reg_nrrr);
+									MMU_DEBUG("\tattr: 0x%08lx, TEX=0x%01lx, C=0x%01lx, B=0x%01lx",
+									          attr, attr_tex, attr_c, attr_b);
+
+									MMU_DEBUG("ir_off=%lu", ir_off);
+
+									if ( ((reg_nrrr & (TEX_REGION_MASK << ir_off))
+									      == (TEX_REGION_NCNB << ir_off)) )
+									{
+										MMU_DEBUG("Non-cacheable");
+									} else if ( ((reg_nrrr & (TEX_REGION_MASK << ir_off))
+									             == (TEX_REGION_WB_WA << ir_off)) )
+									{
+										MMU_DEBUG("Write-Back, Write-Allocate");
+									} else if ( ((reg_nrrr & (TEX_REGION_MASK << ir_off))
+									             == (TEX_REGION_WRITETHROUGH << ir_off)) )
+									{
+										MMU_DEBUG("Write-Through");
+									} else if ( ((reg_nrrr & (TEX_REGION_MASK << ir_off))
+									             == (TEX_REGION_WRITEBACK << ir_off)) )
+									{
+										MMU_DEBUG("Write-Back");
+									}
+								}
+
+								if ( ((reg_nrrr & (TEX_REGION_MASK << or_off))
+								      != (TEX_REGION_NCNB << or_off)) )
+								{
+									MMU_ERROR("Wrong outer cache-attr: should be write-back, write-allocate");
+									MMU_DEBUG("reg PRRR: 0x%08lx", reg_prrr);
+									MMU_DEBUG("reg NRRR: 0x%08lx", reg_nrrr);
+									MMU_DEBUG("\tattr: 0x%08lx, TEX=0x%01lx, C=0x%01lx, B=0x%01lx",
+									          attr, attr_tex, attr_c, attr_b);
+
+									MMU_DEBUG("or_off=%lu", or_off);
+
+									if ( ((reg_nrrr & (TEX_REGION_MASK << or_off))
+									      == (TEX_REGION_NCNB << or_off)) )
+									{
+										MMU_DEBUG("Non-cacheable");
+									} else if ( ((reg_nrrr & (TEX_REGION_MASK << or_off))
+									             == (TEX_REGION_WB_WA << or_off)) )
+									{
+										MMU_DEBUG("Write-Back, Write-Allocate");
+									} else if ( ((reg_nrrr & (TEX_REGION_MASK << or_off))
+									             == (TEX_REGION_WRITETHROUGH << or_off)) )
+									{
+										MMU_DEBUG("Write-Through");
+									} else if ( ((reg_nrrr & (TEX_REGION_MASK << or_off))
+									             == (TEX_REGION_WRITEBACK << or_off)) )
+									{
+										MMU_DEBUG("Write-Back");
+									}
+								}
+							} else if (cache != REGION_IGNORE) {
+								MMU_ERROR("cache type is not supported");
+							}
+
+							/* all pages have same attributes */
+
+							page[k] = attr;
+						} break;
+
+						default:
+							MMU_ERROR("unknown format: 0x%lx", (page[k] & TT_MASK));
+					}
+				}
+			} break;
+
+			case TT_SECTION: {
+				unsigned long *section = &p[i];
+				unsigned long attr = *section & ~TT_SECTION_ATR_AP(AP_MASK);
+
+				if (action == MEM_UNLOCK) {
+					attr |= TT_SECTION_ATR_AP(AP_FULL_ACCESS);
+					attr &= ~TT_SECTION_ATR_AP2;
+				} else if (action == MEM_UNLOCK_AMP) {
+					attr |= TT_SECTION_ATR_AP(AP_FULL_ACCESS);
+					attr &= ~TT_SECTION_ATR_AP2;
+					attr &= ~TT_SECTION_ATR_S;
+				} else if (action == MEM_LOCK) {
+					attr |= TT_SECTION_ATR_AP(AP_PL0_FAULT);
+					attr &= ~TT_SECTION_ATR_AP2;
+				} else if (action == MEM_RDONLY) {
+					attr |= TT_SECTION_ATR_AP(AP_PL0_RDONLY);
+					attr &= ~TT_SECTION_ATR_AP2;
+				} else {
+					MMU_ERROR("control action is not supported: %d", action);
+					break;
+				}
+
+				MMU_DEBUG("\tattr: 0x%08lx", attr);
+
+				*section = attr;
+			} break;
+
+			default:
+				MMU_ERROR("type entry is not supported: 0x%02lx", type);
+		}
+	}
+
+	return 0;
+}
+
+void smp_set_value(void *data)
+{
+	const struct smp_value *smp = (const struct smp_value *)data;
+	size_t size = smp->size;
+	unsigned long *dst = smp->dst;
+	unsigned long *src = smp->src;
+	unsigned long flags;
+	int i;
+
+	spin_lock_irqsave(&amp_thread_lock, flags);
+
+	for (i = 0; i < size; i++) {
+		*dst++ = *src++;
+	}
+
+	v7_dma_map_area((void *)((unsigned long)smp->dst & ~(THREAD_SIZE - 1)), THREAD_SIZE, DMA_TO_DEVICE);
+
+	asm volatile ("dmb");
+
+	spin_unlock_irqrestore(&amp_thread_lock, flags);
+}
+
+struct task_struct *
+kthread_amp_create(int cpu, int (*threadfn)(void *data),
+                   void *data,
+                   const char namefmt[])
+{
+	struct amp_kthread *amp;
+
+	amp = (struct amp_kthread *)kmalloc(sizeof(*amp), GFP_KERNEL);
+
+	amp->threadfn = threadfn;
+	amp->data = data;
+	amp->cpu = cpu;
+	amp->namefmt = namefmt;
+
+	init_completion(&amp->done);
+
+	spin_lock(&amp_thread_lock);
+	list_add_tail(&amp->list, &amp_thread_list);
+	spin_unlock(&amp_thread_lock);
+
+	wake_up_process(amp_thread_master);
+	wait_for_completion(&amp->done);
+
+	return amp->result;
+}
+EXPORT_SYMBOL(kthread_amp_create);
+
+static int amp_stack_setup(void)
+{
+	struct amp_queue *q = &amp_queue;
+	uint32_t *queue;
+	int i;
+
+	q->storage = (void *)amp_mem.virt;
+	q->size = amp_mem.size / THREAD_SIZE * sizeof(uint32_t);
+	q->queue = kmalloc(q->size, GFP_KERNEL);
+
+	if (q->queue == NULL) {
+		return -1;
+	}
+
+	q->put = q->queue;
+	q->get = q->put + 1;
+
+	MMU_DEBUG("Storage: 0x%08lx", (unsigned long)q->storage);
+	MMU_DEBUG("Size:    %lu bytes", (unsigned long)q->size);
+	MMU_DEBUG("Queue:   0x%08lx", (unsigned long)q->queue);
+	MMU_DEBUG("get:     0x%08lx", (unsigned long)q->get);
+	MMU_DEBUG("put:     0x%08lx", (unsigned long)q->put);
+
+	queue = q->queue;
+
+	queue[0] = (uint32_t)q->storage;
+
+	for (i = 1; i < q->size / sizeof(uint32_t); i++) {
+		queue[i] = queue[i - 1] + THREAD_SIZE;
+	}
+
+	return 0;
+}
+
+void *amp_stack_alloc(void)
+{
+	struct amp_queue *q = &amp_queue;
+	uint32_t *get;
+
+	spin_lock(&amp_stack_lock);
+
+	get = q->get;
+
+	if (unlikely(++q->get > (uint32_t *)((uint32_t)q->queue + q->size))) {
+		q->get = q->queue;
+	}
+
+	if (q->put == q->get) {
+		spin_unlock(&amp_stack_lock);
+		MMU_ERROR("No free memory for AMP stack");
+		BUG();
+		return NULL;
+	}
+
+	spin_unlock(&amp_stack_lock);
+
+	return (void *)*get;
+}
+
+void amp_stack_free(void *p)
+{
+	struct amp_queue *q = &amp_queue;
+
+	spin_lock(&amp_stack_lock);
+
+	if (q->put == q->get) {
+		spin_unlock(&amp_stack_lock);
+		MMU_ERROR("Invalid free() for AMP stack");
+		BUG();
+		return;
+	}
+
+	*q->put = (uint32_t)p;
+
+	if (unlikely(++q->put > (uint32_t *)((uint32_t)q->queue + q->size))) {
+		q->put = q->queue;
+	}
+
+	spin_unlock(&amp_stack_lock);
+}
+
diff --git a/arch/arm/mach-transcede/pcie-t2200.c b/arch/arm/mach-transcede/pcie-t2200.c
new file mode 100644
index 0000000..9000980
--- /dev/null
+++ b/arch/arm/mach-transcede/pcie-t2200.c
@@ -0,0 +1,1427 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#include <linux/kernel.h>
+#include <linux/version.h>
+#include <linux/init.h>
+#include <linux/pci.h>
+#include <linux/spinlock.h>
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/slab.h>
+#include <linux/delay.h>
+#include <linux/sched.h>
+#include <linux/gpio.h>
+
+#if defined(CONFIG_PCI_MSI)
+#include <linux/msi.h>
+#endif
+
+#include <asm/irq.h>
+#include <asm/delay.h>
+#include <asm/sizes.h>
+#include <asm/mach/pci.h>
+#include <linux/irq.h>
+#include <asm/io.h>
+#include <asm/mach/irq.h>
+#include <mach/transcede-2200.h>
+#include <mach/pcie-t2200.h>
+#include <mach/serdes-t2200.h>
+
+#include <linux/platform_device.h>
+#include <linux/clk.h>
+
+
+#define PCIE_0_SUPPORT_EN
+#define PCIE_1_SUPPORT_EN
+
+#ifndef CONFIG_PCI_MSI
+/**
+ *   Option to let kernel process INTx IRQ
+ */
+//#define PCIE_USE_FAST_INTX_PROC
+#endif
+
+//#define TRANSCEDE_PCIE_DEBUG
+
+
+#ifdef TRANSCEDE_PCIE_DEBUG
+#define PDEBUG(fmt, args...) printk(KERN_INFO "PCIe: %s(): " fmt "\n", __func__, ## args)
+#else  /* !TRANSCEDE_PCIE_DEBUG */
+#define PDEBUG(fmt, args...) do { /* nothing */ } while (0)
+#endif  /* TRANSCEDE_PCIE_DEBUG */
+
+#define PINFO(fmt, args...) printk(KERN_INFO "PCIe: " fmt "\n", ## args)
+#define PERROR(fmt, args...) printk(KERN_ERR "PCIe: error: " fmt "\n", ## args)
+
+
+#ifdef CONFIG_PCI_MSI
+static DECLARE_BITMAP(msi_irq_in_use[NUM_PCIE_PORTS], PCIE_NUM_MSI_IRQS);
+static int transcede_msi_init(struct pcie_port *pp);
+#endif
+
+static void updatel(unsigned long addr, unsigned long clr_mask,  unsigned long set_mask)
+{
+	unsigned long val;
+
+	val = readl(addr);
+	val = (val & ~clr_mask) | set_mask;
+	writel(val, addr);
+}
+
+int pcie_gen1_only = 1;
+static int __init get_pcie_gen_mode(char *str)
+{
+	if (!strcmp(str, "yes"))
+		pcie_gen1_only = 1;
+
+	return 1;
+}
+
+__setup("pcie_gen1_only=", get_pcie_gen_mode);
+
+
+/* Keeping all DDR area of 2GiB accesible for inbound transaction */
+#define INBOUND_ADDR_MASK	0x7FFFFFFF
+
+
+#define PCIE_SETUP_iATU_IB_ENTRY( _pp, _view_port, _base, _limit, _ctl1, _ctl2, _target ) \
+	{ \
+		transcede_dbi_write_reg(_pp, PCIE_iATU_VIEW_PORT, 4, (u32)(_view_port|iATU_VIEW_PORT_IN_BOUND)); \
+		transcede_dbi_write_reg(_pp, PCIE_iATU_CTRL2, 4, 0); \
+		transcede_dbi_write_reg(_pp, PCIE_iATU_SRC_LOW, 4, (u32)_base); \
+		transcede_dbi_write_reg(_pp, PCIE_iATU_SRC_HIGH, 4, 0); \
+		transcede_dbi_write_reg(_pp, PCIE_iATU_LIMIT, 4, (u32)((_base)+(_limit))); \
+		transcede_dbi_write_reg(_pp, PCIE_iATU_TRGT_LOW, 4, (u32)_target); \
+		transcede_dbi_write_reg(_pp, PCIE_iATU_TRGT_HIGH, 4, (u32)0); \
+		transcede_dbi_write_reg(_pp, PCIE_iATU_CTRL1, 4, (u32)_ctl1); \
+		transcede_dbi_write_reg(_pp, PCIE_iATU_CTRL2, 4, (u32)(_ctl2 |iATU_CTRL2_ID_EN) ); \
+	}
+
+#define PCIE_SETUP_iATU_OB_ENTRY( _pp, _view_port, _base, _limit, _ctl1, _ctl2, _target ) \
+	{ \
+		transcede_dbi_write_reg(_pp, PCIE_iATU_VIEW_PORT, 4, (u32)_view_port); \
+		transcede_dbi_write_reg(_pp, PCIE_iATU_CTRL2, 4, 0); \
+		transcede_dbi_write_reg(_pp, PCIE_iATU_SRC_LOW, 4, (u32)_base); \
+		transcede_dbi_write_reg(_pp, PCIE_iATU_SRC_HIGH, 4, (u32)0); \
+		transcede_dbi_write_reg(_pp, PCIE_iATU_LIMIT, 4, ((u32)((_base)+(_limit)))); \
+		transcede_dbi_write_reg(_pp, PCIE_iATU_TRGT_LOW, 4, (u32)_target); \
+		transcede_dbi_write_reg(_pp, PCIE_iATU_TRGT_HIGH, 4, (u32)0); \
+		transcede_dbi_write_reg(_pp, PCIE_iATU_CTRL1, 4, (u32)_ctl1); \
+		transcede_dbi_write_reg(_pp, PCIE_iATU_CTRL2, 4, (u32)(_ctl2 |iATU_CTRL2_ID_EN) ); \
+	}
+
+#define MAX_LINK_UP_WAIT_JIFFIES	HZ /* 1 Second */
+
+static unsigned long pcie_cnf_base_addr[MAX_PCIE_PORTS] =
+{ TRANSCEDE_PCIe0_BASE, TRANSCEDE_PCIe1_BASE };
+static unsigned long pcie_remote_base_addr[MAX_PCIE_PORTS] =
+{ TRANSCEDE_PCIe0_SLAVE, TRANSCEDE_PCIe1_SLAVE };
+static int pcie_msi_base[MAX_PCIE_PORTS] =
+{ PCIE0_MSI_INT_BASE , PCIE1_MSI_INT_BASE };
+static int pcie_intx_base[MAX_PCIE_PORTS] =
+{ PCIE0_INTX_BASE, PCIE1_INTX_BASE };
+static int pcie_irqs[MAX_PCIE_PORTS] =
+{ IRQ_PCIE_X4, IRQ_PCIE_X1 };
+
+
+// context for each PCIe port
+static struct pcie_port pcie_port[MAX_PCIE_PORTS];
+static int  pcie_port_number = 0;   // actual number of PCIe ports
+
+static int pcie_port_is_host( int nr  )
+{
+	struct pcie_port *pp = &pcie_port[nr];
+
+	return ( pp->port_mode == PCIE_PORT_MODE_RC ) ? 1 : 0;
+}
+
+/**
+ *  @brief  This function sets PCIe port mode. Function modifies pcie_port.port_mode
+ *  @param  pp	Pointer to PCIe Port control block.
+ */
+static void pcie_port_set_mode( struct pcie_port *pp, int mode )
+{
+	switch(pp->port) {
+		case PCIE0_PORT_ID:
+			pp->port_mode = mode & DWC_CFG0_DEV_TYPE_MASK;
+			writel(pp->port_mode, TRANSCEDE_RAD_CFG +  0x200);
+			break;
+		case PCIE1_PORT_ID:
+			pp->port_mode = mode & DWC_CFG0_DEV_TYPE_MASK;
+			writel(pp->port_mode, TRANSCEDE_RAD_CFG +  0x78);
+			break;
+		deafult:
+			PERROR("port %d is unknown", pp->port);
+	}
+}
+
+
+/**
+ * This function checks whether link is up or not.
+ * Returns true if link is up otherwise returns false.
+ * @param   pp	Pointer to PCIe Port control block.
+ * @return  Non-zero if link is up
+ */
+static int transcede_pcie_link_up( struct pcie_port *pp  )
+{
+	unsigned long deadline = jiffies + MAX_LINK_UP_WAIT_JIFFIES;
+	unsigned long linkcount = 0, expected = 100;
+
+	do {
+		if (readl( pp->link_status_reg ) & (1 << 16)) {
+			if(++linkcount >= expected) {
+				break;
+			}
+		} else {
+			linkcount = 0;
+		}
+
+		cond_resched();
+		udelay(500);
+	} while (!time_after_eq(jiffies, deadline));
+
+	if (linkcount) {
+		PINFO("Link up success (count %u)", linkcount);
+		return 1;
+	} else {
+		PINFO("No Link");
+		return 0;
+	}
+}
+
+/**
+ * bus_to_port().
+ *
+ */
+static struct pcie_port *bus_to_port(int bus)
+{
+	volatile struct pcie_port *rc;
+	int i;
+
+	for (i = NUM_PCIE_PORTS - 1; i >= 0; i--) {
+		int rbus = pcie_port[i].root_bus_nr;
+		if ( !pcie_port_is_host(i) )
+			continue;
+		if (rbus != -1 && rbus <= bus)
+			break;
+	}
+
+	rc  = i >= 0 ? pcie_port + i : NULL;
+	if (rc == NULL) {
+		printk(KERN_ERR "attempt to access %d %d\n", i, bus);
+		return pcie_port;
+	}
+
+	return i >= 0 ? pcie_port + i : NULL;
+}
+
+
+/**
+ * This function is used to read DBI registers.
+ */
+static void transcede_dbi_read_reg(struct pcie_port *pp, int where, int size,
+                                   u32 *val)
+{
+	u32 va_address;
+
+	va_address = (u32)pp->va_dbi_base + (where & ~0x3);
+	*val = readl_relaxed(va_address);
+
+	if (size == 1)
+		*val = (*val >> (8 * (where & 3))) & 0xff;
+	else if (size == 2)
+		*val = (*val >> (8 * (where & 3))) & 0xffff;
+}
+
+/**
+ * This function is used to write into DBI registers.
+ */
+static void transcede_dbi_write_reg(struct pcie_port *pp, int where, int size,
+                                    u32 val)
+{
+	u32 va_address;
+	int pos, val1, mask = 0;
+
+	va_address = (u32)pp->va_dbi_base + (where & ~0x3);
+	pos = (where & 0x3) << 3;
+	if (size == 4) {
+		val1 = val;
+	} else {
+		if (size == 2)
+			mask = 0xffff;
+		else if (size == 1)
+			mask = 0xff;
+
+		val1 = readl_relaxed(va_address);
+		val1 = ( val1 & ~( mask  << pos ) ) | ( (val & mask) << pos );
+	}
+
+	writel_relaxed(val1, va_address);
+}
+
+/**
+ * This function is used to update DBI registers.
+ */
+static void transcede_dbi_update_reg(struct pcie_port *pp, int where, int size,
+                                     u32 clr_mask, u32 set_mask)
+{
+	u32 val;
+
+	transcede_dbi_read_reg(pp, where, size, &val);
+	val = (val & ~clr_mask) | set_mask;
+	transcede_dbi_write_reg(pp, where, size, val);
+}
+
+
+/**
+ *  @brief  Read configuration space of EP
+ *  @param  pp  Pointer to PCIe control structure
+ *  @param  bus_nr  bus number
+ *  @param  devfn   device (5 bit) and function (3 bit) number
+ *  @param  where   register offset in configuration space
+ *  @param  size    size of read operation in bytes (1, 2 or 4)
+ *  @param  val     pointer to the variable to read value
+ */
+static int transcede_pcie_rd_conf(struct pcie_port *pp, int bus_nr,
+                                  u32 devfn, int where, int size, u32 *val)
+{
+	u32 address;
+	u32 target_address = (u32)(bus_nr << 24) | (PCI_SLOT(devfn) << 19) | (PCI_FUNC(devfn) << 16);
+
+	/* Initialize iATU */
+	if (bus_nr != pp->root_bus_nr) {
+		if (pp->cfg1_prev_taddr != target_address) {
+			/* Type1 configuration request */
+			PCIE_SETUP_iATU_OB_ENTRY( pp, iATU_ENTRY_CNF1, (u32)pp->iatu_cfg1.base, pp->iatu_cfg1.size - 1,
+			                          (AXI_OP_TYPE_CONFIG_RDRW_TYPE1 & iATU_CTRL1_TYPE_MASK), 0, target_address );
+			pp->cfg1_prev_taddr = target_address;
+		}
+
+		address = (u32)pp->va_cfg1_base |(where & 0xFFFC);
+	} else {
+		if (pp->cfg0_prev_taddr != target_address) {
+			/* Type0 configuration request */
+			PCIE_SETUP_iATU_OB_ENTRY( pp, iATU_ENTRY_CNF0, (u32)pp->iatu_cfg0.base, pp->iatu_cfg0.size - 1,
+			                          (AXI_OP_TYPE_CONFIG_RDRW_TYPE0 & iATU_CTRL1_TYPE_MASK), 0, target_address );
+			pp->cfg0_prev_taddr = target_address;
+		}
+
+		address = (u32)pp->va_cfg0_base |(where & 0xFFFC);
+	}
+
+	PDEBUG("rd cfg: %x-b%x-d%x-%x-%x [%x, ta %x]", pp->port, bus_nr, devfn, where, size, address, target_address);
+
+	*val = readl_relaxed(address);
+
+	if (size == 1)
+		*val = (*val >> (8 * (where & 3))) & 0xff;
+	else if (size == 2)
+		*val = (*val >> (8 * (where & 3))) & 0xffff;
+
+	return PCIBIOS_SUCCESSFUL;
+}
+
+static int pcie_read_conf(struct pci_bus *bus, u32 devfn, int where, int size, u32 *val)
+{
+	struct pcie_port *pp = bus_to_port(bus->number);
+	unsigned long flags;
+	int ret;
+
+	/* Make sure that link is up.
+	 * Filter device numbers, unless it's a type1 access
+	 */
+	if ( (!pp->link_state)||
+	     ((bus->number == pp->root_bus_nr) && (PCI_SLOT(devfn) > 0)) ) {
+		*val = 0xffffffff;
+		return PCIBIOS_DEVICE_NOT_FOUND;
+	}
+
+	BUG_ON (((where & 0x3) + size) > 4);
+
+	/* Enter critical section. */
+	spin_lock_irqsave(&pp->conf_lock, flags);
+	ret = transcede_pcie_rd_conf(pp, bus->number, devfn, where, size, val);
+	/* Exit critical section. */
+	spin_unlock_irqrestore(&pp->conf_lock, flags);
+
+	return ret;
+}
+
+/**
+ *  @brief  Write configuration space of EP
+ *  @param  pp  Pointer to PCIe control structure
+ *  @param  bus_nr  bus number (8 bits)
+ *  @param  devfn   device (5 bit) and function (3 bit) number
+ *  @param  where   register offset in configuration space
+ *  @param  size    size of read operation in bytes (1, 2 or 4)
+ *  @param  val     value to be written
+ */
+static int transcede_pcie_wr_conf(struct pcie_port *pp, int bus_nr,
+                                  u32 devfn, int where, int size, u32 val)
+{
+	int ret = PCIBIOS_SUCCESSFUL;
+	u32 address;
+	u32 target_address = (u32)(bus_nr << 24) | (PCI_SLOT(devfn) << 19) | (PCI_FUNC(devfn) << 16);
+
+	/* Initialize iATU */
+	if (bus_nr != pp->root_bus_nr) {
+		if (pp->cfg1_prev_taddr != target_address) {
+			/* Type1 configuration request */
+			PCIE_SETUP_iATU_OB_ENTRY( pp, iATU_ENTRY_CNF1, (u32)pp->iatu_cfg1.base, pp->iatu_cfg1.size - 1,
+			                          (AXI_OP_TYPE_CONFIG_RDRW_TYPE1 & iATU_CTRL1_TYPE_MASK), 0, target_address );
+			pp->cfg1_prev_taddr = target_address;
+		}
+
+		address = (u32)pp->va_cfg1_base |(where & 0xFFFC);
+	} else {
+		if (pp->cfg0_prev_taddr != target_address) {
+			/* Type0 configuration request */
+			PCIE_SETUP_iATU_OB_ENTRY( pp, iATU_ENTRY_CNF0, (u32)pp->iatu_cfg0.base, pp->iatu_cfg0.size - 1,
+			                          (AXI_OP_TYPE_CONFIG_RDRW_TYPE0 & iATU_CTRL1_TYPE_MASK), 0, target_address );
+			pp->cfg0_prev_taddr = target_address;
+		}
+
+		address = (u32)pp->va_cfg0_base |(where & 0xFFFC);
+	}
+
+	PDEBUG("wr cfg: %x-b%x-d%x-%x-%x [%x, ta %x]", pp->port, bus_nr, devfn, where, size, address, target_address);
+
+	if (size == 4)
+		writel_relaxed(val, address);
+	else if (size == 2)
+		writew_relaxed(val, address + (where & 2));
+	else if (size == 1)
+		writeb_relaxed(val, address + (where & 3));
+	else
+		ret = PCIBIOS_BAD_REGISTER_NUMBER;
+
+	return ret;
+}
+
+static int pcie_write_conf(struct pci_bus *bus, u32 devfn, int where, int size, u32 val)
+{
+	struct pcie_port *pp = bus_to_port(bus->number);
+	unsigned long flags;
+	int ret;
+
+	/* Make sure that link is up.
+	 * Filter device numbers, unless it's a type1 access
+	 */
+	if ( (!pp->link_state)||
+	     ((bus->number == pp->root_bus_nr) && (PCI_SLOT(devfn) > 0)) )
+	{
+		return PCIBIOS_DEVICE_NOT_FOUND;
+	}
+
+	BUG_ON (((where & 0x3) + size) > 4);
+
+	/* Enter critical section. */
+	spin_lock_irqsave(&pp->conf_lock, flags);
+	ret = transcede_pcie_wr_conf(pp, bus->number, devfn, where, size, val);
+	if (where == PCI_COMMAND)
+		pp->cmd_reg_val = val & 0xffff;
+	/* Exit critical section. */
+	spin_unlock_irqrestore(&pp->conf_lock, flags);
+
+	return ret;
+}
+
+
+static u8 __init transcede_pcie_swizzle(struct pci_dev *dev, u8 *pin)
+{
+	PDEBUG("pin in %d,  pin out %d (SLOT %d)",  *pin, 1, PCI_SLOT(dev->devfn));
+
+	//
+	// For PCIe 0 & 1 all INTx will be reported over INTA. It should not cause
+	// problem for single EP connection.
+	//
+	*pin = 1;
+
+	return PCI_SLOT(dev->devfn);
+}
+
+static int __init transcede_pcie_map_irq(const struct pci_dev *dev, u8 slot, u8 pin)
+{
+	int irq;
+	struct pcie_port *pp = bus_to_port(dev->bus->number);
+
+#ifdef PCIE_USE_FAST_INTX_PROC
+	/**
+	 *   Let kernet to process INTX interrupts
+	 */
+	irq = (pp->irq);
+#else
+	irq = (pp->intx_base + pin - 1);
+#endif
+
+	BUG_ON (pin > 1);
+	PINFO("slot %d,  pin %d: IRQ %d",  slot, pin, irq);
+
+	return irq;
+}
+
+static int __init transcede_pcie_setup(int nr, struct pci_sys_data *sys)
+{
+	struct pcie_port *pp;
+	u32 val;
+	int rc;
+
+	if ((nr >= NUM_PCIE_PORTS) || !pcie_port_is_host(nr)) {
+		return 0;
+	}
+
+	PDEBUG("NR %d", nr);
+
+	pp = &pcie_port[nr];
+
+	if (!pp->link_state) {
+		return 0;
+	}
+
+	PDEBUG("pp->root_bus_nr = %d, sys->busnr = %d", pp->root_bus_nr, sys->busnr);
+	pp->root_bus_nr = sys->busnr;
+
+	/* Allocate device memory mapped and IO mapped regions */
+	snprintf(pp->mem_space_name, sizeof(pp->mem_space_name),
+	         "PCIe %d MEM", pp->port);
+	pp->mem_space_name[sizeof(pp->mem_space_name) - 1] = 0;
+	pp->res[0].name = pp->mem_space_name;
+	pp->res[0].start = pp->iatu_mem.base;
+	pp->res[0].end = pp->res[0].start + pp->iatu_mem.size - 1;
+	pp->res[0].flags = IORESOURCE_MEM;
+
+	snprintf(pp->io_space_name, sizeof(pp->io_space_name),
+	         "PCIe %d I/O", pp->port);
+	pp->io_space_name[sizeof(pp->io_space_name) - 1] = 0;
+	pp->res[1].name = pp->io_space_name;
+	pp->res[1].start = pp->iatu_io.base;
+	pp->res[1].end = pp->res[1].start + pp->iatu_io.size - 1;
+	pp->res[1].flags = IORESOURCE_IO;
+
+	rc = request_resource(&iomem_resource, &pp->res[0]);
+
+	PDEBUG("MEM SPACE: %s %X..%X [ERROR %d]", pp->res[0].name, pp->res[0].start, pp->res[0].end, rc);
+
+	if (rc) {
+		printk(KERN_ERR "%s:can't allocate PCIe Memory space", __func__);
+		return 0;
+	}
+
+	rc = request_resource(&iomem_resource, &pp->res[1]);
+
+	PDEBUG("MEM SPACE: %s %X..%X [ERROR %d]", pp->res[1].name, pp->res[1].start, pp->res[1].end, rc);
+
+	if (rc) {
+		printk(KERN_ERR "%s:can't allocate PCIe IO space", __func__);
+		return 0;
+	}
+
+	/* Generic PCIe unit setup.*/
+
+	/* Enable own BME. It is necessary to enable own BME to do a
+	 * memory transaction on a downstream device
+	 */
+	transcede_dbi_read_reg(pp, PCI_COMMAND, 2, &val);
+	val |= (PCI_COMMAND_IO | PCI_COMMAND_MEMORY | PCI_COMMAND_MASTER
+	        | PCI_COMMAND_PARITY | PCI_COMMAND_SERR);
+	transcede_dbi_write_reg(pp, PCI_COMMAND, 2, val);
+
+	/* Need to come back here*/
+	sys->resource[0] = &pp->res[0];
+	sys->resource[1] = &pp->res[1];
+	sys->resource[2] = NULL;
+
+	pp->cfg0_prev_taddr = 0xffffffff;
+	pp->cfg1_prev_taddr = 0xffffffff;
+
+	return 1;
+}
+
+static struct pci_ops pcie_ops = {
+	.read = pcie_read_conf,
+	.write = pcie_write_conf,
+};
+
+static struct pci_bus *__init transcede_pcie_scan_bus(int nr, struct pci_sys_data *sys)
+{
+	struct pci_bus *bus;
+
+	PDEBUG("sys->busnr = %d", sys->busnr);
+
+	if ((nr < NUM_PCIE_PORTS) && (pcie_port_is_host(nr))) {
+		bus = pci_scan_bus(sys->busnr, &pcie_ops, sys);
+	} else {
+		bus = pci_scan_bus(sys->busnr, &pcie_ops, sys);
+	}
+
+	return bus;
+}
+
+static struct hw_pci transcede_pcie __initdata = {
+	.nr_controllers	= NUM_PCIE_PORTS,
+	.swizzle = transcede_pcie_swizzle,
+	.map_irq = transcede_pcie_map_irq,
+	.setup = transcede_pcie_setup,
+	.scan = transcede_pcie_scan_bus,
+};
+
+
+#ifdef CONFIG_PCI_MSI
+/**
+ *   MSI int handler. Hanler processes interrupts from set 0 of MSI controller registers
+ */
+static void handle_msi(struct pcie_port *pp)
+{
+	unsigned long val, mask;
+	unsigned int pos = 0, mask0;
+
+	val = readl_relaxed(pp->va_dbi_base + PCIE_MSI_INTR0_STATUS);
+	val &= ~readl_relaxed(pp->va_dbi_base + PCIE_MSI_INTR0_MASK); // do not process masked IRQs
+
+	while (val) {
+		mask0 = 1 << pos;
+
+		if (val & mask0) {
+			/* FIXME : WA for bz69520
+			 * To avoid race condition during ack the interrupt is disabled interrupt before
+			 * Ack and enabled after Ack.
+			 */
+			spin_lock(&pp->intr_lock);
+			mask = readl_relaxed(pp->va_dbi_base + PCIE_MSI_INTR0_ENABLE);
+			writel_relaxed(mask & ~mask0, pp->va_dbi_base + PCIE_MSI_INTR0_ENABLE); // disable MSI IRQ
+			writel_relaxed(mask0, pp->va_dbi_base + PCIE_MSI_INTR0_STATUS);         // ack MSI IRQ
+			//writel_relaxed(mask & mask0, pp->va_dbi_base + PCIE_MSI_INTR0_ENABLE);  // it must be a problem - only 1 interrupt will be enabled after this code execution
+			writel_relaxed(mask, pp->va_dbi_base + PCIE_MSI_INTR0_ENABLE);  // enable
+			spin_unlock(&pp->intr_lock);
+			generic_handle_irq(pp->msi_base	+ pos);
+			val = val & ~mask0;
+		}
+		pos++;
+	}
+}
+#else
+/**
+ *   @brief Stub MSI IRQ code. Not expected to be called!
+ */
+static void handle_msi(struct pcie_port *pp)
+{
+	PDEBUG("Oops: MSI");
+}
+#endif
+
+
+
+#ifndef PCIE_USE_FAST_INTX_PROC
+
+/**
+ *   @brief  PCIe IRQ handler
+ *           T2200 PCIe 0&1 generate IRQ  = INTA|INTB|INTC|INTD|INT_MSI, INTx generation
+ *           is based on assert signal only; deassert signal is ignored
+ */
+static void transcede_pcie_int_handler(unsigned int irq, struct irq_desc *desc)
+{
+	struct pcie_port *pp = irq_get_handler_data(irq);
+	struct irq_chip *chip;
+	unsigned int status, mask;
+
+	chip = irq_desc_get_chip(desc);
+	chained_irq_enter(chip, desc);
+
+	status = readl_relaxed(pp->va_dbi_base + PCIE_MSI_INTR0_STATUS);
+	mask = readl_relaxed(pp->va_dbi_base + PCIE_MSI_INTR0_MASK);
+	if (status & ~mask)
+	{
+		// if any non-masked MSI interrupt status bit
+		handle_msi(pp);
+	}
+	else
+	{
+		//
+		// If not MSI it sould be INTA..INTD, INTX and MSI cannot be enabled at the same time
+		// We use only INTA HW is not able mask inetrrupts INTA..INTD individually
+		//
+		generic_handle_irq(pp->intx_base + 0);
+	}
+
+	chained_irq_exit(chip, desc);
+}
+
+
+static void pcie_nop_intx_irq(struct irq_data *data )
+{
+	return;
+}
+
+static void pcie_unmask_intx_irq( struct irq_data *data )
+{
+	struct pcie_port *pp = data->chip_data;
+
+	spin_lock(&pp->conf_lock);
+	pp->intx_mask = 0;
+	if(pp->intx_enable != 0) {
+		transcede_pcie_wr_conf(pp, pp->root_bus_nr, 0, PCI_COMMAND, 2, (pp->cmd_reg_val & ~PCI_COMMAND_INTX_DISABLE));
+	}
+	spin_unlock(&pp->conf_lock);
+}
+
+static void pcie_enable_intx_irq( struct irq_data *data )
+{
+	struct pcie_port *pp = data->chip_data;
+
+	spin_lock(&pp->conf_lock);
+	pp->intx_enable = 1;
+	if(pp->intx_mask == 0) {
+		transcede_pcie_wr_conf(pp, pp->root_bus_nr, 0, PCI_COMMAND, 2, (pp->cmd_reg_val & ~PCI_COMMAND_INTX_DISABLE));
+	}
+	spin_unlock(&pp->conf_lock);
+}
+
+static void pcie_mask_intx_irq( struct irq_data *data )
+{
+	struct pcie_port *pp = data->chip_data;
+
+	spin_lock(&pp->conf_lock);
+	pp->intx_mask = 1;
+	transcede_pcie_wr_conf(pp, pp->root_bus_nr, 0, PCI_COMMAND, 2, (pp->cmd_reg_val | PCI_COMMAND_INTX_DISABLE));
+	spin_unlock(&pp->conf_lock);
+}
+
+static void pcie_disable_intx_irq( struct irq_data *data )
+{
+	struct pcie_port *pp = data->chip_data;
+
+	spin_lock(&pp->conf_lock);
+	pp->intx_enable = 0;
+	transcede_pcie_wr_conf(pp, pp->root_bus_nr, 0, PCI_COMMAND, 2, (pp->cmd_reg_val | PCI_COMMAND_INTX_DISABLE));
+	spin_unlock(&pp->conf_lock);
+}
+
+static struct irq_chip pcie_intx_chip = {
+	.name = "PCIe INTx",
+	.irq_ack = pcie_nop_intx_irq,
+	.irq_enable = pcie_enable_intx_irq,
+	.irq_disable = pcie_disable_intx_irq,
+	.irq_mask = pcie_mask_intx_irq,
+	.irq_unmask = pcie_unmask_intx_irq,
+};
+
+static struct irqaction pcie_intx = {
+	.name		= "PCIe",
+	.flags		= IRQF_NO_THREAD,
+	.handler	= transcede_pcie_int_handler,
+};
+
+static int transcede_pcie_intx_init(struct pcie_port *pp)
+{
+	int i, irq;
+	struct pcie_app_reg *app_reg;
+	struct irq_desc *desc = irq_to_desc(pp->irq);
+
+	/* TODO: Disable INTX interrupt */
+
+	/* initilize INTX chip here only. MSI chip will be
+	 * initilized dynamically.*/
+	irq = pp->intx_base;
+
+	for (i = 0; i < PCIE_NUM_INTX_IRQS; i++) {
+		irq_set_chip_data(irq + i, pp);
+		irq_set_chip_and_handler(irq + i, &pcie_intx_chip,
+		                         handle_level_irq);
+		set_irq_flags(irq + i, IRQF_VALID);
+	}
+
+	irq_set_handler_data(pp->irq, pp);
+	irq_set_chained_handler(pp->irq, transcede_pcie_int_handler);
+	irq_set_affinity(pp->irq, cpu_all_mask);
+
+	PINFO("PCIe%d registering for IRQ %d", pp->port, pp->irq);
+
+	return 0;
+}
+
+
+#else
+
+static int transcede_pcie_intx_init(struct pcie_port *pp)
+{
+	PINFO("PCIe%d NOT registering for IRQ %d", pp->port, pp->irq);
+
+	return 0;
+}
+
+#endif
+
+static int transcede_pcie_rc_int_init( struct pcie_port *pp )
+{
+	transcede_pcie_intx_init(pp);
+
+#ifdef CONFIG_PCI_MSI
+	transcede_msi_init(pp);
+#endif
+
+	return 0;
+}
+
+#ifdef CONFIG_PCI_MSI
+static int find_valid_pos0(int port, int nvec, int pos, int *pos0)
+{
+	int flag = 1;
+	do {
+		pos = find_next_zero_bit(msi_irq_in_use[port],
+		                         PCIE_NUM_MSI_IRQS, pos);
+		/*if you have reached to the end then get out from here.*/
+		if (pos == PCIE_NUM_MSI_IRQS) {
+			return -ENOSPC;
+		}
+		/* Check if this position is at correct offset.nvec is always a
+		 * power of two. pos0 must be nvec bit alligned.
+		 */
+		if (pos % nvec) {
+			pos += nvec - (pos % nvec);
+		} else {
+			flag = 0;
+		}
+	} while (flag);
+
+	*pos0 = pos;
+	return 0;
+}
+
+#define GET_MSI_INT_REG_POS(_irq) ((_irq >> 5) * 12)
+#define GET_MSI_INT_OFST(_irq) (_irq & 0x1F)
+
+static void transcede_msi_nop(struct irq_data *data)
+{
+	return;
+}
+
+static void transcede_msi_unmask(struct irq_data *data)
+{
+	struct pcie_port *pp = data->chip_data;
+	int irq = data->irq - pp->msi_base;
+	int ofst, pos;
+	unsigned int val;
+	unsigned long flags;
+
+	if (irq >= PCIE_NUM_MSI_IRQS) {
+		return;
+	}
+
+	pos = GET_MSI_INT_REG_POS(irq);
+	ofst = GET_MSI_INT_OFST(irq);
+
+	PDEBUG("%x:%x", pos, ofst);
+
+	spin_lock_irqsave(&pp->intr_lock, flags);
+	transcede_dbi_read_reg(pp, PCIE_MSI_INTR0_MASK + pos, 4, &val);
+	val &= ~(1 << ofst);
+	transcede_dbi_write_reg(pp, PCIE_MSI_INTR0_MASK + pos, 4, val);
+	spin_unlock_irqrestore(&pp->intr_lock, flags);
+
+	return;
+}
+
+static void transcede_msi_mask(struct irq_data *data)
+{
+	struct pcie_port *pp = data->chip_data;
+	int irq = data->irq - pp->msi_base;
+	int ofst, pos;
+	unsigned int val;
+	unsigned long flags;
+
+	if (irq >= PCIE_NUM_MSI_IRQS) {
+		return;
+	}
+
+	pos = GET_MSI_INT_REG_POS(irq);
+	ofst = GET_MSI_INT_OFST(irq);
+
+	PDEBUG("%x:%x", pos, ofst);
+
+	spin_lock_irqsave(&pp->intr_lock, flags);
+	transcede_dbi_read_reg(pp, PCIE_MSI_INTR0_MASK + pos, 4, &val);
+	val |= (1 << ofst);
+	transcede_dbi_write_reg(pp, PCIE_MSI_INTR0_MASK + pos, 4, val);
+	spin_unlock_irqrestore(&pp->intr_lock, flags);
+
+	return;
+}
+
+
+static void transcede_msi_enable(struct irq_data *data)
+{
+	struct pcie_port *pp = data->chip_data;
+	int irq = data->irq - pp->msi_base;
+	int ofst, pos;
+	unsigned int val;
+	unsigned long flags;
+
+	if (irq >= PCIE_NUM_MSI_IRQS) {
+		return;
+	}
+
+	pos = GET_MSI_INT_REG_POS(irq);
+	ofst = GET_MSI_INT_OFST(irq);
+
+	PDEBUG("%x:%x\n", pos, ofst);
+
+	spin_lock_irqsave(&pp->intr_lock, flags);
+	transcede_dbi_read_reg(pp, PCIE_MSI_INTR0_ENABLE + pos, 4, &val);
+	val |= (1 << ofst);
+	transcede_dbi_write_reg(pp, PCIE_MSI_INTR0_ENABLE + pos, 4, val);
+	spin_unlock_irqrestore(&pp->intr_lock, flags);
+
+	return;
+}
+
+static void transcede_msi_disable(struct irq_data *data)
+{
+	struct pcie_port *pp = data->chip_data;
+	int irq = data->irq - pp->msi_base;
+	int ofst, pos;
+	unsigned int val;
+	unsigned long flags;
+
+	if (irq >= PCIE_NUM_MSI_IRQS) {
+		return;
+	}
+
+	pos = GET_MSI_INT_REG_POS(irq);
+	ofst = GET_MSI_INT_OFST(irq);
+
+	PDEBUG("%x:%x", pos, ofst);
+
+	spin_lock_irqsave(&pp->intr_lock, flags);
+	transcede_dbi_read_reg(pp, PCIE_MSI_INTR0_ENABLE + pos, 4, &val);
+	val &= ~(1 << ofst);
+	transcede_dbi_write_reg(pp, PCIE_MSI_INTR0_ENABLE + pos, 4, val);
+	spin_unlock_irqrestore(&pp->intr_lock, flags);
+
+	return;
+}
+
+static struct irq_chip transcede_msi_chip = {
+	.name = "PCI-MSI",
+	.irq_ack = transcede_msi_nop,
+	.irq_enable = transcede_msi_enable,
+	.irq_disable = transcede_msi_disable,
+	.irq_mask = transcede_msi_mask,
+	.irq_unmask = transcede_msi_unmask,
+};
+
+/*
+ * Dynamic irq allocate and deallocation
+ */
+static int get_irq(int nvec, struct msi_desc *desc, int *pos)
+{
+	int irq, pos0, pos1, i;
+	struct pcie_port *pp = bus_to_port(desc->dev->bus->number);
+
+	pos0 = find_first_zero_bit(msi_irq_in_use[pp->port],
+	                           PCIE_NUM_MSI_IRQS);
+	if (pos0 % nvec) {
+		if (find_valid_pos0(pp->port, nvec, pos0, &pos0)) {
+			goto no_valid_irq;
+		}
+	}
+	if (nvec > 1) {
+		pos1 = find_next_bit(msi_irq_in_use[pp->port],
+		                     PCIE_NUM_MSI_IRQS, pos0);
+		/* there must be nvec number of consecutive free bits */
+		while ((pos1 - pos0) < nvec) {
+			if (find_valid_pos0(pp->port, nvec, pos1, &pos0)) {
+				goto no_valid_irq;
+			}
+
+			pos1 = find_next_bit(msi_irq_in_use[pp->port],
+			                     PCIE_NUM_MSI_IRQS, pos0);
+		}
+	}
+
+	irq = pp->msi_base + pos0;
+
+	if ((irq + nvec) > (pp->msi_base + PCIE_NUM_MSI_IRQS)) {
+		goto no_valid_irq;
+	}
+
+	i = 0;
+	while (i < nvec) {
+		set_bit(pos0 + i, msi_irq_in_use[pp->port]);
+		dynamic_irq_init(irq + i);
+		irq_set_chip_data(irq + i, pp);
+		irq_set_chip_and_handler(irq + i, &transcede_msi_chip,
+		                         handle_simple_irq);
+		set_irq_flags(irq + i, IRQF_VALID);
+		i++;
+	}
+
+	irq_set_msi_desc(irq, desc);
+
+	*pos = pos0;
+
+	return irq;
+
+no_valid_irq:
+	printk(KERN_ERR "%s : MSI interrupt allocate failed\n", __func__);
+	*pos = pos0;
+
+	return -ENOSPC;
+}
+
+static void clean_irq(unsigned int irq)
+{
+	int res, bit, val, pos;
+	struct irq_data *data = irq_get_irq_data(irq);
+	struct pcie_port *pp = data->chip_data;
+	unsigned long flags;
+
+	if( !pp )
+		return;
+
+	pos = irq - pp->msi_base;
+
+	dynamic_irq_cleanup(irq);
+
+	spin_lock_irqsave(&pp->msi_map_lock, flags);
+	clear_bit(pos, msi_irq_in_use[pp->port]);
+
+	/* Disable corresponding interrupt on MSI interrupt
+	 * controller.
+	 */
+	res = (pos / 32) * 12;
+	bit = pos % 32;
+	transcede_dbi_read_reg(pp, PCIE_MSI_INTR0_ENABLE + res, 4, &val);
+	val &= ~(1 << bit);
+	transcede_dbi_write_reg(pp, PCIE_MSI_INTR0_ENABLE + res, 4, val);
+	spin_unlock_irqrestore(&pp->msi_map_lock, flags);
+
+}
+
+int arch_setup_msi_irq(struct pci_dev *pdev, struct msi_desc *desc)
+{
+	int irq, pos;
+	struct msi_msg msg;
+	struct pcie_port *pp = bus_to_port(pdev->bus->number);
+	unsigned long flags;
+
+	spin_lock_irqsave(&pp->msi_map_lock, flags);
+	irq = get_irq(1, desc, &pos);
+	spin_unlock_irqrestore(&pp->msi_map_lock, flags);
+
+	if (irq < 0) {
+		return irq;
+	}
+
+	desc->msi_attrib.multiple = 0;
+
+	/* An EP will modify lower 8 bits(max) of msi data while
+	 * sending any msi interrupt
+	 */
+	msg.address_hi = 0x0;
+	msg.address_lo = pp->msi_mbox_handle;
+	msg.data = pos;
+
+	write_msi_msg(irq, &msg);
+
+	if (desc->msi_attrib.is_msix) {
+		/**
+		 *   MSI-X masked by default.
+		 *   Changes in kernel msi required because kernel will mask MSI-X later
+		 */
+		unmask_msi_irq(irq_get_irq_data(irq));
+	}
+
+	return 0;
+}
+
+void arch_teardown_msi_irq(unsigned int irq)
+{
+	clean_irq(irq);
+}
+
+static int transcede_msi_init(struct pcie_port *pp)
+{
+	int i;
+
+	pp->msi_mbox_baseaddr = dma_alloc_coherent(NULL, sizeof(u32), &pp->msi_mbox_handle, GFP_KERNEL);
+	if (!pp->msi_mbox_baseaddr) {
+		printk(KERN_ERR "PCIe %d: failed to allocate msi mailbox coherent memory\n", pp->port);
+		goto err;
+	}
+
+	PDEBUG("MSI box address: 0x%08X", pp->msi_mbox_handle);
+
+	transcede_dbi_write_reg(pp, PCIE_MSI_ADDR_LO, 4, pp->msi_mbox_handle);
+	transcede_dbi_write_reg(pp, PCIE_MSI_ADDR_HI, 4, 0);
+	transcede_dbi_read_reg(pp, PCIE_MSI_ADDR_LO, 4, &i);
+	return 0;
+
+err:
+	return -1;
+}
+#endif
+
+/**
+ *   @brief  Initialize PCIe for RC mode. PCIe 0 0r one selection is based on
+ *           pp->port value. Should be called after SERDES initialization
+ */
+static void transcede_pcie_rc_init(struct pcie_port *pp)
+{
+	unsigned int val;
+	unsigned int lm, lanes = 4;
+	unsigned long TD = 1, flip_lanes = 0, g1 = pcie_gen1_only;
+
+	if (pp->port == 0) {
+		//  PCIe 0 specific initialization
+		val =                         //(mode << 0) |     // RC/EP mode selection
+			(0x07 << 4) |     // set signals to not active state
+			(0x03 << 7) |     // map DECERR -> UR, SLVERR->UR
+			(0x3f << 9) |     // map all CPL errors -> AXI SLVERR
+			(TD << 23)  |     // add digest into read response
+			(1 << 27);        // TODO: this bit is reserved, but set in VLSI code
+		updatel(RAD_CFG_PCIE_X4_CFG0, 0xFFFFFFF0, val);
+		writel((TD << 7),   RAD_CFG_PCIE_X4_CFG1);        // AXI read transactions params
+		writel((TD << 8),   RAD_CFG_PCIE_X4_CFG2);        // write response params
+		writel((TD << 7),   RAD_CFG_PCIE_X4_CFG3);        // AXI write transaction params
+		writel((1 << 4),    RAD_CFG_PCIE_X4_CFG4);        // sys_aux_pwr_det
+		writel(0,           RAD_CFG_PCIE_X4_CFG5);        // disable ltssm
+		writel(3,           RAD_CFG_PCIE_X4_CFG7);        // from VLSI code, no information on signals in documentation
+
+		if (flip_lanes) {
+			updatel(RAD_CFG_PCIE_X4_CFG4, 0, 6);
+		}
+
+#if 0
+		// limit lane number
+		transcede_dbi_update_reg(pp, PCIE_G2CTRL_REG, 4, 0x0001ff00, (lanes & 7) << 8);
+		lm = (lanes * 2) - 1;
+		transcede_dbi_update_reg(pp, PCIE_CTL_REG, 4, 0x003f0000, lm << 16);
+#endif
+
+	} else {
+		/* PCIe 1 specific initialization */
+		writel(0,           RAD_CFG_PCIE_X1_CFG1);        // disable ltssm
+	}
+
+
+	/* Debugging code: Enable completions with EP (poisoned) */
+
+	/* transcede_dbi_update_reg(pp, 0x71c, 4, 0, 0xffff0000); */
+	/* transcede_dbi_update_reg(pp, 0x720, 4, 0, 0xf); */
+
+	/* advertize MTU size = 1024 */
+	/* transcede_dbi_update_reg(pp, PCIE_DCNT_REG, 4, 7 << 5, 3 << 5); */
+
+	if (g1) {
+		/* advertize PCIe g.1 only */
+		transcede_dbi_update_reg(pp, PCIE_LCAP_REG, 4, 0xf, 0x1);
+		transcede_dbi_update_reg(pp, PCIE_LCNT2_REG, 4, 0xf, 0x1);
+	}
+
+	transcede_dbi_update_reg(pp, PCIE_CTL_REG, 4, 1 << 22, 0);      // disable crosslink
+	transcede_dbi_write_reg(pp, PCIE_AFL0L1_REG, 4, 0x3f161601);    // incr l0&l1 entry times
+	transcede_dbi_update_reg(pp, PCIE_G2CTRL_REG, 4, 0xff, 0x40);   // Set number of FTS (Fast training sequences) symbols to 64 for Gen2
+
+	transcede_dbi_update_reg(pp, PCIE_DCNT_REG-4, 4, 0X7, 0);
+	transcede_dbi_update_reg(pp, PCIE_DCNT_REG, 4, 0X70f0, 0);
+
+	/* transcede_dbi_update_reg(pp, 0X818, 4, 0XF, 0); */
+	/* transcede_dbi_update_reg(pp, 0X81C, 4, 0, 1); */
+
+	/* TODO: enable DLL in DLL Link Enable (0x710) */
+
+	if(pp->port == 0) {
+		writel((1 << 1), RAD_CFG_PCIE_X4_CFG5);        // enable ltssm, PCIe0
+	}
+	else {
+		writel((1 << 0), RAD_CFG_PCIE_X1_CFG1);        // enable ltssm, PCIe1
+	}
+
+	udelay(100);
+}
+
+
+/**
+ *   @brief  Initialize PCIe port structure for the selected mode
+ *   @param  pp  pointer to the pcie_port structure, it will be cleared
+ *           befor setting all the actual parameters
+ *   @param  pcie_id identifier of PCIe engine (0 - PCIe X4, 1 -PCIe X1)
+ *   @param  mode    PCIe code mode (only RC mode supported)
+ */
+static int pcie_app_init(struct pcie_port *pp, int pcie_id, int mode)
+{
+
+	PDEBUG("port %d, mode %d", pcie_id, mode);
+
+	if (pcie_id != PCIE0_PORT_ID && pcie_id != PCIE1_PORT_ID) {
+		PERROR("Invalid PCIe port: %d", pcie_id);
+		goto err0;
+	}
+
+	if (mode != CFG0_DEV_TYPE_RC) {
+		PERROR("Unsupported mode selected mode: %d", mode);
+		goto err0;
+	}
+
+	memset(pp, 0, sizeof(struct pcie_port));
+
+	pp->port = pcie_id;
+	pp->root_bus_nr = pcie_id;
+	pp->base = pcie_cnf_base_addr[pcie_id];
+	pp->app_base = (unsigned long)TRANSCEDE_RAD_CFG;
+	pp->va_app_base = (void __iomem *)TRANSCEDE_RAD_CFG;
+	pp->remote_mem_baseaddr = pcie_remote_base_addr[pcie_id];
+
+	pp->iatu_mem.base = iATU_GET_MEM_BASE(pp->remote_mem_baseaddr, pp->port);
+	pp->iatu_mem.size = iATU_MEM_SIZE(pp->port);
+	pp->iatu_io.base = iATU_GET_IO_BASE(pp->remote_mem_baseaddr, pp->port);
+	pp->iatu_io.size = iATU_IO_SIZE(pp->port);
+	pp->iatu_cfg0.base = iATU_GET_CFG0_BASE(pp->remote_mem_baseaddr, pp->port);
+	pp->iatu_cfg0.size = iATU_CFG0_SIZE(pp->port);
+	pp->iatu_cfg1.base = iATU_GET_CFG1_BASE(pp->remote_mem_baseaddr, pp->port);
+	pp->iatu_cfg1.size = iATU_CFG1_SIZE(pp->port);
+
+	PINFO("PCIe%d: MEM  0x%08X..+0x%X", pp->port, pp->iatu_mem.base, pp->iatu_mem.size);
+	PINFO("PCIe%d: IO   0x%08X..+0x%X", pp->port, pp->iatu_io.base, pp->iatu_io.size);
+	PINFO("PCIe%d: CFG0 0x%08X..+0x%X", pp->port, pp->iatu_cfg0.base, pp->iatu_cfg0.size);
+	PINFO("PCIe%d: CFG1 0x%08X..+0x%X", pp->port, pp->iatu_cfg1.base, pp->iatu_cfg1.size);
+
+	if (pcie_id == PCIE0_PORT_ID) {
+		pp->va_dbi_base = (void __iomem *)TRANSCEDE_PCIe0_BASE;
+		pp->link_status_reg = TRANSCEDE_RAD_CFG + 0x224; //RAD_CFG_PCIE_X4_STAT0;
+
+	} else {
+		pp->va_dbi_base = (void __iomem *)TRANSCEDE_PCIe1_BASE;
+		pp->link_status_reg = TRANSCEDE_RAD_CFG + 0x100; //RAD_CFG_PCIE_X1_STAT0;
+	}
+
+	pp->va_cfg0_base = (void __iomem *)ioremap(pp->iatu_cfg0.base, pp->iatu_cfg0.size);
+	if (!pp->va_cfg0_base) {
+		PERROR("error with ioremap va_cfg0_base");
+		goto err0;
+	}
+
+	pp->va_cfg1_base = (void __iomem *)		ioremap(pp->iatu_cfg1.base, pp->iatu_cfg1.size);
+	if (!pp->va_cfg1_base) {
+		PERROR("error with ioremap va_cfg1_base");
+		goto err1;
+	}
+
+	PINFO("PCIe%d: CFG0 0x%08X..+0x%X VA 0x%08X", pp->port, pp->iatu_cfg0.base, pp->iatu_cfg0.size, pp->va_cfg0_base);
+	PINFO("PCIe%d: CFG1 0x%08X..+0x%X VA 0x%08X", pp->port, pp->iatu_cfg1.base, pp->iatu_cfg1.size, pp->va_cfg1_base);
+
+	pp->intx_base = pcie_intx_base[pcie_id];
+	pp->msi_base = pcie_msi_base[pcie_id];
+	pp->irq = pcie_irqs[pcie_id];
+
+	PINFO("PCIe%d: HW irq=%d", pcie_id, pp->irq);
+
+	spin_lock_init(&pp->conf_lock);
+	spin_lock_init(&pp->intr_lock);
+	spin_lock_init(&pp->msi_map_lock);
+	memset(pp->res, 0, sizeof(pp->res));
+
+	pcie_port_set_mode(pp, mode);
+
+	return 0;
+
+err1:
+	iounmap(pp->va_cfg0_base);
+
+err0:
+	return -1;
+
+}
+
+/**
+ *   @brief  Initialize SerDes for the PCIe port, initialize PCIe, wait for the link
+ *   @param  pp  pointer to the initialized pcie_port structure
+ */
+static int transcede_pcie_bsp_link_init(struct pcie_port *pp)
+{
+	int axi_pcie_component;
+	int pcie_component;
+	int serdes_component;
+	int rc;
+	int if_err = 1;
+	static int pcie_reset_done = 0;
+
+	// init SerDes (PHY)
+	serdes_pcie_phy_init(pp->port);
+	mdelay(1);
+
+	if (!pcie_reset_done) {
+		/* Reset PCIe card over GPIO. EVM uses the same GPIO for all PCIe slots, so
+		 * reset should be done only once
+		 */
+		pcie_reset_done = 1;
+		if (gpio_request_one(PCIE_GPIO_RESET, GPIOF_DIR_OUT | GPIOF_INIT_LOW, "gpio-pcie-reset")) {
+			printk(KERN_ERR "error: failed to request PCIE RESET GPIO\n");
+		} else {
+			t2200_gpio_pin_stat.t2200_gpio_pins |=  (1<<PCIE_GPIO_RESET); /* reserve */
+			mdelay(1);
+			gpio_set_value(PCIE_GPIO_RESET, 1);
+			mdelay(1);
+			gpio_free(PCIE_GPIO_RESET); /* free for sysfs */
+		}
+	}
+
+	// init PCIE core
+	transcede_pcie_rc_init( pp );
+
+	// start link initialization
+	pp->link_state = transcede_pcie_link_up( pp );
+
+	if (!pp->link_state) {
+		PINFO("PCIe%d: No Link up", pp->port);
+		if_err = 0;
+		goto err0;
+	}
+
+	/* setup iATU for outbound translation */
+	PCIE_SETUP_iATU_OB_ENTRY( pp, iATU_ENTRY_MEM, pp->iatu_mem.base, pp->iatu_mem.size - 1,
+	                          0, 0, pp->iatu_mem.base );
+	PCIE_SETUP_iATU_OB_ENTRY( pp, iATU_ENTRY_IO, pp->iatu_io.base, pp->iatu_io.size - 1,
+	                          (AXI_OP_TYPE_IO_RDRW & iATU_CTRL1_TYPE_MASK), 0, pp->iatu_io.base );
+	PCIE_SETUP_iATU_IB_ENTRY( pp, 0, 0, INBOUND_ADDR_MASK, 0, 0, TRANSCEDE_SDRAM_BASE);
+
+	return 0;
+
+err0:
+	if (if_err)
+		return -1;
+	else
+		return 0;
+}
+
+
+/**
+ *   @brief  Initialize selected PCIe port in RC mode, wait for the link
+ *   @param  pp  pointer to the pcie_port port structure (it will be initialized)
+ *   @param  pcie_id ID of PCIe device to be used
+ *   @return 0 if PCIe successfully initialized and link is up
+ */
+static int transcede_pcie_bsp_init(struct pcie_port *pp, int pcie_id)
+{
+	int ret;
+
+	if (pcie_id >= NUM_PCIE_PORTS) {
+		PERROR("Invalid PCIe port number: %d", pcie_id);
+		goto err0;
+	}
+
+	// init driver control structure
+	ret = pcie_app_init(pp, pcie_id, CFG0_DEV_TYPE_RC);
+	if(ret == -1)
+		goto err0;
+
+	// init PHY and PCIe, wait for link
+	ret = transcede_pcie_bsp_link_init(pp);
+	if(ret == -1)
+		goto err1;;
+
+	if (pp->link_state)
+		goto linkup;
+
+	//
+	//TODO: implement cycle with polarity change here
+	//TODO: change polarity and try to set the link up
+	//
+	PERROR("PCIe%d: Link Up Failed", pcie_id);
+
+err1:
+	iounmap(pp->va_cfg0_base);
+	iounmap(pp->va_cfg1_base);
+	pp->port_mode = PCIE_PORT_MODE_NONE;
+
+err0:
+	return -1;
+
+linkup:
+	PINFO("PCIe%d: Link Up Success", pcie_id);
+	PINFO("PCIe%d: Gen1 mode: %d", pcie_id, pcie_gen1_only);
+
+	// init IRQ support
+	transcede_pcie_rc_int_init(pp);
+
+	return 0;
+}
+
+static unsigned long pcie = 0;
+
+static int __init set_pcie(char *str)
+{
+	pcie = simple_strtoul(str, NULL, 0);
+
+	return 0;
+}
+
+__setup("pcie=", set_pcie);
+
+/**
+ *   This function is called by kernel first. It is responsible for
+ *   PCIe 0&1 initialization and export of PCIe-related information
+ *   (hw_pci structure) to the kernel
+ */
+static int __init transcede_pcie_init(void)
+{
+	struct pcie_port *pp;
+
+	pcie_port_number = 0;
+
+	if (pcie & (1 << PCIE0_PORT_ID)) {
+#ifdef PCIE_0_SUPPORT_EN
+		PINFO("PCIe0: enabled, context %d", pcie_port_number);
+		pp = &pcie_port[pcie_port_number];
+		transcede_pcie_bsp_init(pp, 0);
+		pcie_port_number++;
+#else
+		PINFO("PCIe0 is not supported");
+#endif
+	}
+
+	if (pcie & (1 << PCIE1_PORT_ID)) {
+#ifdef PCIE_1_SUPPORT_EN
+		PINFO("PCIe1: enabled, context %d", pcie_port_number);
+		pp = &pcie_port[pcie_port_number];
+		transcede_pcie_bsp_init(pp, 1);
+		pcie_port_number++;
+#else
+		PINFO("PCIe1 is not supported");
+#endif
+	}
+
+	transcede_pcie.nr_controllers = pcie_port_number;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,1,0)
+	/* kernel version >= 3.1 */
+	pcibios_min_io = iATU_GET_IO_BASE(TRANSCEDE_AXI_PCIe0_SLAVE_BASE);
+	pcibios_min_mem = TRANSCEDE_AXI_PCIe0_SLAVE_BASE;
+	pci_add_flags(PCI_REASSIGN_ALL_RSRC);
+#endif
+
+	pci_common_init(&transcede_pcie);
+
+	return 0;
+}
+subsys_initcall(transcede_pcie_init);
diff --git a/arch/arm/mach-transcede/periodic_task.c b/arch/arm/mach-transcede/periodic_task.c
new file mode 100644
index 0000000..80ba6ba
--- /dev/null
+++ b/arch/arm/mach-transcede/periodic_task.c
@@ -0,0 +1,545 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#include <linux/module.h>
+#include <linux/io.h>
+#include <linux/init.h>
+#include <linux/irq.h>
+#include <linux/hardirq.h>
+#include <linux/smp.h>
+#include <linux/proc_fs.h>
+#include <linux/slab.h>
+
+#include <asm/localtimer.h>
+#include <asm/mach-types.h>
+#include <asm/sched_clock.h>
+#include <asm/smp_twd.h>
+#include <asm/hardware/gic.h>
+#include <asm/uaccess.h>
+
+#include <mach/hardware.h>
+#include <mach/periodic_task.h>
+
+#ifdef CONFIG_TRANSCEDE_MIPS_MONITOR
+#include <mach/mips_monitor.h>
+#endif
+
+enum {
+	kPROC_ACTIVE_CPU,
+	kPROC_PERIOD_MS,
+	kPROC_COUNT
+};
+
+struct periodic_sched {
+	char name[PERIODIC_NAME_LEN + 1];
+	void (*action)(unsigned long);
+	unsigned long data;
+	unsigned long period;
+
+	unsigned long expires;
+	unsigned long count;
+	unsigned int cpu;
+	unsigned long tick;
+
+	struct proc_dir_entry *proc;
+	struct proc_dir_entry *proc_i[kPROC_COUNT];
+	char proc_name[PERIODIC_NAME_LEN + 10 + 1];
+
+	struct list_head list;
+};
+
+#define periodic_head(cpu) (&periodic_queue[cpu].list)
+
+static unsigned int global_timer_irqs[NR_CPUS];
+static struct periodic_sched periodic_queue[NR_CPUS];
+static DEFINE_SPINLOCK(periodic_task_lock);
+static struct proc_dir_entry *periodic_proc;
+
+static void *periodic_task_get_id(struct periodic_sched *task)
+{
+	return (void *)(~(unsigned long)task);
+}
+
+static struct periodic_sched *periodic_task_get_task(void *id)
+{
+	return (struct periodic_sched *)(~(unsigned long)id);
+}
+
+static int periodic_proc_read_cpu(char *page, char **start, off_t off,
+                                  int count, int *eof, void *data)
+{
+	struct periodic_sched *q = (struct periodic_sched *)data;
+	int len = 0;
+
+	if (off > 0) {
+		return 0;
+	}
+
+	len += sprintf(page + len, "%d\n", q->cpu);
+
+	*start = NULL;
+
+	if (count <= len) {
+		*eof = 1;
+	}
+
+	return len;
+}
+
+static int periodic_proc_write_cpu(struct file *file, const char __user *buffer,
+                                   unsigned long count, void *data)
+{
+	struct periodic_sched *q = (struct periodic_sched *)data;
+	unsigned int cpu;
+	unsigned long flags;
+
+	char *tmp = kmalloc(count + 1, GFP_KERNEL);
+
+	if (copy_from_user(tmp, buffer, count)) {
+		printk(KERN_ERR "copy failed: 0x%08lx\n", (unsigned long)buffer);
+		kfree(tmp);
+		return 0;
+	}
+
+	tmp[count] = '\0';
+
+	cpu = simple_strtoul(tmp, NULL, 10);
+
+	if ((cpu != q->cpu) && (cpu < NR_CPUS)) {
+		spin_lock_irqsave(&periodic_task_lock, flags);
+
+		list_del(&q->list);
+		list_add_tail(&q->list, periodic_head(cpu));
+
+		q->cpu = cpu;
+
+		spin_unlock_irqrestore(&periodic_task_lock, flags);
+	}
+
+	kfree(tmp);
+
+	return count;
+}
+
+static int periodic_proc_read_period(char *page, char **start, off_t off,
+                                     int count, int *eof, void *data)
+{
+	struct periodic_sched *q = (struct periodic_sched *)data;
+	int len = 0;
+
+	if (off > 0) {
+		return 0;
+	}
+
+	len += sprintf(page + len, "%lu\n", q->period);
+
+	*start = NULL;
+
+	if (count <= len) {
+		*eof = 1;
+	}
+
+	return len;
+}
+
+static int periodic_proc_write_period(struct file *file, const char __user *buffer,
+                                      unsigned long count, void *data)
+{
+	struct periodic_sched *q = (struct periodic_sched *)data;
+	unsigned long period;
+	unsigned long flags;
+
+	char *tmp = kmalloc(count + 1, GFP_KERNEL);
+
+	if (copy_from_user(tmp, buffer, count)) {
+		printk(KERN_ERR "copy failed: 0x%08lx\n", (unsigned long)buffer);
+		kfree(tmp);
+		return 0;
+	}
+
+	tmp[count] = '\0';
+
+	period = simple_strtoul(tmp, NULL, 10);
+
+	if ((period != q->period) && (period != 0)) {
+		spin_lock_irqsave(&periodic_task_lock, flags);
+
+		q->period = period;
+
+		spin_unlock_irqrestore(&periodic_task_lock, flags);
+	}
+
+	kfree(tmp);
+
+	return count;
+}
+
+static int create_periodic_proc(struct periodic_sched *q)
+{
+	int ret = 0;
+
+	q->proc_i[kPROC_ACTIVE_CPU] = create_proc_entry("active_cpu", 0600, q->proc);
+
+	if (!q->proc_i[kPROC_ACTIVE_CPU]) {
+		ret = -1;
+	} else {
+		q->proc_i[kPROC_ACTIVE_CPU]->read_proc = periodic_proc_read_cpu;
+		q->proc_i[kPROC_ACTIVE_CPU]->write_proc = periodic_proc_write_cpu;
+		q->proc_i[kPROC_ACTIVE_CPU]->data = q;
+	}
+
+	q->proc_i[kPROC_PERIOD_MS] = create_proc_entry("period_ms", 0600, q->proc);
+
+	if (!q->proc_i[kPROC_PERIOD_MS]) {
+		ret = -1;
+	} else {
+		q->proc_i[kPROC_PERIOD_MS]->read_proc = periodic_proc_read_period;
+		q->proc_i[kPROC_PERIOD_MS]->write_proc = periodic_proc_write_period;
+		q->proc_i[kPROC_PERIOD_MS]->data = q;
+	}
+
+	return ret;
+}
+
+static int remove_periodic_proc(struct periodic_sched *q)
+{
+	int ret = 0;
+
+	remove_proc_entry("period_ms", q->proc);
+	remove_proc_entry("active_cpu", q->proc);
+	remove_proc_entry(q->proc_name, periodic_proc);
+
+	return ret;
+}
+
+static int periodic_task_proc(char *page, char **start, off_t off,
+                              int count, int *eof, void *data)
+{
+	struct periodic_sched *q;
+	unsigned int cpu;
+	int len = 0;
+
+	if (off > 0) {
+		/* FIXME: too many clients may force to change this */
+		return 0;
+	}
+
+	len += sprintf(page + len, "ID:      CPU:     count    period      tick name\n");
+
+	for_each_possible_cpu(cpu) {
+		list_for_each_entry(q, periodic_head(cpu), list) {
+			len += sprintf(page + len, "%p  %2d: %9lu %9lu %9lu %s\n",
+			               periodic_task_get_id(q),
+			               cpu, q->count, q->period, q->tick, q->name);
+		}
+	}
+
+	*start = NULL;
+
+	if (count <= len) {
+		*eof = 1;
+	}
+
+	return len;
+}
+
+int periodic_task_setup(void)
+{
+	unsigned int cpu;
+
+	for_each_possible_cpu(cpu) {
+		memset(&periodic_queue[cpu], 0, sizeof(struct periodic_sched));
+		INIT_LIST_HEAD(periodic_head(cpu));
+	}
+
+	periodic_proc = proc_mkdir("periodic_task", NULL);
+	create_proc_read_entry("list", 0, periodic_proc, periodic_task_proc, (void *)periodic_queue);
+
+	return 0;
+}
+EXPORT_SYMBOL(periodic_task_setup);
+
+void *periodic_task_add(unsigned int cpu,
+                        unsigned long period, /* ms */
+                        void (*action)(unsigned long),
+                        unsigned long data,
+                        const char *name)
+{
+	struct periodic_sched *q = NULL;
+	unsigned long flags;
+
+	if (unlikely((cpu >= NR_CPUS) || (action == NULL) || (name == NULL))) {
+		printk(KERN_ERR "null context: failed to add task on cpu%d\n", cpu);
+		return NULL;
+	}
+
+	if (period == 0) {
+		printk(KERN_ERR "null context: period == 0 is not supported\n");
+		return NULL;
+	}
+
+	q = (struct periodic_sched *)kzalloc(sizeof(*q), GFP_KERNEL);
+
+	if (unlikely(q == NULL)) {
+		printk(KERN_ERR "no memory: failed to add task on cpu%d\n", cpu);
+		return NULL;
+	}
+
+	q->cpu = cpu;
+	q->action = action;
+	q->data = data;
+	q->period = period;
+
+	strncpy(q->name, name, PERIODIC_NAME_LEN);
+	strncpy(q->proc_name, name, PERIODIC_NAME_LEN);
+	snprintf(q->proc_name + strlen(q->proc_name), 10, "-%08lx", (unsigned long)periodic_task_get_id(q));
+
+	spin_lock_irqsave(&periodic_task_lock, flags);
+	list_add_tail(&q->list, periodic_head(cpu));
+	spin_unlock_irqrestore(&periodic_task_lock, flags);
+
+	q->proc = proc_mkdir(q->proc_name, periodic_proc);
+
+	if (!q->proc) {
+		printk(KERN_ERR "Failed to create proc dir for task %p\n", periodic_task_get_id(q));
+	} else if (create_periodic_proc(q)) {
+		printk(KERN_ERR "Failed to create proc entry for task %p\n", periodic_task_get_id(q));
+	}
+
+	printk(KERN_INFO "Periodic task `%s' has been added on cpu%d. ID: %p\n",
+	       q->name, cpu, periodic_task_get_id(q));
+
+	return periodic_task_get_id(q);
+}
+EXPORT_SYMBOL(periodic_task_add);
+
+int periodic_task_remove(void *task)
+{
+	struct periodic_sched *q = NULL;
+	struct periodic_sched *t = periodic_task_get_task(task);
+	unsigned long flags;
+	unsigned int cpu;
+	int ret = -1;
+
+	if (unlikely(task == NULL)) {
+		return 0;
+	}
+
+	spin_lock_irqsave(&periodic_task_lock, flags);
+
+	for_each_possible_cpu(cpu) {
+		list_for_each_entry(q, periodic_head(cpu), list) {
+			if (q == t) {
+				list_del(&q->list);
+				remove_periodic_proc(q);
+				kfree(q);
+				ret = 0;
+				break;
+			}
+		}
+	}
+
+	spin_unlock_irqrestore(&periodic_task_lock, flags);
+
+	if (!ret) {
+		printk(KERN_INFO "Periodic task 0x%lx has been removed", (unsigned long)task);
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL(periodic_task_remove);
+
+int periodic_task_remove_all(unsigned int cpu)
+{
+	struct periodic_sched *q, *next;
+	unsigned long flags;
+
+	spin_lock_irqsave(&periodic_task_lock, flags);
+
+	list_for_each_entry_safe(q, next, periodic_head(cpu), list) {
+		list_del(&q->list);
+		kfree(q);
+	}
+
+	spin_unlock_irqrestore(&periodic_task_lock, flags);
+
+	return 0;
+}
+EXPORT_SYMBOL(periodic_task_remove_all);
+
+unsigned long periodic_task_get_period(void *task)
+{
+	struct periodic_sched *q = NULL;
+	struct periodic_sched *t = periodic_task_get_task(task);
+	unsigned int cpu;
+
+	for_each_possible_cpu(cpu) {
+		list_for_each_entry(q, periodic_head(cpu), list) {
+			if (q == t) {
+				return q->period;
+			}
+		}
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(periodic_task_get_period);
+
+unsigned long periodic_task_get_cpu(void *task)
+{
+	struct periodic_sched *q = NULL;
+	struct periodic_sched *t = periodic_task_get_task(task);
+	unsigned int cpu;
+
+	for_each_possible_cpu(cpu) {
+		list_for_each_entry(q, periodic_head(cpu), list) {
+			if (q == t) {
+				return cpu;
+			}
+		}
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(periodic_task_get_cpu);
+
+int periodic_task_get_name(void *task, char *name)
+{
+	struct periodic_sched *q = NULL;
+	struct periodic_sched *t = periodic_task_get_task(task);
+	unsigned int cpu;
+
+	for_each_possible_cpu(cpu) {
+		list_for_each_entry(q, periodic_head(cpu), list) {
+			if (q == t) {
+				strncpy(name, q->name, PERIODIC_NAME_LEN);
+				return 0;
+			}
+		}
+	}
+
+	return -1;
+}
+EXPORT_SYMBOL(periodic_task_get_name);
+
+static int global_timer_ack(void)
+{
+	if (readl(TRANSCEDE_TIMER_STATUS)) {
+		writel(1, TRANSCEDE_TIMER_STATUS);
+
+		return 1;
+	}
+
+	return 0;
+}
+
+#define POLL_INCREMENT (1)		/* 1ms */
+
+static void ppi_timer(unsigned int cpu)
+{
+	struct periodic_sched *q;
+
+	list_for_each_entry(q, periodic_head(cpu), list) {
+		q->expires += POLL_INCREMENT;
+
+		if (q->expires >= q->period) {
+			unsigned long t = get_tick();
+			q->count++;
+			q->action(q->data);
+			q->tick = get_tick() - t;
+			q->expires = 0;
+		}
+	}
+}
+
+asmlinkage void __exception_irq_entry do_global_timer(int ppinr, struct pt_regs *regs)
+{
+	struct pt_regs *old_regs = set_irq_regs(regs);
+	unsigned int cpu = smp_processor_id();
+	unsigned long flags;
+
+	if (global_timer_ack()) {
+		global_timer_irqs[cpu]++;
+		irq_enter();
+		local_irq_save(flags);
+#ifdef CONFIG_TRANSCEDE_MIPS_MONITOR
+        mips_monitor_tick_timer_update(*((volatile unsigned int*)TIMER0_CURR_COUNT), cpu);
+#endif
+		ppi_timer(cpu);			/* in_irq() == in_interrupt() == true */
+		local_irq_restore(flags);
+		irq_exit();
+	}
+
+	set_irq_regs(old_regs);
+}
+
+void show_global_irqs(struct seq_file *p, int prec)
+{
+	unsigned int cpu;
+
+	seq_printf(p, "%*s: ", prec, "GBL");
+
+	for_each_present_cpu(cpu)
+		seq_printf(p, "%10u ", global_timer_irqs[cpu]);
+
+	seq_printf(p, " Global timer interrupts\n");
+}
+
+/** @brief This function is IPI handler and called in linux interrupt mode
+	    to process global timer callbacks on this(local) core
+    @param cpu - the current cpu id (smp_processor_id())
+*/
+void ipi_gtcore_sync(int cpu)
+{
+    unsigned long flags;
+    global_timer_irqs[cpu]++;
+    local_irq_save(flags);
+    ppi_timer(cpu);			/* in_irq() == in_interrupt() == true */
+    local_irq_restore(flags);
+}
+
+void global_timer_send_sync(void)
+{
+    // this function sends notification to other ARM core
+    // to process global timer callbacks on that core
+    // it's designed to be called directly by driver
+    // to be able to implement more flexible logic
+    // and to minimize the number of syncronizations
+    smp_send_gtcore_sync(smp_processor_id() ? 0 : 1);
+}
+EXPORT_SYMBOL(global_timer_send_sync);
+
+/** @brief This function is designed to be called in Linux interrupt mode
+	    by some specific driver to start global timer callbacks processing
+	    on this(local) ARM core
+*/
+void global_timer_proc (void)
+{
+    unsigned long flags;
+    unsigned int cpu = smp_processor_id();
+
+    global_timer_irqs[cpu]++;
+    local_irq_save(flags);
+    ppi_timer(cpu);			/* in_irq() == in_interrupt() == true */
+    local_irq_restore(flags);
+}
+EXPORT_SYMBOL(global_timer_proc);
diff --git a/arch/arm/mach-transcede/platsmp-t2200.c b/arch/arm/mach-transcede/platsmp-t2200.c
new file mode 100644
index 0000000..58bc9af
--- /dev/null
+++ b/arch/arm/mach-transcede/platsmp-t2200.c
@@ -0,0 +1,208 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#include <linux/init.h>
+#include <linux/errno.h>
+#include <linux/smp.h>
+#include <linux/io.h>
+#include <linux/delay.h>
+#include <linux/jiffies.h>
+#include <linux/module.h>
+
+#include <asm/cacheflush.h>
+#include <mach/hardware.h>
+#include <asm/hardware/gic.h>
+#include <asm/mach-types.h>
+#include <asm/smp_scu.h>
+#include <asm/unified.h>
+
+#include "core.h"
+
+/* Required by L-ARM loader */
+volatile int   asmp_core_ready = -1;
+volatile void (*asmp_core_boot)(void) = NULL;
+volatile int   asmp_core_reset = 0;
+
+EXPORT_SYMBOL(asmp_core_ready);
+EXPORT_SYMBOL(asmp_core_boot);
+EXPORT_SYMBOL(asmp_core_reset);
+
+
+extern void transcede_secondary_startup(void);
+
+static void __iomem *scu_base_addr(void)
+{
+	return (void *)(TRANSCEDE_SCU_BASE);
+}
+
+#define hard_smp_processor_id()                 \
+        ({                                              \
+                unsigned int cpunum;                    \
+                __asm__("mrc p15, 0, %0, c0, c0, 5"     \
+                        : "=r" (cpunum));               \
+                cpunum &= 0x0F;                         \
+        })
+
+void raise_icpu(const struct cpumask *mask, unsigned int irq)
+{
+        if ( hard_smp_processor_id() )
+            *(volatile unsigned*) 0xFE43003C = 1;       // notify core #0
+        else
+            *(volatile unsigned*) 0xFE43003C = 2;       // notify core #1
+}
+
+
+/*
+ * Initialise the CPU possible map early - this describes the CPUs
+ * which may be present or become present in the system.
+ */
+void __init smp_init_cpus(void)
+{
+	void __iomem *scu_base = scu_base_addr();
+	unsigned int i, ncores;
+
+	ncores = scu_base ? scu_get_core_count(scu_base) : 1;
+
+	if (ncores > NR_CPUS) {
+		printk(KERN_WARNING
+		       "Realview: no. of cores (%d) greater than configured "
+		       "maximum of %d - clipping\n",
+		       ncores, NR_CPUS);
+		ncores = NR_CPUS;
+	}
+
+	for (i = 0; i < ncores; i++)
+		set_cpu_possible(i, true);
+#ifdef CONFIG_MACH_M84XXX
+	set_smp_cross_call(raise_icpu);
+#else
+        set_smp_cross_call(gic_raise_softirq);
+#endif
+}
+
+void __init platform_smp_prepare_cpus(unsigned int max_cpus)
+{
+	int i;
+
+	/*
+	 * Initialise the present map, which describes the set of CPUs
+	 * actually populated at the present time.
+	 */
+	for (i = 0; i < max_cpus; i++)
+		set_cpu_present(i, true);
+
+	scu_enable(scu_base_addr());
+
+	/*
+	 * Write the address of secondary startup into the
+	 * system-wide flags register. The BootMonitor waits
+	 * until it receives a soft interrupt, and then the
+	 * secondary CPU branches to this address.
+	 */
+
+
+	__raw_writel(BSYM(virt_to_phys(transcede_secondary_startup)) ,TRANSCEDE_IRAM_BASE);
+}
+
+/*
+ * control for which core is the next to come out of the secondary
+ * boot "holding pen"
+ */
+volatile int __cpuinitdata pen_release = -1;
+
+/*
+ * Write pen_release in a way that is guaranteed to be visible to all
+ * observers, irrespective of whether they're taking part in coherency
+ * or not.  This is necessary for the hotplug code to work reliably.
+ */
+static void __cpuinit write_pen_release(int val)
+{
+        pen_release = val;
+        smp_wmb();
+        __cpuc_flush_dcache_area((void *)&pen_release, sizeof(pen_release));
+        outer_clean_range(__pa(&pen_release), __pa(&pen_release + 1));
+}
+
+static DEFINE_RAW_SPINLOCK(boot_lock);
+
+void __cpuinit platform_secondary_init(unsigned int cpu)
+{
+        /*
+         * if any interrupts are already enabled for the primary
+         * core (e.g. timer irq), then they will not have been enabled
+         * for us: do so
+         */
+        gic_secondary_init(0);
+
+        /*
+         * let the primary processor know we're out of the
+         * pen, then head off into the C entry point
+         */
+        write_pen_release(-1);
+
+        /*
+         * Synchronise with the boot thread.
+         */
+        raw_spin_lock(&boot_lock);
+        raw_spin_unlock(&boot_lock);
+}
+
+int __cpuinit boot_secondary(unsigned int cpu, struct task_struct *idle)
+{
+        unsigned long timeout;
+
+
+        /*
+         * Set synchronisation state between this boot processor
+         * and the secondary one
+         */
+        raw_spin_lock(&boot_lock);
+
+        /*
+         * This is really belt and braces; we hold unintended secondary
+         * CPUs in the holding pen until we're ready for them.  However,
+         * since we haven't sent them a soft interrupt, they shouldn't
+         * be there.
+         */
+        write_pen_release(cpu);
+
+        /*
+         * Send the secondary CPU a soft interrupt, thereby causing
+         * the boot monitor to read the system wide flags register,
+         * and branch to the address found there.
+         */
+        timeout = jiffies + (1 * HZ);
+        while (time_before(jiffies, timeout)) {
+                smp_rmb();
+                if (pen_release == -1)
+                        break;
+
+                udelay(10);
+        }
+        /*
+         * now the secondary core is starting up let it run its
+         * calibrations, then wait for it to finish
+         */
+        raw_spin_unlock(&boot_lock);
+
+        return pen_release != -1 ? -ENOSYS : 0;
+}
diff --git a/arch/arm/mach-transcede/platsmp-t4000.c b/arch/arm/mach-transcede/platsmp-t4000.c
new file mode 100644
index 0000000..044a861
--- /dev/null
+++ b/arch/arm/mach-transcede/platsmp-t4000.c
@@ -0,0 +1,187 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#include <linux/delay.h>
+#include <linux/device.h>
+#include <linux/errno.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/io.h>
+#include <linux/smp.h>
+#include <linux/spinlock.h>
+
+#include <asm/cacheflush.h>
+#include <asm/irq_regs.h>
+#include <asm/smp_scu.h>
+#include <asm/unified.h>
+#include <asm/hardware/gic.h>
+#include <mach/hardware.h>
+
+#include "core.h"
+
+/* Required by L-ARM loader */
+volatile int   asmp_core_ready = -1;
+volatile void (*asmp_core_boot)(void) = NULL;
+volatile int   asmp_core_reset = 0;
+
+EXPORT_SYMBOL(asmp_core_ready);
+EXPORT_SYMBOL(asmp_core_boot);
+EXPORT_SYMBOL(asmp_core_reset);
+
+extern void transcede_secondary_startup(void);
+
+volatile int __cpuinitdata pen_release = -1;
+
+static DEFINE_RAW_SPINLOCK(boot_lock);
+
+struct ipi_data {
+	raw_spinlock_t lock;
+	unsigned int bits;
+};
+
+static DEFINE_PER_CPU(struct ipi_data, ipi_data) = {
+	.lock = __RAW_SPIN_LOCK_UNLOCKED(ipi_data.lock),
+};
+
+static void __iomem *scu_base_addr(void)
+{
+	return (void __iomem *) __io_address(TRANSCEDE_SCU_BASE);
+}
+
+static void transcede_smp_cross_call(const struct cpumask *mask, unsigned int ipinr)
+{
+	unsigned int cpu;
+
+	for_each_cpu(cpu, mask) {
+		struct ipi_data *ipi = &per_cpu(ipi_data, cpu);
+
+		raw_spin_lock(&ipi->lock);
+		ipi->bits |= 1 << ipinr;
+		raw_spin_unlock(&ipi->lock);
+	}
+
+	writel(*cpus_addr(*mask), TRANSCEDE_ARM_IRQ_SET);
+}
+
+extern void kernel_do_IPI(unsigned int cpu, int ipinr);
+
+irqreturn_t transcede_ipi_irq(int irq, void *dev)
+{
+	unsigned int cpu = smp_processor_id();
+	struct ipi_data *ipi = &per_cpu(ipi_data, cpu);
+	unsigned int bits, ipinr;
+
+	raw_spin_lock(&ipi->lock);
+	bits = ipi->bits;
+	ipi->bits = 0;
+	raw_spin_unlock(&ipi->lock);
+
+	while (bits) {
+		ipinr = bits & -bits;
+		bits &= ~ipinr;
+		ipinr = ffz(~ipinr);
+
+		kernel_do_IPI(cpu, ipinr);
+	}
+
+	writel(1 << cpu, TRANSCEDE_ARM_IRQ_CLR);
+
+	return IRQ_HANDLED;
+}
+
+static void __cpuinit write_pen_release(int val)
+{
+	pen_release = val;
+	smp_wmb();
+	__cpuc_flush_dcache_area((void *)&pen_release, sizeof(pen_release));
+	outer_clean_range(__pa(&pen_release), __pa(&pen_release + 1));
+}
+
+void __cpuinit platform_secondary_init(unsigned int cpu)
+{
+	gic_secondary_init(0);
+
+	write_pen_release(-1);
+	smp_wmb();
+
+	raw_spin_lock(&boot_lock);
+	raw_spin_unlock(&boot_lock);
+}
+
+int __cpuinit boot_secondary(unsigned int cpu, struct task_struct *idle)
+{
+	unsigned long timeout;
+
+	raw_spin_lock(&boot_lock);
+
+	write_pen_release(cpu);
+
+#ifdef CONFIG_TRANSCEDE_DUALCORE
+	transcede_smp_cross_call(cpumask_of(cpu), 3);
+#endif
+	timeout = jiffies + (1 * HZ);
+	while (time_before(jiffies, timeout)) {
+		smp_rmb();
+		if (pen_release == -1)
+			break;
+
+		udelay(10);
+	}
+
+	raw_spin_unlock(&boot_lock);
+
+	return pen_release != -1 ? -ENOSYS : 0;
+}
+
+void __init smp_init_cpus(void)
+{
+	unsigned int i, ncores = scu_get_core_count(scu_base_addr());
+
+	if (ncores > NR_CPUS)
+		ncores = NR_CPUS;
+
+	for (i = 0; i < ncores; i++)
+		set_cpu_possible(i, true);
+
+#ifdef CONFIG_TRANSCEDE_DUALCORE
+	set_smp_cross_call(transcede_smp_cross_call);
+#else
+	set_smp_cross_call(gic_raise_softirq);
+#endif
+}
+
+void __init platform_smp_prepare_cpus(unsigned int max_cpus)
+{
+	int i;
+
+	for (i = 0; i < max_cpus; i++)
+		set_cpu_present(i, true);
+
+	scu_enable(scu_base_addr());
+
+	/* nobody is to be released from the pen yet */
+	pen_release = -1;
+
+	writel(BSYM(virt_to_phys(transcede_secondary_startup)), IRAM_BASE_VADDR);
+
+	mb();
+}
diff --git a/arch/arm/mach-transcede/platsmp.c b/arch/arm/mach-transcede/platsmp.c
new file mode 100644
index 0000000..6765c17
--- /dev/null
+++ b/arch/arm/mach-transcede/platsmp.c
@@ -0,0 +1,32 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#if defined(CONFIG_MACH_M84XXX)
+
+#include "platsmp-t4000.c"
+
+#else
+
+#include "platsmp-t2200.c"
+
+#endif
+
diff --git a/arch/arm/mach-transcede/revision.c b/arch/arm/mach-transcede/revision.c
new file mode 100644
index 0000000..86d5215
--- /dev/null
+++ b/arch/arm/mach-transcede/revision.c
@@ -0,0 +1,76 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#include <linux/module.h>
+#include <linux/proc_fs.h>
+#include <asm/io.h>
+
+#include <mach/revision.h>
+
+static int transcede_revision = T2200_REV_UNDEF;
+
+inline int get_t2200_rev(void)
+{
+	return transcede_revision;
+}
+EXPORT_SYMBOL(get_t2200_rev);
+
+int __get_t2200_rev(void) {
+	int rev;
+	volatile unsigned int *pRom;
+
+	pRom = (volatile unsigned int *)ioremap(0xFF000000, 0x10000);
+
+	if (*(pRom + 0x20/4) == 0x8b32)
+		rev = T2200_REV_X2_1;
+	else if (*(pRom + 0xC0/4) == 0xFF003EFC)
+		rev = T2200_REV_X1;
+	else if (*(pRom + 0xC0/4) == 0xFF000158)
+		rev = T2200_REV_X2;
+	else
+		rev = T2200_REV_UNKNOWN;
+
+	iounmap(pRom);
+
+	return rev;
+}
+
+static int transcede_rev_read(char *buf, char **start, off_t offset, int count, int *eof, void *data) {
+	memcpy(buf, (const void *)&transcede_revision, sizeof(transcede_revision));
+
+	return sizeof (transcede_revision);
+}
+
+int revision_init(void)
+{
+	struct proc_dir_entry *entry;
+
+	// Get revision and store it
+	transcede_revision = __get_t2200_rev();
+
+	// Create procfs file for userspace
+	entry = create_proc_read_entry(T2200_REV_PROCFS_NAME, 0, NULL, transcede_rev_read, NULL);
+	if (!entry)
+		return -1;
+
+	return 0;
+}
diff --git a/arch/arm/mach-transcede/serdes-t2200.c b/arch/arm/mach-transcede/serdes-t2200.c
new file mode 100644
index 0000000..ee9b26a
--- /dev/null
+++ b/arch/arm/mach-transcede/serdes-t2200.c
@@ -0,0 +1,292 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#include <asm/uaccess.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <asm/types.h>
+#include <asm/io.h>
+#include <mach/transcede-2200.h>
+#include <linux/delay.h>
+#include <linux/sched.h>
+#include <asm/irq.h>
+#include <asm/delay.h>
+#include <asm/sizes.h>
+#include <mach/serdes-t2200.h>
+#include <mach/transcede-2200.h>
+
+
+#define HARD_RST_SERDES0        // do SerDes0 HARD reset before initialization
+#define HARD_RST_SERDES1        // do SerDes1 HARD reset before initialization
+
+
+/**
+ *   @brief  Set serdes rate settings for PCIe mode
+ *   @param  id - SERDES ID (0 - SERDES0 for PCIe0, 1 - SERDES1 for PCIe1 multisandard)
+ */
+static int serdes_pcie_rate_init(int id)
+{
+	unsigned long baseaddr, laneaddr, lanes, lane;
+
+	switch (id) {
+		case 0: {
+			baseaddr = TRANSCEDE_SERDES0;
+			laneaddr = TRANSCEDE_RAD_CFG + 0x044;
+			lanes = 4;
+		} break;
+
+		case 1: {
+			baseaddr = TRANSCEDE_SERDES1;
+			laneaddr = TRANSCEDE_RAD_CFG + 0x040;
+			lanes = 1;
+		} break;
+
+		default: {
+			return -1;
+		}
+	}
+
+	for (lane = 0; lane < lanes; lane++) {
+		writel((lane), (laneaddr));     // select lane 0 - 3
+
+		// original, from document
+		writel((0xB7), (baseaddr + (101<<2)));
+		writel((0xC), (baseaddr + (102<<2)));
+		writel((0xC), (baseaddr + (103<<2)));
+		writel((0x1A), (baseaddr + (104<<2)));
+		writel((0x1A), (baseaddr + (105<<2)));
+		writel((0x2), (baseaddr + (106<<2)));
+		writel((0x11), (baseaddr + (107<<2)));
+		writel((0xCF), (baseaddr + (108<<2)));
+
+		// proposed by Martin
+		writel((0x0), (baseaddr + (0x14<<2)));   //0x01
+		writel((0x0), (baseaddr + (0x13<<2)));   //0x6
+		writel((0x61), (baseaddr + (0x12<<2)));  //0xdf,0x69 0x29
+
+		writel((0), (baseaddr + (0x15<<2)));     //0
+		writel((0x1), (baseaddr + (0x19<<2)));
+		writel((0x0), (baseaddr + (0x16<<2)));
+		//writel((0x42), (baseaddr + (314<<2)));
+	}
+
+	writel((4), (laneaddr));            // select common lane
+	writel((0x99), (baseaddr + (111<<2)));
+	writel((0x0), (baseaddr + (112<<2)));
+	writel((0x76), (baseaddr + (113<<2))); // 52
+
+	// original
+	writel((0xB7), (baseaddr + (114<<2)));
+	writel((0xC), (baseaddr + (115<<2)));
+	writel((0xC), (baseaddr + (116<<2)));
+	writel((0x1A), (baseaddr + (117<<2)));
+	writel((0x1A), (baseaddr + (118<<2)));
+
+	writel((0x6), (baseaddr + (119<<2)));
+	writel((0x16), (baseaddr + (120<<2)));
+	writel((0x8), (baseaddr + (121<<2)));
+	writel((0x0), (baseaddr + (122<<2)));
+	writel((0x8), (baseaddr + (123<<2)));
+	writel((0x0), (baseaddr + (124<<2)));
+	writel((0xFF), (baseaddr + (125<<2)));
+	writel((0xB3), (baseaddr + (126<<2)));
+	writel((0xf6), (baseaddr + (127<<2)));
+	writel((0xD0), (baseaddr + (128<<2)));
+	writel((0xEf), (baseaddr + (129<<2)));
+	writel((0xFB), (baseaddr + (130<<2)));
+	writel((0xFF), (baseaddr + (131<<2)));
+	writel((0xFF), (baseaddr + (132<<2)));
+	writel((0xFF), (baseaddr + (133<<2)));
+	writel((0xFF), (baseaddr + (134<<2)));
+	writel((0xFF), (baseaddr + (135<<2)));
+	writel((0x17), (baseaddr + (136<<2))); //??
+	writel((0xd5), (baseaddr + (137<<2))); //??
+	writel((0xE2), (baseaddr + (138<<2)));
+	writel((0xEF), (baseaddr + (139<<2)));
+	writel((0xFB), (baseaddr + (140<<2)));
+	writel((0xFB), (baseaddr + (141<<2)));
+	writel((0xFF), (baseaddr + (142<<2)));
+	writel((0xEF), (baseaddr + (143<<2)));
+	writel((0xFF), (baseaddr + (144<<2)));
+	writel((0xFF), (baseaddr + (145<<2)));
+	writel((0x17), (baseaddr + (146<<2)));
+	writel((0xD5), (baseaddr + (147<<2)));
+	writel((0xE2), (baseaddr + (148<<2)));
+	writel((0xEF), (baseaddr + (149<<2)));
+	writel((0xFB), (baseaddr + (150<<2)));
+	writel((0xFB), (baseaddr + (151<<2)));
+	writel((0xFF), (baseaddr + (152<<2)));
+	writel((0xEF), (baseaddr + (153<<2)));
+	writel((0xFF), (baseaddr + (154<<2)));
+	writel((0xFF), (baseaddr + (155<<2)));
+	writel((0xFB), (baseaddr + (156<<2)));
+	writel((0xFF), (baseaddr + (157<<2)));
+	writel((0x3F), (baseaddr + (158<<2)));
+	writel((0x0), (baseaddr + (159<<2)));
+	writel((0x32), (baseaddr + (160<<2)));
+	writel((0x0), (baseaddr + (161<<2)));
+	writel(0x5, (baseaddr + (162<<2)));
+	writel((0x5), (baseaddr + (163<<2)));
+	writel((0x4), (baseaddr + (164<<2)));
+	writel((0x0), (baseaddr + (165<<2)));
+	writel((0x0), (baseaddr + (166<<2)));
+	writel((0x8), (baseaddr + (167<<2)));
+	writel((0x4), (baseaddr + (168<<2)));
+	writel((0x0), (baseaddr + (169<<2)));
+	writel((0x0), (baseaddr + (170<<2)));
+	writel((0x4), (baseaddr + (171<<2)));
+	writel((0x4), (baseaddr + (172<<2)));
+
+	return 0;
+}
+
+
+/**
+ *   @brief  SERDES fix for dynamic rate change process (that takes place in case of switching to 5Gbps mode for PCIe g.2)
+ *   @param  id - SERDES ID (0 - SERDES0 for PCIe0, 1 - SERDES1 for PCIe1 multisandard)
+ */
+static int serdes_pcie_fix(int id)
+{
+	unsigned int r1,r2;
+	unsigned long baseaddr, laneaddr;
+
+	switch (id) {
+		case 0: {
+			baseaddr = TRANSCEDE_SERDES0;
+			laneaddr = TRANSCEDE_RAD_CFG + 0x044;
+		} break;
+
+		case 1: {
+			baseaddr = TRANSCEDE_SERDES0;
+			laneaddr = TRANSCEDE_RAD_CFG + 0x040;
+		} break;
+
+		default: {
+			return -1;
+		}
+	}
+
+	// 14) Setup overrides for signals
+	writel(0, laneaddr);
+	r1 = readl(baseaddr +(211<<2));
+	r2 = readl(baseaddr +(216<<2));
+
+	writel(7, laneaddr);
+	writel(r1 & (~(1 << 7)), baseaddr + (211<<2));           //211[7:7] RSTPDOVR_RX_AETRREGRX_DROPLEV                   7  0
+	//212[0:0] RSTPDOVR_RX_AETRRXPHD_MUTE                      7  0
+	//212[1:1] RSTPDOVR_RX_AETRRX_CDRFBDIVCKEN                 7  1
+	//212[2:2] RSTPDOVR_RX_AETRRX_TERMHIZ_EN                   7  1
+	//212[3:3] RSTPDOVR_RX_APDREGRX                            7  1
+	//212[4:4] RSTPDOVR_RX_APDRXIASMIRROR                      7  1
+	//212[5:5] RSTPDOVR_RX_APDRX_DFE                           7  1
+	//212[6:6] RSTPDOVR_RX_APDRX_HIFREQAGC                     7  1
+	writel(0xfe, baseaddr + (212<<2));                        //212[7:7] RSTPDOVR_RX_APDRX_LOSDET                        7  1
+	//213[0:0] RSTPDOVR_RX_APDRX_ROAMCOMP                      7  1
+	//213[1:1] RSTPDOVR_RX_APDRX_S2PA                          7  1
+	//213[2:2] RSTPDOVR_RX_APDRX_S2PB                          7  1
+	//213[3:3] RSTPDOVR_RX_APDRX_SIGDET                        7  1
+	//213[4:4] RSTPDOVR_RX_APDRX_VCO                           7  1
+	//213[5:5] RSTPDOVR_RX_ARSTRX_CDRFBDIV                     7  1
+	//213[6:6] RSTPDOVR_RX_ARSTRX_PDET                         7  1
+	writel(0xff, baseaddr + (213<<2));                        //213[7:7] RSTPDOVR_RX_ARSTRX_PFD                          7  1
+	//214[0:0] RSTPDOVR_RX_ARSTRX_REFDIV             //          7  1
+	//214[1:1] RSTPDOVR_RX_ARSTRX_S2PA                         7  1
+	//214[2:2] RSTPDOVR_RX_ARSTRX_S2PB                         7  1
+	//214[3:3] RSTPDOVR_RX_ARSTRX_VCO                          7  1
+	//214[4:4] RSTPDOVR_RX_CLKGENMUXSELPCSRXWORD_KEEPALIVESEL  7  1
+	//214[5:5] RSTPDOVR_RX_CLKGENMUXSELREFRX_KEEPALIVESEL      7  1
+	//214[6:6] RSTPDOVR_RX_DRSTRX_CALDUTYFSM                   7  1
+	writel(0xff, baseaddr + (214<<2));                        //214[7:7] RSTPDOVR_RX_DRSTRX_CALDUTYMEASCYCLE             7  1
+	//215[0:0] RSTPDOVR_RX_DRSTRX_CALFOSCFSM                   7  1
+	//215[1:1] RSTPDOVR_RX_DRSTRX_CALFOSCMEASCYCLE             7  1
+	//215[2:2] RSTPDOVR_RX_DRSTRX_CALOFFSETFSM                 7  1
+	//215[3:3] RSTPDOVR_RX_DRSTRX_CALOFFSETMEASCYCLE           7  1
+	//215[4:4] RSTPDOVR_RX_DRSTRX_CALROAMDELAYFSM              7  1
+	//215[5:5] RSTPDOVR_RX_DRSTRX_CALROAMDELAYMEASCYCLE        7  1
+	//215[6:6] RSTPDOVR_RX_DRSTRX_CALROAMEYEMEASCYCLE          7  1
+	writel(0xff, baseaddr + (215<<2));                        //215[7:7] RSTPDOVR_RX_DRSTRX_DATAPATHA                    7  1
+	//216[0:0] RSTPDOVR_RX_DRSTRX_DATAPATHB                    7  1
+	//216[1:1] RSTPDOVR_RX_DRSTRX_DPIF                         7  1
+	//216[2:2] RSTPDOVR_RX_DRSTRX_EYEDIAGFSM                   7  1
+	//216[3:3] RSTPDOVR_RX_DRSTRX_FBDIVSLIP                    7  1
+	//216[4:4] RSTPDOVR_RX_DRSTRX_PPM                          7  1
+	writel(r2 | 0x3f, baseaddr + (216<<2));                   //216[5:5] RSTPDOVR_RX_DRSTRX_SIGDET                       7  1
+
+	//15) Enable overrides for signals
+	writel(0, laneaddr);
+	r1 = readl(baseaddr + (216<<2));
+	writel(7, laneaddr);
+	writel(r1 | (1 << 6), baseaddr + (216<<2));              //216[6:6] RSTPDOVR_RX_OVREN                               7  1
+
+	udelay(5);
+}
+
+/**
+ *   @brief  Initialize SERDES for 2500/5000 PCIE rate
+ *   @param  id - SERDES ID (0 - SERDES0 for PCIe0, 1 - SERDES1 for PCIe1 multisandard)
+ */
+int serdes_pcie_phy_init(int id)
+{
+	int rc;
+
+	switch (id) {
+		case 0: {
+			writel(0x000, (TRANSCEDE_RAD_CFG + 0x0c0));     // clock selection - default
+			writel(0x000, (TRANSCEDE_RAD_CFG + 0x220));     // set PIPE RST
+
+#ifdef HARD_RST_SERDES0
+			writel(0x000, (TRANSCEDE_RAD_CFG + 0x008));     // set POR
+			mdelay(5);
+#endif
+
+			writel(0x001, (TRANSCEDE_RAD_CFG + 0x008));     // remove POR
+			mdelay(5);
+			rc = serdes_pcie_rate_init(id);
+			writel(0x00f, (TRANSCEDE_RAD_CFG + 0x220));     // remove PIPE RST
+			mdelay(5);
+			writel(0x100, (TRANSCEDE_RAD_CFG + 0x0c0));
+		} break;
+
+		case 1: {
+			writel(0x222, TRANSCEDE_RAD_CFG  +  0x98);
+
+#ifdef HARD_RST_SERDES1
+			writel(0x000, (TRANSCEDE_RAD_CFG + 0x004));     // set POR + PIPE RST
+			mdelay(5);
+#endif
+
+			writel(0x001, (TRANSCEDE_RAD_CFG + 0x004));     // remove POR
+			mdelay(5);
+			rc = serdes_pcie_rate_init(id);
+			writel(0x00F, (TRANSCEDE_RAD_CFG + 0x004));     // remove PIPE RST
+			mdelay(5);
+		} break;
+
+		default:
+			rc = -1;
+	}
+
+	return rc;
+}
+
+EXPORT_SYMBOL(serdes_pcie_phy_init);
diff --git a/arch/arm/mach-transcede/sysheap.c b/arch/arm/mach-transcede/sysheap.c
new file mode 100644
index 0000000..b501a29
--- /dev/null
+++ b/arch/arm/mach-transcede/sysheap.c
@@ -0,0 +1,689 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+  *
+  * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+  *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+  *
+  * You should have received a copy of the GNU General Public License
+  * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+  */
+
+#include <linux/init.h>
+#include <linux/errno.h>
+#include <linux/smp.h>
+#include <linux/io.h>
+#include <linux/delay.h>
+#include <linux/jiffies.h>
+#include <linux/module.h>
+
+#include <asm/cacheflush.h>
+#include <mach/hardware.h>
+#include <mach/sysheap.h>
+#include <asm/hardware/gic.h>
+#include <asm/mach-types.h>
+#include <asm/smp_scu.h>
+#include <asm/unified.h>
+
+typedef unsigned long (*addr_convert)(unsigned long);
+
+static unsigned SysCreateHeap(unsigned pStorage, unsigned HeapSize, unsigned HeapAlign, void* SpinLock, addr_convert kheap_phys_to_virt, addr_convert kheap_virt_to_phys);
+static unsigned SysDestroyHeap(unsigned hHeap);
+static void* SysHeapAlloc(unsigned hHeap, unsigned nSize, addr_convert kheap_phys_to_virt, addr_convert kheap_virt_to_phys);
+static unsigned SysHeapFree(unsigned hHeap, void* pData, addr_convert kheap_phys_to_virt, addr_convert kheap_virt_to_phys);
+
+
+
+#define ALLOC_HEADER_SIZE (sizeof(unsigned))
+
+static unsigned long sys_get_lock(volatile unsigned int *sema_ptr)
+{
+    unsigned long irq = 0;
+
+    local_irq_save(irq);
+    barrier();
+    while (*sema_ptr)
+        asm("nop");
+
+    return irq;
+}
+
+static void sys_release_lock(volatile unsigned int *sema_ptr, unsigned long irq)
+{
+    *sema_ptr = 0;
+    barrier();
+    local_irq_restore(irq);
+}
+
+#define ROUND(x, dx)            (((x) + ((dx) - 1) ) & ~((dx) - 1))
+
+static void *convert_addr(void *addr, addr_convert func)
+{
+	if (addr) {
+		return (void *)func((unsigned long)addr);
+	} else {
+		return NULL; // addr == 0
+	}
+}
+
+
+/** @brief The function creates a heap.
+
+ @param pStorage           [in] - the pointer to memory (heap storage)
+ @param nSize              [in] - the storage size in bytes
+ @param nAlign             [in] - the alignment in bytes
+ @param kheap_phys_to_virt [in] - pointer to funtion to convert physical address to virtual
+ @param kheap_virt_to_phys [in] - pointer to funtion to convert virtual address to physical
+
+ @return [u32] error code
+*/
+static unsigned SysCreateHeap(unsigned pStorage, unsigned HeapSize, unsigned HeapAlign, void* SpinLock, addr_convert kheap_phys_to_virt, addr_convert kheap_virt_to_phys)
+{
+    PHEAPDESC pHeap = (PHEAPDESC)pStorage;
+    unsigned RealSize, HeapAddr, HeapDescSize;
+//    printk(KERN_ERR "SysCreateHeap(unsigned pStorage(0x%x), unsigned HeapSize(0x%x), unsigned HeapAlign(0x%x), unsigned SpinLockId(0x%x))\n", pStorage, HeapSize, HeapAlign, SpinLockId);
+
+    // Heap alignment should be divisible by 4 bytes
+    // 0, 4, 8, 12, 16 ...
+    HeapAlign = ROUND(HeapAlign, 4);
+
+    // Address of heap buffer should be alignment on HeapAlign size
+    HeapAddr = ROUND((unsigned)pStorage, HeapAlign);
+
+    // The real size of heap buffer can be less than
+    // nSize because of address alignment
+    RealSize = HeapSize - (HeapAddr - (unsigned)pStorage);
+
+#ifdef HEAP_PATTERN_INIT
+    memset((void*)HeapAddr, HEAP_PATTERN_INIT, RealSize);
+#endif
+    HeapDescSize = ROUND(sizeof(*pHeap), HeapAlign);
+
+    pHeap->HeapStorage = (void*)kheap_virt_to_phys(HeapAddr);
+    pHeap->FreeBlock   = (PMEMBLOCK)(kheap_virt_to_phys(HeapAddr) + HeapDescSize);
+    pHeap->FreeSize    = RealSize;
+    pHeap->Align       = HeapAlign;
+    pHeap->SpinLock    = SpinLock; // use as a pointer to system semaphore
+
+    ((PMEMBLOCK)kheap_phys_to_virt((unsigned long)pHeap->FreeBlock))->NextFree = 0;
+    ((PMEMBLOCK)kheap_phys_to_virt((unsigned long)pHeap->FreeBlock))->Size     = RealSize;
+
+    return (unsigned)pHeap;
+}
+
+/** @brief The function removes created heap.
+
+ @param hHeap [in] - the heap handle
+
+ @return [2] error code
+*/
+static unsigned SysDestroyHeap(unsigned hHeap)
+{
+    return 0;
+}
+
+/** @brief The function allocates a block of memory of reqired size and returns a pointer to
+   the allocated memory or 0. The size is alignment and the memory block can be bigger that required.
+
+ @param hHeap              [in] - the heap handle
+ @param nSize              [in] - the size of block in bytes
+ @param kheap_phys_to_virt [in] - pointer to funtion to convert physical address to virtual
+ @param kheap_virt_to_phys [in] - pointer to funtion to convert virtual address to physical
+
+ @return [LPVOID] a pointer to allocated block or 0
+ */
+static void* SysHeapAlloc(unsigned hHeap, unsigned nSize, addr_convert kheap_phys_to_virt, addr_convert kheap_virt_to_phys)
+{
+    unsigned dx;
+    void* pMemBlk = 0;
+    PHEAPDESC pHeap = (PHEAPDESC)hHeap;
+    PMEMBLOCK pBlock, pPrev, pNewFreeBlock;
+    unsigned oldirq = 0;
+
+    // The allocated block has a header (an allocated size)
+    // the pointer to allocated memory will be immediately after
+    // block header with specified heap alignment
+
+    nSize += ROUND(ALLOC_HEADER_SIZE, pHeap->Align);
+
+    // The total size should be alignment as well
+    nSize = ROUND(nSize, pHeap->Align);
+
+    if (pHeap->FreeSize < nSize)
+    {
+        return NULL;
+    }
+
+    if (pHeap->SpinLock)
+        oldirq = sys_get_lock(pHeap->SpinLock);
+
+    pBlock = convert_addr(pHeap->FreeBlock, kheap_phys_to_virt);
+    pPrev = 0;
+
+    while (pBlock != 0)
+    {
+        if (pBlock->Size >= nSize)
+        {
+            dx = (pBlock->Size - nSize);
+
+            if (dx < ROUND(sizeof(MEMBLOCK), pHeap->Align))
+            {
+                pNewFreeBlock = convert_addr(pBlock->NextFree, kheap_phys_to_virt);
+
+                pHeap->FreeSize -= pBlock->Size;
+
+                nSize = pBlock->Size;
+            }
+            else
+            {
+                // The size is aligned with heap's alingment
+                // so this operation moves pointer to the next aligned block
+
+                pNewFreeBlock = (PMEMBLOCK)((unsigned char *) pBlock + nSize);
+                pNewFreeBlock->NextFree = pBlock->NextFree;
+                pNewFreeBlock->Size = pBlock->Size - nSize;
+
+                pHeap->FreeSize -= nSize;
+            }
+
+            if (pHeap->FreeBlock == (void *)kheap_virt_to_phys((unsigned long)pBlock))
+            {
+                pHeap->FreeBlock = convert_addr(pNewFreeBlock, kheap_virt_to_phys);
+            }
+            else if (pPrev != NULL)
+            {
+                pPrev->NextFree = convert_addr(pNewFreeBlock, kheap_virt_to_phys);
+            }
+
+            // store allocated size (user size + size of allocated
+            // block header (4 bytes))
+            pBlock->Size = nSize;
+
+            // return pointer to allocated memory
+            pMemBlk = (unsigned char*)pBlock + ROUND(ALLOC_HEADER_SIZE, pHeap->Align);
+            break;
+        }
+
+        pPrev = pBlock;
+        pBlock = convert_addr(pBlock->NextFree, kheap_phys_to_virt);
+    }
+
+    if (pHeap->SpinLock)
+        sys_release_lock(pHeap->SpinLock, oldirq);
+
+    return pMemBlk;
+}
+
+/** @brief The function frees allocated block of memory.
+
+ @param hHeap              [in] - the heap handle
+ @param pData              [in] - the pointer to allocated block of memory
+ @param kheap_phys_to_virt [in] - pointer to funtion to convert physical address to virtual
+ @param kheap_virt_to_phys [in] - pointer to funtion to convert virtual address to physical
+
+ @return [u32] error code
+ */
+static unsigned SysHeapFree(unsigned hHeap, void* pData, addr_convert kheap_phys_to_virt, addr_convert kheap_virt_to_phys)
+{
+    PHEAPDESC pHeap = (PHEAPDESC)hHeap;
+    PMEMBLOCK pBlock, pTmp, pBeginPack = 0;
+    unsigned Num;
+    unsigned oldirq = 0;
+
+    pBlock = (PMEMBLOCK)((unsigned char*)pData - ROUND(ALLOC_HEADER_SIZE, pHeap->Align));
+
+    if (pHeap->SpinLock)
+        oldirq = sys_get_lock(pHeap->SpinLock);
+
+    pHeap->FreeSize += pBlock->Size;
+
+    if ((unsigned)pBlock < kheap_phys_to_virt((unsigned)pHeap->FreeBlock) || pHeap->FreeBlock == 0)
+    {
+        pBlock->NextFree = pHeap->FreeBlock;
+        pHeap->FreeBlock = (void *)kheap_virt_to_phys((unsigned long)pBlock);
+
+        pBeginPack = pBlock;
+    }
+    else
+    {
+        /* looking for the best place in chain of free blocks */
+
+        pTmp = convert_addr(pHeap->FreeBlock, kheap_phys_to_virt);
+
+        while (pTmp)
+        {
+	        if ((unsigned)pTmp < (unsigned)pBlock
+	            && ((unsigned)convert_addr(pTmp->NextFree, kheap_phys_to_virt) > (unsigned)pBlock
+	                || pTmp->NextFree == 0))
+            {
+                pBeginPack = pTmp;
+
+                pBlock->NextFree = pTmp->NextFree;
+                pTmp->NextFree = convert_addr(pBlock, kheap_virt_to_phys);
+
+                break;
+            }
+
+            pTmp = convert_addr(pTmp->NextFree, kheap_phys_to_virt);
+        }
+    }
+
+    // Packing adjacent blocks
+
+    Num = 0;
+
+    // The number of iteration less than 3 times!
+
+    while (pBeginPack)
+    {
+        while (((unsigned)pBeginPack + pBeginPack->Size) == (unsigned)convert_addr(pBeginPack->NextFree, kheap_phys_to_virt))
+        {
+            pBeginPack->Size += ((struct tMEMBLOCK*)kheap_phys_to_virt((unsigned)pBeginPack->NextFree))->Size;
+            pBeginPack->NextFree = ((struct tMEMBLOCK*)kheap_phys_to_virt((unsigned)pBeginPack->NextFree))->NextFree;
+        }
+
+        pBeginPack = (++Num < 2) ? convert_addr(pBeginPack->NextFree, kheap_phys_to_virt) : 0;
+    }
+
+    if (pHeap->SpinLock)
+        sys_release_lock(pHeap->SpinLock, oldirq);
+
+    return 0;
+}
+
+/** @brief The function is bypass address convertion for 1 to 1 mappings like CRAM, IRAM
+
+ @param addr [in] - the pointer to memory we need to convert
+
+ @return [u32] converted address (same as input)
+*/
+unsigned long kheap_null_address_convert(unsigned long addr) {
+	return addr;
+}
+EXPORT_SYMBOL(kheap_null_address_convert);
+
+#ifdef CONFIG_MACH_M822XX
+/** @brief The function creates a heap in CRAM.
+
+ @param pStorage [in] - the pointer to memory (heap storage)
+ @param nSize    [in] - the storage size in bytes
+ @param nAlign   [in] - the alignment in bytes
+
+ @return [u32] error code
+*/
+static unsigned cram_storage = 0;
+unsigned long cram_create_heap(unsigned pStorage, unsigned HeapSize, unsigned HeapAlign, unsigned SpinLockId)
+{
+	return cram_storage = SysCreateHeap(pStorage, HeapSize, HeapAlign, (void*)TRANSCEDE_SEMAARMCTRL(SpinLockId), kheap_null_address_convert, kheap_null_address_convert);
+}
+EXPORT_SYMBOL(cram_create_heap);
+
+/** @brief The function allocates a block of memory of reqired size in CRAM and returns a pointer to
+   the allocated memory or 0. The size is alignment and the memory block can be bigger that required.
+
+ @param nSize [in] - the size of block in bytes
+
+ @return [LPVOID] a pointer to allocated block or 0
+ */
+void* cram_heap_alloc(unsigned nSize)
+{
+	BUG_ON(!cram_storage);
+
+	return SysHeapAlloc(cram_storage, nSize, kheap_null_address_convert, kheap_null_address_convert);
+}
+EXPORT_SYMBOL(cram_heap_alloc);
+
+
+/** @brief The function frees allocated block of memory in CRAM.
+
+ @param pData [in] - the pointer to allocated block of memory
+
+ @return [u32] error code
+ */
+unsigned cram_heap_free(void* pData)
+{
+	BUG_ON(!cram_storage);
+
+	return SysHeapFree(cram_storage, pData, kheap_null_address_convert, kheap_null_address_convert);
+}
+EXPORT_SYMBOL(cram_heap_free);
+
+/** @brief The function removes created heap in CRAM.
+
+ @return [2] error code
+*/
+unsigned cram_destroy_heap(void)
+{
+    return SysDestroyHeap(cram_storage);
+}
+EXPORT_SYMBOL(cram_destroy_heap);
+
+/** @brief The function creates a heap in IRAM.
+
+ @param pStorage [in] - the pointer to memory (heap storage)
+ @param nSize    [in] - the storage size in bytes
+ @param nAlign   [in] - the alignment in bytes
+
+ @return [u32] error code
+*/
+static unsigned iram_storage = 0;
+unsigned long iram_create_heap(unsigned pStorage, unsigned HeapSize, unsigned HeapAlign, unsigned SpinLockId)
+{
+	return iram_storage = SysCreateHeap(pStorage, HeapSize, HeapAlign, (void*)TRANSCEDE_SEMAARMCTRL(SpinLockId), kheap_null_address_convert, kheap_null_address_convert);
+}
+EXPORT_SYMBOL(iram_create_heap);
+
+/** @brief The function allocates a block of memory of reqired size in IRAM and returns a pointer to
+   the allocated memory or 0. The size is alignment and the memory block can be bigger that required.
+
+ @param nSize [in] - the size of block in bytes
+
+ @return [LPVOID] a pointer to allocated block or 0
+ */
+void* iram_heap_alloc(unsigned nSize)
+{
+	BUG_ON(!iram_storage);
+
+	return SysHeapAlloc(iram_storage, nSize, kheap_null_address_convert, kheap_null_address_convert);
+}
+EXPORT_SYMBOL(iram_heap_alloc);
+
+/** @brief The function frees allocated block of memory in IRAM.
+
+ @param pData [in] - the pointer to allocated block of memory
+
+ @return [u32] error code
+ */
+unsigned iram_heap_free(void* pData)
+{
+	BUG_ON(!iram_storage);
+
+	return SysHeapFree(iram_storage, pData, kheap_null_address_convert, kheap_null_address_convert);
+}
+EXPORT_SYMBOL(iram_heap_free);
+
+/** @brief The function removes created heap in IRAM.
+
+ @return [2] error code
+*/
+unsigned iram_destroy_heap(void)
+{
+    return SysDestroyHeap(iram_storage);
+}
+EXPORT_SYMBOL(iram_destroy_heap);
+
+/** @brief The function creates a heap in DDR.
+
+ @param pStorage [in] - the pointer to memory (heap storage)
+ @param nSize    [in] - the storage size in bytes
+ @param nAlign   [in] - the alignment in bytes
+
+ @return [u32] error code
+*/
+static unsigned ddr_heap_virt_base = 0;
+static unsigned ddr_storage = 0;
+unsigned long ddr_create_heap(unsigned pStorage, unsigned HeapSize, unsigned HeapAlign, unsigned SpinLockId)
+{
+	ddr_heap_virt_base = pStorage;
+	return ddr_storage = SysCreateHeap(pStorage, HeapSize, HeapAlign, (void*)TRANSCEDE_SEMAARMCTRL(SpinLockId), ddr_heap_phys_to_virt, ddr_heap_virt_to_phys);
+}
+EXPORT_SYMBOL(ddr_create_heap);
+
+unsigned long ddr_heap_virt_to_phys(unsigned long addr)
+{
+	return (addr - ddr_heap_virt_base + TRANSCEDE_DDRHEAP_BASE);
+}
+EXPORT_SYMBOL(ddr_heap_virt_to_phys);
+unsigned long ddr_heap_phys_to_virt(unsigned long addr)
+{
+	return (addr - TRANSCEDE_DDRHEAP_BASE + ddr_heap_virt_base);
+}
+EXPORT_SYMBOL(ddr_heap_phys_to_virt);
+
+
+/** @brief The function allocates a block of memory of reqired size in DDR and returns a pointer to
+   the allocated memory or 0. The size is alignment and the memory block can be bigger that required.
+
+ @param nSize [in] - the size of block in bytes
+
+ @return [LPVOID] a pointer to allocated block or 0
+ */
+void* ddr_heap_alloc(unsigned nSize)
+{
+	BUG_ON(!ddr_storage);
+
+	return SysHeapAlloc(ddr_storage, nSize, ddr_heap_phys_to_virt, ddr_heap_virt_to_phys);
+}
+EXPORT_SYMBOL(ddr_heap_alloc);
+
+
+/** @brief The function frees allocated block of memory in DDR.
+
+ @param pData [in] - the pointer to allocated block of memory
+
+ @return [u32] error code
+ */
+unsigned ddr_heap_free(void* pData)
+{
+	BUG_ON(!ddr_storage);
+
+	return SysHeapFree(ddr_storage, pData, ddr_heap_phys_to_virt, ddr_heap_virt_to_phys);
+}
+EXPORT_SYMBOL(ddr_heap_free);
+
+/** @brief The function removes created heap in DDR.
+
+ @return [2] error code
+*/
+unsigned ddr_destroy_heap(void)
+{
+    return SysDestroyHeap(ddr_storage);
+}
+EXPORT_SYMBOL(ddr_destroy_heap);
+
+/** @brief The function creates a heap in DDR (cache-able via kheap only).
+
+ @param pStorage [in] - the pointer to memory (heap storage)
+ @param nSize    [in] - the storage size in bytes
+ @param nAlign   [in] - the alignment in bytes
+
+ @return [u32] error code
+*/
+
+static unsigned ddrcb_heap_virt_base = 0;
+static unsigned ddrcb_storage = 0;
+
+unsigned long ddrcb_create_heap(unsigned pStorage, unsigned HeapSize,
+                           unsigned HeapAlign, unsigned SpinLockId)
+{
+	ddrcb_heap_virt_base = pStorage;
+	return ddrcb_storage = SysCreateHeap(pStorage, HeapSize, HeapAlign, (void*)TRANSCEDE_SEMAARMCTRL(SpinLockId), ddrcb_heap_phys_to_virt, ddrcb_heap_virt_to_phys);
+}
+EXPORT_SYMBOL(ddrcb_create_heap);
+
+unsigned long ddrcb_heap_virt_to_phys(unsigned long addr)
+{
+	return (addr - ddrcb_heap_virt_base + TRANSCEDE_DDRCBHEAP_BASE);
+}
+EXPORT_SYMBOL(ddrcb_heap_virt_to_phys);
+
+unsigned long ddrcb_heap_phys_to_virt(unsigned long addr)
+{
+	return (addr - TRANSCEDE_DDRCBHEAP_BASE + ddrcb_heap_virt_base);
+}
+EXPORT_SYMBOL(ddrcb_heap_phys_to_virt);
+
+
+/** @brief The function allocates a block of memory of reqired size in
+   DDR and returns a pointer to the allocated memory or 0. The size is
+   alignment and the memory block can be bigger that required.
+
+ @param nSize [in] - the size of block in bytes
+
+ @return [LPVOID] a pointer to allocated block or 0
+ */
+void* ddrcb_heap_alloc(unsigned nSize)
+{
+	BUG_ON(!ddrcb_storage);
+
+	return SysHeapAlloc(ddrcb_storage, nSize, ddrcb_heap_phys_to_virt, ddrcb_heap_virt_to_phys);
+}
+EXPORT_SYMBOL(ddrcb_heap_alloc);
+
+
+/** @brief The function frees allocated block of memory in DDR.
+
+ @param pData [in] - the pointer to allocated block of memory
+
+ @return [u32] error code
+ */
+unsigned ddrcb_heap_free(void* pData)
+{
+	BUG_ON(!ddrcb_storage);
+
+	return SysHeapFree(ddrcb_storage, pData, ddrcb_heap_phys_to_virt, ddrcb_heap_virt_to_phys);
+}
+EXPORT_SYMBOL(ddrcb_heap_free);
+
+/** @brief The function removes created heap in DDR.
+
+ @return [2] error code
+*/
+unsigned ddrcb_destroy_heap(void)
+{
+    return SysDestroyHeap(ddrcb_storage);
+}
+EXPORT_SYMBOL(ddrcb_destroy_heap);
+
+
+/** @brief The function creates a heap in ICC.
+
+    @param pStorage [in] - the pointer to memory (heap storage)
+    @param nSize    [in] - the storage size in bytes
+    @param nAlign   [in] - the alignment in bytes
+
+    @return [u32] error code
+*/
+static unsigned icc_storage = 0;
+unsigned long icc_create_heap(unsigned pStorage, unsigned HeapSize,
+                         unsigned HeapAlign, unsigned SpinLockId)
+{
+	return icc_storage = SysCreateHeap(pStorage, HeapSize, HeapAlign, (void*)TRANSCEDE_SEMAARMCTRL(SpinLockId), kheap_null_address_convert, kheap_null_address_convert);
+}
+EXPORT_SYMBOL(icc_create_heap);
+
+/** @brief The function allocates a block of memory of reqired size in
+    ICC and returns a pointer to the allocated memory or 0. The size is
+    alignment and the memory block can be bigger that required.
+
+    @param nSize [in] - the size of block in bytes
+
+    @return [LPVOID] a pointer to allocated block or 0
+*/
+void* icc_heap_alloc(unsigned nSize)
+{
+	BUG_ON(!icc_storage);
+
+	return SysHeapAlloc(icc_storage, nSize, kheap_null_address_convert, kheap_null_address_convert);
+}
+EXPORT_SYMBOL(icc_heap_alloc);
+
+
+/** @brief The function frees allocated block of memory in ICC.
+
+    @param pData [in] - the pointer to allocated block of memory
+
+    @return [u32] error code
+*/
+void icc_heap_free(const void* pData)
+{
+	BUG_ON(!icc_storage);
+
+	SysHeapFree(icc_storage, (void *)pData, kheap_null_address_convert, kheap_null_address_convert);
+}
+EXPORT_SYMBOL(icc_heap_free);
+
+/** @brief The function removes created heap in ICC.
+
+    @return [2] error code
+*/
+unsigned icc_destroy_heap(void)
+{
+	return SysDestroyHeap(icc_storage);
+}
+EXPORT_SYMBOL(icc_destroy_heap);
+
+/** @brief The function creates a heap in AMP.
+
+    @param pStorage [in] - the pointer to memory (heap storage)
+    @param nSize    [in] - the storage size in bytes
+    @param nAlign   [in] - the alignment in bytes
+
+    @return [u32] error code
+*/
+static unsigned amp_storage[NR_CPUS];
+unsigned long amp_create_heap(unsigned cpuid,
+                              unsigned pStorage, unsigned HeapSize,
+                              unsigned HeapAlign, unsigned SpinLockId)
+{
+	return amp_storage[cpuid] = SysCreateHeap(pStorage, HeapSize, HeapAlign, (void*)TRANSCEDE_SEMAARMCTRL(SpinLockId), kheap_null_address_convert, kheap_null_address_convert);
+}
+EXPORT_SYMBOL(amp_create_heap);
+
+/** @brief The function allocates a block of memory of reqired size in
+    AMP and returns a pointer to the allocated memory or 0. The size is
+    alignment and the memory block can be bigger that required.
+
+    @param nSize [in] - the size of block in bytes
+
+    @return [LPVOID] a pointer to allocated block or 0
+*/
+void* amp_heap_alloc(unsigned nSize)
+{
+	unsigned cpuid = smp_processor_id();
+
+	BUG_ON(!amp_storage[cpuid]);
+
+	return SysHeapAlloc(amp_storage[cpuid], nSize, kheap_null_address_convert, kheap_null_address_convert);
+}
+EXPORT_SYMBOL(amp_heap_alloc);
+
+
+/** @brief The function frees allocated block of memory in AMP.
+
+    @param pData [in] - the pointer to allocated block of memory
+
+    @return [u32] error code
+*/
+void amp_heap_free(const void* pData)
+{
+	unsigned cpuid = smp_processor_id();
+
+	BUG_ON(!amp_storage[cpuid]);
+
+	SysHeapFree(amp_storage[cpuid], (void *)pData, kheap_null_address_convert, kheap_null_address_convert);
+}
+EXPORT_SYMBOL(amp_heap_free);
+
+/** @brief The function removes created heap in AMP.
+
+    @return [2] error code
+*/
+unsigned amp_destroy_heap(unsigned cpuid)
+{
+	return SysDestroyHeap(amp_storage[cpuid]);
+}
+EXPORT_SYMBOL(amp_destroy_heap);
+
+#endif
diff --git a/arch/arm/mach-transcede/syslib.c b/arch/arm/mach-transcede/syslib.c
new file mode 100644
index 0000000..a5fc485
--- /dev/null
+++ b/arch/arm/mach-transcede/syslib.c
@@ -0,0 +1,269 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+#include <mach/syslib.h>
+#include <linux/slab.h>
+
+#define SFL_memcpy  memcpy
+
+/******************************************************************************
+*                                                                             *
+*     Generic fast queue that operates with pointers (derviced from ICC)      *
+*                                                                             *
+******************************************************************************/
+int	SFL_Enqueue(PFASTQUEUE pq, PVOID pData)
+{
+	U32 put = pq->put;
+	U32 new_put = put + 1;
+	if (new_put >= pq->size)
+		new_put = 0;
+
+	if (new_put != pq->get)
+	{ // the queue is not full
+		pq->pStorage[ put ] = pData;
+
+		DSB();
+		pq->put = new_put;
+		return TRUE;
+	}
+	return FALSE;
+}
+
+PVOID SFL_Dequeue(PFASTQUEUE pq)
+{
+	PVOID p;
+	U32   get = pq->get;
+
+	if ((pq->put - get) != 0)
+	{
+		p = pq->pStorage[get++];
+		if (get >= pq->size)
+			get = 0;
+
+		DSB();
+		pq->get = get;
+		return p;
+	}
+	return NULL;
+}
+
+
+int	SFL_Enqueue_NoSync(PFASTQUEUE pq, PVOID pData)
+{
+	U32 put = pq->put;
+	U32 new_put = put + 1;
+	if (new_put >= pq->size)
+		new_put = 0;
+
+	if (new_put != pq->get)
+	{ // the queue is not full
+		pq->pStorage[ put ] = pData;
+		pq->put = new_put;
+		return TRUE;
+	}
+	return FALSE;
+}
+
+PVOID SFL_Dequeue_NoSync(PFASTQUEUE pq)
+{
+	PVOID p;
+	U32   get = pq->get;
+
+	if ((pq->put - get) != 0)
+	{
+		p = pq->pStorage[get++];
+		if (get >= pq->size)
+			get = 0;
+		pq->get = get;
+		return p;
+	}
+	return NULL;
+}
+
+void SFL_DefQueue(PFASTQUEUE pq, void *pStorage, int StorageSize)
+{
+	memset( pq, 0x00, sizeof( FASTQUEUE) );
+	// always define storage as U32 []
+	pq->size = StorageSize >> (sizeof(PVOID) >> 1);
+	pq->pStorage = pStorage;
+}
+
+
+static U32 sfl_SafeQueueLevel(U32 put, U32 get, U32 size)
+{
+	U32 nItems;
+
+	if (put >= get)
+		nItems = put - get;
+	else
+		nItems = size + put - get;
+
+	return nItems;
+}
+
+
+U32 SFL_GetNumItemsInTheQueue(FASTQUEUE *fpq)
+{
+	return sfl_SafeQueueLevel(fpq->put, fpq->get, fpq->size);
+}
+
+
+
+
+U32 SFL_Queue_BatchRead( PFASTQUEUE pq, U32 *pDestArr, U32 Count)
+{
+	if (Count)
+	{
+		U32 write_index = 0;
+		U32 nReads = 0;
+		//U32 iMask = SFL_IDisable();
+		U32 put = pq->put; // fetch the put atomicly (as app may change it!)
+		U32 get = pq->get; // cache the volatile "get index"
+
+		//printf("nItems_%d ", SFL_GetNumItemsInTheQueue(pq));
+
+		if ( (nReads = sfl_SafeQueueLevel(put, get, pq->size)) < Count )
+			Count = nReads;
+		else
+			nReads = Count;
+
+		if (Count >= pq->size - get)
+		{
+			U32 n = pq->size - get;
+			SFL_memcpy( pDestArr, &pq->pStorage[get], sizeof(pDestArr[0]) * n);
+			get = 0;
+			Count -= n;
+			write_index += n;
+		}
+
+		if (Count)
+		{
+			SFL_memcpy( &pDestArr[write_index], &pq->pStorage[get], sizeof(pDestArr[0]) * Count);
+			get += Count;
+		}
+
+		DSB();
+		pq->get = get;
+
+		//printf("nItems_%d ", SFL_GetNumItemsInTheQueue(pq));
+
+		//SFL_IControl(iMask);
+
+		return nReads;
+	}
+	return FALSE;
+}
+
+
+// the routine does not keep the fifo order (it is used to take items away from the queue)
+U32 SFL_Queue_BatchUnload(PFASTQUEUE pq, U32 *pDestArr, U32 Count)
+{
+	if (Count)
+	{
+		U32 write_index = 0;
+		U32 nReads = 0;
+		//U32 iMask = SFL_IDisable();
+		U32 put = pq->put; // lets cache the volatile "put index"
+		U32 get = pq->get; // fetch the get index atomicly (as app may change it)
+
+		//printf("nItems_%d ", SFL_GetNumItemsInTheQueue(pq));
+
+		nReads = sfl_SafeQueueLevel(put, get, pq->size);
+		if (nReads)
+			nReads -= 1; // decrement is used to cover the case when a reader already started reading from head
+
+		if ( nReads < Count )
+			Count = nReads;
+		else
+			nReads = Count;
+
+		if (!put)
+			put = pq->size;
+
+		if (Count >= put)
+		{
+			U32 n = put;
+			SFL_memcpy( pDestArr, &pq->pStorage[0], sizeof(pDestArr[0]) * n);
+			put = pq->size;
+			Count -= n;
+			write_index += n;
+		}
+
+		if (Count)
+		{
+			put -= Count;
+			SFL_memcpy( &pDestArr[write_index], &pq->pStorage[put], sizeof(pDestArr[0]) * Count);
+		}
+
+		if (put >= pq->size)
+			put = 0;
+
+		DSB();
+		pq->put = put;
+
+		//printf("nItems_%d ", SFL_GetNumItemsInTheQueue(pq));
+
+		//SFL_IControl(iMask);
+
+		return nReads;
+	}
+	return FALSE;
+}
+
+
+U32 SFL_Queue_BatchWrite( PFASTQUEUE pq, U32 *pSrcArr, U32 Count)
+{
+
+	U32 nWrites = Count;
+
+	if (Count)
+	{
+		U32 read_index = 0;
+		U32 put = pq->put;
+		//U32 iMask = SFL_IDisable();
+
+		if (pq->size - put <= Count)
+		{
+			U32 n = pq->size - put;
+			SFL_memcpy( &pq->pStorage[put], pSrcArr, sizeof(pSrcArr[0]) * n);
+			put = 0;
+			Count -= n;
+			read_index += n;
+		}
+
+		if (Count)
+		{
+			SFL_memcpy( &pq->pStorage[put], &pSrcArr[read_index], sizeof(pSrcArr[0]) * Count);
+			put += Count;
+		}
+
+		DSB();
+		pq->put = put;
+
+		//SFL_IControl(iMask);
+		return nWrites;
+	}
+	return 0;
+}
+
+
+
+
diff --git a/arch/arm/mach-transcede/transcede-2200.c b/arch/arm/mach-transcede/transcede-2200.c
new file mode 100644
index 0000000..cdba49c
--- /dev/null
+++ b/arch/arm/mach-transcede/transcede-2200.c
@@ -0,0 +1,1831 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+// TODO: hardcoded initialization shold be moved to u-boot or radio module
+#define TRANSCEDE_RADIO_INIT
+
+#include <linux/clockchips.h>
+#include <linux/i2c.h>
+#include <linux/init.h>
+#include <linux/serial_8250.h>
+#include <linux/smp.h>
+#include <linux/mtd/mtd.h>
+#include <linux/mtd/physmap.h>
+#include <linux/mtd/partitions.h>
+#include <linux/mtd/nand.h>
+#include <linux/proc_fs.h>
+
+#include <linux/i2c/at24.h>
+#include <linux/spi/spi.h>
+
+#include <asm/system.h>
+#include <asm/mach-types.h>
+#include <asm/smp_twd.h>
+#include <asm/localtimer.h>
+#include <asm/hardware/gic.h>
+#include <asm/pmu.h>
+#include <asm/uaccess.h>
+
+#ifdef TRANSCEDE_RADIO_INIT
+#include <linux/delay.h>
+#endif
+#include <asm/mach/arch.h>
+#include <asm/mach/map.h>
+#include <asm/mach/time.h>
+#include <asm/sched_clock.h>
+#include <mach/hardware.h>
+#include <mach/sysheap.h>
+#include <asm/hardware/cache-l2x0.h>
+#include <linux/memblock.h>
+
+#include <linux/gpio.h>
+#include <mach/spa.h>
+#include <mach/clkrst.h>
+#include <linux/dma-mapping.h>
+#include <mach/usb_mmap.h>
+
+#include <mach/ddr_protection.h>
+
+#ifdef CONFIG_AMP_STACK
+#include <mach/mmu_protect.h>
+#endif	/* CONFIG_AMP_STACK */
+
+#include <mach/drv_if.h>
+
+struct t2200_gpio_pin_stat_info t2200_gpio_pin_stat = {
+	.t2200_gpio_pins = 0x0,
+};
+EXPORT_SYMBOL(t2200_gpio_pin_stat);
+
+static struct map_desc transcede_io_desc[] __initdata = {
+	{
+		.virtual	= AAB_XP_BASE_VADDR,
+		.pfn		= __phys_to_pfn(TRANSCEDE_AAB_XP_BASE),
+		.length		= TRANSCEDE_AAB_XP_SIZE,		/* 0xFE000000 .. 0xFF000000 */
+		.type		= MT_DEVICE
+	},
+	{
+		.virtual	= TRANSCEDE_SCU_BASE,
+		.pfn		= __phys_to_pfn(TRANSCEDE_SCU_BASE),
+		.length		= SZ_128K,				/* 0xFFF00000 .. 0xFFF20000 */
+		.type		= MT_DEVICE,
+	},
+	{
+		.virtual	= IRAM_BASE_VADDR,
+		.pfn		= __phys_to_pfn(TRANSCEDE_IRAM_BASE),
+		.length		= TRANSCEDE_IRAM_SIZE,			/* 0xF4000000 .. 0xF4040000 */
+		.type		= MT_DEVICE
+	},
+	{
+		.virtual	= TRANSCEDE_CRAM_BASE,
+		.pfn		= __phys_to_pfn(TRANSCEDE_CRAM_BASE),
+		.length		= TRANSCEDE_CRAM_SIZE,			/* 0xF3000000 .. 0xF30C0000 */
+		.type		= MT_DEVICE
+	},
+	{
+		.virtual	= 0xF0D00000,
+		.pfn		= __phys_to_pfn(0xF0D00000),
+		.length		= 0x00030000,				/* 0xF0D00000 .. 0xF0D30000 */
+		.type		= MT_DEVICE
+	},
+	{
+		.virtual	= 0xF3D10000,
+		.pfn		= __phys_to_pfn(0xF3D10000),
+		.length		= 0x002F0000,				/* 0xF3D10000 .. 0xF4000000 */
+		.type		= MT_DEVICE
+	},
+	{
+		.virtual	= 0xF4C00000,
+		.pfn		= __phys_to_pfn(0xF4C00000),
+		.length		= 0x00120000,				/* 0xF4C00000 .. 0xF4D20000 */
+		.type		= MT_DEVICE
+	},
+	{
+		.virtual	= TRANSCEDE_SEMAARM_BASEADDR,
+		.pfn		= __phys_to_pfn(TRANSCEDE_SEMAARM_BASEADDR),
+		.length		= SZ_64K,				/* 0xF4F00000 .. 0xF4F10000 */
+		.type		= MT_DEVICE
+	},
+	{
+		.virtual	= 0xF5000000,
+		.pfn		= __phys_to_pfn(0xF5000000),
+		.length		= SZ_16M,				/* 0xF5000000 .. 0xF6000000 */
+		.type		= MT_DEVICE
+	},
+	{
+		.virtual        = TRANSCEDE_IPSEC_REG_BASE,
+		.pfn            = __phys_to_pfn(TRANSCEDE_IPSEC_REG_BASE),
+		.length         = SZ_512K,				/* 0xFBD80000 .. 0xFBE00000 */
+		.type           = MT_DEVICE
+	},
+/*
+  #ifdef CONFIG_TRANSCEDE_GEMAC_0
+  {
+  .virtual	=  TRANSCEDE_GEM0,
+  .pfn		= __phys_to_pfn(TRANSCEDE_GEM0),
+  .length		= SZ_64K,
+  .type		= MT_DEVICE
+  },
+  #endif
+*/
+	{
+		.virtual	= TRANSCEDE_PCIESlv_CPRISlv,
+		.pfn		= __phys_to_pfn(TRANSCEDE_PCIESlv_CPRISlv),
+		.length		= SZ_16M,					/* 0xF6000000 .. 0xF7000000 */
+		.type		= MT_MEMORY_STRONGLY_ORDERED,
+	},
+#ifdef CONFIG_PCI
+	{
+		.virtual    = TRANSCEDE_PCIe1_BASE,
+		.pfn        = __phys_to_pfn(TRANSCEDE_PCIe1_BASE),
+		.length     = 0x03000000, /* 0xFA000000 .. 0xFD000000 */
+		.type       = MT_DEVICE
+	}
+#endif
+};
+
+#if defined(CONFIG_TRANSCEDE_IPSEC_HW_SUPPORT)
+
+static struct resource transcede_espah_resources[] = {
+	{
+		.name = "base_core",
+		.start  = (TRANSCEDE_SPA_REG_BASE + TRANSCEDE_SPA_ESPAH_REG_OFFSET),
+		.end = (TRANSCEDE_SPA_REG_BASE + TRANSCEDE_SPA_ESPAH_REG_OFFSET + TRANSCEDE_SPA_ESPAH_REG_SIZE),
+		.flags  = IORESOURCE_MEM,
+	},
+
+	{
+		.name   = "irq_core",
+		.start  = IRQ_ESPAH,
+		.end    = IRQ_ESPAH,
+		.flags  = IORESOURCE_IRQ
+	}
+};
+
+static struct platform_device  transcede_espah_device = {
+	.name                   = "Transcede ESPAH",
+	.id                     = 0,
+	.num_resources          = ARRAY_SIZE(transcede_espah_resources),
+	.resource               = transcede_espah_resources,
+	.dev = {
+		.coherent_dma_mask      = 0xffffffff,
+	},
+};
+#endif
+
+#if defined(CONFIG_TRANSCEDE_ELP_SPACC)
+static struct resource transcede_elp_spacc_resources[] = {
+	{
+		.name = "base_core",
+		.start  = TRANSCEDE_SPA_REG_BASE + TRANSCEDE_SPA_SPACC_REG_OFFSET,
+		.end = TRANSCEDE_SPA_REG_BASE + TRANSCEDE_SPA_SPACC_REG_OFFSET + TRANSCEDE_SPA_SPACC_REG_SIZE,
+		.flags  = IORESOURCE_MEM,
+	},
+
+	{
+		.name   = "irq_core",
+		.start  = IRQ_SPAcc_IRQ0,
+		.end    = IRQ_SPAcc_IRQ0,
+		.flags  = IORESOURCE_IRQ
+	}
+};
+
+static struct platform_device  transcede_elp_spacc_device = {
+	.name                   = "spacc",
+	.id                     = (0x1805 << 16) /* EPN */ | (0 & 0xF) /* virt */,
+	.num_resources          = ARRAY_SIZE(transcede_elp_spacc_resources),
+	.resource               = transcede_elp_spacc_resources,
+};
+#endif
+
+#if defined(CONFIG_TRANSCEDE_ELP_CLP30)
+static struct resource transcede_elp_clp30_resources[] = {
+        {
+                .name = "base_core",
+                .start  = TRANSCEDE_IPSEC_REG_BASE,
+                .end = TRANSCEDE_IPSEC_REG_BASE + TRANSCEDE_SPA_ESPAH_REG_SIZE,
+                .flags  = IORESOURCE_MEM,
+        },
+
+        {
+                .name   = "irq_core",
+                .start  = IRQ_ESPAH,
+                .end    = IRQ_ESPAH,
+                .flags  = IORESOURCE_IRQ
+        }
+};
+
+static struct platform_device  transcede_elp_clp30_device = {
+    .name                   = "clp30",
+    .num_resources          = ARRAY_SIZE(transcede_elp_clp30_resources),
+    .resource               = transcede_elp_clp30_resources,
+    .dev = {
+       .coherent_dma_mask   = 0xffffffff,
+    },
+};
+#endif
+
+#if defined(CONFIG_TRANSCEDE_ELP_PDU)
+static struct platform_device  transcede_elp_pdu_device = {
+	.name		= "elppdu",
+	.dev = {
+		.coherent_dma_mask      = 0xffffffff,
+	},
+};
+#endif
+
+#if defined(CONFIG_TRANSCEDE_ELP_TRNG)
+static struct resource transcede_elp_trng_resources[] = {
+        {
+                .name = "base_core",
+                .start  = TRNG_BASEADDR,
+                .end = TRNG_BASEADDR + TRANSCEDE_SPA_TRNG_REG_SIZE,
+                .flags  = IORESOURCE_MEM,
+        },
+/*
+        {
+                .name   = "irq_core",
+                .start  = IRQ_SASPA,
+                .end    = IRQ_SASPA,
+                .flags  = IORESOURCE_IRQ
+        }
+*/
+};
+
+static struct platform_device  transcede_elp_trng_device = {
+    .name                   = "trng",
+    .num_resources          = ARRAY_SIZE(transcede_elp_trng_resources),
+    .resource               = transcede_elp_trng_resources,
+};
+#endif
+
+struct transcede_eth_platform_data transcede_gem0_pdata = {
+	.name = "eth1",
+	.device_flags = CONFIG_TRANSCEDE_GEMAC,
+	.phy_flags = GEMAC_PHY_1000 | GEMAC_NO_PHY,
+	.bus_id = "0",
+	.phy_id = 0,
+	.gem_id = 0,
+	.mac_addr = (u8[]){ 0x00, 0xED, 0xCD, 0xEF, 0xAA, 0xCC },
+};
+
+struct transcede_eth_platform_data transcede_gem1_pdata = {
+	.name = "eth2",
+	.device_flags = CONFIG_TRANSCEDE_GEMAC,
+	.phy_flags = GEMAC_PHY_1000,
+	.bus_id = "0",
+	.phy_id = 5,
+	.gem_id = 1,
+	.mac_addr = (u8[]){ 0x00, 0x2A, 0x2B, 0x2C, 0x2D, 0x2E },
+};
+
+#if defined(CONFIG_TRANSCEDE_GEMAC_1)
+static struct resource transcede_eth1_resources[] = {
+	{
+		.name   = "gemac",
+		.start  = TRANSCEDE_GEM1,
+		.end    = TRANSCEDE_GEM1 + SZ_64K - 1,
+		.flags  = IORESOURCE_MEM,
+	},
+	{
+		.name   = "gemac_desc_mem",
+#if defined(CONFIG_TRANSCEDE_GEMAC_DESC_IRAM)
+		.start  = TRANSCEDE_IRAM_BASE,
+		.end    = TRANSCEDE_IRAM_BASE + 0x7FFE, /* 32 KiB */
+#else
+		.start  = 0,
+		.end    = (32 * 1024) - 1,
+#endif
+		.flags  = IORESOURCE_MEM,
+	},
+	{
+		.name   = "rx",
+		.start  = IRQ_GEM1,
+		.flags  = IORESOURCE_IRQ,
+	},
+};
+
+static struct platform_device transcede_eth1_device = {
+	.name   = "t2200-eth",
+	.id     = 1,
+	.resource       = transcede_eth1_resources,
+	.num_resources  = ARRAY_SIZE(transcede_eth1_resources),
+	.dev = {
+		.platform_data = &transcede_gem1_pdata,
+	},
+};
+#endif
+
+#if defined(CONFIG_TRANSCEDE_GEMAC_0)
+static struct resource transcede_eth0_resources[] = {
+	{
+		.name   = "gemac",
+		.start  = TRANSCEDE_GEM0,
+		.end    = TRANSCEDE_GEM0 + SZ_64K - 1,
+		.flags  = IORESOURCE_MEM,
+	},
+	{
+		.name   = "gemac_desc_mem",
+#if defined(CONFIG_TRANSCEDE_GEMAC_DESC_IRAM)
+		.start  = TRANSCEDE_IRAM_BASE,
+		.end    = TRANSCEDE_IRAM_BASE + 0x7FFE, /* 32 KiB */
+#else
+		.start  = 0,
+		.end    = (32 * 1024) - 1,
+#endif
+		.flags  = IORESOURCE_MEM,
+	},
+	{
+		.name   = "rx",
+		.start  = IRQ_GEM0,
+		.flags  = IORESOURCE_IRQ,
+	},
+};
+
+static struct platform_device transcede_eth0_device = {
+	.name   = "t2200-eth",
+	.id     = 0,
+	.resource       = transcede_eth0_resources,
+	.num_resources  = ARRAY_SIZE(transcede_eth0_resources),
+	.dev = {
+		.platform_data = &transcede_gem0_pdata,
+	},
+};
+#endif
+
+#if defined(CONFIG_TRANSCEDE_GEM_PHY)
+static struct transcede_mdio_data transcede_mdio_pdata = {
+	.phy_mask = 0xFFFFFF00, /* Allow PHY addresses 0-7, bits 0-7 as 0, other addresses masked invalid as 1 */
+	.mdc_div = 64,
+};
+
+static struct resource transcede_mdio_resources[] = {
+	{
+		.start	= TRANSCEDE_GEM0 + 0xE000,
+		.end	= TRANSCEDE_GEM0 + 0xE400 - 1,
+		.flags	= IORESOURCE_MEM,
+	},
+};
+
+static struct platform_device transcede_mdio_device = {
+	.name           = "transcede-mdio",
+	.id	            = 0,
+	.resource	    = transcede_mdio_resources,
+	.num_resources	= ARRAY_SIZE(transcede_mdio_resources),
+	.dev = {
+		.platform_data = &transcede_mdio_pdata,
+	},
+};
+#endif
+
+#ifdef CONFIG_USB_SUPPORT
+
+static u64 transcede_dwc_otg_dmamask = DMA_BIT_MASK(32);
+
+static struct resource transcede_dwc_otg_resources[] = {
+	{
+		.start  = USB0_CFG_BASE,
+		.end    = USB0_CFG_BASE + 0xFFFFFF,
+		.flags  = IORESOURCE_MEM,
+	},
+	{
+		.start  = IRQ_USB,
+		.end    = IRQ_USB,
+		.flags  = IORESOURCE_IRQ,
+	},
+};
+
+static struct platform_device transcede_dwc_otg = {
+	.name           = "dwc_otg",
+	.resource       = transcede_dwc_otg_resources,
+	.id             = -1,
+	.num_resources  = ARRAY_SIZE(transcede_dwc_otg_resources),
+	.dev = {
+		.platform_data     = NULL,
+		.dma_mask          = &transcede_dwc_otg_dmamask,
+		.coherent_dma_mask = DMA_BIT_MASK(32),
+	}
+};
+
+#endif	/* CONFIG_USB_SUPPORT */
+
+void* (*ICC_KDII_Create)(const char *name, u32 QueueSize, u32 BlockSize, u32 *Ports, u32 mode) = NULL;
+int   (*ICC_KDII_Destroy)(void* h) = NULL;
+
+EXPORT_SYMBOL(ICC_KDII_Create);
+EXPORT_SYMBOL(ICC_KDII_Destroy);
+
+typedef struct _T2K_XFRM_ELM_
+{
+    void*	xfrm;		// The pointer to the registered <struct xfrm_state*> elm
+    uint	d_ip;		// The source IPv4 address
+    uint	s_ip;		// The destination IPv4 address
+
+}T2K_XFRM_ELM;
+
+// This array contains all the XFRM contexts registerd by IPSec driver
+// and used to find/get it by other drivers, the size of array is set here
+// and macro is not needed because the code auto-detects a number of elements in this array
+static T2K_XFRM_ELM t2k_xfrm_array[32];
+static volatile uint t2k_xfrm_elm_num = 0;
+
+int t2k_reg_xfrm_ipv4(void * pxfrm, uint d_ip, uint s_ip)
+{
+    uint i;
+
+    if (t2k_xfrm_elm_num >= ARRAY_SIZE(t2k_xfrm_array))
+	return -1;
+
+    // to find the gap in the array and to put a new pointer
+    // to the unused cell
+
+    for (i = 0; i < ARRAY_SIZE(t2k_xfrm_array); i++)
+    {
+	if (t2k_xfrm_array[i].xfrm == NULL)
+	    break;
+    }
+
+    t2k_xfrm_array[i].xfrm = pxfrm;
+    t2k_xfrm_array[i].s_ip = s_ip;
+    t2k_xfrm_array[i].d_ip = d_ip;
+    smp_wmb();
+
+    t2k_xfrm_elm_num ++;
+    smp_wmb();
+    return 0;
+}
+EXPORT_SYMBOL(t2k_reg_xfrm_ipv4);
+
+int t2k_unreg_xfrm(void* pxfrm)
+{
+    uint i;
+
+    // to find the xfrm in the array and to put NULL
+    // to this cell
+    for (i = 0; i < ARRAY_SIZE(t2k_xfrm_array); i++)
+    {
+	if (t2k_xfrm_array[i].xfrm == pxfrm)
+	{
+	    t2k_xfrm_array[i].xfrm = NULL;
+	    t2k_xfrm_array[i].s_ip = 0;
+	    t2k_xfrm_array[i].d_ip = 0;
+	    smp_wmb();
+
+	    t2k_xfrm_elm_num --;
+	    smp_wmb();
+	    return 0;
+	}
+    }
+    return -1;
+}
+EXPORT_SYMBOL(t2k_unreg_xfrm);
+
+void* t2k_find_xfrm_by_ipv4(uint d_ip, uint s_ip)
+{
+    uint i;
+    uint proc_num = 0;
+
+    // to find the xfrm in the array specified by the IPs
+    for (i = 0; i < ARRAY_SIZE(t2k_xfrm_array) && proc_num < t2k_xfrm_elm_num; i++)
+    {
+	if (t2k_xfrm_array[i].s_ip == s_ip && t2k_xfrm_array[i].d_ip == d_ip)
+	{
+	    return t2k_xfrm_array[i].xfrm;
+	}
+
+	if (t2k_xfrm_array[i].xfrm != NULL)
+	{
+	    proc_num++;
+	}
+    }
+    return NULL;
+}
+EXPORT_SYMBOL(t2k_find_xfrm_by_ipv4);
+
+void* t2k_get_xfrm_by_idx(uint idx)
+{
+    if (idx < ARRAY_SIZE(t2k_xfrm_array))
+	return t2k_xfrm_array[idx].xfrm;
+
+    return NULL;
+}
+EXPORT_SYMBOL(t2k_get_xfrm_by_idx);
+
+uint t2k_get_reg_xfrm_num(void)
+{
+    return t2k_xfrm_elm_num;
+}
+EXPORT_SYMBOL(t2k_get_reg_xfrm_num);
+
+#ifdef CONFIG_GLOBAL_POLLING
+static int __cpuinit global_timer_setup(void)
+{
+	unsigned long long comparator, increment;
+
+	increment = ClkRstGetFreq(CR_CA9_MC_MPU_PERIPH) / 1000; /* 1ms */
+
+	writel(0, TRANSCEDE_TIMER_CTRL);
+
+	comparator = readl(TRANSCEDE_LOW_TIMER_COUNTER);
+	comparator += ((unsigned long long)readl(TRANSCEDE_UP_TIMER_COUNTER)) << 32;
+	comparator += increment;
+
+	writel(increment, TRANSCEDE_AUTO_INCREMENT);
+	writel(comparator, TRANSCEDE_LOW_COMPARATOR);
+	writel(comparator >> 32, TRANSCEDE_UP_COMPARATOR);
+	writel(0
+	       | GLOBAL_TIMER_AUTO_INC_ENA
+	       | GLOBAL_TIMER_IRQ_ENA
+	       | GLOBAL_TIMER_COMPARATOR_ENA
+	       | GLOBAL_TIMER_ENA,
+	       TRANSCEDE_TIMER_CTRL);
+
+	gic_enable_ppi(IRQ_GLOBALTIMER);
+
+	return 0;
+}
+
+void global_timer_enable (void)
+{
+    writel(0
+       | GLOBAL_TIMER_AUTO_INC_ENA
+       | GLOBAL_TIMER_IRQ_ENA
+       | GLOBAL_TIMER_COMPARATOR_ENA
+       | GLOBAL_TIMER_ENA,
+       TRANSCEDE_TIMER_CTRL);
+}
+EXPORT_SYMBOL(global_timer_enable);
+
+void global_timer_disable(void)
+{
+    writel(0
+       | GLOBAL_TIMER_AUTO_INC_ENA
+       | GLOBAL_TIMER_COMPARATOR_ENA
+       | GLOBAL_TIMER_ENA,
+       TRANSCEDE_TIMER_CTRL);
+}
+EXPORT_SYMBOL(global_timer_disable);
+
+#endif	/* CONFIG_GLOBAL_POLLING */
+
+#define NOR_BASE_ADDR	0xA0000000
+#define NAND_BASE_ADDR	0xAC000000
+#define NOR8_SIZE	(512*1024)
+#define NOR16_SIZE	(64*1024*1024)
+
+#define NAND_ADDR_ALE   (1<<25)
+#define NAND_ADDR_CLE   (1<<24)
+
+
+#if defined(CONFIG_MTD_TRANSCEDE_NOR_8)
+static struct mtd_partition transcede_nor8_parts[] = {
+	{
+		.name	= "boot",
+		.offset	= 0,
+		.size	= MTDPART_SIZ_FULL,
+		.mask_flags = MTD_WRITEABLE,
+	}
+};
+
+static struct physmap_flash_data transcede_nor8_data = {
+	.width = 1,
+	.parts = transcede_nor8_parts,
+	.nr_parts = ARRAY_SIZE(transcede_nor8_parts)
+};
+
+static struct resource transcede_nor8_resources[] = {
+	{
+		.start	= NOR_BASE_ADDR,
+		.end	= NOR_BASE_ADDR + NOR8_SIZE - 1,
+		.flags	= IORESOURCE_MEM
+	},
+};
+
+struct platform_device transcede_nor8_flash_device = {
+	.name		= "physmap-flash",
+	.id		= 0,
+	.resource	= transcede_nor8_resources,
+	.num_resources	= ARRAY_SIZE(transcede_nor8_resources),
+	.dev = {
+		.platform_data = &transcede_nor8_data
+	},
+};
+#endif
+
+
+#if defined(CONFIG_MTD_TRANSCEDE_NOR_16)
+static struct mtd_partition transcede_nor16_parts[] = {
+	{
+		.name = "boot",
+		.offset = 0,
+		.size = 512*1024,
+		.mask_flags = MTD_WRITEABLE,
+	}, {
+		.name = "kernel",
+		.size = 3*1024*1024,
+		.offset = MTDPART_OFS_APPEND,
+	}, {
+		.name = "fs",
+		.size = MTDPART_SIZ_FULL,
+		.offset = MTDPART_OFS_APPEND,
+	},
+};
+
+static struct physmap_flash_data transcede_nor16_data = {
+	.width = 2,
+	.parts = transcede_nor16_parts,
+	.nr_parts = ARRAY_SIZE(transcede_nor16_parts)
+};
+
+static struct resource transcede_nor16_resources[] = {
+	{
+		.start = NOR_BASE_ADDR,
+		.end = NOR_BASE_ADDR + NOR16_SIZE - 1,
+		.flags = IORESOURCE_MEM
+	},
+};
+
+struct platform_device transcede_nor16_flash_device = {
+	.name		= "physmap-flash",
+	.id		= 0,
+	.resource	= transcede_nor16_resources,
+	.num_resources	= ARRAY_SIZE(transcede_nor16_resources),
+	.dev = {
+		.platform_data = &transcede_nor16_data
+	},
+};
+#endif
+
+#ifdef CONFIG_I2C
+static struct resource transcede_i2c_resources[] = {
+	{
+		.start  = TRANSCEDE_I2C,
+		.end    = TRANSCEDE_I2C + SZ_4K - 1,
+		.flags  = IORESOURCE_MEM,
+	},
+	{
+		.start  = IRQ_I2C,
+		.flags  = IORESOURCE_IRQ,
+	},
+};
+static struct platform_device transcede_i2c = {
+	.name           = "transcede_i2c",
+	.id             = 0,
+	.num_resources  = ARRAY_SIZE(transcede_i2c_resources),
+	.resource       = transcede_i2c_resources,
+};
+
+/*
+static struct at24_platform_data transcede_i2c_eeprom = {
+	.byte_len       = 64*1024,
+	.page_size      = 128,
+	.flags          = AT24_FLAG_ADDR16,
+};
+
+static struct i2c_board_info __initdata transcede_i2c_info[] = {
+	{
+		I2C_BOARD_INFO("at24", 0x50),
+		.platform_data = &transcede_i2c_eeprom,
+	},
+};
+*/
+#endif
+
+#if defined(CONFIG_TRANSCEDE_USIM_SUPPORT)
+static struct resource transcede_usim_resources[] = {
+	{
+		.start  = TRANSCEDE_USIM,
+		.end    = TRANSCEDE_USIM + SZ_1K - 1,
+		.flags  = IORESOURCE_MEM,
+	},
+	{
+		.start  = IRQ_USIM,
+		.flags  = IORESOURCE_IRQ,
+	},
+};
+static struct platform_device transcede_usim = {
+	.name           = "transcede_usim",
+	.id             = 0,
+	.num_resources  = ARRAY_SIZE(transcede_usim_resources),
+	.resource       = transcede_usim_resources,
+};
+#endif
+
+#if defined(CONFIG_SPI_TRANSCEDE)
+static struct resource transcede_spi0_resources[] = {
+	{
+		.start  = TRANSCEDE_SPI0,
+		.end    = TRANSCEDE_SPI0 + SZ_4K - 1,
+		.flags  = IORESOURCE_MEM,
+	},
+	{
+		.start  = IRQ_SPI0,
+		.flags  = IORESOURCE_IRQ,
+	},
+};
+
+static struct resource transcede_spi1_resources[] = {
+	{
+		.start  = TRANSCEDE_SPI1,
+		.end    = TRANSCEDE_SPI1 + SZ_4K - 1,
+		.flags  = IORESOURCE_MEM,
+	},
+	{
+		.start  = IRQ_SPI1,
+		.flags  = IORESOURCE_IRQ,
+	},
+};
+
+static struct platform_device transcede_spi0 = {
+	.name = "transcede_spi",
+	.id = 0,
+	.num_resources = ARRAY_SIZE(transcede_spi0_resources),
+	.resource = transcede_spi0_resources,
+};
+
+static struct platform_device transcede_spi1 = {
+	.name = "transcede_spi",
+	.id = 1,
+	.num_resources = ARRAY_SIZE(transcede_spi1_resources),
+	.resource = transcede_spi1_resources,
+};
+
+static struct spi_board_info transcede_spi_info[] __initdata = {
+	{
+		.modalias = "spidev",
+		.chip_select = 4,
+		.max_speed_hz = 4000000,
+		.bus_num = 0,
+		.mode = SPI_MODE_0,
+	},
+	{
+		.modalias = "spidev",
+		.chip_select = 2,
+		.max_speed_hz = 4000000,
+		.bus_num = 1,
+		.mode = SPI_MODE_0,
+	},
+};
+
+#endif
+
+#if defined(CONFIG_MTD_NAND_TRANSCEDE)
+static struct mtd_partition transcede_nand_parts[] = {
+	{
+		.name	= "boot",
+		.offset	= 0,
+		.size	= 512*1024,
+		.mask_flags = MTD_WRITEABLE,
+	}, {
+		.name	= "kernel",
+		.size	= 3*1024*1024,
+		.offset	= MTDPART_OFS_APPEND,
+	}, {
+		.name	= "fs",
+		.size	= MTDPART_SIZ_FULL,
+		.offset	= MTDPART_OFS_APPEND,
+	}
+};
+
+static struct resource transcede_nand_resources[] = {
+	{
+		.start	= NAND_BASE_ADDR,
+		.end	= NAND_BASE_ADDR + 32 - 1,
+		.flags	= IORESOURCE_MEM,
+	}
+};
+
+static u32  nand_ale_base;
+static u32  nand_cle_base;
+
+static void transcede_nand_ctrl(struct mtd_info *mtd, int cmd, unsigned int ctrl)
+{
+	struct nand_chip* this = mtd->priv;
+	u32 addr = (u32)this->IO_ADDR_W;
+
+//	if (ctrl & NAND_CTRL_CHANGE)
+	{
+		if (ctrl & NAND_NCE)
+			gpio_set_value(NAND_GPIO_CS, 0);
+		else
+			gpio_set_value(NAND_GPIO_CS, 1);
+
+		if ( (ctrl & NAND_CTRL_CLE) == NAND_CTRL_CLE ) {// old bitmask
+			addr = nand_cle_base;
+		}
+		if ( (ctrl & NAND_CTRL_ALE) == NAND_CTRL_ALE ) {// old bitmask
+			addr = nand_ale_base;
+		}
+	}
+
+	if (cmd != NAND_CMD_NONE) {
+		*(volatile u8*)((u32) addr)= cmd;
+	}
+}
+
+static int transcede_nand_dev_ready(struct mtd_info* mtd)
+{
+	return gpio_get_value(NAND_GPIO_RDY);
+}
+
+#ifdef CONFIG_MTD_PARTITIONS
+const char *part_probes[] = { "cmdlinepart", NULL };
+#endif
+
+struct platform_nand_data transcede_nand_data = {
+	.chip = {
+		.nr_chips = 1,
+		.chip_offset = 0,
+		.nr_partitions = ARRAY_SIZE(transcede_nand_parts),
+		.partitions = transcede_nand_parts,
+		.chip_delay = 20,
+#ifdef CONFIG_MTD_PARTITIONS
+		.part_probe_types = part_probes,
+#endif
+	},
+	.ctrl = {
+		.hwcontrol = 0,
+		.dev_ready = transcede_nand_dev_ready,
+		.select_chip = 0,
+		.cmd_ctrl = transcede_nand_ctrl,
+	}
+};
+
+static struct platform_device transcede_nand = {
+	.name		= "gen_nand",
+	.id		= -1,
+	.resource	= transcede_nand_resources,
+	.num_resources	= ARRAY_SIZE(transcede_nand_resources),
+	.dev = {
+		.platform_data = &transcede_nand_data,
+	},
+};
+#endif
+
+static struct resource transcede_pmu_resources[] = {
+	{
+		.start = IRQ_PMU0,
+		.end   = IRQ_PMU0,
+		.flags = IORESOURCE_IRQ,
+	},
+	{
+		.start = IRQ_PMU1,
+		.end   = IRQ_PMU1,
+		.flags = IORESOURCE_IRQ,
+	},
+};
+
+static struct platform_device transcede_pmu = {
+	.name = "arm-pmu",
+	.id = ARM_PMU_DEVICE_CPU,
+	.num_resources = ARRAY_SIZE(transcede_pmu_resources),
+	.resource = transcede_pmu_resources,
+};
+
+/***************************************************************************/
+static void __init transcede_map_io(void)
+{
+	iotable_init(transcede_io_desc, ARRAY_SIZE(transcede_io_desc));
+}
+
+#include <linux/smp.h>
+#include <linux/cpumask.h>
+#include <linux/io.h>
+
+#include <asm/irq.h>
+#include <asm/mach/irq.h>
+#include <asm/hardware/gic.h>
+
+extern void gic_clear_cpu(struct irq_data *d, cpumask_t mask_val);
+
+static void __init transcede_init_irq(void)
+{
+	unsigned irq;
+	gic_init(0, 7, (void *)(TRANSCEDE_GIC_DIST_BASE), (void *)(TRANSCEDE_GIC_CPU_BASE));
+
+	for (irq = 32; irq < (128 + 16 + 16); irq++)
+	{
+		struct irq_desc *desc = irq_to_desc( irq );
+		if (irq != 132 && irq != 146 && irq != 7) {
+			gic_clear_cpu(&desc->irq_data, cpumask_of_cpu(irq));
+		}
+	}
+}
+
+static cycle_t transcede_get_cycles(struct clocksource *cs)
+{
+	return readl(TRANSCEDE_LOW_TIMER_COUNTER);
+}
+
+static struct clocksource clocksource_transcede = {
+	.name = "global_timer",
+	.rating = 200,
+	.read = transcede_get_cycles,
+	.mask = CLOCKSOURCE_MASK(32),
+	.shift = 24,
+	.flags = CLOCK_SOURCE_IS_CONTINUOUS,
+};
+
+static DEFINE_CLOCK_DATA(cd);
+
+unsigned long long notrace sched_clock(void)
+{
+	return cyc_to_sched_clock(&cd, readl(TRANSCEDE_LOW_TIMER_COUNTER), (u32)~0);
+}
+
+static void notrace transcede_update_sched_clock(void)
+{
+	update_sched_clock(&cd, readl(TRANSCEDE_LOW_TIMER_COUNTER), (u32)~0);
+}
+
+void __init clocksource_init(void)
+{
+	/* Disable timer & comparator */
+	writel(0x00000000, TRANSCEDE_TIMER_CTRL);
+	/* Reset timer counter s*/
+	writel(0x00000000, TRANSCEDE_LOW_TIMER_COUNTER);
+	writel(0x00000000, TRANSCEDE_UP_TIMER_COUNTER);
+	/* Enable timer */
+	writel(0x00000001, TRANSCEDE_TIMER_CTRL);
+
+	clocksource_transcede.mult = clocksource_khz2mult(CLOCK_TICK_RATE / 1000, clocksource_transcede.shift);
+	clocksource_register(&clocksource_transcede);
+
+	init_sched_clock(&cd, transcede_update_sched_clock, 32, CLOCK_TICK_RATE);
+}
+
+int __cpuinit local_timer_setup(struct clock_event_device *evt)
+{
+	/* if ::irq is set do not setup timer again */
+	if (!evt->irq) {
+		evt->irq = IRQ_LOCALTIMER;
+		twd_timer_setup(evt);
+#ifdef CONFIG_GLOBAL_POLLING
+		global_timer_setup();
+#endif	/* CONFIG_GLOBAL_POLLING */
+	}
+
+	return 0;
+}
+
+static void __init transcede_timer_init(void)
+{
+#ifdef CONFIG_GLOBAL_POLLING
+	periodic_task_setup();
+#endif	/* CONFIG_GLOBAL_POLLING */
+
+	clocksource_init();
+
+#ifdef CONFIG_LOCAL_TIMERS
+	twd_base = (void *)(TRANSCEDE_TWD_BASE);
+	twd_timer_rate = CLOCK_TICK_RATE;
+	percpu_timer_setup();
+#endif
+}
+
+static struct sys_timer transcede_timer = {
+	.init   = transcede_timer_init
+};
+
+
+#ifdef CONFIG_CACHE_L2X0
+
+static int l2x0_disabled = 0;
+static int l2x0_write_through = 0;
+
+// static int l2x0_aux_val = (L2CC_AUX_CODE_PREF | L2CC_AUX_DATA_PREF | L2CC_AUX_WRITE_ALLOC | L2CC_AUX_DOUBLE_LINE_FILL);
+static int l2x0_aux_val = 0;
+static int l2x0_aux_mask = 0;
+
+// L2CC Aux Control bits
+#define L2CC_AUX_DOUBLE_LINE_FILL       (1<<30)
+#define L2CC_AUX_CODE_PREF              (1<<29)
+#define L2CC_AUX_DATA_PREF              (1<<28)
+#define L2CC_AUX_WRITE_ALLOC            (2<<23)
+#define L2CC_AUX_CTRL_EMBUS             (1<<20)
+
+static int __init transcede_l2_cache_init(void)
+{
+	void __iomem *l2x0_base =(void *)(TRANSCEDE_L310_BASE);
+
+	if (l2x0_disabled)
+		return 0;
+
+	printk(KERN_ERR "transcede_l2_cache_init\n");
+
+	if (!(readl(l2x0_base + L2X0_CTRL) & 1)) {
+		writel(0x111, l2x0_base + L2X0_TAG_LATENCY_CTRL);
+		writel(0x111, l2x0_base + L2X0_DATA_LATENCY_CTRL);
+		if (l2x0_write_through)
+			writel(3, l2x0_base + L2X0_DEBUG_CTRL);
+	}
+
+	l2x0_init(l2x0_base, l2x0_aux_val, ~l2x0_aux_mask);
+	printk(KERN_INFO "l2x0: init with aux: %x, mask: %x\n", l2x0_aux_val, l2x0_aux_mask);
+
+	printk(KERN_INFO "l2x0: write-%s mode\n", l2x0_write_through ? "through":"back");
+
+	return 0;
+}
+pure_initcall(transcede_l2_cache_init);
+
+static int __init l2x0_disable(char *unused)
+{
+	l2x0_disabled = 1;
+
+	return 0;
+}
+static int __init l2x0_force_write_through(char *unused)
+{
+	l2x0_write_through = 1;
+
+	return 0;
+}
+early_param("nol2x0", l2x0_disable);
+early_param("l2x0wt", l2x0_force_write_through);
+
+static int __init set_l2x0_aux_val(char *str)
+{
+	unsigned long tmp = simple_strtoul(str, NULL, 0);
+	l2x0_aux_val = (__u32)tmp;
+
+	return 0;
+}
+
+__setup("l2x0_aux_val=", set_l2x0_aux_val);
+
+static int __init set_l2x0_aux_mask(char *str)
+{
+        unsigned long tmp = simple_strtoul(str, NULL, 0);
+        l2x0_aux_mask = (__u32)tmp;
+
+        return 0;
+}
+
+__setup("l2x0_aux_mask=", set_l2x0_aux_mask);
+
+#endif
+
+/* T3300 slave boot arg managing */
+unsigned t3300_slave = 0;
+EXPORT_SYMBOL(t3300_slave);
+
+static int __init t3300_slave_enable(char *unused)
+{
+	t3300_slave = 1;
+	return 0;
+}
+
+__setup("t3300_slave", t3300_slave_enable);
+
+/* CRAM heap offset parameter */
+unsigned long transcede_cram_heap_offset = TRANSCEDE_CRAMHEAP_DEF_OFFSET;	/* please use TRANSCEDE_CRAMHEAP_OFFSET definition instead of this variable */
+EXPORT_SYMBOL(transcede_cram_heap_offset);
+
+/* DDR heap parameters */
+unsigned long transcede_ddr_common_base;
+EXPORT_SYMBOL(transcede_ddr_common_base);
+unsigned long transcede_ddr_heap_base;
+EXPORT_SYMBOL(transcede_ddr_heap_base);
+unsigned long transcede_ddr_heap_size = TRANSCEDE_DDRHEAP_DEF_SIZE;
+EXPORT_SYMBOL(transcede_ddr_heap_size);
+unsigned long transcede_ddrcb_heap_base;
+EXPORT_SYMBOL(transcede_ddrcb_heap_base);
+unsigned long transcede_ddrcb_heap_size = TRANSCEDE_DDRCBHEAP_DEF_SIZE;
+EXPORT_SYMBOL(transcede_ddrcb_heap_size);
+unsigned long transcede_ddr_ceva_shared_size = TRANSCEDE_DDR_CEVA_SHARED_DEF_SIZE;
+EXPORT_SYMBOL(transcede_ddr_ceva_shared_size);
+
+/* ICX parameters */
+unsigned long transcede_icx_base;
+EXPORT_SYMBOL(transcede_icx_base);
+
+/* ICC parameters */
+unsigned long transcede_icc_amp_base;
+EXPORT_SYMBOL(transcede_icc_amp_base);
+#ifdef CONFIG_AMP_STACK
+unsigned long transcede_amp_base;
+EXPORT_SYMBOL(transcede_amp_base);
+#endif	/* CONFIG_AMP_STACK */
+unsigned long transcede_icc_heap_base;
+EXPORT_SYMBOL(transcede_icc_heap_base);
+unsigned long transcede_icc_amp_heap_size = TRANSCEDE_ICC_AMP_DEF_SIZE;
+EXPORT_SYMBOL(transcede_icc_amp_heap_size);
+unsigned long transcede_icc_heap_size = TRANSCEDE_ICCHEAP_DEF_SIZE;
+EXPORT_SYMBOL(transcede_icc_heap_size);
+unsigned long transcede_icc_local_part_size = TRANSCEDE_ICC_LOCAL_PART_DEF_SIZE;
+EXPORT_SYMBOL(transcede_icc_local_part_size);
+unsigned long transcede_icc_remote_part_size = TRANSCEDE_ICC_REMOTE_PART_DEF_SIZE;
+EXPORT_SYMBOL(transcede_icc_remote_part_size);
+unsigned long transcede_icc_virt;
+EXPORT_SYMBOL(transcede_icc_virt);
+unsigned long transcede_icc_amp_virt;
+EXPORT_SYMBOL(transcede_icc_amp_virt);
+
+
+/* IRAM */
+unsigned long transcede_reserved_size = TRANSCEDE_RESERVED_SEC_SIZE;
+EXPORT_SYMBOL(transcede_reserved_size);
+
+static int __init cram_offset_change(char *str)
+{
+	unsigned long tmp = simple_strtoul(str, NULL, 0);
+
+	if (tmp < TRANSCEDE_CRAM_SIZE) {
+		transcede_cram_heap_offset = tmp;
+	}
+
+	return 0;
+}
+
+__setup("cram_offset=", cram_offset_change);
+
+static int __init set_icc_amp_heap_size(char *str)
+{
+	unsigned long size = memparse(str, NULL);
+
+	if (size > 0) {
+		transcede_icc_amp_heap_size = size;
+	}
+
+	return 0;
+}
+
+__setup("icc_amp_heap_size=", set_icc_amp_heap_size);
+
+static int __init set_icc_heap_size(char *str)
+{
+	unsigned long size = memparse(str, NULL);
+
+	if (size > 0) {
+		transcede_icc_heap_size = size;
+	}
+
+	return 0;
+}
+
+static int __init set_icc_part_size(char *str)
+{
+	unsigned long size = memparse(str, NULL);
+
+	if (size > 0) {
+		transcede_icc_local_part_size = size;
+		transcede_icc_remote_part_size = size;
+	}
+
+	return 0;
+}
+
+__setup("icc_heap_size=", set_icc_heap_size);
+__setup("icc_part_size=", set_icc_part_size);
+
+static int __init set_ddr_heap_size(char *str)
+{
+	unsigned long size = memparse(str, NULL);
+
+	if (size > 0) {
+		transcede_ddr_heap_size = size;
+	}
+
+	return 0;
+}
+
+__setup("ddr_heap_size=", set_ddr_heap_size);
+
+static int __init set_ddr_ceva_shared_size(char *str)
+{
+	unsigned long size = memparse(str, NULL);
+
+	if (size > 0) {
+		transcede_ddr_ceva_shared_size = size;
+	}
+
+	if (transcede_ddr_ceva_shared_size > TRANSCEDE_DDR_CEVA_SHARED_MAX_SIZE) {
+		printk(KERN_WARNING "ddr_ceva_size=%lu is too big, truncating\n", transcede_ddr_ceva_shared_size);
+		transcede_ddr_ceva_shared_size = TRANSCEDE_DDR_CEVA_SHARED_MAX_SIZE;
+	}
+
+	return 0;
+}
+
+early_param("ddr_ceva_size", set_ddr_ceva_shared_size);
+
+// GEMAC1 (RGMII) usage forced for Slave cluster
+static unsigned rgmii = 0;
+
+static int __init force_rgmii(char *unused)
+{
+        rgmii = 1;
+        return 0;
+}
+
+__setup("rgmii", force_rgmii);
+
+/* --------------------------------------------------------------------
+ *  Serial interface
+ * -------------------------------------------------------------------- */
+#if defined(CONFIG_TRANSCEDE_UART0_SUPPORT) || defined(CONFIG_TRANSCEDE_UART1_SUPPORT)|| defined(CONFIG_TRANSCEDE_UART2_SUPPORT)
+static struct plat_serial8250_port transcede_uart_data[] = {
+#ifdef CONFIG_TRANSCEDE_UART0_SUPPORT
+	{
+		.mapbase	= TRANSCEDE_UART0,
+		.membase	= (void *)AAB_XP_VADDR(TRANSCEDE_UART0),
+		.irq		= IRQ_UART0,
+		.flags		= UPF_BOOT_AUTOCONF | UPF_SKIP_TEST,
+		.iotype		= UPIO_DWAPB,
+		.regshift	= 2,
+//		.uartclk	= TRANSCEDE_AXICLK_HZ, // it is set lated becaule TRANSCEDE_AXICLK_HZ is dyn variable
+		.private_data	= (void *)AAB_XP_VADDR(TRANSCEDE_UART0 + 0x7C),
+	},
+#endif
+#ifdef CONFIG_TRANSCEDE_UART1_SUPPORT
+	{
+		.mapbase	= TRANSCEDE_UART1,
+		.membase	= (void *)AAB_XP_VADDR(TRANSCEDE_UART1),
+		.irq		= IRQ_UART1,
+		.flags		= UPF_BOOT_AUTOCONF | UPF_SKIP_TEST,
+		.iotype		= UPIO_DWAPB,
+		.regshift	= 2,
+//		.uartclk	= TRANSCEDE_AXICLK_HZ, // it is set lated becaule TRANSCEDE_AXICLK_HZ is dyn variable
+		.private_data	= (void *)AAB_XP_VADDR(TRANSCEDE_UART1 + 0x7C),
+	},
+#endif
+	{
+		.flags = 0,
+	},
+};
+
+static struct platform_device transcede_uart = {
+	.name	= "serial8250",
+	.id	= PLAT8250_DEV_PLATFORM,
+	.dev = {
+		.platform_data	= transcede_uart_data,
+	},
+};
+#endif
+
+#ifdef CONFIG_MPCORE_WATCHDOG
+static struct resource transcede_wdt_resources[] = {
+	{
+		.name	= "wdt-irq",
+		.start	= IRQ_LOCALWDOG,
+		.flags	= IORESOURCE_IRQ,
+	},
+	{
+		.name	= "wdt-regs",
+		.start	= TRANSCEDE_TWD_BASE,
+		.end	= TRANSCEDE_TWD_BASE + 0xA00 - 1,
+		.flags	= IORESOURCE_MEM,
+	},
+};
+
+static struct platform_device transcede_wdt = {
+	.name	= "mpcore_wdt",
+	.id	= -1,
+	.resource = transcede_wdt_resources,
+	.num_resources = ARRAY_SIZE(transcede_wdt_resources),
+};
+#endif
+
+static struct platform_device *transcede_devices[] __initdata = {
+#if defined(CONFIG_TRANSCEDE_UART0_SUPPORT) || defined(CONFIG_TRANSCEDE_UART1_SUPPORT) || defined(CONFIG_TRANSCEDE_UART1_SUPPORT)
+	&transcede_uart,
+#endif
+#if defined(CONFIG_TRANSCEDE_IPSEC_HW_SUPPORT)
+	&transcede_espah_device,
+#endif
+#if defined(CONFIG_TRANSCEDE_ELP_CLP30)
+	&transcede_elp_clp30_device,
+#endif
+#if defined(CONFIG_TRANSCEDE_ELP_PDU)
+	&transcede_elp_pdu_device,
+#endif
+#if defined(CONFIG_TRANSCEDE_ELP_SPACC)
+	&transcede_elp_spacc_device,
+#endif
+#if defined(CONFIG_TRANSCEDE_ELP_TRNG)
+	&transcede_elp_trng_device,
+#endif
+#ifdef CONFIG_MPCORE_WATCHDOG
+	&transcede_wdt,
+#endif
+#ifdef CONFIG_PERF_EVENTS
+	&transcede_pmu,
+#endif	/* CONFIG_PERF_EVENTS */
+};
+
+extern void larm_init(void);
+
+#if defined(CONFIG_MTD_NAND_TRANSCEDE) || defined(CONFIG_MTD_NAND_TRANSCEDE_MODULE)
+static struct platform_device *transcede_nand_dev[] __initdata = {
+	&transcede_nand,
+};
+#endif
+
+#if defined(CONFIG_MTD_TRANSCEDE_NOR_8)
+static struct platform_device *transcede_nor8_flash_dev[] __initdata = {
+	&transcede_nor8_flash_device,
+};
+#endif
+
+#if defined(CONFIG_MTD_TRANSCEDE_NOR_16)
+static struct platform_device *transcede_nor16_flash_dev[] __initdata = {
+	&transcede_nor16_flash_device,
+};
+#endif
+
+#if defined(CONFIG_TRANSCEDE_GEMAC_0)
+static struct platform_device *transcede_eth0_dev[] __initdata = {
+	&transcede_eth0_device,
+};
+#endif
+
+#if defined(CONFIG_TRANSCEDE_GEMAC_1)
+static struct platform_device *transcede_eth1_dev[] __initdata = {
+	&transcede_eth1_device,
+};
+#endif
+
+#if defined(CONFIG_TRANSCEDE_GEM_PHY)
+static struct platform_device *transcede_mdio_dev[] __initdata = {
+	&transcede_mdio_device,
+};
+#endif
+
+#if defined(CONFIG_I2C)
+static struct platform_device *transcede_i2c_dev[] __initdata = {
+	&transcede_i2c,
+};
+#endif
+
+#if defined(CONFIG_TRANSCEDE_USIM_SUPPORT)
+static struct platform_device *transcede_usim_dev[] __initdata = {
+	&transcede_usim,
+};
+#endif
+
+#if defined(CONFIG_SPI_TRANSCEDE)
+static struct platform_device *transcede_spi0_dev[] __initdata = {
+	&transcede_spi0,
+};
+
+static struct platform_device *transcede_spi1_dev[] __initdata = {
+	&transcede_spi1,
+};
+#endif
+
+#if defined(CONFIG_USB_SUPPORT)
+static struct platform_device *transcede_usb_dev[] __initdata = {
+	&transcede_dwc_otg,
+};
+#endif	/* CONFIG_USB_SUPPORT */
+
+static struct resource * transcede_request_region(resource_size_t start,
+                                                  resource_size_t n,
+                                                  const char *name)
+{
+	struct resource * region = request_mem_region(start, n, name);
+
+	printk(KERN_INFO "%25s: start: 0x%08lx, end: 0x%08lx, size: %lu\n", name,
+	       (unsigned long)start, (unsigned long)(start + n - 1), (unsigned long)n);
+
+	if (!region) {
+		printk(KERN_ERR "error: memory region %s is already in use.\n", name);
+	}
+
+	return region;
+}
+
+extern int revision_init(void);
+void SysInfo(void);
+
+void print_world(void)
+{
+	if (readl(TRANSCEDE_SNSAC_BASE) == 0) {
+		printk(KERN_INFO "World: Secure\n");
+	} else {
+		printk(KERN_INFO "World: Normal\n");
+		transcede_reserved_size = TRANSCEDE_RESERVED_NORM_SIZE;
+	}
+}
+
+static int cram_proc_read_offset(char *page, char **start, off_t off,
+                                 int count, int *eof, void *data)
+{
+	int len = 0;
+
+	if (off > 0) {
+		return 0;
+	}
+
+	len += sprintf(page + len, "0x%lx\n", TRANSCEDE_CRAMHEAP_OFFSET);
+
+	*start = NULL;
+
+	if (count <= len) {
+		*eof = 1;
+	}
+
+	return len;
+}
+
+static int cram_proc_write_offset(struct file *file, const char __user *buffer,
+                                  unsigned long count, void *data)
+{
+	unsigned long offset = 0;
+	char *tmp = kmalloc(count + 1, GFP_KERNEL);
+
+	if (tmp == NULL) {
+		printk(KERN_ERR "no memory for tmp buffer\n");
+		return 0;
+	}
+
+	if (copy_from_user(tmp, buffer, count)) {
+		printk(KERN_ERR "copy failed: 0x%08lx\n", (unsigned long)buffer);
+		kfree(tmp);
+		return 0;
+	}
+
+	tmp[count] = '\0';
+
+	offset = simple_strtoul(tmp, NULL, 0);
+
+	if (offset < TRANSCEDE_CRAMHEAP_SIZE) {
+		transcede_cram_heap_offset = offset;
+		printk(KERN_INFO "CRAM heap recreating: 0x%08lX, 0x%08lX, 0x%X, %u",
+		       TRANSCEDE_CRAMHEAP, TRANSCEDE_CRAMHEAP_SIZE,
+		       TRANSCEDE_CRAMHEAP_ALIGN, TRANSCEDE_SYSLOCKID_CRAMHEAP);
+		cram_create_heap(TRANSCEDE_CRAMHEAP, TRANSCEDE_CRAMHEAP_SIZE,
+		                 TRANSCEDE_CRAMHEAP_ALIGN, TRANSCEDE_SYSLOCKID_CRAMHEAP);
+	} else {
+		printk(KERN_ERR "error: offset is too big: 0x%08lx\n", offset);
+	}
+
+	kfree(tmp);
+
+	return count;
+}
+
+static int cram_proc_read_info(char *page, char **start, off_t off,
+                               int count, int *eof, void *data)
+{
+	PHEAPDESC pHeap = (PHEAPDESC)TRANSCEDE_CRAMHEAP;
+	int len = 0;
+
+	if (off > 0) {
+		return 0;
+	}
+
+	len += sprintf(page + len, "Storage address: 0x%p\n", pHeap->HeapStorage);
+	len += sprintf(page + len, "Heap size (bytes): %lu\n", TRANSCEDE_CRAMHEAP_SIZE);
+	len += sprintf(page + len, "Free size (bytes): %u\n", pHeap->FreeSize);
+	len += sprintf(page + len, "Free size (%%): %lu%%\n", pHeap->FreeSize * 100UL / TRANSCEDE_CRAMHEAP_SIZE);
+
+	*start = NULL;
+
+	if (count <= len) {
+		*eof = 1;
+	}
+
+	return len;
+}
+
+int cram_set_proc(void)
+{
+	static struct proc_dir_entry *cram_proc;
+	static struct proc_dir_entry *cram_offset_proc;
+	static struct proc_dir_entry *cram_info_proc;
+
+	cram_proc = proc_mkdir("cram", NULL);
+	cram_offset_proc = create_proc_entry("offset", 0600, cram_proc);
+
+	cram_offset_proc->read_proc = cram_proc_read_offset;
+	cram_offset_proc->write_proc = cram_proc_write_offset;
+	cram_offset_proc->data = NULL;
+
+	cram_info_proc = create_proc_entry("info", 0400, cram_proc);
+	cram_info_proc->read_proc = cram_proc_read_info;
+	cram_info_proc->write_proc = NULL;
+	cram_info_proc->data = NULL;
+
+	return 0;
+}
+
+static void __init transcede_init(void)
+{
+	unsigned ddr_heap_virt;
+	int i;
+
+	/* Get board revision */
+	revision_init();
+
+	print_world();
+
+	/* Get each bloc clocks and print them */
+	printk("System clocks:\n");
+	SysInfo();
+
+	printk(KERN_INFO "%25s: start: 0x%08lx, end: 0x%08lx, size: %lu\n",
+	       "ddr ceva",
+	       (unsigned long)TRANSCEDE_DDR_CEVA_SHARED_BASE,
+	       (unsigned long)(TRANSCEDE_DDR_CEVA_SHARED_BASE + TRANSCEDE_DDR_CEVA_SHARED_SIZE - 1),
+	       TRANSCEDE_DDR_CEVA_SHARED_SIZE);
+
+	/* We set the order right here:
+	 * 1. DDR heap
+	 * 2. DDR CB heap
+	 * 3. ICX
+	 * 4. AMP
+	 * 5. ICC
+	 */
+
+	transcede_ddr_heap_base = transcede_ddr_common_base;
+	transcede_ddrcb_heap_base = TRANSCEDE_DDRHEAP_BASE + TRANSCEDE_DDRHEAP_SIZE;
+	transcede_icx_base = TRANSCEDE_DDRCBHEAP_BASE + TRANSCEDE_DDRCBHEAP_SIZE;
+	transcede_icc_amp_base = TRANSCEDE_ICX_BASE + TRANSCEDE_ICX_SIZE;
+#ifdef CONFIG_AMP_STACK
+	transcede_amp_base = TRANSCEDE_ICC_AMP_BASE + TRANSCEDE_ICC_AMP_SIZE;
+	transcede_icc_heap_base = TRANSCEDE_AMP_BASE + TRANSCEDE_AMP_SIZE;
+#else  /* !CONFIG_AMP_STACK */
+	transcede_icc_heap_base = TRANSCEDE_ICC_AMP_BASE + TRANSCEDE_ICC_AMP_SIZE;
+#endif /* CONFIG_AMP_STACK */
+
+	/* make sure nobody uses our regions */
+
+	transcede_request_region(TRANSCEDE_DDRHEAP_BASE, TRANSCEDE_DDRHEAP_SIZE, "ddr heap");
+	transcede_request_region(TRANSCEDE_DDRCBHEAP_BASE, TRANSCEDE_DDRCBHEAP_SIZE, "ddr cb heap");
+	transcede_request_region(TRANSCEDE_ICX_BASE, TRANSCEDE_ICX_SIZE, "icx");
+	transcede_request_region(TRANSCEDE_ICC_AMP_BASE, TRANSCEDE_ICC_AMP_SIZE, "icc amp");
+#ifdef CONFIG_AMP_STACK
+	transcede_request_region(TRANSCEDE_AMP_BASE, TRANSCEDE_AMP_SIZE, "amp");
+#endif	/* CONFIG_AMP_STACK */
+	transcede_request_region(TRANSCEDE_ICC_BASE, TRANSCEDE_ICC_SIZE, "icc");
+
+	/* Early ICC remap to guarantee identical virtual addresses on
+	 * both clusters */
+
+	transcede_icc_virt = (unsigned long)ioremap_nocache(TRANSCEDE_ICC_BASE,
+	                                                    TRANSCEDE_ICC_SIZE);
+	if (!transcede_icc_virt) {
+		printk(KERN_ERR "ICC remap failed\n");
+	}
+
+	transcede_icc_amp_virt = (unsigned long)ioremap_nocache(TRANSCEDE_ICC_AMP_BASE,
+	                                                        TRANSCEDE_ICC_AMP_SIZE);
+
+	if (!transcede_icc_amp_virt) {
+		printk(KERN_ERR "ICC AMP remap failed\n");
+	}
+
+	if (cram_set_proc()) {
+		printk(KERN_ERR "error: failed to setup /proc for CRAM\n");
+	}
+
+#ifdef CONFIG_AMP_STACK
+	if (mmu_protect_set_proc()) {
+		printk(KERN_ERR "error: failed to setup /proc for MMU protection\n");
+	}
+#endif	/* CONFIG_AMP_STACK */
+
+#ifdef TRANSCEDE_RADIO_INIT
+	/* MCLK reset */
+	*(volatile u32*)(0xF5E01010) = 3;
+	udelay(1000);
+	*(volatile u32*)(0xF5E01010) = 0;
+
+	if (gpio_request_one(RADIO_GPIO_RESET, GPIOF_DIR_OUT | GPIOF_INIT_LOW, "gpio-rad")) {
+		printk(KERN_ERR "error: failed to request RAD Reset GPIO\n");
+	} else {
+		mdelay(10);
+		gpio_set_value(RADIO_GPIO_RESET, 1);	/* out of reset */
+		mdelay(10);
+
+		t2200_gpio_pin_stat.t2200_gpio_pins |= GPIO_3; /* reserve */
+		gpio_free(RADIO_GPIO_RESET);							   /* free for sysfs */
+	}
+
+	/* T2200 for SPI interface, change default of SPI slave select
+	   from debug mode to enable Slave Select 3 for SPI bus 0 */
+	*(volatile u32*)(TRANSCEDE_MISC_PIN_SELECT) |= 1;
+	*(volatile u32*)(TRANSCEDE_PAD_CONFIG1) = 0x33;
+#endif	/* TRANSCEDE_RADIO_INIT */
+
+	printk("CRAM heap creating (0x%08lX, 0x%08lX, 0x%X, %u)...", TRANSCEDE_CRAMHEAP, TRANSCEDE_CRAMHEAP_SIZE, TRANSCEDE_CRAMHEAP_ALIGN, TRANSCEDE_SYSLOCKID_CRAMHEAP);
+	cram_create_heap(TRANSCEDE_CRAMHEAP, TRANSCEDE_CRAMHEAP_SIZE, TRANSCEDE_CRAMHEAP_ALIGN, TRANSCEDE_SYSLOCKID_CRAMHEAP);
+	printk(" done.\n");
+
+	printk("IRAM heap creating (0x%08X, 0x%08lX, 0x%X, %u)...", TRANSCEDE_IRAMHEAP, TRANSCEDE_IRAMHEAP_SIZE, TRANSCEDE_IRAMHEAP_ALIGN, TRANSCEDE_SYSLOCKID_IRAMHEAP);
+	iram_create_heap(TRANSCEDE_IRAMHEAP, TRANSCEDE_IRAMHEAP_SIZE, TRANSCEDE_IRAMHEAP_ALIGN, TRANSCEDE_SYSLOCKID_IRAMHEAP);
+	printk(" done.\n");
+
+	ddr_heap_virt = (unsigned long)ioremap_nocache(TRANSCEDE_DDRHEAP_BASE, TRANSCEDE_DDRHEAP_SIZE);
+
+	if (ddr_heap_virt) {
+		printk("DDR heap creating (0x%08lX (virt = 0x%08X), 0x%08lX, 0x%X, %u)...", TRANSCEDE_DDRHEAP_BASE, ddr_heap_virt, TRANSCEDE_DDRHEAP_SIZE, TRANSCEDE_DDRHEAP_ALIGN, TRANSCEDE_SYSLOCKID_DDRL1HEAP);
+		ddr_create_heap(ddr_heap_virt, TRANSCEDE_DDRHEAP_SIZE, TRANSCEDE_DDRHEAP_ALIGN, TRANSCEDE_SYSLOCKID_DDRL1HEAP);
+		printk(" done.\n");
+	} else {
+		printk("DDR heap allocation failed. (try changing vmalloc=<size>)\n");
+	}
+
+	ddr_heap_virt = (unsigned long)ioremap_nocache(TRANSCEDE_DDRCBHEAP_BASE, TRANSCEDE_DDRCBHEAP_SIZE);
+
+	if (ddr_heap_virt) {
+		printk("DDR CB heap creating (0x%08lX (virt = 0x%08X), 0x%08lX, 0x%X, %u)...", TRANSCEDE_DDRCBHEAP_BASE, ddr_heap_virt, TRANSCEDE_DDRCBHEAP_SIZE, TRANSCEDE_DDRCBHEAP_ALIGN, TRANSCEDE_SYSLOCKID_DDRL2HEAP);
+		ddrcb_create_heap(ddr_heap_virt, TRANSCEDE_DDRCBHEAP_SIZE, TRANSCEDE_DDRCBHEAP_ALIGN, TRANSCEDE_SYSLOCKID_DDRL2HEAP);
+		printk(" done.\n");
+	} else {
+		printk("DDR CB heap allocation failed.\n");
+	}
+
+	// Setting UART clk to AXI
+	for (i = 0; i < ARRAY_SIZE(transcede_uart_data) - 1; i++) {
+		transcede_uart_data[i].uartclk = TRANSCEDE_AXICLK_HZ;
+	}
+
+	// registering platfor devices
+	platform_add_devices(transcede_devices, ARRAY_SIZE(transcede_devices));
+
+//	larm_init();
+#if defined(CONFIG_SPI) && defined(CONFIG_SPI_TRANSCEDE)
+	spi_register_board_info(transcede_spi_info, ARRAY_SIZE(transcede_spi_info));
+#endif
+
+#if defined(CONFIG_TRANSCEDE_GEMAC_0)
+	platform_add_devices(transcede_eth0_dev, 1);
+#endif
+
+#if defined(CONFIG_TRANSCEDE_GEMAC_1)
+	if (!t3300_slave || rgmii) {
+		platform_add_devices(transcede_eth1_dev, 1);
+	}
+#endif
+
+	if (!t3300_slave) {
+		/* avoid registering NOR, NAND, GEMAC on t3300 slave device */
+#if defined(CONFIG_MTD_NAND_TRANSCEDE) || defined(CONFIG_MTD_NAND_TRANSCEDE_MODULE)
+		platform_add_devices(transcede_nand_dev, 1);
+#endif
+#if defined(CONFIG_MTD_TRANSCEDE_NOR_8)
+		platform_add_devices(transcede_nor8_flash_dev, 1);
+#endif
+#if defined(CONFIG_MTD_TRANSCEDE_NOR_16)
+		platform_add_devices(transcede_nor16_flash_dev, 1);
+#endif
+#if defined(CONFIG_I2C)
+		platform_add_devices(transcede_i2c_dev, 1);
+#endif
+#if defined(CONFIG_TRANSCEDE_USIM_SUPPORT)
+		platform_add_devices(transcede_usim_dev, 1);
+#endif
+#if defined(CONFIG_TRANSCEDE_GEM_PHY)
+		platform_add_devices(transcede_mdio_dev, 1);
+#endif
+#if defined(CONFIG_USB_SUPPORT)
+		platform_add_devices(transcede_usb_dev, ARRAY_SIZE(transcede_usb_dev));
+#endif	/* CONFIG_USB_SUPPORT */
+	}
+
+#if defined(CONFIG_SPI_TRANSCEDE)
+	platform_add_devices(transcede_spi0_dev, ARRAY_SIZE(transcede_spi0_dev));
+	platform_add_devices(transcede_spi1_dev, ARRAY_SIZE(transcede_spi1_dev));
+#endif
+
+#if defined(CONFIG_MTD_NAND_TRANSCEDE)
+	nand_ale_base = (u32) ioremap(NAND_BASE_ADDR | NAND_ADDR_ALE, 4096);
+	nand_cle_base = (u32) ioremap(NAND_BASE_ADDR | NAND_ADDR_CLE, 4096);
+#endif
+
+
+	// GPIO init for T3k ICCOM
+	if (t3300_slave) {
+		if (gpio_request_one(ICCOM_S_GPIO_TX, GPIOF_DIR_OUT | GPIOF_INIT_LOW, "gpio-iccom-tx")) {
+			printk(KERN_ERR "error: failed to request ICCOM TX GPIO\n");
+		} else {
+			t2200_gpio_pin_stat.t2200_gpio_pins |= (1 << ICCOM_S_GPIO_TX); /* reserve */
+			gpio_free(ICCOM_S_GPIO_TX);	/* free for sysfs */
+		}
+
+		if (gpio_request_one(ICCOM_S_GPIO_RX, GPIOF_DIR_IN, "gpio-iccom-rx")) {
+			printk(KERN_ERR "error: failed to request ICCOM RX GPIO\n");
+		} else {
+			t2200_gpio_pin_stat.t2200_gpio_pins |= (1 << ICCOM_S_GPIO_RX); /* reserve */
+			gpio_free(ICCOM_S_GPIO_RX);	/* free for sysfs */
+		}
+
+		REG32UPD(TRANSCEDE_GPIO_INT_CFG_REG, 3 << (ICCOM_S_GPIO_RX << 1), 1 << (ICCOM_S_GPIO_RX << 1));	// rising edge
+		REG32(TRANSCEDE_GPIO_INT_CLEAR_REG) |= (1 << ICCOM_S_GPIO_RX);					// clear INT status if any
+		REG32(TRANSCEDE_GPIO_INT_MASK_REG) |= (1 << ICCOM_S_GPIO_RX);					// enable interrupts for this GPIO
+	} else {
+		if (gpio_request_one(ICCOM_GPIO_TX, GPIOF_DIR_OUT | GPIOF_INIT_LOW, "gpio-iccom-tx")) {
+			printk(KERN_ERR "error: failed to request ICCOM TX GPIO\n");
+		} else {
+			t2200_gpio_pin_stat.t2200_gpio_pins |= (1 << ICCOM_GPIO_TX); /* reserve */
+			gpio_free(ICCOM_GPIO_TX);	/* free for sysfs */
+		}
+
+		if (gpio_request_one(ICCOM_GPIO_RX, GPIOF_DIR_IN, "gpio-iccom-rx")) {
+			printk(KERN_ERR "error: failed to request ICCOM RX GPIO\n");
+		} else {
+			t2200_gpio_pin_stat.t2200_gpio_pins |= (1 << ICCOM_GPIO_RX); /* reserve */
+			gpio_free(ICCOM_GPIO_RX);	/* free for sysfs */
+		}
+
+		REG32UPD(TRANSCEDE_GPIO_INT_CFG_REG, 3 << (ICCOM_GPIO_RX << 1), 1 << (ICCOM_GPIO_RX << 1));	// rising edge
+		REG32(TRANSCEDE_GPIO_INT_CLEAR_REG) |= (1 << ICCOM_GPIO_RX);					// clear INT status if any
+		REG32(TRANSCEDE_GPIO_INT_MASK_REG) |= (1 << ICCOM_GPIO_RX);					// enable interrupts for this GPIO
+	}
+
+	/* Setup DDR controller protection */
+	ddr_protect_ports();
+
+	if (ddr_protect_set_proc()) {
+		printk(KERN_ERR "error: failed to setup /proc for DDR protection\n");
+	}
+}
+
+unsigned long transcede_axi_clk_hz;
+EXPORT_SYMBOL(transcede_axi_clk_hz);
+unsigned long transcede_arm_clk_hz;
+EXPORT_SYMBOL(transcede_arm_clk_hz);
+unsigned long transcede_arm_peripheral_clk_hz;
+EXPORT_SYMBOL(transcede_arm_peripheral_clk_hz);
+
+static void __init transcede_reserve(void)
+{
+	/* Allocate DDR block used by CEVAs, the base address is fixed.
+	 * BUG() on failure, cannot proceed further. */
+
+	if (memblock_reserve(TRANSCEDE_DDR_CEVA_SHARED_BASE,
+	                     TRANSCEDE_DDR_CEVA_SHARED_SIZE) < 0)
+	{
+		BUG();
+	}
+
+	if (memblock_free(TRANSCEDE_DDR_CEVA_SHARED_BASE,
+	                  TRANSCEDE_DDR_CEVA_SHARED_SIZE) < 0)
+	{
+		BUG();
+	}
+
+	if (memblock_remove(TRANSCEDE_DDR_CEVA_SHARED_BASE,
+	                    TRANSCEDE_DDR_CEVA_SHARED_SIZE) < 0)
+	{
+		BUG();
+	}
+}
+
+void SysInfo(void)
+{
+	const char* BootModeNames[] = {"SPI", "I2C", "PCIe", "UART", "<?>", "<?>", "NOR8", "NOR16"};
+
+	printk("PLL0    %4d, PLL1    %4d, PLL2   %4d, PLL3    %4d\n",
+			FREQ_MHZ(CR_PLL_0), FREQ_MHZ(CR_PLL_1), FREQ_MHZ(CR_PLL_2), FREQ_MHZ(CR_PLL_3));
+	printk("ARM     %4d, ACP     %4d, PERIP  %4d, L2CC    %4d\n",
+			FREQ_MHZ(CR_CA9_MC), FREQ_MHZ(CR_CA9_MC_MPU_ACP), FREQ_MHZ(CR_CA9_MC_MPU_PERIPH), FREQ_MHZ(CR_L2CC));
+	printk("CEVA    %4d, FFT     %4d, CRP    %4d\n",
+			FREQ_MHZ(CR_CEVA), FREQ_MHZ(CR_FFT), FREQ_MHZ(CR_CRP));
+	printk("IPSEC   %4d, SPACC   %4d\n",
+			FREQ_MHZ(CR_IPSEC), FREQ_MHZ(CR_SPACC));
+	printk("FEC DL  %4d, FEC UL  %4d\n",
+			FREQ_MHZ(CR_FEC_DL), FREQ_MHZ(CR_FEC_UL));
+	printk("SYS AXI %4d, CEVA BM %4d\n",
+			FREQ_MHZ(CR_SYS_AXI), FREQ_MHZ(CR_CEVA_BM_AXI));
+	printk("DDR     %4d\n",
+			FREQ_MHZ(CR_DDR3) * 2);
+	printk("L2CC to AXI clk: %sSYNC\n", (REG32(SYSCONF_STAT_REG) & (1 << 3)) ? "" : "A");
+	printk("Boot mode:       %s\n", BootModeNames[REG32(SYSCONF_STAT_REG) & 7]);
+
+	transcede_axi_clk_hz = ClkRstGetFreq(CR_SYS_AXI);
+	transcede_arm_clk_hz = ClkRstGetFreq(CR_CA9_MC);
+	transcede_arm_peripheral_clk_hz = ClkRstGetFreq(CR_CA9_MC_MPU_PERIPH);
+}
+
+MACHINE_START(TRANSCEDE, "Transcede 2200/3300")
+	/* Maintainer: Intel Corporation */
+	.boot_params  = PHYS_OFFSET + 0x00000100,
+	.map_io       = transcede_map_io,
+	.init_irq     = transcede_init_irq,
+	.timer        = &transcede_timer,
+	.init_machine = transcede_init,
+	.reserve      = transcede_reserve,
+MACHINE_END
diff --git a/arch/arm/mach-transcede/transcede-4000.c b/arch/arm/mach-transcede/transcede-4000.c
new file mode 100644
index 0000000..7f1c0d5
--- /dev/null
+++ b/arch/arm/mach-transcede/transcede-4000.c
@@ -0,0 +1,1292 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#include <linux/clockchips.h>
+#include <linux/i2c.h>
+#include <linux/init.h>
+#include <linux/irq.h>
+#include <linux/memblock.h>
+#include <linux/serial_8250.h>
+#include <linux/smp.h>
+#include <linux/i2c/at24.h>
+#include <linux/spi/spi.h>
+#include <linux/uio_driver.h>
+#include <linux/mtd/mtd.h>
+#include <linux/mtd/physmap.h>
+#include <linux/mtd/partitions.h>
+#include <linux/mtd/nand.h>
+
+#include <asm/localtimer.h>
+#include <asm/mach-types.h>
+#include <asm/sched_clock.h>
+#include <asm/sections.h>
+#include <asm/smp_twd.h>
+#include <asm/hardware/gic.h>
+#include <asm/hardware/cache-l2x0.h>
+#include <asm/pmu.h>
+
+#include <asm/mach/arch.h>
+#include <asm/mach/map.h>
+#include <asm/mach/time.h>
+
+#include <mach/hardware.h>
+#include <mach/spa.h>
+
+int transcede_revision;
+
+static struct map_desc transcede_io_desc[] __initdata = {
+	{
+		.virtual	= AAB_XP_BASE_VADDR,
+		.pfn		= __phys_to_pfn(TRANSCEDE_AAB_XP_BASE),
+		.length		= TRANSCEDE_AAB_XP_SIZE,
+		.type		= MT_DEVICE
+	},
+
+	{
+		.virtual	= TRANSCEDE_SCU_BASE,
+		.pfn		= __phys_to_pfn(TRANSCEDE_SCU_BASE),
+		.length		= SZ_128K,
+		.type		= MT_DEVICE,
+	}, 
+
+	{
+		.virtual	= IRAM_BASE_VADDR,
+		.pfn		= __phys_to_pfn(TRANSCEDE_IRAM_BASE),
+		.length		= TRANSCEDE_IRAM_SIZE,
+		.type		= MT_DEVICE
+	},
+
+	{
+		.virtual	= TRANSCEDE_SEMA_BASE,
+		.pfn		= __phys_to_pfn(TRANSCEDE_SEMA_BASE),
+		.length		= SZ_16K,
+		.type		= MT_DEVICE
+	},
+	{
+		.virtual	= TRANSCEDE_IPSEC_REG_BASE,
+		.pfn		= __phys_to_pfn(TRANSCEDE_IPSEC_REG_BASE),
+		.length		= SZ_512K,
+		.type		= MT_DEVICE
+	},
+
+
+};
+
+#if defined(CONFIG_TRANSCEDE_IPSEC_HW_SUPPORT)
+
+static struct resource transcede_espah_resources[] = {
+        {
+                .name = "base_core",
+                .start  = (TRANSCEDE_SPA_REG_BASE + TRANSCEDE_SPA_ESPAH_REG_OFFSET),
+                .end = (TRANSCEDE_SPA_REG_BASE + TRANSCEDE_SPA_ESPAH_REG_OFFSET + TRANSCEDE_SPA_ESPAH_REG_SIZE),
+                .flags  = IORESOURCE_MEM,
+        },
+
+        {
+                .name   = "irq_core",
+                .start  = IRQ_ESPAH,
+                .end    = IRQ_ESPAH,
+                .flags  = IORESOURCE_IRQ
+        }
+};
+
+static struct platform_device  transcede_espah_device = {
+    .name                   = "Transcede ESPAH",
+    .id                     = 0,
+    .num_resources          = ARRAY_SIZE(transcede_espah_resources),
+    .resource               = transcede_espah_resources,
+    .dev = {
+        .coherent_dma_mask      = 0xffffffff,
+    },
+};
+#endif
+
+#if defined(CONFIG_TRANSCEDE_ELP_CLP30)
+static struct resource transcede_elp_clp30_resources[] = {
+	{
+		.name	= "base_core",
+		.start	= TRANSCEDE_IPSEC_REG_BASE,
+		.end	= TRANSCEDE_IPSEC_REG_BASE + TRANSCEDE_SPA_ESPAH_REG_SIZE,
+		.flags	= IORESOURCE_MEM,
+	},
+
+	{
+		.name   = "irq_core",
+		.start  = IRQ_ESPAH,
+		.end    = IRQ_ESPAH,
+		.flags  = IORESOURCE_IRQ
+	}
+};
+
+static struct platform_device  transcede_elp_clp30_device = {
+	.name		= "clp30",
+	.num_resources	= ARRAY_SIZE(transcede_elp_clp30_resources),
+	.resource	= transcede_elp_clp30_resources,
+	.dev = {
+		.coherent_dma_mask      = 0xffffffff,
+	},
+};
+#endif
+
+#if defined(CONFIG_TRANSCEDE_ELP_PDU)
+static struct platform_device  transcede_elp_pdu_device = {
+	.name		= "elppdu",
+	.dev = {
+		.coherent_dma_mask      = 0xffffffff,
+	},
+};
+#endif
+
+#if defined(CONFIG_TRANSCEDE_ELP_TRNG)
+static struct resource transcede_elp_trng_resources[] = {
+	{
+		.name = "base_core",
+		.start  = TRNG_BASEADDR,
+		.end = TRNG_BASEADDR + TRANSCEDE_SPA_TRNG_REG_SIZE,
+		.flags  = IORESOURCE_MEM,
+	},
+};
+
+static struct platform_device  transcede_elp_trng_device = {
+	.name		= "trng",
+	.num_resources	= ARRAY_SIZE(transcede_elp_trng_resources),
+	.resource	= transcede_elp_trng_resources,
+};
+#endif
+
+struct transcede_eth_platform_data transcede_gem0_pdata = {
+	.name = "eth1",
+	.device_flags = CONFIG_TRANSCEDE_GEMAC,
+	.phy_flags = GEMAC_PHY_1000,
+	.bus_id = "0",
+	.phy_id = 0,
+	.gem_id = 0,
+	.mac_addr = (u8[]){ 0x00, 0xED, 0xCD, 0xEF, 0xAA, 0xCC },
+};
+
+struct transcede_eth_platform_data transcede_gem1_pdata = {
+	.name = "eth2",
+	.device_flags = CONFIG_TRANSCEDE_GEMAC,
+	.phy_flags = GEMAC_PHY_1000 | GEMAC_NO_PHY,
+	.bus_id = "0",
+	.phy_id = 0,
+	.gem_id = 1,
+	.mac_addr = (u8[]){ 0x00, 0x2A, 0x2B, 0x2C, 0x2D, 0x2E },
+};
+
+
+#if defined(CONFIG_TRANSCEDE_GEMAC_0)
+static struct resource transcede_eth0_resources[] = {
+	{
+		.name	= "gemac",
+		.start	= TRANSCEDE_GEM0,
+		.end	= TRANSCEDE_GEM0 + SZ_64K - 1,
+		.flags	= IORESOURCE_MEM,
+	},
+	{
+		.name	= "descriptors",
+		.start	= TRANSCEDE_IRAM_BASE,
+#if defined(CONFIG_TRANSCEDE_GEMAC_1)
+		.end	= TRANSCEDE_IRAM_BASE + 16*1024 - 1,
+#else
+		.end	= TRANSCEDE_IRAM_BASE + 32*1024 - 1,
+#endif
+		.flags  = IORESOURCE_MEM,
+	},
+	{
+		.name	= "irq",
+		.start	= IRQ_GEM0,
+		.flags	= IORESOURCE_IRQ,
+	},
+};
+
+static struct platform_device transcede_eth0_device = {
+	.name	= "c4000-eth",
+	.id	= 0,
+	.resource	= transcede_eth0_resources,
+	.num_resources	= ARRAY_SIZE(transcede_eth0_resources),
+	.dev = {
+		.platform_data = &transcede_gem0_pdata,
+	},
+};
+#endif
+
+#if defined(CONFIG_TRANSCEDE_GEMAC_1)
+static struct resource transcede_eth1_resources[] = {
+	{
+		.name	= "gemac",
+		.start	= TRANSCEDE_GEM1,
+		.end	= TRANSCEDE_GEM1 + SZ_64K - 1,
+		.flags	= IORESOURCE_MEM,
+	},
+	{
+		.name	= "descriptors",
+#if defined(CONFIG_TRANSCEDE_GEMAC_0)
+		.start	= TRANSCEDE_IRAM_BASE + 32*1024,
+		.end	= TRANSCEDE_IRAM_BASE + 64*1024 - 1,
+#else
+		.start	= TRANSCEDE_IRAM_BASE,
+		.end	= TRANSCEDE_IRAM_BASE + 32*1024 - 1,
+#endif
+		.flags  = IORESOURCE_MEM,
+	},
+	{
+		.name	= "irq",
+		.start	= IRQ_GEM1,
+		.flags	= IORESOURCE_IRQ,
+	},
+};
+
+static struct platform_device transcede_eth1_device = {
+	.name	= "c4000-eth",
+	.id	= 1,
+	.resource	= transcede_eth1_resources,
+	.num_resources	= ARRAY_SIZE(transcede_eth1_resources),
+	.dev = {
+		.platform_data = &transcede_gem1_pdata,
+	},
+};
+#endif
+
+#ifdef CONFIG_TRANSCEDE_GEM_PHY
+static struct transcede_mdio_data transcede_mdio_pdata = {
+	.phy_mask = 0xFFFFFFFC,
+	.mdc_div = 96,
+};
+
+static struct resource transcede_mdio_resources[] = {
+	{
+		.start	= TRANSCEDE_GEM0 + 0xE000,
+		.end	= TRANSCEDE_GEM0 + 0xE400 - 1,
+		.flags	= IORESOURCE_MEM,
+	},
+};
+
+static struct platform_device transcede_mdio_device = {
+	.name		= "transcede-mdio",
+	.id		= 0,
+	.resource	= transcede_mdio_resources,
+	.num_resources	= ARRAY_SIZE(transcede_mdio_resources),
+	.dev = {
+		.platform_data = &transcede_mdio_pdata,
+	},
+};
+#endif
+
+#ifdef CONFIG_GLOBAL_POLLING
+static int __cpuinit global_timer_setup(void)
+{
+	unsigned long long comparator, increment;
+
+	increment = CLOCK_TICK_RATE / 1000; /* 1ms */
+
+	writel(0, TRANSCEDE_TIMER_CTRL);
+
+	comparator = readl(TRANSCEDE_LOW_TIMER_COUNTER);
+	comparator += ((unsigned long long)readl(TRANSCEDE_UP_TIMER_COUNTER)) << 32;
+	comparator += increment;
+
+	writel(increment, TRANSCEDE_AUTO_INCREMENT);
+	writel(comparator, TRANSCEDE_LOW_COMPARATOR);
+	writel(comparator >> 32, TRANSCEDE_UP_COMPARATOR);
+	writel(0
+	       | GLOBAL_TIMER_AUTO_INC_ENA
+	       | GLOBAL_TIMER_IRQ_ENA
+	       | GLOBAL_TIMER_COMPARATOR_ENA
+	       | GLOBAL_TIMER_ENA,
+	       TRANSCEDE_TIMER_CTRL);
+
+	gic_enable_ppi(IRQ_GLOBALTIMER);
+
+	return 0;
+}
+#endif	/* CONFIG_GLOBAL_POLLING */
+
+#define NOR_BASE_ADDR	0xA0000000
+#define NAND_BASE_ADDR	0xA9000000
+#define NOR8_SIZE	(512*1024)
+#define NOR16_SIZE	(64*1024*1024)
+
+
+#if defined(CONFIG_MTD_TRANSCEDE_NOR_8)
+static struct mtd_partition transcede_nor8_parts[] = {
+	{
+		.name	= "boot",
+		.offset	= 0,
+		.size	= MTDPART_SIZ_FULL,
+		.mask_flags = MTD_WRITEABLE,
+	}
+};
+
+static struct physmap_flash_data transcede_nor8_data = {
+	.width = 1,
+	.parts = transcede_nor8_parts,
+	.nr_parts = ARRAY_SIZE(transcede_nor8_parts)
+};
+
+static struct resource transcede_nor8_resources[] = {
+	{
+		.start	= NOR_BASE_ADDR,
+		.end	= NOR_BASE_ADDR + NOR8_SIZE - 1,
+		.flags	= IORESOURCE_MEM
+	},
+};
+
+struct platform_device transcede_nor8_flash_device = {
+	.name		= "physmap-flash",
+	.id		= 0,
+	.resource	= transcede_nor8_resources,
+	.num_resources	= ARRAY_SIZE(transcede_nor8_resources),
+	.dev = {
+		.platform_data = &transcede_nor8_data
+	},
+};
+#endif
+
+
+#if defined(CONFIG_MTD_TRANSCEDE_NOR_16)
+static struct mtd_partition transcede_nor16_parts[] = {
+	{
+		.name = "boot",
+		.offset = 0,
+		.size = 512*1024,
+		.mask_flags = MTD_WRITEABLE,
+	}, {
+		.name = "kernel",
+		.size = 3*1024*1024,
+		.offset = MTDPART_OFS_APPEND,
+	}, {
+		.name = "fs",
+		.size = MTDPART_SIZ_FULL,
+		.offset = MTDPART_OFS_APPEND,
+	},
+};
+
+static struct physmap_flash_data transcede_nor16_data = {
+	.width = 2,
+	.parts = transcede_nor16_parts,
+	.nr_parts = ARRAY_SIZE(transcede_nor16_parts)
+};
+
+static struct resource transcede_nor16_resources[] = {
+	{
+		.start = NOR_BASE_ADDR,
+		.end = NOR_BASE_ADDR + NOR16_SIZE - 1,
+		.flags = IORESOURCE_MEM
+	},
+};
+
+struct platform_device transcede_nor16_flash_device = {
+	.name		= "physmap-flash",
+	.id		= 0,
+	.resource	= transcede_nor16_resources,
+	.num_resources	= ARRAY_SIZE(transcede_nor16_resources),
+	.dev = {
+		.platform_data = &transcede_nor16_data
+	},
+};
+#endif
+
+#if defined(CONFIG_MTD_NAND_TRANSCEDE)
+static struct mtd_partition transcede_nand_parts[] = {
+	{
+		.name	= "boot",
+		.offset	= 0,
+		.size	= 512*1024,
+		.mask_flags = MTD_WRITEABLE,
+	}, {
+		.name	= "kernel",
+		.size	= 3*1024*1024,
+		.offset	= MTDPART_OFS_APPEND,
+	}, {
+		.name	= "fs",
+		.size	= MTDPART_SIZ_FULL,
+		.offset	= MTDPART_OFS_APPEND,
+	}
+};
+
+static struct resource transcede_nand_resources[] = {
+	{
+		.start	= NAND_BASE_ADDR,
+		.end	= NAND_BASE_ADDR + 32 - 1,
+		.flags	= IORESOURCE_MEM,
+	}
+};
+
+static void transcede_nand_ctrl(struct mtd_info *mtd, int cmd, unsigned int ctrl)
+{
+	struct nand_chip *this = mtd->priv;
+
+	if (ctrl & NAND_CTRL_CHANGE) {
+		if (ctrl & NAND_NCE)
+			*(volatile u32*) TRANSCEDE_GPIO_OUTPUT_REG &= ~EXP_NAND_CS_N;
+		else
+			*(volatile u32*) TRANSCEDE_GPIO_OUTPUT_REG |= EXP_NAND_CS_N;
+
+		if (ctrl & NAND_CLE)
+			*(volatile u32*) TRANSCEDE_GPIO_OUTPUT_REG |= EXP_NAND_CLE;
+		else
+			*(volatile u32*) TRANSCEDE_GPIO_OUTPUT_REG &= ~EXP_NAND_CLE;
+
+		if (ctrl & NAND_ALE)
+			*(volatile u32*) TRANSCEDE_GPIO_EXP_MUX |= 1;
+		else
+			*(volatile u32*) TRANSCEDE_GPIO_EXP_MUX &= ~1;
+	}
+	if (cmd != NAND_CMD_NONE)
+		*(volatile u8*)this->IO_ADDR_W = cmd;
+}
+
+static int transcede_nand_dev_ready(struct mtd_info* mtd)
+{
+	return *(volatile u32*) TRANSCEDE_GPIO_INPUT_REG & EXP_RDY_BSY;
+}
+
+#ifdef CONFIG_MTD_PARTITIONS
+const char *part_probes[] = { "cmdlinepart", NULL };
+#endif
+
+struct platform_nand_data transcede_nand_data = {
+	.chip = {
+		.nr_chips = 1,
+		.chip_offset = 0,
+		.nr_partitions = ARRAY_SIZE(transcede_nand_parts),
+		.partitions = transcede_nand_parts,
+		.chip_delay = 20,
+#ifdef CONFIG_MTD_PARTITIONS
+		.part_probe_types = part_probes,
+#endif
+	},
+	.ctrl = {
+		.hwcontrol = 0,
+		.dev_ready = transcede_nand_dev_ready,
+		.select_chip = 0,
+		.cmd_ctrl = transcede_nand_ctrl,
+	}
+};
+
+static struct platform_device transcede_nand = {
+	.name		= "gen_nand",
+	.id		= -1,
+	.resource	= transcede_nand_resources,
+	.num_resources	= ARRAY_SIZE(transcede_nand_resources),
+	.dev = {
+		.platform_data = &transcede_nand_data,
+	},
+};
+#endif
+
+
+static void __init transcede_map_io(void)
+{
+	iotable_init(transcede_io_desc, ARRAY_SIZE(transcede_io_desc));
+}
+
+static void __init transcede_reserve(void)
+{
+	memblock_reserve(0, PAGE_SIZE);
+}
+
+extern irqreturn_t transcede_ipi_irq(int irq, void *dev);
+#ifdef CONFIG_TRANSCEDE_DUALCORE
+static struct irqaction ipi_irq = {
+	.name		= "IPI",
+	.flags		= IRQF_NO_THREAD,
+	.handler	= transcede_ipi_irq,
+};
+#endif
+
+static void __init transcede_init_irq(void)
+{
+	u32 mask;
+
+#ifdef CONFIG_GLOBAL_POLLING
+	gic_init(0, IRQ_GLOBALTIMER, __io_address(TRANSCEDE_GIC_DIST_BASE), __io_address(TRANSCEDE_GIC_CPU_BASE));
+#else  /* !CONFIG_GLOBAL_POLLING */
+	gic_init(0, IRQ_LOCALTIMER, __io_address(TRANSCEDE_GIC_DIST_BASE), __io_address(TRANSCEDE_GIC_CPU_BASE));
+#endif	/* CONFIG_GLOBAL_POLLING */
+
+	mask = 0;
+#ifdef CONFIG_TRANSCEDE_DUALCORE
+	setup_irq(IRQ_INTER_ARM0, &ipi_irq);
+	irq_set_affinity(IRQ_INTER_ARM0, cpumask_of(0));
+	setup_irq(IRQ_INTER_ARM1, &ipi_irq);
+	irq_set_affinity(IRQ_INTER_ARM1, cpumask_of(1));
+
+	/* enable inter-ARM0,1 SPIs */
+	mask = readl(TRANSCEDE_GIC_DIST_BASE + GIC_DIST_ENABLE_SET + 4);
+	mask |= 3;
+	writel(mask, TRANSCEDE_GIC_DIST_BASE + GIC_DIST_ENABLE_SET + 4);
+#endif
+}
+
+static cycle_t transcede_get_cycles(struct clocksource *cs)
+{
+	return readl(TRANSCEDE_LOW_TIMER_COUNTER);
+}
+
+static struct clocksource clocksource_transcede = {
+	.name = "global_timer",
+	.rating = 200,
+	.read = transcede_get_cycles,
+	.mask = CLOCKSOURCE_MASK(32),
+	.shift = 24,
+	.flags = CLOCK_SOURCE_IS_CONTINUOUS,
+};
+
+static DEFINE_CLOCK_DATA(cd);
+
+unsigned long long notrace sched_clock(void)
+{
+	return cyc_to_sched_clock(&cd, __raw_readl(TRANSCEDE_LOW_TIMER_COUNTER), (u32)~0);
+}
+
+static void notrace transcede_update_sched_clock(void)
+{
+	update_sched_clock(&cd, __raw_readl(TRANSCEDE_LOW_TIMER_COUNTER), (u32)~0);
+}
+
+void __init clocksource_init(void)
+{
+	/*Disable timer first*/
+	writel(0x00000000, TRANSCEDE_TIMER_CTRL);
+	/*Set counter to 0 */
+	writel(0x00000000, TRANSCEDE_LOW_TIMER_COUNTER);
+	writel(0x00000000, TRANSCEDE_UP_TIMER_COUNTER);
+	/*Re-enable timer */
+	writel(0x00000001, TRANSCEDE_TIMER_CTRL);
+
+	clocksource_transcede.mult = clocksource_khz2mult(CLOCK_TICK_RATE / 1000, clocksource_transcede.shift);
+	clocksource_register(&clocksource_transcede);
+
+	init_sched_clock(&cd, transcede_update_sched_clock, 32, CLOCK_TICK_RATE);
+}
+
+int __cpuinit local_timer_setup(struct clock_event_device *evt)
+{
+	/* if ::irq is set do not setup timer again */
+	if (!evt->irq) {
+		evt->irq = IRQ_LOCALTIMER;
+		twd_timer_setup(evt);
+#ifdef CONFIG_GLOBAL_POLLING
+		global_timer_setup();
+#endif	/* CONFIG_GLOBAL_POLLING */
+	}
+
+	return 0;
+}
+
+static void __init transcede_timer_init(void)
+{
+#ifdef CONFIG_GLOBAL_POLLING
+	periodic_task_setup();
+#endif	/* CONFIG_GLOBAL_POLLING */
+
+	clocksource_init();
+
+#ifdef CONFIG_LOCAL_TIMERS
+	twd_base = __io_address(TRANSCEDE_TWD_BASE);
+	twd_timer_rate = CLOCK_TICK_RATE;
+	percpu_timer_setup();
+#endif
+}
+
+static struct sys_timer transcede_timer = {
+	.init	= transcede_timer_init
+};
+
+#ifdef CONFIG_CACHE_L2X0
+
+static int l2x0_disabled = 0;
+static int l2x0_write_through = 0;
+
+// L2CC Aux Control bits
+#define L2CC_AUX_DOUBLE_LINE_FILL       (1<<30)
+#define L2CC_AUX_CODE_PREF              (1<<29)
+#define L2CC_AUX_DATA_PREF              (1<<28)
+#define L2CC_AUX_WRITE_ALLOC            (2<<23)
+#define L2CC_AUX_CTRL_EMBUS             (1<<20)
+
+// static int l2x0_aux_val = (L2CC_AUX_CODE_PREF | L2CC_AUX_DATA_PREF | L2CC_AUX_WRITE_ALLOC | L2CC_AUX_DOUBLE_LINE_FILL);
+static int l2x0_aux_val = 0;
+static int l2x0_aux_mask = 0;
+
+
+static int __init transcede_l2_cache_init(void)
+{
+	void __iomem *l2x0_base =__io_address(TRANSCEDE_L310_BASE);
+
+	if (l2x0_disabled)
+		return 0;
+
+	if (!(readl(l2x0_base + L2X0_CTRL) & 1)) {
+		writel(0x111, l2x0_base + L2X0_TAG_LATENCY_CTRL);
+		writel(0x111, l2x0_base + L2X0_DATA_LATENCY_CTRL);
+		if (l2x0_write_through)
+			writel(3, l2x0_base + L2X0_DEBUG_CTRL);
+	}
+
+	l2x0_init(l2x0_base, l2x0_aux_val, ~l2x0_aux_mask);
+	printk(KERN_INFO "l2x0: init with aux: %x, mask: %x\n", l2x0_aux_val, l2x0_aux_mask);
+
+	return 0;
+}
+early_initcall(transcede_l2_cache_init);
+
+static int __init l2x0_disable(char *unused)
+{
+	l2x0_disabled = 1;
+
+	return 0;
+}
+static int __init l2x0_force_write_through(char *unused)
+{
+	l2x0_write_through = 1;
+
+	return 0;
+}
+early_param("nol2x0", l2x0_disable);
+early_param("l2x0wt", l2x0_force_write_through);
+
+static int __init set_l2x0_aux_val(char *str)
+{
+        unsigned long tmp = simple_strtoul(str, NULL, 0);
+        l2x0_aux_val = (__u32)tmp;
+
+        return 0;
+}
+
+__setup("l2x0_aux_val=", set_l2x0_aux_val);
+
+static int __init set_l2x0_aux_mask(char *str)
+{
+        unsigned long tmp = simple_strtoul(str, NULL, 0);
+        l2x0_aux_mask = (__u32)tmp;
+
+        return 0;
+}
+
+__setup("l2x0_aux_mask=", set_l2x0_aux_mask);
+
+#endif
+
+static char ddr_protection_enable = 0;
+
+static int ddr_protection(char *unused)
+{
+	ddr_protection_enable = 1;
+	return 0;
+};
+
+static void __init transcede_enable_ddr_protection(void)
+{
+	volatile u8 *ddr0_cfg;
+	u32 start, start_phys;
+	u32 end, end_phys;
+
+	if (!ddr_protection_enable)
+		return;
+
+	ddr0_cfg = ioremap_nocache(TRANSCEDE_DDR0_CFG, PAGE_SIZE);
+
+	if (!ddr0_cfg)
+		return;
+
+	/* calculate start/end addresses assuming 64K granularity */
+	start_phys = virt_to_phys(_text);
+	end_phys = virt_to_phys(_etext - 1);
+
+	start = start_phys >> 16;
+	if (start_phys & 0xFFFF)
+		start++;	/* round up to avoid protecting pages below the start */
+
+	end = end_phys >> 16;
+	if ((end_phys & 0xFFFF) != 0xFFFF)
+		end--;		/* round down to avoid protecting pages above the end */
+
+	if (start > end)
+		return;
+
+	start <<= 16;
+	end <<= 16;
+
+	printk(KERN_ERR "DDR Protection: enabled for 0x%08X..0x%08X range (kernel text is 0x%08X..0x%08X)\n",
+		start, end, start_phys, end_phys);
+
+	/* disable protection */
+	ddr0_cfg[0x054] = 0;
+
+	/* setup start address */
+	ddr0_cfg[0x1B9] = 0;
+	ddr0_cfg[0x1BA] = 0;
+	ddr0_cfg[0x1BB] = start >> 16;
+	ddr0_cfg[0x1BC] = start >> 24;
+	ddr0_cfg[0x1BD] = 0;
+
+	/* setup end address */
+	ddr0_cfg[0x1C3] = 0;
+	ddr0_cfg[0x1C4] = 0;
+	ddr0_cfg[0x1C5] = end >> 16;
+	ddr0_cfg[0x1C6] = end >> 24;
+	ddr0_cfg[0x1C7] = 0;
+
+	/* enable protection */
+	ddr0_cfg[0x54] = 1;
+
+	iounmap(ddr0_cfg);
+}
+
+
+early_param("ddrwp", ddr_protection);
+
+/* --------------------------------------------------------------------
+ *  Serial interface
+ * -------------------------------------------------------------------- */
+#if defined(CONFIG_TRANSCEDE_UART0_SUPPORT) || defined(CONFIG_TRANSCEDE_UART1_SUPPORT)|| defined(CONFIG_TRANSCEDE_UART2_SUPPORT)
+static struct plat_serial8250_port transcede_uart_data[] = {
+#ifdef CONFIG_TRANSCEDE_UART0_SUPPORT
+	{
+		.mapbase	= TRANSCEDE_UART0,
+		.membase	= (void *)AAB_XP_VADDR(TRANSCEDE_UART0),
+		.irq		= IRQ_UART0,
+		.flags		= UPF_BOOT_AUTOCONF | UPF_SKIP_TEST,
+		.iotype		= UPIO_DWAPB,
+		.regshift	= 2,
+		.uartclk	= TRANSCEDE_AHBCLK_HZ,
+		.private_data	= (void *)AAB_XP_VADDR(TRANSCEDE_UART0 + 0x7C),
+	},
+#endif
+#ifdef CONFIG_TRANSCEDE_UART1_SUPPORT
+	{
+		.mapbase	= TRANSCEDE_UART1,
+		.membase	= (void *)AAB_XP_VADDR(TRANSCEDE_UART1),
+		.irq		= IRQ_UART1,
+		.flags		= UPF_BOOT_AUTOCONF | UPF_SKIP_TEST,
+		.iotype		= UPIO_DWAPB,
+		.regshift	= 2,
+		.uartclk	= TRANSCEDE_AHBCLK_HZ,
+		.private_data	= (void *)AAB_XP_VADDR(TRANSCEDE_UART1 + 0x7C),
+	},
+#endif
+#ifdef CONFIG_TRANSCEDE_UART2_SUPPORT
+	{
+		.mapbase	= TRANSCEDE_UART2,
+		.membase	= (void *)AAB_XP_VADDR(TRANSCEDE_UART2),
+		.irq		= IRQ_UART2,
+		.flags		= UPF_BOOT_AUTOCONF | UPF_SKIP_TEST,
+		.iotype		= UPIO_DWAPB,
+		.regshift	= 2,
+		.uartclk	= TRANSCEDE_AHBCLK_HZ,
+		.private_data	= (void *)AAB_XP_VADDR(TRANSCEDE_UART2 + 0x7C),
+	},
+#endif
+	{
+		.flags = 0,
+	},
+};
+
+static struct platform_device transcede_uart = {
+	.name	= "serial8250",
+	.id	= PLAT8250_DEV_PLATFORM,
+	.dev = {
+		.platform_data	= transcede_uart_data,
+	},
+};
+#endif
+
+
+/* --------------------------------------------------------------------
+ *  UIO configuration
+ * -------------------------------------------------------------------- */
+static struct resource ddr0_res = {
+	.name = "ddr0",
+	.start = TRANSCEDE_DDR0_BASE,
+	.end = TRANSCEDE_DDR0_BASE + TRANSCEDE_DDR0_SIZE - 1,
+	.flags = IORESOURCE_MEM
+};
+
+static struct uio_info uio_ddr0_info = {
+	.name = "ddr0",
+	.version = "0.1.5",
+	.irq = UIO_IRQ_CUSTOM
+};
+
+static struct platform_device transcede_ddr0_uio = {
+	.name 	= "uio_pdrv",
+	.id	= 0,
+	.dev	= {
+		.platform_data = &uio_ddr0_info,
+	},
+	.resource = &ddr0_res,
+	.num_resources = 1,
+};
+
+static struct resource ddr1_res = {
+	.name = "ddr1",
+	.start = TRANSCEDE_DDR1_BASE,
+	.end = TRANSCEDE_DDR1_BASE + TRANSCEDE_DDR1_SIZE - 1,
+	.flags = IORESOURCE_MEM
+};
+
+static struct uio_info uio_ddr1_info = {
+	.name = "ddr1",
+	.version = "0.1.5",
+	.irq = UIO_IRQ_CUSTOM
+};
+
+static struct platform_device transcede_ddr1_uio = {
+	.name 	= "uio_pdrv",
+	.id	= 1,
+	.dev	= {
+		.platform_data = &uio_ddr1_info,
+	},
+	.resource = &ddr1_res,
+	.num_resources = 1,
+};
+
+static struct resource cram_res = {
+	.name = "cram",
+	.start = TRANSCEDE_CRAM_BASE,
+	.end = TRANSCEDE_CRAM_BASE + TRANSCEDE_CRAM_SIZE - 1,
+	.flags = IORESOURCE_MEM
+};
+
+static struct uio_info uio_cram_info = {
+	.name = "cram",
+	.version = "0.1.5",
+	.irq = UIO_IRQ_CUSTOM
+};
+
+static struct platform_device transcede_cram_uio = {
+	.name 	= "uio_pdrv",
+	.id	= 2,
+	.dev	= {
+		.platform_data = &uio_cram_info,
+	},
+	.resource = &cram_res,
+	.num_resources = 1,
+};
+
+static struct resource iram_res = {
+	.name = "iram",
+	.start = TRANSCEDE_IRAM_BASE,
+	.end = TRANSCEDE_IRAM_BASE + TRANSCEDE_IRAM_SIZE - 1,
+	.flags = IORESOURCE_MEM
+};
+
+static struct uio_info uio_iram_info = {
+	.name = "iram",
+	.version = "0.1.5",
+	.irq = UIO_IRQ_CUSTOM
+};
+
+static struct platform_device transcede_iram_uio = {
+	.name 	= "uio_pdrv",
+	.id	= 3,
+	.dev	= {
+		.platform_data = &uio_iram_info, 
+	},
+	.resource = &iram_res,
+	.num_resources = 1,
+};
+
+static struct resource jram_res = {
+	.name = "jram",
+	.start = TRANSCEDE_JRAM_BASE,
+	.end = TRANSCEDE_JRAM_BASE + TRANSCEDE_JRAM_SIZE - 1,
+	.flags = IORESOURCE_MEM
+};
+
+static struct uio_info uio_jram_info = {
+	.name = "jram",
+	.version = "0.1.5",
+	.irq = UIO_IRQ_CUSTOM
+};
+
+static struct platform_device transcede_jram_uio = {
+	.name 	= "uio_pdrv",
+	.id	= 4,
+	.dev	= {
+		.platform_data = &uio_jram_info,
+	},
+	.resource = &jram_res,
+	.num_resources = 1,
+};
+
+static struct resource xp_timer_res = {
+	.name = "xp_timer",
+	.start = TRANSCEDE_TIMER,
+	.end = TRANSCEDE_TIMER + SZ_64K - 1,
+	.flags = IORESOURCE_MEM
+};
+
+static struct uio_info uio_xp_timer_info = {
+	.name = "xp_timer",
+	.version = "0.1.5",
+	.irq = UIO_IRQ_CUSTOM
+};
+
+static struct platform_device transcede_xp_timer_uio = {
+	.name 	= "uio_pdrv",
+	.id	= 5,
+	.dev	= {
+		.platform_data = &uio_xp_timer_info,
+	},
+	.resource = &xp_timer_res,
+	.num_resources = 1,
+};
+
+static struct resource sema_res = {
+	.name = "semaphore",
+	.start = TRANSCEDE_SEMA_BASE,
+	.end =  TRANSCEDE_SEMA_BASE + TRANSCEDE_SEMA_SIZE - 1,
+	.flags = IORESOURCE_MEM
+};
+
+static struct uio_info uio_sema_info = {
+	.name = "semaphore",
+	.version = "0.1.5",
+	.irq = UIO_IRQ_CUSTOM
+};
+
+static struct platform_device transcede_sema_uio = {
+	.name 	= "uio_pdrv",
+	.id	= 6,
+	.dev	= {
+		.platform_data = &uio_sema_info,
+	},
+	.resource = &sema_res,
+	.num_resources = 1,
+};
+
+static struct resource intc_res = {
+	.name = "intc",
+	.start = TRANSCEDE_CFG_SYS,
+	.end =  TRANSCEDE_CFG_SYS + SZ_64K - 1,
+	.flags = IORESOURCE_MEM
+};
+
+static struct uio_info uio_intc_info = {
+	.name = "intc",
+	.version = "0.1.5",
+	.irq = UIO_IRQ_CUSTOM
+};
+
+static struct platform_device transcede_intc_uio = {
+	.name 	= "uio_pdrv",
+	.id	= 7,
+	.dev	= {
+		.platform_data = &uio_intc_info,
+	},
+	.resource = &intc_res,
+	.num_resources = 1,
+};
+
+#if defined(CONFIG_SPI_TRANSCEDE)
+static struct resource transcede_spi0_resources[] = {
+        {
+                .start	= TRANSCEDE_SPI,
+                .end	= TRANSCEDE_SPI + SZ_4K - 1,
+                .flags	= IORESOURCE_MEM,
+        },
+        {
+                .start	= IRQ_SPI,
+                .flags	= IORESOURCE_IRQ,
+        },
+};
+
+static struct platform_device transcede_spi0 = {
+        .name = "transcede_spi",
+        .id = 0,
+        .num_resources = ARRAY_SIZE(transcede_spi0_resources),
+        .resource = transcede_spi0_resources,
+};
+
+static struct spi_board_info transcede_spi_info[] __initdata = {
+#if defined(CONFIG_SPI_CDCE62005) || defined(CONFIG_SPI_CDCE62005_MODULE)
+	{
+		.modalias = "cdce62005",
+		.chip_select = SPI_DEJITTER_CS,
+		.max_speed_hz = 4000000,
+		.bus_num = 0,
+		.irq = -1,
+		.mode = SPI_MODE_0,
+	},
+#else
+	{
+		.modalias = "spidev",
+		.chip_select = SPI_DEJITTER_CS,
+		.max_speed_hz = 4000000,
+		.bus_num = 0,
+		.mode = SPI_MODE_0,
+	},
+#endif
+};
+#endif
+
+#ifdef CONFIG_I2C
+static struct resource transcede_i2c_resources[] = {
+	{
+		.start  = TRANSCEDE_I2C,
+		.end    = TRANSCEDE_I2C + SZ_4K - 1,
+		.flags  = IORESOURCE_MEM,
+	},
+	{
+		.start  = IRQ_I2C,
+		.flags  = IORESOURCE_IRQ,
+	},
+};
+static struct platform_device transcede_i2c = {
+	.name		= "transcede_i2c",
+	.id		= 0,
+	.num_resources	= ARRAY_SIZE(transcede_i2c_resources),
+	.resource	= transcede_i2c_resources,
+};
+
+static struct at24_platform_data transcede_i2c_eeprom = {
+	.byte_len	= 64*1024,
+	.page_size	= 128,
+	.flags		= AT24_FLAG_ADDR16,
+};
+
+static struct i2c_board_info __initdata transcede_i2c_info[] = {
+	{
+		I2C_BOARD_INFO("at24", 0x50),
+		.platform_data = &transcede_i2c_eeprom,
+	},
+};
+#endif
+
+/* Unfortunately there is a single PMU IRQ for all cores,
+and this doesn't fit well with the generic PMU support.
+We declare a single PMU IRQ, but then hack the code
+to be able to set it's affinity manually and profile
+any of the cores (still with the restriction of only
+one core at a time) */
+#ifdef CONFIG_TRANSCEDE_DUALCORE
+static struct resource transcede_pmu_resources[] = {
+	{
+		.start	= IRQ_ARM2_MONITOR,
+		.end	= IRQ_ARM2_MONITOR,
+		.flags	= IORESOURCE_IRQ,
+	},
+};
+#else
+static struct resource transcede_pmu_resources[] = {
+	{
+		.start	= IRQ_ARM4_MONITOR,
+		.end	= IRQ_ARM4_MONITOR,
+		.flags	= IORESOURCE_IRQ,
+	},
+};
+#endif
+
+static struct platform_device transcede_pmu = {
+	.name		= "arm-pmu",
+	.id		= ARM_PMU_DEVICE_CPU,
+	.num_resources	= ARRAY_SIZE(transcede_pmu_resources),
+	.resource	= transcede_pmu_resources,
+};
+
+
+#ifdef CONFIG_MPCORE_WATCHDOG
+static struct resource transcede_wdt_resources[] = {
+	{
+	        .name	= "wdt-irq",
+		.start	= IRQ_LOCALWDOG,
+		.flags	= IORESOURCE_IRQ,
+	},
+	{
+	        .name	= "wdt-regs",
+		.start	= TRANSCEDE_TWD_BASE,
+		.end	= TRANSCEDE_TWD_BASE + 0xA00 - 1,
+		.flags	= IORESOURCE_MEM,
+	},
+};
+
+static struct platform_device transcede_wdt = {
+	.name	= "mpcore_wdt",
+	.id	= -1,
+	.resource = transcede_wdt_resources,
+	.num_resources = ARRAY_SIZE(transcede_wdt_resources),
+};
+#endif
+
+static struct platform_device *transcede_devices[] __initdata = {
+#if defined(CONFIG_TRANSCEDE_UART0_SUPPORT) || defined(CONFIG_TRANSCEDE_UART1_SUPPORT) || defined(CONFIG_TRANSCEDE_UART1_SUPPORT)
+	&transcede_uart,
+#endif
+
+#if defined(CONFIG_MTD_TRANSCEDE_NOR_8)
+	&transcede_nor8_flash_device,
+#endif
+
+#if defined(CONFIG_MTD_TRANSCEDE_NOR_16)
+	&transcede_nor16_flash_device,
+#endif
+
+#if defined(CONFIG_TRANSCEDE_GEMAC_0)
+	&transcede_eth0_device,
+#endif	
+
+#if defined(CONFIG_TRANSCEDE_GEMAC_1)
+	&transcede_eth1_device,
+#endif
+
+#if defined(CONFIG_TRANSCEDE_GEM_PHY)
+	&transcede_mdio_device,
+#endif
+
+#if defined(CONFIG_I2C)
+	&transcede_i2c,
+#endif
+
+#if defined(CONFIG_SPI_TRANSCEDE)
+	&transcede_spi0,
+#endif
+
+#if defined(CONFIG_TRANSCEDE_IPSEC_HW_SUPPORT)
+	&transcede_espah_device,
+#endif
+#if defined(CONFIG_TRANSCEDE_ELP_CLP30)
+	&transcede_elp_clp30_device,
+#endif
+#if defined(CONFIG_TRANSCEDE_ELP_PDU)
+	&transcede_elp_pdu_device,
+#endif
+#if defined(CONFIG_TRANSCEDE_ELP_TRNG)
+	&transcede_elp_trng_device,
+#endif
+	&transcede_ddr0_uio,
+	&transcede_ddr1_uio,
+	&transcede_cram_uio,
+	&transcede_iram_uio,
+	&transcede_jram_uio,
+	&transcede_xp_timer_uio,
+	&transcede_sema_uio,
+	&transcede_intc_uio,
+	&transcede_pmu,
+
+#ifdef CONFIG_MPCORE_WATCHDOG
+	&transcede_wdt,
+#endif
+
+};
+
+#if defined(CONFIG_MTD_NAND_TRANSCEDE) || defined(CONFIG_MTD_NAND_TRANSCEDE_MODULE)
+static struct platform_device *transcede_nand_device[] __initdata = {
+	&transcede_nand,
+};
+#endif
+
+static u32 __init ddr_phy_setup_access(u32 addr)
+{
+	u32 data;
+
+	data = readl(TRANSCEDE_CFG_0);
+
+	if (addr & 3) {
+		writel(data | 0x80000000, TRANSCEDE_CFG_0);
+
+		addr -= 2;
+	} else {
+		writel(data & 0x7FFFFFFF, TRANSCEDE_CFG_0);
+	}
+
+	return addr;
+}
+
+static u32 __init ddr_phy_read(u32 addr)
+{
+	u32 data;
+
+	addr = ddr_phy_setup_access(addr);
+	data = readl(addr);
+
+	return data & 0xFFFF;
+}
+
+static int __init transcede_get_revision(void)
+{
+	u32 data1, data2, backup;
+
+	if ((__raw_readl(TRANSCEDE_GPIO_DEVICE_ID_REG) & 0xFFE) != 0x45A)
+		return 4;
+
+	backup = readl(TRANSCEDE_CFG_0);
+
+	data1 = ddr_phy_read(TRANSCEDE_DDRPHY0 + 0x22);
+	data2 = ddr_phy_read(TRANSCEDE_DDRPHY0 + 0x20);
+
+	writel(backup, TRANSCEDE_CFG_0);
+
+	if (data1 == data2)
+		return 6;
+
+	return 7;
+}
+
+static void __init transcede_fixup(struct machine_desc *desc, struct tag *tags, char **cmdline, struct meminfo *mi)
+{
+	transcede_revision = transcede_get_revision();
+	printk(KERN_INFO "Device revision: x%d\n", transcede_revision);
+}
+
+/* RAM parameter */
+unsigned long transcede_ddr_common_base;
+EXPORT_SYMBOL(transcede_ddr_common_base);
+unsigned long transcede_public_virt = 0xffffffffU;
+EXPORT_SYMBOL(transcede_public_virt);
+unsigned long transcede_public_size;
+EXPORT_SYMBOL(transcede_public_size);
+
+static void __init transcede_init(void)
+{
+	transcede_enable_ddr_protection();
+
+#if defined(CONFIG_I2C)
+	i2c_register_board_info(0, transcede_i2c_info, ARRAY_SIZE(transcede_i2c_info));
+#endif
+
+	platform_add_devices(transcede_devices, ARRAY_SIZE(transcede_devices));
+
+#if defined(CONFIG_MTD_NAND_TRANSCEDE) || defined(CONFIG_MTD_NAND_TRANSCEDE_MODULE)
+	/* avoid registering NAND support on x6 devices */
+	if (transcede_revision > 6)
+		platform_add_devices(transcede_nand_device, 1);
+#endif
+
+#if defined(CONFIG_SPI)
+	spi_register_board_info(transcede_spi_info, ARRAY_SIZE(transcede_spi_info));
+#endif
+}
+
+MACHINE_START(TRANSCEDE, "Transcede")
+	/* Maintainer: Intel */
+	.boot_params	= PHYS_OFFSET + 0x01000100,
+	.fixup		= transcede_fixup,
+	.map_io		= transcede_map_io,
+	.reserve	= transcede_reserve,
+	.init_irq	= transcede_init_irq,
+	.timer		= &transcede_timer,
+	.init_machine	= transcede_init,
+MACHINE_END
diff --git a/arch/arm/mach-transcede/u-arm.c b/arch/arm/mach-transcede/u-arm.c
new file mode 100644
index 0000000..6bf3211
--- /dev/null
+++ b/arch/arm/mach-transcede/u-arm.c
@@ -0,0 +1,328 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#include <linux/cdev.h>
+#include <linux/device.h>
+#include <linux/interrupt.h>
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <linux/fs.h>
+#include <linux/uaccess.h>
+#include <asm/delay.h>
+#include <asm/io.h>
+#include <asm/cacheflush.h>
+#include <asm/hardware/cache-l2x0.h>
+#include <asm/mach/map.h>
+
+#include "core.h"
+#include "u-arm.h"
+
+
+static struct class	*uarm_class;
+static dev_t		uarm_devt;
+static int		uarm_major;
+
+static struct device	*uarm_device;
+static struct cdev	uarm_cdev;
+static atomic_t		uarm_refs;
+static struct page	*uarm_shm_pages;
+static u32		uarm_shm_size;
+static u32		uarm_shm_order;
+
+
+static int uarm_open(struct inode *inode, struct file *file)
+{
+	atomic_inc(&uarm_refs);
+
+	return 0;
+}
+
+static int uarm_close(struct inode *inode, struct file *file)
+{
+	if (atomic_dec_return(&uarm_refs) == 0) {
+		if (uarm_shm_pages) {
+			__free_pages(uarm_shm_pages, uarm_shm_order);
+			uarm_shm_pages = NULL;
+			uarm_shm_size = uarm_shm_order = 0;
+		}
+	}
+
+	return 0;
+}
+
+static int uarm_ioctl_shm_create(u32 size_in_pages, u32 *phys_addr)
+{
+	if (uarm_shm_pages) {
+		dev_err(uarm_device, "%u pages already allocated\n", 1 << uarm_shm_order);
+		return -EBUSY;
+	}
+
+	if (size_in_pages > 16*1024*1024/PAGE_SIZE) {
+		dev_err(uarm_device, "invalid size requsted: %lu, valid range 4K..16M\n", size_in_pages*PAGE_SIZE);
+		return -EINVAL;
+	}
+
+	uarm_shm_size = size_in_pages * PAGE_SIZE;
+	uarm_shm_order = get_order(uarm_shm_size);
+	uarm_shm_pages = alloc_pages(GFP_KERNEL | __GFP_ZERO, uarm_shm_order);
+	if (!uarm_shm_pages) {
+		uarm_shm_size = 0;
+		uarm_shm_order = 0;
+		dev_err(uarm_device, "failed to allocate %u pages\n", 1 << uarm_shm_order);
+		return -ENOMEM;
+	}
+
+	if (phys_addr)
+		*phys_addr = page_to_phys(&uarm_shm_pages[0]);
+
+	return 0;
+}
+
+static void __uarm_cache_clean(u32 start_virt, u32 start_phys, u32 size)
+{
+	u32 end;
+
+	end = start_virt + size;
+	while (start_virt < end) {
+		asm volatile ("mcr p15, 0, %0, c7, c10, 1\n" : : "r" (start_virt) : "cc");
+		start_virt += 32;
+	}
+
+#ifdef CONFIG_CACHE_L3X0
+	if (!l2x0_disabled)
+		outer_clean_range(start_phys, start_phys+size);
+#endif
+}
+
+static int uarm_ioctl_cache_clean(u32 *user_start_end)
+{
+	u32 start_virt, start_phys, end;
+
+	if (get_user(start_virt, user_start_end))
+		return -EFAULT;
+	if (get_user(end, user_start_end+1))
+		return -EFAULT;
+
+	start_virt &= ~31;
+	if (start_virt >= end)
+		return -EINVAL;
+
+	start_phys = page_to_phys(&uarm_shm_pages[0]);
+
+	__uarm_cache_clean(start_virt, start_phys, end-start_virt);
+	dsb();
+
+	return 0;
+}
+
+static void __uarm_cache_invalidate(u32 start_virt, u32 start_phys, u32 size)
+{
+	u32 end;
+
+	end = start_virt + size;
+	while (start_virt < end) {
+		asm volatile ("mcr p15, 0, %0, c7, c6, 1\n" : : "r" (start_virt) : "cc");
+		start_virt += 32;
+	}
+
+#ifdef CONFIG_CACHE_L3X0
+	if (!l2x0_disabled)
+		outer_inv_range(start_phys, start_phys+size);
+#endif
+}
+
+static int uarm_ioctl_cache_invalidate(u32 *user_start_end)
+{
+	u32 start_virt, start_phys, end;
+
+	if (get_user(start_virt, user_start_end))
+		return -EFAULT;
+	if (get_user(end, user_start_end+1))
+		return -EFAULT;
+
+	start_virt &= ~31;
+	if (start_virt >= end)
+		return -EINVAL;
+
+	start_phys = page_to_phys(&uarm_shm_pages[0]);
+
+	__uarm_cache_invalidate(start_virt, start_phys, end-start_virt);
+	dsb();
+
+	return 0;
+}
+
+static int uarm_ioctl_cache_flush(u32 *user_start_end)
+{
+	u32 start_virt, start_phys, end;
+
+	if (get_user(start_virt, user_start_end))
+		return -EFAULT;
+	if (get_user(end, user_start_end+1))
+		return -EFAULT;
+
+	start_virt &= ~31;
+	if (start_virt >= end)
+		return -EINVAL;
+
+	start_phys = page_to_phys(&uarm_shm_pages[0]);
+
+	__uarm_cache_clean(start_virt, start_phys, end-start_virt);
+	__uarm_cache_invalidate(start_virt, start_phys, end-start_virt);
+	dsb();
+
+	return 0;
+}
+
+static long uarm_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	int res;
+	u32 u, phys;
+
+	switch (cmd) {
+	case UARM_IOCTL_SHM_CREATE:
+		if (get_user(u, (u32*)arg))
+			return -EFAULT;
+		res = uarm_ioctl_shm_create(u, &phys);
+		if (!res)
+			res = put_user(phys, (u32*)arg);
+		return res;
+
+	case UARM_IOCTL_CACHE_CLEAN:
+		return uarm_ioctl_cache_clean((u32*)arg);
+
+	case UARM_IOCTL_CACHE_INVALIDATE:
+		return uarm_ioctl_cache_invalidate((u32*)arg);
+
+	case UARM_IOCTL_CACHE_FLUSH:
+		return uarm_ioctl_cache_flush((u32*)arg);
+	}
+
+	return -EOPNOTSUPP;
+}
+
+static int uarm_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	int err = 0, i;
+	int size = vma->vm_end - vma->vm_start;
+	ulong start = vma->vm_start;
+
+	if (!vma->vm_pgoff) {	/* remap buffer in system memory */
+		if (!uarm_shm_pages)
+			return -ENODEV;
+
+		if (size <= 0 || size > PAGE_SIZE * (1 << uarm_shm_order))
+			return -EINVAL;
+
+		for (i = 0; i*PAGE_SIZE < size; i++) {
+			err = vm_insert_page(vma, start, &uarm_shm_pages[i]);
+			if (err) {
+				dev_err(uarm_device, "failed to map page\n");
+				break;
+			}
+		}
+	} else {		/* remap device memory with cacheable attributes */
+	}
+
+	return err;
+}
+
+static ssize_t uarm_info_show(struct device *dev, struct device_attribute *attr, char *buf)
+{
+	int n = 0;
+
+	if (!uarm_shm_size)
+		n = sprintf(buf, "No buffer\n");
+	else
+		n = sprintf(buf,
+				"Phys: 0x%08X\n"
+				"Size: %u pages, %u bytes\n",
+				page_to_phys(&uarm_shm_pages[0]), 1 << uarm_shm_order, uarm_shm_size);
+
+	return n;
+}
+
+struct file_operations uarm_fops = {
+	.open		= uarm_open,
+	.release	= uarm_close,
+	.unlocked_ioctl	= uarm_ioctl,
+	.mmap		= uarm_mmap,
+};
+
+struct device_attribute uarm_info = {
+	.attr = {
+		.name = "info",
+		.mode = S_IRUGO,
+	},
+	.show   = uarm_info_show,
+	.store  = NULL,
+};
+
+static int __init uarm_init(void)
+{
+	int err;
+
+	uarm_class = class_create(THIS_MODULE, "u-arm");
+	if (IS_ERR(uarm_class))
+		return PTR_ERR(uarm_class);
+
+	err = alloc_chrdev_region(&uarm_devt, 0, 1, "u-arm");
+	if (err)
+		goto err0;
+
+	uarm_major = MAJOR(uarm_devt);
+
+	cdev_init(&uarm_cdev, &uarm_fops);
+	if (cdev_add(&uarm_cdev, uarm_devt, 1))
+		goto err1;
+
+	uarm_device = device_create(uarm_class, NULL, MKDEV(uarm_major, 0), NULL, "u-arm");
+	if (IS_ERR(uarm_device))
+		goto err2;
+
+	if (device_create_file(uarm_device, &uarm_info))
+		goto err3;
+
+	return 0;
+
+err3:
+	device_destroy(uarm_class, MKDEV(uarm_major, 0));
+err2:
+	unregister_chrdev_region(uarm_devt, 1);
+err1:
+	cdev_del(&uarm_cdev);
+err0:
+	class_destroy(uarm_class);
+	return -1;
+}
+
+static void __exit uarm_exit(void)
+{
+	device_remove_file(uarm_device, &uarm_info);
+	device_destroy(uarm_class, MKDEV(uarm_major, 0));
+	cdev_del(&uarm_cdev);
+	unregister_chrdev_region(uarm_devt, 1);
+	class_destroy(uarm_class);
+}
+
+module_init(uarm_init);
+module_exit(uarm_exit);
diff --git a/arch/arm/mach-transcede/u-arm.h b/arch/arm/mach-transcede/u-arm.h
new file mode 100644
index 0000000..dfba06a
--- /dev/null
+++ b/arch/arm/mach-transcede/u-arm.h
@@ -0,0 +1,37 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef __MACH_U_ARM_H
+#define __MACH_U_ARM_H
+
+#ifdef __KERNEL__
+#include <linux/ioctl.h>
+#else
+#include <sys/ioctl.h>
+#endif
+
+#define UARM_IOCTL_SHM_CREATE		_IOWR('U', 1, unsigned int*)
+#define UARM_IOCTL_CACHE_CLEAN		_IOR ('U', 2, unsigned int*)
+#define UARM_IOCTL_CACHE_INVALIDATE	_IOR ('U', 3, unsigned int*)
+#define UARM_IOCTL_CACHE_FLUSH		_IOR ('U', 4, unsigned int*)
+
+#endif
diff --git a/arch/arm/mm/Kconfig b/arch/arm/mm/Kconfig
index 0074b8d..29653a0 100644
--- a/arch/arm/mm/Kconfig
+++ b/arch/arm/mm/Kconfig
@@ -417,7 +417,7 @@ config CPU_V6K
 
 # ARMv7
 config CPU_V7
-	bool "Support ARM V7 processor" if ARCH_INTEGRATOR || MACH_REALVIEW_EB || MACH_REALVIEW_PBX
+	bool "Support ARM V7 processor" if ARCH_INTEGRATOR || MACH_REALVIEW_EB || MACH_REALVIEW_PBX || ARCH_TRANSCEDE
 	select CPU_32v6K
 	select CPU_32v7
 	select CPU_ABRT_EV7
@@ -821,7 +821,7 @@ config CACHE_L2X0
 	depends on REALVIEW_EB_ARM11MP || MACH_REALVIEW_PB11MP || MACH_REALVIEW_PB1176 || \
 		   REALVIEW_EB_A9MP || SOC_IMX35 || SOC_IMX31 || MACH_REALVIEW_PBX || \
 		   ARCH_NOMADIK || ARCH_OMAP4 || ARCH_EXYNOS4 || ARCH_TEGRA || \
-		   ARCH_U8500 || ARCH_VEXPRESS_CA9X4 || ARCH_SHMOBILE
+		   ARCH_U8500 || ARCH_VEXPRESS_CA9X4 || ARCH_SHMOBILE || ARCH_TRANSCEDE
 	default y
 	select OUTER_CACHE
 	select OUTER_CACHE_SYNC
diff --git a/arch/arm/mm/dma-mapping.c b/arch/arm/mm/dma-mapping.c
index f96d2c7..cd09fc6 100644
--- a/arch/arm/mm/dma-mapping.c
+++ b/arch/arm/mm/dma-mapping.c
@@ -422,6 +422,15 @@ void dma_free_coherent(struct device *dev, size_t size, void *cpu_addr, dma_addr
 }
 EXPORT_SYMBOL(dma_free_coherent);
 
+unsigned long (*__dma_ext_map_mem)(const void * kaddr, size_t size, uint dir)=NULL;
+unsigned long (*__dma_ext_unmap_mem)(const void * kaddr, size_t size, uint dir)=NULL;
+unsigned long (*__dma_cpu_sync)(const void * kaddr, size_t size, uint dir)=NULL;
+unsigned long (*__dma_dev_sync)(const void * kaddr, size_t size, uint dir)=NULL;
+EXPORT_SYMBOL(__dma_ext_map_mem);
+EXPORT_SYMBOL(__dma_ext_unmap_mem);
+EXPORT_SYMBOL(__dma_cpu_sync);
+EXPORT_SYMBOL(__dma_dev_sync);
+
 /*
  * Make an area consistent for devices.
  * Note: Drivers should NOT use this function directly, as it will break
diff --git a/arch/arm/mm/fault.c b/arch/arm/mm/fault.c
index a763335..a9ce01f 100644
--- a/arch/arm/mm/fault.c
+++ b/arch/arm/mm/fault.c
@@ -548,6 +548,12 @@ hook_fault_code(int nr, int (*fn)(unsigned long, unsigned int, struct pt_regs *)
 	fsr_info[nr].name = name;
 }
 
+#if defined(CONFIG_TRANSCEDE_MLOG)
+#include <mach/mlog.h>
+#include <mach/tcb.h>
+#endif
+
+
 /*
  * Dispatch a data abort to the relevant handler.
  */
@@ -556,9 +562,26 @@ do_DataAbort(unsigned long addr, unsigned int fsr, struct pt_regs *regs)
 {
 	const struct fsr_info *inf = fsr_info + fsr_fs(fsr);
 	struct siginfo info;
+#if defined(CONFIG_TRANSCEDE_MLOG)
+	unsigned long tt, t = get_tick();
+	unsigned int mlogVars[10], mlogVarsCnt = 0;
+#endif
 
-	if (!inf->fn(addr, fsr & ~FSR_LNX_PF, regs))
+	if (!inf->fn(addr, fsr & ~FSR_LNX_PF, regs)) {
+#if defined(CONFIG_TRANSCEDE_MLOG)
+		tt = get_tick();
+		mlogVars[mlogVarsCnt++] = MLOG_VAR_DABT;
+		mlogVars[mlogVarsCnt++] = addr;
+		mlogVars[mlogVarsCnt++] = fsr;
+		mlogVars[mlogVarsCnt++] = regs->ARM_pc;
+		mlogVars[mlogVarsCnt++] = regs->ARM_lr;
+		mlogVars[mlogVarsCnt++] = regs->ARM_r0;
+
+		MLogAddVariables(mlogVarsCnt, mlogVars, tt);
+		MLogTask(TASK_ID_IRQ, RESOURCE_LARM, t, tt);
+#endif
 		return;
+	}
 
 	printk(KERN_ALERT "Unhandled fault: %s (0x%03x) at 0x%08lx\n",
 		inf->name, fsr, addr);
diff --git a/arch/arm/mm/init.c b/arch/arm/mm/init.c
index 4a4eba5..3d10e26 100644
--- a/arch/arm/mm/init.c
+++ b/arch/arm/mm/init.c
@@ -720,6 +720,13 @@ void free_initmem(void)
 		totalram_pages += free_area(__phys_to_pfn(__pa(__init_begin)),
 					    __phys_to_pfn(__pa(__init_end)),
 					    "init");
+
+#ifdef CONFIG_ARCH_TRANSCEDE
+	if (!machine_is_integrator() && !machine_is_cintegrator())
+		totalram_pages += free_area(__phys_to_pfn(__pa(__initramfs_begin)),
+					    __phys_to_pfn(__pa(__initramfs_end)),
+					    "init ramfs");
+#endif
 }
 
 #ifdef CONFIG_BLK_DEV_INITRD
diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index ab50627..405482d 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -201,11 +201,13 @@ void __iomem * __arm_ioremap_pfn_caller(unsigned long pfn,
 	if (pfn >= 0x100000 && (__pfn_to_phys(pfn) & ~SUPERSECTION_MASK))
 		return NULL;
 
+#ifndef CONFIG_ARCH_TRANSCEDE
 	/*
 	 * Don't allow RAM to be mapped - this causes problems with ARMv6+
 	 */
 	if (WARN_ON(pfn_valid(pfn)))
 		return NULL;
+#endif
 
 	type = get_mem_type(mtype);
 	if (!type)
diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 594d677..1975973 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -273,6 +273,12 @@ static struct mem_type mem_types[] = {
 		.prot_l1   = PMD_TYPE_TABLE,
 		.domain    = DOMAIN_KERNEL,
 	},
+	[MT_MEMORY_STRONGLY_ORDERED] = {
+		.prot_sect = PMD_TYPE_SECT |
+				PMD_SECT_AP_WRITE | PMD_SECT_AP_READ |
+				PMD_SECT_UNCACHED,
+		.domain    = DOMAIN_IO,
+	},
 };
 
 const struct mem_type *get_mem_type(unsigned int type)
@@ -727,7 +733,11 @@ void __init iotable_init(struct map_desc *io_desc, int nr)
 		create_mapping(io_desc + i);
 }
 
+#ifdef CONFIG_MACH_M822XX
+static void * __initdata vmalloc_min = (void *)(VMALLOC_END - (SZ_256M + SZ_64M));
+#else
 static void * __initdata vmalloc_min = (void *)(VMALLOC_END - SZ_128M);
+#endif
 
 /*
  * vmalloc=size forces the vmalloc area to be exactly 'size'
diff --git a/arch/arm/mm/proc-v7.S b/arch/arm/mm/proc-v7.S
index 21cd298..def339d 100644
--- a/arch/arm/mm/proc-v7.S
+++ b/arch/arm/mm/proc-v7.S
@@ -46,7 +46,7 @@ ENTRY(cpu_v7_proc_fin)
 	mrc	p15, 0, r0, c1, c0, 0		@ ctrl register
 	bic	r0, r0, #0x1000			@ ...i............
 	bic	r0, r0, #0x0006			@ .............ca.
-	mcr	p15, 0, r0, c1, c0, 0		@ disable caches
+#	mcr	p15, 0, r0, c1, c0, 0		@ disable caches
 	mov	pc, lr
 ENDPROC(cpu_v7_proc_fin)
 
@@ -191,6 +191,7 @@ cpu_v7_name:
 	 *   WRITETHROUGH	010	10	10	10
 	 *   WRITEBACK		011	10	11	11
 	 *   reserved		110
+	 *   WRITEALLOC_INNER	101	10	01	00
 	 *   WRITEALLOC		111	10	01	01
 	 *   DEV_SHARED		100	01
 	 *   DEV_NONSHARED	100	01
@@ -205,8 +206,8 @@ cpu_v7_name:
 	 *   NS1 = PRRR[19] = 1		- normal shareable property
 	 *   NOS = PRRR[24+n] = 1	- not outer shareable
 	 */
-.equ	PRRR,	0xff0a81a8
-.equ	NMRR,	0x40e040e0
+.equ	PRRR,	0xff0a89a8
+.equ	NMRR,	0x40e044e0
 
 /* Suspend/resume support: derived from arch/arm/mach-s5pv210/sleep.S */
 .globl	cpu_v7_suspend_size
diff --git a/drivers/char/random.c b/drivers/char/random.c
index 15c8c3c..7b1b605 100644
--- a/drivers/char/random.c
+++ b/drivers/char/random.c
@@ -131,6 +131,9 @@
  *	void add_interrupt_randomness(int irq, int irq_flags);
  * 	void add_disk_randomness(struct gendisk *disk);
  *
+ *      void random_input_words(__u32 *buf, size_t wordcount, int ent_count)
+ *      int random_input_wait(void);
+ *
  * add_device_randomness() is for adding data to the random pool that
  * is likely to differ between two devices (or possibly even per boot).
  * This would be things like MAC addresses or serial numbers, or the
@@ -152,6 +155,13 @@
  * seek times do not make for good sources of entropy, as their seek
  * times are usually fairly consistent.
  *
+ * random_input_words() just provides a raw block of entropy to the input
+ * pool, such as from a hardware entropy generator.
+ *
+ * random_input_wait() suspends the caller until such time as the
+ * entropy pool falls below the write threshold, and returns a count of how
+ * much entropy (in bits) is needed to sustain the pool.
+ *
  * All of these routines try to estimate how many bits of randomness a
  * particular randomness source.  They do this by keeping track of the
  * first and second order deltas of the event timings.
@@ -800,6 +810,63 @@ void add_disk_randomness(struct gendisk *disk)
 }
 #endif
 
+/*
+ * random_input_words - add bulk entropy to pool
+ *
+ * @buf: buffer to add
+ * @wordcount: number of __u32 words to add
+ * @ent_count: total amount of entropy (in bits) to credit
+ *
+ * this provides bulk input of entropy to the input pool
+ *
+ */
+void random_input_words(__u32 *buf, size_t wordcount, int ent_count)
+{
+	mix_pool_bytes(&input_pool, buf, wordcount*4, NULL);
+
+	credit_entropy_bits(&input_pool, ent_count);
+
+	DEBUG_ENT("crediting %d bits => %d\n",
+		  ent_count, input_pool.entropy_count);
+	/*
+	 * Wake up waiting processes if we have enough
+	 * entropy.
+	 */
+	if (input_pool.entropy_count >= random_read_wakeup_thresh)
+		wake_up_interruptible(&random_read_wait);
+}
+EXPORT_SYMBOL(random_input_words);
+
+/*
+ * random_input_wait - wait until random needs entropy
+ *
+ * this function sleeps until the /dev/random subsystem actually
+ * needs more entropy, and then return the amount of entropy
+ * that it would be nice to have added to the system.
+ */
+int random_input_wait(void)
+{
+	int count;
+
+	wait_event_interruptible(random_write_wait,
+			 input_pool.entropy_count < random_write_wakeup_thresh);
+
+	count = random_write_wakeup_thresh - input_pool.entropy_count;
+
+        /* likely we got woken up due to a signal */
+	if (count <= 0) count = random_read_wakeup_thresh;
+
+	DEBUG_ENT("requesting %d bits from input_wait()er %d<%d\n",
+		  count,
+		  input_pool.entropy_count, random_write_wakeup_thresh);
+
+	return count;
+}
+EXPORT_SYMBOL(random_input_wait);
+
+
+#define EXTRACT_SIZE 10
+
 /*********************************************************************
  *
  * Entropy extraction routines
diff --git a/drivers/crypto/Kconfig b/drivers/crypto/Kconfig
index 98caccf..e5deb76 100644
--- a/drivers/crypto/Kconfig
+++ b/drivers/crypto/Kconfig
@@ -293,4 +293,14 @@ config CRYPTO_DEV_S5P
 	  Select this to offload Samsung S5PV210 or S5PC110 from AES
 	  algorithms execution.
 
+config CRYPTO_DEV_TRANSCEDE
+        tristate "Acrypto driver for Comcerto & Transcede LC device"
+        select CRYPTO_DES
+        select CRYPTO_HASH
+        select CRYPTO_ALGAPI
+        select CRYPTO_AUTHENC
+        help
+          This option allows you to have acrypto suport on Transcede
+
+
 endif # CRYPTO_HW
diff --git a/drivers/crypto/Makefile b/drivers/crypto/Makefile
index 53ea501..c5df040 100644
--- a/drivers/crypto/Makefile
+++ b/drivers/crypto/Makefile
@@ -13,3 +13,5 @@ obj-$(CONFIG_CRYPTO_DEV_OMAP_SHAM) += omap-sham.o
 obj-$(CONFIG_CRYPTO_DEV_OMAP_AES) += omap-aes.o
 obj-$(CONFIG_CRYPTO_DEV_PICOXCELL) += picoxcell_crypto.o
 obj-$(CONFIG_CRYPTO_DEV_S5P) += s5p-sss.o
+obj-$(CONFIG_CRYPTO_DEV_TRANSCEDE) += transcede_crypto.o
+transcede_crypto-y := comcerto_crypto.o comcerto_espah.o
diff --git a/drivers/crypto/comcerto_crypto.c b/drivers/crypto/comcerto_crypto.c
new file mode 100644
index 0000000..d510d5d
--- /dev/null
+++ b/drivers/crypto/comcerto_crypto.c
@@ -0,0 +1,1112 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+#include <linux/version.h>
+#include <linux/dma-mapping.h>
+#include <linux/dmapool.h>
+#include <linux/crypto.h>
+#include <linux/kernel.h>
+#include <linux/rtnetlink.h>
+#include <linux/interrupt.h>
+#include <linux/spinlock.h>
+#include <linux/ip.h>
+#include <linux/ipv6.h>
+
+#include <crypto/ctr.h>
+#include <crypto/des.h>
+#include <crypto/aes.h>
+#include <crypto/sha.h>
+#include <crypto/algapi.h>
+#include <crypto/aead.h>
+#include <crypto/authenc.h>
+#include <crypto/scatterwalk.h>
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,15)
+#include <linux/platform_device.h>
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,0,0)
+#include <mach/spa.h>
+#else
+#include <mach/comcerto-4000/spa.h>
+#endif
+
+#include "comcerto_espah.h"
+#include "comcerto_crypto.h"
+
+extern volatile int local_espah_debug;
+extern volatile int *espah_debug;
+
+static struct platform_device *comcerto_acrypto_platform_dev;
+
+
+static int comcerto_crypto_remove(struct platform_device *pdev);
+
+
+static inline u32 cipher_cfg(struct crypto_tfm *tfm)
+{
+	return container_of(tfm->__crt_alg, struct comcerto_alg, crypto)->cfg_cipher;
+}
+
+static inline u32 digest_cfg(struct crypto_tfm *tfm)
+{
+	return container_of(tfm->__crt_alg, struct comcerto_alg, crypto)->cfg_digest;
+}
+
+
+/* ------------------------------------------------------------------- ENCRYPTION ENTRY POINTS */
+
+static int ablk_init(struct crypto_tfm *tfm)
+{
+	return 0;
+}
+
+static void ablk_exit(struct crypto_tfm *tfm)
+{
+}
+
+static int ablk_setkey(struct crypto_ablkcipher *tfm, const u8 *key,
+			unsigned int key_len)
+{
+	return 0;
+}
+
+#if 0
+static int ablk_rfc3686_setkey(struct crypto_ablkcipher *tfm, const u8 *key,
+		unsigned int key_len)
+{
+
+	/* the nonce is stored in bytes at end of key */
+	if (key_len < CTR_RFC3686_NONCE_SIZE)
+		return -EINVAL;
+
+	key_len -= CTR_RFC3686_NONCE_SIZE;
+	return ablk_setkey(tfm, key, key_len);
+}
+#endif
+
+/*!
+ *	Encryption entry point
+ *	\param	req
+ *	\param	encrypt 1 means encryption and 0 means decryption
+  *	\retval
+ */
+static int ablk_perform(struct ablkcipher_request *req, int encrypt)
+{
+	return 0;
+}
+
+/*!
+ *	Encryption entry point
+ *	\param	req
+ *	\retval
+ */
+static int ablk_encrypt(struct ablkcipher_request *req)
+{
+	return ablk_perform(req, 1);
+}
+
+/*!
+ *	Encryption entry point
+ *	\param	req
+ *	\retval
+ */
+static int ablk_decrypt(struct ablkcipher_request *req)
+{
+	return ablk_perform(req, 0);
+}
+
+#if 0
+static int ablk_rfc3686_crypt(struct ablkcipher_request *req)
+{
+	return 0;
+}
+#endif
+
+
+/* ------ aead entry points ------ */
+
+
+/*!
+ *	Acrypto Authentication entry point
+ *	\param	tfm
+ *	\retval	0 is success <0 if error
+ */
+static int aead_init(struct crypto_tfm *tfm)
+{
+	struct comcerto_ctx *ctx;
+	int retval;
+
+	DPRINTF(ESPAH_DBG, "%s\n", __FUNCTION__);
+
+	ctx = crypto_tfm_ctx(tfm);
+
+	tfm->crt_aead.reqsize = sizeof(struct aead_ctx);
+
+	/* get packet engine reference */
+	ctx->sc = platform_get_drvdata(comcerto_acrypto_platform_dev);
+	if (!ctx->sc) {
+		retval = -EAGAIN; /* ERROR; */
+		goto err;
+	}
+
+	/* allocate SA DMA pool for ipsec context */
+	ctx->sa_dma_addr_virt = dma_pool_alloc(ctx->sc->sa_dma_pool, GFP_DMA, &ctx->sa_dma_addr_phys);
+	if (ctx->sa_dma_addr_virt == NULL) {
+		DPRINTF(ESPAH_ERR, "!ERR! %s() SA DMA pool allocate fail\n", __FUNCTION__);
+		retval = -ENOMEM;
+		goto err;
+	}
+
+	retval = 0;
+
+err:
+	return(retval);
+}
+
+
+/*!
+ *	Acrypto Authentication entry point
+ *	\param	tfm
+ *	\retval	0 is success <0 if error
+ */
+static void aead_exit(struct crypto_tfm *tfm)
+{
+	struct comcerto_ctx *ctx;
+
+	DPRINTF(ESPAH_DBG, "%s\n", __FUNCTION__);
+
+	ctx = crypto_tfm_ctx(tfm);
+	tfm->crt_aead.reqsize = sizeof(struct aead_ctx);
+
+	ctx->sc = platform_get_drvdata(comcerto_acrypto_platform_dev);
+
+	if (ctx->sc)
+		dma_pool_free(ctx->sc->sa_dma_pool, ctx->sa_dma_addr_virt, ctx->sa_dma_addr_phys);
+}
+
+
+/*!
+ *	Acrypto Authentication entry point => returns encrypt & authentication keys for AH
+ *	\param	tfm
+ *	\param	authsize
+ *	\retval	0 is success <0 if error
+ */
+static int aead_setauthsize(struct crypto_aead *tfm, unsigned int authsize)
+{
+	struct comcerto_ctx *ctx = crypto_aead_ctx(tfm);
+
+	DPRINTF(ESPAH_DBG, "%s\n", __FUNCTION__);
+
+	ctx->authsize = authsize;
+
+	return 0;
+}
+
+
+/*!
+ *	Acrypto Authentication entry point => returns encrypt & authentication keys for ESP
+ *	\param tfm
+ *	\param key
+ *	\param keylen Length of the while key buffer
+ *	\retval	0 is success <0 if error
+ */
+static int aead_setkey(struct crypto_aead *tfm, const u8 *key, unsigned int keylen)
+{
+	struct comcerto_ctx *ctx;
+	struct rtattr *rta = (void *)key;
+	struct crypto_authenc_key_param *param;
+	unsigned int auth_keylen;
+	unsigned int enc_keylen;
+
+	DPRINTF(ESPAH_DBG, "%s\n", __FUNCTION__);
+
+	ctx = crypto_aead_ctx(tfm);
+
+	if (!RTA_OK(rta, keylen))
+		goto badkey;
+
+	if (rta->rta_type != CRYPTO_AUTHENC_KEYA_PARAM)
+		goto badkey;
+
+	if (RTA_PAYLOAD(rta) < sizeof(*param))
+		goto badkey;
+
+	param = RTA_DATA(rta); /* param points to util values */
+	enc_keylen = be32_to_cpu(param->enckeylen);
+
+	key += RTA_ALIGN(rta->rta_len); /* skip rtattr header */
+	keylen -= RTA_ALIGN(rta->rta_len);
+
+	if (keylen < enc_keylen)
+		goto badkey;
+
+	auth_keylen = keylen - enc_keylen;
+
+	if (enc_keylen > COMCERTO_MAX_KEY_SIZE)
+		goto badkey;
+
+	ctx->cipher_alg = cipher_cfg(&tfm->base);
+	ctx->digest_alg = digest_cfg(&tfm->base);
+
+	DPRINTF(ESPAH_DBG, "%s alg = %i ,keylen=%i, enc_keylen=%i, auth_keylen=%i\n", __FUNCTION__, ctx->cipher_alg, keylen, enc_keylen, auth_keylen);
+
+	if (ctx->cipher_alg == ELP_CIPHER_AES128){
+		switch (enc_keylen){
+			case  AES_KEYSIZE_128:
+					ctx->cipher_alg = ELP_CIPHER_AES128;
+					break;
+			case  AES_KEYSIZE_192:
+					ctx->cipher_alg = ELP_CIPHER_AES192;
+					break;
+			case  AES_KEYSIZE_256:
+					ctx->cipher_alg = ELP_CIPHER_AES256;
+					break;
+			default:
+				goto badkey;
+		}
+	}
+
+	if (ctx->cipher_alg == ELP_CIPHER_DES)
+		if (enc_keylen != DES_KEY_SIZE)
+			goto badkey;
+
+	if (ctx->cipher_alg == ELP_CIPHER_3DES)
+		if (enc_keylen != DES3_EDE_KEY_SIZE)
+			goto badkey;
+
+	DPRINTF(ESPAH_DBG, "%s ELP cipher alg = %i ,keylen=%i\n", __FUNCTION__,ctx->cipher_alg,keylen);
+	DPRINTF(ESPAH_DBG, "%s ELP digest alg = %i ,keylen=%i\n", __FUNCTION__,ctx->digest_alg,keylen);
+
+	/* save to context */
+	ctx->enc_keylen = keylen; /* total key buffer length */
+	memcpy(ctx->keyval, key, keylen); /* save both encryption and authentication key values */
+
+	ctx->auth_keylen = auth_keylen; /* authentication/digest key length */
+	memcpy(ctx->auth_keyval, key, ctx->auth_keylen);
+	key += ctx->auth_keylen;
+
+	ctx->enc_keylen = enc_keylen; /* encryption key length */
+	memcpy(ctx->enc_keyval, key, ctx->enc_keylen);
+
+	DPRINTF(ESPAH_DBG, "ENC_KEYVAL = %02x %02x ...\n", ctx->enc_keyval[0], ctx->enc_keyval[1]);
+
+#if 0
+        {
+		int i;
+		DPRINTF(ESPAH_DBG, "ENC_KEYVAL (%d) = ", ctx->enc_keylen);
+		for (i=0; i<ctx->enc_keylen; i++)
+			DPRINTF(ESPAH_DBG, "%02x ", ctx->enc_keyval[i]);
+		printk(KERN_ERR "\n");
+
+		DPRINTF(ESPAH_DBG, "AUTH_KEYVAL (%d) = ", ctx->auth_keylen);
+		for (i=0; i<ctx->auth_keylen; i++)
+			DPRINTF(ESPAH_DBG, "%02x ", ctx->auth_keyval[i]);
+		printk(KERN_ERR "\n");
+	}
+#endif
+	DPRINTF(ESPAH_DBG, "%s PASSED\n", __FUNCTION__);
+
+	return 0;
+
+badkey:
+	crypto_aead_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
+	return -EINVAL;
+}
+
+static void 
+elp_sa_build_clp30(elp_sa_clp30_t *sa, struct comcerto_ctx *ctx)
+{
+	/* auth key */
+	sa->auth_key0 = ntohl(*(U32*)&ctx->auth_keyval[0]);
+	sa->auth_key1 = ntohl(*(U32*)&ctx->auth_keyval[4]);
+	sa->auth_key2 = ntohl(*(U32*)&ctx->auth_keyval[8]);
+	sa->auth_key3 = ntohl(*(U32*)&ctx->auth_keyval[12]);
+	sa->auth_key4 = ntohl(*(U32*)&ctx->auth_keyval[16]);
+
+	if (ctx->auth_keylen > 20) {
+		sa->ext_auth_key0 = ntohl(*(U32*)&ctx->auth_keyval[20]);
+		sa->ext_auth_key1 = ntohl(*(U32*)&ctx->auth_keyval[24]);
+		sa->ext_auth_key2 = ntohl(*(U32*)&ctx->auth_keyval[28]);
+	}
+
+	/* cipher key */
+	sa->cipher0 = ntohl(*(U32*)&ctx->enc_keyval[0]);
+	sa->cipher1 = ntohl(*(U32*)&ctx->enc_keyval[4]);
+	sa->cipher2 = ntohl(*(U32*)&ctx->enc_keyval[8]);
+	sa->cipher3 = ntohl(*(U32*)&ctx->enc_keyval[12]);
+	sa->cipher4 = ntohl(*(U32*)&ctx->enc_keyval[16]);
+	sa->cipher5 = ntohl(*(U32*)&ctx->enc_keyval[20]);
+	sa->cipher6 = ntohl(*(U32*)&ctx->enc_keyval[24]);
+	sa->cipher7 = ntohl(*(U32*)&ctx->enc_keyval[28]);
+
+	sa->hard_ttl_hi = ntohl((u32)(0xffffffff));
+	sa->hard_ttl_lo = ntohl((u32)(0xffffffff));
+	sa->soft_ttl_hi = ntohl((u32)(0xffffffff));
+	sa->soft_ttl_lo = ntohl((u32)(0xffffffff));
+
+	/* Secure Policy Identificator to be set only for encrypt */
+	sa->spi = ntohl(ctx->spi);
+
+	sa->algo = ctx->digest_alg | (ctx->cipher_alg << 4);
+
+	sa->flags = ESPAH_SEQ_ROLL_ALLOWED | ESPAH_ENABLED;
+	if(ctx->ip_family == ESPAH_IPV6)
+		sa->flags |= ESPAH_IPV6_ENABLE;
+
+#if 0
+	if (ctx->lifetime)
+		sa->flags |= ESPAH_TTL_ENABLE;
+
+	if (ctx->ipsec_protocol == CRYPTO_AH)
+		sa->flags |= ESPAH_AH_MODE;
+
+	if (ctx->anti_replay)
+		sa->flags |= ESPAH_ANTI_REPLAY_ENABLE;
+	else
+		sa->flags |= ESPAH_SEQ_ROLL_ALLOWED;
+#endif
+
+	DPRINTF(ESPAH_DBG, "sa->flags = %08x", sa->flags);
+
+	/* CLP30 HW uses SA DB caching.
+	 * Below we ask HW to update cache.
+	 */
+	while (elp_read(ctx->sc, ESPAH_SA_CACHE_READY) == 0);
+	elp_write(ctx->sc, ESPAH_SA_CACHE_FLUSH, (uint32_t)sa); /* Any value causes flush-all. We write SA address for debug purposes */
+	while (elp_read(ctx->sc, ESPAH_SA_CACHE_READY) == 0);
+}
+
+static void 
+elp_sa_build_clp36(elp_sa_clp36_t *sa, struct comcerto_ctx *ctx)
+{
+	int i;
+
+	/* auth key */
+	for(i=0;i<5;i++)
+		*(U32*)&sa->auth_key[i*4] = ntohl(*(U32*)&ctx->auth_keyval[i*4]);
+
+	if (ctx->auth_keylen > 20) {
+		for(i=0;i<((ctx->auth_keylen-20)/4);i++)
+			*(U32*)&sa->ext_auth_key[i*4] = ntohl(*(U32*)&ctx->auth_keyval[20+i*4]);
+	}
+
+	/* cipher key */
+	for(i=0;i<8;i++)
+		*(U32*)&sa->cipher_key[i*4] = ntohl(*(U32*)&ctx->enc_keyval[i*4]);
+
+	sa->cipher_token = (U16) 0;
+	sa->auth_token = (U16) 0;
+	sa->thread_id = ((U16) sa);
+
+	sa->hard_ttl_hi = ntohl((u32)(0xffffffff));
+	sa->hard_ttl_lo = ntohl((u32)(0xffffffff));
+	sa->soft_ttl_hi = ntohl((u32)(0xffffffff));
+	sa->soft_ttl_lo = ntohl((u32)(0xffffffff));
+
+	/* Secure Policy Identificator to be set only for encrypt */
+	sa->spi = ntohl(ctx->spi);
+
+	sa->algo = ctx->digest_alg | (ctx->cipher_alg << 4);
+
+	sa->flags = ESPAH_SEQ_ROLL_ALLOWED | ESPAH_ENABLED;
+	if(ctx->ip_family == ESPAH_IPV6)
+		sa->flags |= ESPAH_IPV6_ENABLE;
+
+#if 0
+	if (ctx->lifetime)
+		sa->flags |= ESPAH_TTL_ENABLE;
+
+	if (ctx->ipsec_protocol == CRYPTO_AH)
+		sa->flags |= ESPAH_AH_MODE;
+
+	if (ctx->anti_replay)
+		sa->flags |= ESPAH_ANTI_REPLAY_ENABLE;
+	else
+		sa->flags |= ESPAH_SEQ_ROLL_ALLOWED;
+#endif
+
+	DPRINTF(ESPAH_DBG, "sa->flags = %08x", sa->flags);
+}
+
+/*!
+ *	Acrypto Authentication entry point
+ *	\param sa	pointer to security association virtual address (returned by dma_pool_alloc)
+ *	\param ctx	ipsec context
+ *	\retval	0 is success <0 if error
+write to Elliptic key information + iv
+write 1st packet to elliptic
+ */
+void send_keys_to_acrypto_hw(elp_sa_t *sa, struct comcerto_ctx *ctx)
+{
+	printk("comcerto-crypto: offloading SPI=0x%08x\n", (unsigned int) ntohl(ctx->spi));
+
+	memset(sa, 0, sizeof(elp_sa_t));
+
+	if(elp_get_device_id() == ELP_DEVICE_ID_30)
+		elp_sa_build_clp30(&sa->clp30, ctx);
+	else
+		elp_sa_build_clp36(&sa->clp36, ctx);
+
+#if 0
+	if(*espah_debug & ESPAH_DBG)
+	{
+		int i;
+		u32 *ptr = (u32 *) sa;
+
+		for (i=0; i<32; i++) {
+			DPRINTF(ESPAH_DBG, "[ %p ] = %08x\n", ptr, *ptr);
+			ptr++;
+		}
+	}
+#endif
+}
+
+
+
+/*!
+ *	Acrypto Authentication entry point
+ *	\param req
+ *	\retval	0 is success <0 if error
+ */
+static int aead_encrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm;
+	struct comcerto_ctx *ctx;
+
+	struct iphdr *ipv4h;
+	struct ipv6hdr *ipv6h;
+	struct ip_esp_hdr *esphdr;
+	unsigned char *iph;
+	unsigned int iphlen;
+	unsigned short ip_total_len;
+
+	unsigned int ivsize;
+	unsigned int authsize;
+	unsigned int cryptlen;
+	int retval = 0;
+
+	DPRINTF(ESPAH_DBG, "%s req %p\n", __FUNCTION__, req);
+
+	tfm = crypto_aead_reqtfm(req);
+	ctx = crypto_aead_ctx(tfm); /* current ipsec session context */
+
+	ivsize = crypto_aead_ivsize(tfm);
+	memcpy(ctx->iv, req->iv, ivsize);
+
+	authsize = crypto_aead_authsize(tfm);
+	cryptlen = req->cryptlen;
+
+	DPRINTF(ESPAH_DBG, "%s: ivsize %d, authsize %d, cryptlen %d\n", __FUNCTION__, ivsize, authsize, cryptlen);
+
+	esphdr = (struct ip_esp_hdr *)(sg_virt(req->assoc));
+	DPRINTF(ESPAH_DBG, "esp header @ %p: 0x%08x 0x%08x\n", esphdr, ntohl(esphdr->spi), ntohl(esphdr->seq_no));
+	ctx->spi = ntohl(esphdr->spi);
+
+	ipv4h = (struct iphdr *)sg_virt(req->ip);
+
+	if (ipv4h->version == 4) {
+		ipv4h = (struct iphdr *)sg_virt(req->ip);
+		iph = (u8*)ipv4h;
+		iphlen = (ipv4h->ihl*4);	/* ADFIXME */
+		DPRINTF(ESPAH_DBG, "ip @ %p: version 4 (hdr len %d bytes) - next header: %d\n", iph, iphlen, ipv4h->protocol);
+	}
+	else
+	{
+		ipv6h = (struct ipv6hdr *)sg_virt(req->ip);
+		iph = (u8*)ipv6h;
+		iphlen = sizeof(*ipv6h);
+		DPRINTF(ESPAH_DBG, "ip @ %p: version 6 (hdr len %d bytes)- next header: %d\n", iph, iphlen, ipv6h->nexthdr);
+		ctx->ip_family = ESPAH_IPV6;
+        }
+#if 0
+	if(*espah_debug & ESPAH_DBG) {
+		for(i=0; i < iphlen; i+=2)
+			DPRINTF(ESPAH_DBG, "%02x%02x\n", iph[i],iph[i+1]);
+        }
+#endif
+
+	DPRINTF(ESPAH_DBG, "cipher_alg: 0x%08x digest_alg: 0x%08x\n",  ctx->cipher_alg, ctx->digest_alg);
+
+        if (!ctx->first_packet_received) {
+		DPRINTF(ESPAH_DBG, "%s - ctx=%08x, sa_dma_addr_virt=%08x, sa_dma_addr_phys=%08x\n", __FUNCTION__, (unsigned int) ctx, (unsigned int) ctx->sa_dma_addr_virt, (unsigned int) ctx->sa_dma_addr_phys);
+
+		ctx->first_packet_received = 1;
+
+
+		send_keys_to_acrypto_hw(ctx->sa_dma_addr_virt, ctx);
+        }
+
+        /* chain ip header and payload */
+	DPRINTF(ESPAH_DBG, "%s: sg ip %d - sg src %d - sg dst %d \n", __FUNCTION__, req->ip->length, req->src->length, req->dst->length);
+	ip_total_len = req->ip->length + req->src->length;
+	retval = send_data_to_acrypto_hw(ctx->sc, ELP_SESDIR_OUTBOUND, req->src, ip_total_len, req->dst, ip_total_len, ctx->sa_dma_addr_phys, (void *)req);
+
+	DPRINTF(ESPAH_DBG, "%s req %p  DONE\n", __FUNCTION__, req);
+
+	return retval;
+}
+
+/*!
+ *	Acrypto Authentication entry point
+ *	\param req
+ *	\retval	0 is success <0 if error
+write to Elliptic key information + iv
+write 1st packet to elliptic
+ */
+static int aead_decrypt(struct aead_request *req)
+{
+	struct crypto_aead *tfm;
+	struct comcerto_ctx *ctx;
+	unsigned int ivsize;
+	unsigned int authsize;
+	unsigned int cryptlen;
+	struct ip_esp_hdr *esphdr;
+	struct iphdr *ipv4h;
+	struct ipv6hdr *ipv6h;
+	unsigned char *iph;
+	unsigned int iphlen;
+	unsigned short ip_total_len;
+
+	int retval;
+
+	unsigned char *ptr;
+
+	DPRINTF(ESPAH_DBG, "%s req %p\n", __FUNCTION__, req);
+
+	tfm = crypto_aead_reqtfm(req);
+	ctx = crypto_aead_ctx(tfm); /* current ipsec session context */
+
+	ivsize = crypto_aead_ivsize(tfm);
+	memcpy(ctx->iv, req->iv, ivsize);
+	DPRINTF(ESPAH_DBG, "%s: iv (%d bytes)\n",__FUNCTION__, ivsize);
+#if 0
+	if(*espah_debug & ESPAH_DBG) {
+		for(i=0; i<ivsize; i++)
+			printk(KERN_ERR "%02x\n", req->iv[i]);
+	}
+#endif
+	authsize = crypto_aead_authsize(tfm);
+	cryptlen = req->cryptlen; /* req->cryptlen includes the authsize when decrypting */
+
+	if (req->src != req->dst) {
+		BUG(); /* -ENOTSUP because of my lazyness */
+	}
+
+	ipv4h = (struct iphdr *)sg_virt(req->src);
+	if(ipv4h->version == 4) {
+		iph = (u8*)ipv4h;
+		iphlen = (ipv4h->ihl*4);
+		ip_total_len = ntohs(ipv4h->tot_len);
+		DPRINTF(ESPAH_DBG, "ip @ %p: version 4 (hdr len %d bytes - payload len %d - next header: %d\n", iph, iphlen, ip_total_len, ipv4h->protocol);
+	}
+	else
+	{
+		ipv6h = (struct ipv6hdr *)sg_virt(req->src);
+		iph = (u8*)ipv6h;
+		iphlen = sizeof(*ipv6h);
+		ip_total_len = iphlen + ntohs(ipv6h->payload_len);
+		DPRINTF(ESPAH_DBG, "ip @ %p: version 6 (hdr len %d bytes) - payload len %d - next header: %d\n", iph, iphlen, ip_total_len, ipv6h->nexthdr);
+		ctx->ip_family = ESPAH_IPV6;
+	}
+#if 0
+	if(*espah_debug & ESPAH_DBG) {
+		for(i=0; i < iphlen; i++)
+			printk(KERN_ERR "%02x\n", iph[i]);
+	}
+#endif
+	esphdr = (struct ip_esp_hdr *)(iph + iphlen);
+	DPRINTF(ESPAH_DBG, "esp header @ %p: 0x%08x 0x%08x\n", esphdr, ntohl(esphdr->spi), ntohl(esphdr->seq_no));
+	ctx->spi = ntohl(esphdr->spi);
+#if 0
+	DPRINTF(ESPAH_DBG, "%s: in packet iv (%d bytes)\n",__FUNCTION__, ivsize);
+	if(*espah_debug & ESPAH_DBG) {
+		for(i=0; i < ivsize; i++)
+			printk(KERN_ERR "%02x\n", esphdr->enc_data[i]);
+	}
+#endif
+	DPRINTF(ESPAH_DBG, "%s: data (%d bytes) (authsize %d bytes)\n",__FUNCTION__, (ip_total_len - iphlen - ESP_HDRLEN - ivsize), authsize);
+	ptr = iph + (iphlen + ESP_HDRLEN + ivsize);
+#if 0
+	if(*espah_debug & ESPAH_DBG) {
+		for(i=0; i < (ip_total_len - iphlen - ESP_HDRLEN - ivsize); i+=2)
+			printk(KERN_ERR "%02x%02x\n", ptr[i],ptr[i+1]);
+	}
+#endif
+
+	DPRINTF(ESPAH_DBG, "cipher_alg: 0x%08x digest_alg: 0x%08x\n",  ctx->cipher_alg, ctx->digest_alg);
+
+	if (!ctx->first_packet_received) {
+		DPRINTF(ESPAH_DBG, "ESPAH - %s - 01 - ctx=%08x, sa_dma_addr_virt=%08x, sa_dma_addr_phys=%08x\n", __FUNCTION__, (unsigned int) ctx, (unsigned int) ctx->sa_dma_addr_virt, (unsigned int) ctx->sa_dma_addr_phys);
+
+		ctx->first_packet_received = 1;
+
+
+		send_keys_to_acrypto_hw(ctx->sa_dma_addr_virt, ctx);
+	}
+
+	retval = send_data_to_acrypto_hw(ctx->sc, ELP_SESDIR_INBOUND, req->src, ip_total_len, req->src, ip_total_len, ctx->sa_dma_addr_phys, (void *)req);
+	DPRINTF(ESPAH_DBG, "%s req %p  DONE\n", __FUNCTION__, req);
+	return retval;
+}
+
+
+/*!
+ *	Acrypto Authentication entry point
+ *	\param req
+ *	\retval	0 is success <0 if error
+ */
+static int aead_givencrypt(struct aead_givcrypt_request *req)
+{
+	int result;
+	DPRINTF(ESPAH_DBG, "%s req %p\n", __FUNCTION__, req);
+	result = aead_encrypt(&req->areq);
+	return result;
+}
+
+
+
+/* ---------------------------------------------------- */
+
+/*!
+ *	Acrypto Authentication entry point
+ *	\param req
+ *	\retval	0 is success <0 if error
+ */
+void comcerto_crypto_done(struct elp_softc *sc, void *context, int err)
+{
+	struct aead_request *areq;
+
+	areq = context; /* get ipsec context (aead request) */
+
+	DPRINTF(ESPAH_DBG, "%s: sc=%p context=%p err=%d\n", __FUNCTION__, sc, context, err);
+
+	areq->base.offloaded = 1;
+
+	aead_request_complete(areq, err);
+}
+
+static struct comcerto_alg comcerto_algos[] = {
+{
+	.crypto = {
+		.cra_name       = "authenc(digest_null,ecb(cipher_null))",
+		.cra_blocksize  = NULL_BLOCK_SIZE,
+		.cra_u          = { .aead = {
+			.ivsize         = 0,
+			.maxauthsize    = 0,
+			}
+		}
+	},
+	.cfg_digest = 0,
+	.cfg_cipher = 0 ,
+	.type = TYPE_ESPAH,
+}, {
+	.crypto = {
+		.cra_name       = "authenc(digest_null,cbc(des))",
+		.cra_blocksize  = DES_BLOCK_SIZE,
+		.cra_u          = { .aead = {
+			.ivsize         = DES_BLOCK_SIZE,
+			.maxauthsize    = NULL_DIGEST_SIZE,
+			}
+		}
+	},
+	.cfg_digest = ELP_HMAC_NULL,
+	.cfg_cipher = ELP_CIPHER_DES ,
+	.type = TYPE_ESPAH,
+}, {
+	.crypto = {
+		.cra_name       = "authenc(digest_null,cbc(des3_ede))",
+		.cra_blocksize  = DES3_EDE_BLOCK_SIZE,
+		.cra_u          = { .aead = {
+			.ivsize         = DES3_EDE_BLOCK_SIZE,
+			.maxauthsize    = NULL_DIGEST_SIZE,
+			}
+		}
+	},
+	.cfg_digest = ELP_HMAC_NULL,
+	.cfg_cipher = ELP_CIPHER_3DES ,
+	.type = TYPE_ESPAH,
+}, {
+	.crypto = {
+		.cra_name       = "authenc(digest_null,cbc(aes))",
+		.cra_blocksize  = AES_BLOCK_SIZE,
+		.cra_u          = { .aead = {
+			.ivsize         = AES_BLOCK_SIZE,
+			.maxauthsize    = NULL_DIGEST_SIZE,
+			}
+		}
+	},
+	.cfg_digest = ELP_HMAC_NULL,
+	.cfg_cipher = ELP_CIPHER_AES128 ,
+	.type = TYPE_ESPAH,
+}, {
+	.crypto = {
+                .cra_name       = "authenc(hmac(md5),cbc(aes))",
+                .cra_blocksize  = AES_BLOCK_SIZE,
+                .cra_u          = { .aead = {
+                        .ivsize         = AES_BLOCK_SIZE,
+                        .maxauthsize    = MD5_DIGEST_SIZE,
+                        }
+                }
+        },
+        .cfg_digest = ELP_HMAC_MD5,
+        .cfg_cipher = ELP_CIPHER_AES128 ,
+        .type = TYPE_ESPAH,
+}, {
+        .crypto = {
+                .cra_name       = "authenc(hmac(sha1),cbc(aes))",
+                .cra_blocksize  = AES_BLOCK_SIZE,
+                .cra_u          = { .aead = {
+                        .ivsize         = AES_BLOCK_SIZE,
+                        .maxauthsize    = SHA1_DIGEST_SIZE,
+                        }
+                }
+        },
+        .cfg_digest = ELP_HMAC_SHA1,
+        .cfg_cipher = ELP_CIPHER_AES128 ,
+        .type = TYPE_ESPAH,
+}, {
+        .crypto = {
+                .cra_name       = "authenc(hmac(md5),cbc(des))",
+                .cra_blocksize  = DES_BLOCK_SIZE,
+                .cra_u          = { .aead = {
+                        .ivsize         = DES_BLOCK_SIZE,
+                        .maxauthsize    = MD5_DIGEST_SIZE,
+                        }
+                }
+        },
+        .cfg_digest = ELP_HMAC_MD5,
+        .cfg_cipher = ELP_CIPHER_DES,
+        .type = TYPE_ESPAH,
+}, {
+        .crypto = {
+                .cra_name       = "authenc(hmac(sha1),cbc(des))",
+                .cra_blocksize  = DES_BLOCK_SIZE,
+                .cra_u          = { .aead = {
+                        .ivsize         = DES_BLOCK_SIZE,
+                        .maxauthsize    = SHA1_DIGEST_SIZE,
+                        }
+                }
+        },
+        .cfg_digest = ELP_HMAC_SHA1,
+        .cfg_cipher = ELP_CIPHER_DES,
+        .type = TYPE_ESPAH,
+}, {
+        .crypto = {
+                .cra_name       = "authenc(hmac(md5),cbc(des3_ede))",
+                .cra_blocksize  = DES3_EDE_BLOCK_SIZE,
+                .cra_u          = { .aead = {
+                        .ivsize         = DES3_EDE_BLOCK_SIZE,
+                        .maxauthsize    = MD5_DIGEST_SIZE,
+                        }
+                }
+        },
+        .cfg_digest = ELP_HMAC_MD5,
+        .cfg_cipher = ELP_CIPHER_3DES,
+        .type = TYPE_ESPAH,
+}, {
+        .crypto = {
+                .cra_name       = "authenc(hmac(sha1),cbc(des3_ede))",
+                .cra_blocksize  = DES3_EDE_BLOCK_SIZE,
+                .cra_u          = { .aead = {
+                        .ivsize         = DES3_EDE_BLOCK_SIZE,
+                        .maxauthsize    = SHA1_DIGEST_SIZE,
+                        }
+                }
+        },
+        .cfg_digest = ELP_HMAC_SHA1,
+        .cfg_cipher = ELP_CIPHER_3DES,
+        .type = TYPE_ESPAH,
+}
+
+};
+
+
+
+
+#define TRANSCEDE_POSTFIX "-transcede"
+/*!
+ *	This function registers the supported algos to the Linux kernel
+ *	\param pdev Pointer to the platform_device instance
+ *	\retval	0 on success, <0 on error
+ */
+static int crypto_alg_init(struct platform_device *pdev)
+{
+	int num = ARRAY_SIZE(comcerto_algos);
+	int i, retval;
+
+	for (i=0; i< num; i++) {
+		struct crypto_alg *cra = &comcerto_algos[i].crypto;
+
+		cra->offloaded = 1;
+
+		if (snprintf(cra->cra_driver_name, CRYPTO_MAX_ALG_NAME,
+			"%s"TRANSCEDE_POSTFIX, cra->cra_name) >=
+			CRYPTO_MAX_ALG_NAME)
+		{
+			continue;
+		}
+
+		if (comcerto_algos[i].type == TYPE_RAW_CRYPTO) {
+			/* block ciphers */
+			cra->cra_type = &crypto_ablkcipher_type;
+			cra->cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER |
+					 CRYPTO_ALG_ASYNC;
+			if (!cra->cra_ablkcipher.setkey)
+				cra->cra_ablkcipher.setkey = ablk_setkey;
+			if (!cra->cra_ablkcipher.encrypt)
+				cra->cra_ablkcipher.encrypt = ablk_encrypt;
+			if (!cra->cra_ablkcipher.decrypt)
+				cra->cra_ablkcipher.decrypt = ablk_decrypt;
+			cra->cra_init = ablk_init;
+			cra->cra_exit = ablk_exit;
+		} else {
+			/* authenc */
+			cra->cra_type = &crypto_aead_type;
+			cra->cra_flags = CRYPTO_ALG_TYPE_AEAD |
+					 CRYPTO_ALG_ASYNC;
+			cra->cra_aead.setkey = aead_setkey;
+			cra->cra_aead.setauthsize = aead_setauthsize;
+			cra->cra_aead.encrypt = aead_encrypt;
+			cra->cra_aead.decrypt = aead_decrypt;
+			cra->cra_aead.givencrypt = aead_givencrypt;
+			cra->cra_init = aead_init;
+			cra->cra_exit = aead_exit;
+		}
+
+		cra->cra_ctxsize = sizeof(struct comcerto_ctx); /* context passed to cra */
+
+		cra->cra_module = THIS_MODULE;
+
+		cra->cra_alignmask = 3;
+		cra->cra_priority = 300;
+
+		if (crypto_register_alg(cra)) {
+                        DPRINTF(ESPAH_ERR, "Failed to register %s\n", cra->cra_name);
+                }
+		else
+			comcerto_algos[i].registered = 1;
+	}
+
+	retval = 0;
+	DPRINTF(ESPAH_DBG, "%s passed\n", __FUNCTION__);
+
+	return(retval);
+}
+
+/*!
+ *
+ *	\return none
+ */
+static void crypto_alg_exit(void)
+{
+	int num = ARRAY_SIZE(comcerto_algos);
+	int i;
+
+	for (i=0; i< num; i++) {
+		if (comcerto_algos[i].registered)
+			crypto_unregister_alg(&comcerto_algos[i].crypto);
+	}
+}
+
+
+/* ---------------------------------------------------- */
+
+
+/*!
+ *	This function is the platform_driver probe entry point.
+ *	The function registers the module as a Platform Driver.
+ *	It allocates memory and initializes the sc structure.
+ *	It finally calls the espah_init() function to complete the ESPAH init.
+ *	\param	pdev platform device assigned and provided by Linux
+ *	\return	0
+ */
+static int comcerto_crypto_probe(struct platform_device *pdev)
+{
+	struct elp_softc *sc;
+	struct resource *r;
+	int retval;
+
+	DPRINTF(ESPAH_DBG, "%s\n", __FUNCTION__);
+
+	sc = (struct elp_softc *) kmalloc(sizeof(*sc), GFP_KERNEL);
+
+	if (!sc) {
+		retval = -ENOMEM;
+		goto err;
+	}
+	memset(sc, 0, sizeof(struct elp_softc));
+
+	sc->sc_irq = -1;
+	sc->sc_cid = -1;
+
+	sc->pdev = pdev; /* save/store the platform device pointer provided by Linux */
+	platform_set_drvdata(sc->pdev, sc);
+	comcerto_acrypto_platform_dev = pdev;
+
+	/* get the ESPAH register base address */
+	r = platform_get_resource_byname(pdev, IORESOURCE_MEM, "base_core");
+	if (!r) {
+		DPRINTF(ESPAH_ERR, "%s: elp %d missing resource information\n", __func__, pdev->id);
+		retval = -EINVAL;
+		goto err;
+	}
+
+	/* get & save the ESPAH register base address */
+	sc->espah_base_addr = TRANSCEDE_IPSEC_REG_BASE;
+
+	if (!sc->espah_base_addr) {
+		DPRINTF(ESPAH_ERR, "%s: elp (dev id %d) ioremap fail\n", __func__, pdev->id);
+		retval = -EINVAL;
+		goto err;
+	}
+
+	DPRINTF(ESPAH_DBG, "%s sc->espah_base_addr = %08x\n", __FUNCTION__, sc->espah_base_addr);
+
+	/* get the ESPAH irq line */
+	r = platform_get_resource_byname(pdev, IORESOURCE_IRQ, "irq_core");
+	if (!r) {
+		DPRINTF(ESPAH_ERR, "%s: elp %d missing resource information\n", __func__, pdev->id);
+		retval = -EINVAL;
+		goto err;
+	}
+
+	/* check the specified irq number is valid */
+	sc->espah_irq = r->start;
+
+	if (sc->espah_irq < 0) {
+		DPRINTF(ESPAH_ERR, "%s: comcerto-spa-espah-irq %d invalid\n", __func__, sc->espah_irq);
+		retval = -EINVAL;
+		goto err;
+	}
+
+	DPRINTF(ESPAH_DBG, "%s sc->espah_irq = %d\n", __FUNCTION__, sc->espah_irq);
+
+	spin_lock_init(&sc->reg_lock);
+
+	if (elp_espah_init(sc) != 0)
+	{
+		DPRINTF(ESPAH_ERR, "elp_espah_init failed\n");
+		retval = -EINVAL;
+		goto err;
+	}
+
+	if (crypto_alg_init(pdev) != 0)
+	{
+		DPRINTF(ESPAH_ERR, "crypto_alg_init failed\n");
+		retval = -EINVAL;
+		goto err;
+	}
+
+	DPRINTF(ESPAH_DBG, "%s passed\n", __FUNCTION__);
+	return (0);
+
+err:
+	DPRINTF(ESPAH_ERR, "%s !ERR!\n", __FUNCTION__);
+	comcerto_crypto_remove(pdev);
+	return(retval);
+}
+
+/*!
+ *	This function is called when unloading the module from kernel space
+ *	\param	pdev platform device pointer
+ *	\return	0
+ */
+static int comcerto_crypto_remove(struct platform_device *pdev)
+{
+	struct elp_softc *sc = platform_get_drvdata(pdev);
+
+	DPRINTF(ESPAH_DBG, "%s() irq %d\n", __FUNCTION__, sc->espah_irq);
+
+	elp_espah_exit(pdev); /*  */
+
+	crypto_alg_exit();
+
+	if (sc->espah_irq != -1)
+		free_irq(sc->espah_irq, pdev);
+
+/* There is an issue with this code - it causes kernel oops.
+ * This code has never been called before, because the module has been embedded into kernel.
+ * The issue has revealed when we started using driver as a module.
+ * Have no time to investigate this issue.
+ */
+#if 0
+	if (sc->espah_base_addr)
+		iounmap((void *) sc->espah_base_addr);
+
+	if(sc != NULL)
+		kfree(sc);
+#endif
+
+	return 0;
+}
+
+
+/* ---------------------------------------------------- */
+
+
+/*! Structure for a platform device driver */
+static struct platform_driver comcerto_crypto_driver = {
+	.probe = comcerto_crypto_probe,
+	.remove = comcerto_crypto_remove,
+	.driver = {
+		.name = "Transcede ESPAH",
+	}
+};
+
+/*!
+ *	This function is the kernel module entry point.
+ *	The function registers the module as a Platform Driver.
+ *	\retval	Value returned by plaftorm_driver_register()
+ */
+static int __init comcerto_crypto_init(void)
+{
+	int rc = 1;
+#if 0
+	espah_debug = (int *)0xFB01FFF0;
+	*espah_debug = 0;
+#else
+	espah_debug = &local_espah_debug;
+#endif
+	rc = platform_driver_register(&comcerto_crypto_driver);
+	DPRINTF(ESPAH_ERR, "%s returns rc=%d\n", __FUNCTION__, rc);
+	return rc;
+}
+
+/*!
+ *	This function is the kernel module exit point.
+ *	The function unregisters the module as a Platform Driver.
+ *	\retval	None
+ */
+static void __exit comcerto_crypto_exit(void)
+{
+	DPRINTF(ESPAH_ERR, "%s\n", __FUNCTION__);
+	platform_driver_unregister(&comcerto_crypto_driver);
+}
+
+
+module_init(comcerto_crypto_init);
+module_exit(comcerto_crypto_exit);
+
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Intel");
+MODULE_DESCRIPTION("ESPAH driver for T2K Families");
diff --git a/drivers/crypto/comcerto_crypto.h b/drivers/crypto/comcerto_crypto.h
new file mode 100644
index 0000000..42e9643
--- /dev/null
+++ b/drivers/crypto/comcerto_crypto.h
@@ -0,0 +1,92 @@
+
+#ifndef _COMCERTO_CRYPTO_H_
+#define _COMCERTO_CRYPTO_H_
+
+#include "comcerto_types.h"
+
+#define TYPE_RAW_CRYPTO         0
+#define TYPE_ESPAH              1
+
+#define ESPAH_IPV4 		0
+#define ESPAH_IPV6 		1
+
+#define ESP_HDRLEN      	8
+
+#define MAX_KEYLEN 		32
+#define MAX_IVLEN   		16
+
+#define COMCERTO_CRA_PRIORITY	3000
+#define COMCERTO_MAX_KEY_SIZE	32
+#define COMCERTO_MAX_IV_LENGTH	16 /* max of AES_BLOCK_SIZE, DES3_EDE_BLOCK_SIZE */
+
+#define MD5_DIGEST_SIZE         16
+#define NULL_KEY_SIZE           0
+#define NULL_BLOCK_SIZE         1
+#define NULL_DIGEST_SIZE        0
+#define NULL_IV_SIZE            0
+
+
+struct ablk_ctx {
+	struct buffer_desc *src;
+	struct buffer_desc *dst;
+};
+
+struct aead_ctx {
+	struct buffer_desc *buffer;
+	struct scatterlist ivlist;
+	/* used when the hmac is not on one sg entry */
+	u8 *hmac_virt;
+	int encrypt;
+};
+
+struct comcerto_ctx {
+	struct platform_device *pdev;
+
+	struct elp_softc *sc;
+
+	dma_addr_t sa_dma_addr_phys;
+	void *sa_dma_addr_virt;
+	elp_sa_t *sa_hw;
+
+	int first_packet_received;
+
+	__be32 desc_hdr_template;
+
+	u8 keyval[COMCERTO_MAX_KEY_SIZE*2];
+	u8 auth_keyval[COMCERTO_MAX_KEY_SIZE];
+	u8 enc_keyval[COMCERTO_MAX_KEY_SIZE];
+
+	unsigned int key_len;
+	unsigned int auth_keylen;
+	unsigned int enc_keylen;
+
+	unsigned int authsize;
+
+	u32 digest_alg; /* authentication algo */
+	u32 cipher_alg; /* encryption/decryption cipher algo */
+
+	u32 spi; /* Secure Policy Identificator */
+
+	u8 ip_family;
+
+#if 0
+	u8 anti_replay;
+	u8 ipsec_protocol;
+	u8 lifetime;
+#endif
+
+	u8 iv[COMCERTO_MAX_IV_LENGTH];
+
+	struct scatterlist sg_dst;
+};
+
+
+struct comcerto_alg {
+	struct crypto_alg crypto;
+	u32 cfg_digest;
+	u32 cfg_cipher;
+	u32 type;
+	int registered;
+};
+
+#endif
diff --git a/drivers/crypto/comcerto_espah.c b/drivers/crypto/comcerto_espah.c
new file mode 100644
index 0000000..bf8aec3
--- /dev/null
+++ b/drivers/crypto/comcerto_espah.c
@@ -0,0 +1,894 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/spinlock.h>
+#include <linux/random.h>
+#include <linux/skbuff.h>
+#include <asm/scatterlist.h>
+#include <linux/dma-mapping.h>
+#include <linux/uio.h>
+#include <linux/dmapool.h>
+#include <net/ipv6.h>
+#include <net/ip.h>
+#include <asm/io.h>
+#include <linux/version.h>
+
+#include <crypto/scatterwalk.h>
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,15)
+#include <linux/platform_device.h>
+#endif
+
+#ifdef CONFIG_MACH_M822XX
+#include <mach/sysheap.h>
+#include <mach/hardware.h>
+#include <mach/revision.h>
+#endif
+
+#include "comcerto_espah.h"
+#include "comcerto_crypto.h"
+
+volatile int local_espah_debug = 0;// = ESPAH_ERR | ESPAH_DBG;
+volatile int *espah_debug;
+
+elp_device_id_t elp_get_device_id(void)
+{
+#ifdef CONFIG_MACH_M822XX
+   if (get_t2200_rev() == T2200_REV_X2_1)
+      return ELP_DEVICE_ID_30;
+#endif
+   return ELP_DEVICE_ID_36;
+}
+
+/*!
+ *	elp_map_scatterlist() is called to perform encryption and/or authentication operations on h/w
+ *	\param	buf Pointer to the Elliptic operand
+ *	\param	sg Pointer to the scatterlist
+ *	\param	nbytes ......
+ *	\retval	0 on success otherwise <0
+ */
+struct elp_operand *elp_map_scatterlist(struct elp_softc *sc, struct elp_operand *buf, struct scatterlist *sg, unsigned nbytes, struct scatterlist *sg_ip)
+{
+	unsigned len;
+	void *ptr;
+
+	DPRINTF(ESPAH_DBG, "%s - nbytes = %d sg %p length %d\n", __FUNCTION__, nbytes, sg, sg->length);
+
+	if(sg_ip != NULL){
+		DPRINTF(ESPAH_DBG, "%s add ip header buffer %p (%d bytes)\n", __FUNCTION__, sg_ip, sg_ip->length);
+
+		len = sg_ip->length;
+		buf->sg = sg_ip;
+		buf->sg_len += len;
+
+		ptr = page_address(sg_page(sg_ip)) + sg_ip->offset;
+		sg_dma_address(sg_ip) = dma_map_single(&sc->pdev->dev, ptr, len+ELP_MAX_ESPAH_OVERHEAD, DMA_BIDIRECTIONAL);
+		buf->segs[buf->nsegs].ds_addr = sg_dma_address(sg_ip);
+
+		buf->segs[buf->nsegs].ds_len = len;
+
+		buf->nsegs++;
+	} else {
+		buf->sg = sg;
+	}
+
+	while (sg) {
+		len = sg->length;
+		buf->sg_len += len;
+
+		DPRINTF(ESPAH_DBG, "%s add buffer %p (%d bytes)\n", __FUNCTION__, sg, len);
+
+		ptr = page_address(sg_page(sg)) + sg->offset;
+		sg_dma_address(sg) = dma_map_single(&sc->pdev->dev, ptr, len+ELP_MAX_ESPAH_OVERHEAD, DMA_BIDIRECTIONAL);
+		buf->segs[buf->nsegs].ds_addr = sg_dma_address(sg);
+
+		buf->segs[buf->nsegs].ds_len = len;
+
+		buf->nsegs++;
+		sg = scatterwalk_sg_next(sg);
+	}
+
+	return buf;
+}
+
+
+#if 1
+/*!
+*	elp_unmap_scatterlist() is called to perform encryption and/or authentication operations on h/w
+ *	\param	buf Pointer to the Elliptic operand
+ *	\retval	0 on success otherwise <0
+ */
+static void elp_unmap_scatterlist(struct elp_softc *sc, struct elp_operand *buf)
+{
+	int i;
+
+        DPRINTF(ESPAH_DBG, "%s\n", __FUNCTION__);
+
+	for (i=0; i<buf->nsegs; i++) {
+		dma_unmap_single(&sc->pdev->dev, buf->segs[i].ds_addr, buf->segs[i].ds_len, DMA_BIDIRECTIONAL);
+	}
+}
+#endif
+#if 0
+static void elp_sync_scatterlist(struct elp_softc *sc, struct elp_operand *buf)
+{
+#ifndef CONFIG_CPU_DCACHE_DISABLE
+	int i;
+
+        DPRINTF(ESPAH_DBG, "%s\n", __FUNCTION__);
+
+	for (i=0; i<buf->nsegs; i++) {
+		dma_sync_single_for_device(&sc->pdev->dev, buf->segs[i].ds_addr, buf->segs[i].ds_len, DMA_BIDIRECTIONAL);
+	}
+#endif
+}
+#endif
+
+static void elp_sync_cpu_scatterlist(struct elp_softc *sc, struct elp_operand *buf)
+{
+#ifndef CONFIG_CPU_DCACHE_DISABLE
+	int i;
+
+        DPRINTF(ESPAH_DBG, "%s\n", __FUNCTION__);
+
+	for (i=0; i<buf->nsegs; i++) {
+		dma_sync_single_for_cpu(&sc->pdev->dev, buf->segs[i].ds_addr, buf->segs[i].ds_len, DMA_BIDIRECTIONAL);
+	}
+#endif
+}
+
+
+/*!
+ *	elp_wrapper_enqueue() is ......
+ *	\param	sc Pointer to the main ESPAH structure
+ *	\param	desc Pointer to the Elliptic descriptor
+ *	\retval	0 on success otherwise <0
+ */
+static int elp_wrapper_enqueue(struct elp_softc *sc, struct elp_desc *desc, int direction,U8 id)
+{
+
+	if (direction == ELP_SESDIR_INBOUND) {
+		DPRINTF(ESPAH_DBG, "INBOUND src 0x%x  dst 0x%x sai 0x%x\n", desc->d_src_ddt_addr,desc->d_dst_ddt_addr,desc->d_sa_addr);
+		elp_write(sc, ESPAH_IN_OFFSET, 0);
+		elp_write(sc, ESPAH_IN_SRC_PTR, desc->d_src_ddt_addr);
+		elp_write(sc, ESPAH_IN_DST_PTR, desc->d_dst_ddt_addr);
+		elp_write(sc, ESPAH_IN_ID,id);
+		elp_write(sc, ESPAH_IN_SAI, desc->d_sa_addr);
+	}
+	else {
+		DPRINTF(ESPAH_DBG, "OUTBND src 0x%x  dst 0x%x sai 0x%x\n", desc->d_src_ddt_addr,desc->d_dst_ddt_addr,desc->d_sa_addr);
+		elp_write(sc, ESPAH_OUT_OFFSET, 0);
+		elp_write(sc, ESPAH_OUT_SRC_PTR, desc->d_src_ddt_addr);
+		elp_write(sc, ESPAH_OUT_DST_PTR, desc->d_dst_ddt_addr);
+		elp_write(sc, ESPAH_OUT_ID,id);
+		elp_write(sc, ESPAH_OUT_SAI, desc->d_sa_addr);
+	}
+	return(0);
+
+}
+
+
+
+/*!
+ *	clp30_feed() is called to perform encryption and/or authentication operations on h/w
+ *	\param	sc Pointer to the main ESPAH structure
+ *	\param	pe Pointer to the packet engine
+ *	\param	re Pointer to the ring entry
+ *	\retval	0 on success otherwise <0
+ */
+static int clp30_feed(struct elp_softc *sc, struct elp_packet_engine * pe, struct elp_ringentry *re, dma_addr_t sa_dma_addr_phys, int direction)
+{
+	int i;
+	int retval;
+	struct elp_ddtdesc *srcfree;
+	struct elp_ddtdesc *dstfree;
+
+        DPRINTF(ESPAH_DBG, "%s\n", __FUNCTION__);
+
+	if (((re->re_src.nsegs+1) <= pe->pe_src_ddtavail) && ((re->re_dst.nsegs+1) <= pe->pe_dst_ddtavail)) {
+
+		if (  ((pe->pe_src_ddtringtop - pe->pe_src_ddtfree ) < (re->re_src.nsegs+1 ) ) ){
+			DPRINTF(ESPAH_ERR, "%s  switched  to the  begining of the ring in direction %i\n", __FUNCTION__,direction);
+
+			if (pe->pe_src_ddtavail < (pe->pe_src_ddtringtop - pe->pe_src_ddtfree+re->re_src.nsegs+1)){
+				DPRINTF(ESPAH_ERR, "%s  switching  to the  begining of the ring in direction %i FAILED\n", __FUNCTION__,direction);
+				goto over;
+			}
+			srcfree = pe->pe_src_ddtring;
+		}else{
+			srcfree = pe->pe_src_ddtfree;
+		}
+
+		if (  ((pe->pe_dst_ddtringtop - pe->pe_dst_ddtfree ) < (re->re_dst.nsegs+1 ) ) ){
+			DPRINTF(ESPAH_ERR, "%s  switched  to the  begining of the ring in direction %i\n", __FUNCTION__,direction);
+			if (pe->pe_dst_ddtavail < (pe->pe_dst_ddtringtop - pe->pe_dst_ddtfree+re->re_dst.nsegs+1)){
+				DPRINTF(ESPAH_ERR, "%s  switching  to the  begining of the ring in direction %i FAILED\n", __FUNCTION__,direction);
+				goto over;
+			}
+			dstfree = pe->pe_dst_ddtring;
+		}else{
+			dstfree = pe->pe_dst_ddtfree;
+		}
+
+
+		re->re_desc.d_src_ddt_addr = pe->pe_src_ddt_dma + ((caddr_t) srcfree - (caddr_t) pe->pe_src_ddtring);
+		re->re_desc.d_src_ddt_size = re->re_src.nsegs + 1;
+
+		srcfree->frag_addr = re->re_src.segs[0].ds_addr;
+		srcfree->frag_size = re->re_src.segs[0].ds_len;
+
+		if (++(srcfree) == pe->pe_src_ddtringtop)
+			srcfree = pe->pe_src_ddtring;
+
+		for (i=1;i<re->re_src.nsegs;i++) {
+			srcfree->frag_addr = re->re_src.segs[i].ds_addr;
+			srcfree->frag_size = re->re_src.segs[i].ds_len;
+                        if (++(srcfree) == pe->pe_src_ddtringtop)
+				srcfree = pe->pe_src_ddtring;
+		}
+		srcfree->frag_addr = 0;
+		srcfree->frag_size = 0;
+		if (++(srcfree) == pe->pe_src_ddtringtop)
+			srcfree = pe->pe_src_ddtring;
+
+		re->re_desc.d_dst_ddt_addr = pe->pe_dst_ddt_dma + ((caddr_t) dstfree - (caddr_t) pe->pe_dst_ddtring);
+		re->re_desc.d_dst_ddt_size = re->re_dst.nsegs + 1;
+
+                dstfree->frag_addr = re->re_dst.segs[0].ds_addr;
+		dstfree->frag_size = re->re_dst.segs[0].ds_len;
+
+                if (++(dstfree) == pe->pe_dst_ddtringtop)
+			dstfree = pe->pe_dst_ddtring;
+
+		for (i=1;i<re->re_dst.nsegs;i++) {
+			dstfree->frag_addr = re->re_dst.segs[i].ds_addr;
+			dstfree->frag_size = re->re_dst.segs[i].ds_len;
+			if (++(dstfree) == pe->pe_dst_ddtringtop)
+				dstfree = pe->pe_dst_ddtring;
+		}
+
+		dstfree->frag_addr = 0;
+		dstfree->frag_size = 0;
+		if (++(dstfree) == pe->pe_dst_ddtringtop)
+			dstfree = pe->pe_dst_ddtring;
+
+		re->re_desc.d_sa_addr = sa_dma_addr_phys; /* save DMA pool address used by the hw */
+		re->re_desc.d_status = 0;
+
+		if (elp_wrapper_enqueue(sc, &re->re_desc, direction,(pe->pe_front - pe->pe_ring)&0xFF) != 0) {
+			DPRINTF(ESPAH_ERR,"%s() CFG fifo wrapper overflow !!!  \n", __FUNCTION__ );
+			retval = -ENOMEM;
+			goto err;
+		}
+
+		pe->pe_nqchip++;
+
+		pe->pe_src_ddtavail -= re->re_desc.d_src_ddt_size;
+		pe->pe_src_ddtfree = srcfree;
+
+		pe->pe_dst_ddtavail -= re->re_desc.d_dst_ddt_size;
+		pe->pe_dst_ddtfree = dstfree;
+
+
+		DPRINTF(ESPAH_DBG, "%s - PASSED\n", __FUNCTION__);
+		return(0);
+	}
+over:
+	DPRINTF(ESPAH_ERR, "%s() packet engine ring overflow !!!  \n", __FUNCTION__ );
+	retval = -ENOMEM;
+	return(retval);
+
+err:
+
+	return(retval);
+}
+
+
+/*!
+ *	send_data_to_acrypto_hw() is the ....
+ *	\param	sc Address/pointer to the main ESPAH structure
+ *	\param	pe Pointer to the packet engine
+ *	\param	re Pointer to the ring entry
+ *	\retval	None
+ */
+
+int send_data_to_acrypto_hw(struct elp_softc *sc, int direction, struct scatterlist *scatter_src, int scatter_src_len, struct scatterlist *scatter_dst, int scatter_dst_len, dma_addr_t sa_dma_addr_phys, void *context)
+{
+	struct elp_packet_engine *pe;
+	struct elp_ringentry *re;
+	struct elp_operand *op;
+	int retval;
+	int re_flags;
+	int pe_nqchip;
+
+	struct aead_request *req = (struct aead_request *)context;
+
+	DPRINTF(ESPAH_DBG, "%s\n", __FUNCTION__);
+
+	/* which PE engine to use */
+	if (direction == ELP_SESDIR_OUTBOUND) {
+		pe = &sc->pe[PE_OUTBOUND];
+		DPRINTF(ESPAH_DBG, "%s() outbound processing \n", __FUNCTION__ );
+	}
+	else {
+		pe = &sc->pe[PE_INBOUND];
+		DPRINTF(ESPAH_DBG, "%s() inbound processing \n", __FUNCTION__ );
+	}
+
+	spin_lock_irqsave(&pe->pe_ringmtx,pe->pe_flags);
+
+	re = pe->pe_front; /* get ring entry */
+	if (!re) {
+		retval = -EAGAIN; /* ERROR; */
+		goto err;
+	}
+
+	re_flags = re->re_flags;
+	pe_nqchip = pe->pe_nqchip;
+
+	if (pe_nqchip>(ESPAH_WRP_FIFO_DEPTH-1)){
+		retval = -EAGAIN; /* ERROR; */
+		DPRINTF(ESPAH_ERR, "%s()  processing OVERFLOW dir %i pe_nqchip  %i\n", __FUNCTION__ ,direction,pe_nqchip);
+		goto err;
+	}
+
+	if (re_flags){
+		retval = -EAGAIN; /* ERROR; */
+		DPRINTF(ESPAH_ERR, "%s()  processing OVERFLOW dir re_flags  %i pe_nqchip  %i\n", __FUNCTION__ ,direction,pe_nqchip);
+		goto err;
+	}
+
+	re->context = context;
+	re->re_src.nsegs = 0;
+	re->re_dst.nsegs = 0;
+
+	/* WA - Save a copy of the pointer to the esp4 callback (esp_output_done or esp_input_done) in case of last 2 bytes corruption */
+	if(direction == ELP_SESDIR_OUTBOUND)
+		sc->aead_request_complete = req->base.complete;
+	else
+		sc->aead_request_complete2 = req->base.complete;
+
+	DPRINTF(ESPAH_DBG, "%s() src sglist %d bytes\n", __FUNCTION__,scatter_src_len);
+
+	if(direction == ELP_SESDIR_OUTBOUND)
+		op = elp_map_scatterlist(sc, &re->re_src, scatter_src, scatter_src_len, req->ip);
+	else
+		op = elp_map_scatterlist(sc, &re->re_src, scatter_src, scatter_src_len, NULL);
+
+	if (!op) {
+		retval = -ENOMEM;
+		goto err;
+	}
+
+	DPRINTF(ESPAH_DBG, "%s() dst sglist %d bytes \n", __FUNCTION__,scatter_dst_len);
+	op = elp_map_scatterlist(sc, &re->re_dst, scatter_dst, scatter_dst_len, NULL);
+	if (!op) {
+		retval = -ENOMEM;
+		goto err;
+	}
+
+	retval = clp30_feed(sc, pe, re, sa_dma_addr_phys, direction);
+	if (retval != 0) {
+		retval = -ENOMEM;
+		goto err;
+	}
+
+	re->re_flags = 1;
+
+	if (++(pe->pe_front) == pe->pe_ringtop)
+		pe->pe_front = pe->pe_ring;
+
+	spin_unlock_irqrestore(&pe->pe_ringmtx,pe->pe_flags);
+	retval = -EINPROGRESS;
+	return(retval);
+err:
+	spin_unlock_irqrestore(&pe->pe_ringmtx,pe->pe_flags);
+	return(retval);
+}
+
+
+/*!
+ *	elp_callback() is the callback function
+ *	\param	sc Address/pointer to the main ESPAH structure
+ *	\param	pe Pointer to the packet engine
+ *	\param	re Pointer to the ring entry
+ *	\retval	None
+ */
+static void elp_callback(struct elp_softc *sc, struct elp_packet_engine *pe, struct elp_ringentry *re)
+{
+	uint16_t ret_code;
+	int failed;
+
+	DPRINTF(ESPAH_DBG, "%s\n", __FUNCTION__);
+
+	failed = -EBADMSG;
+
+	ret_code = (re->re_desc.d_status & ESPAH_STAT_RET_CODE_MASK) >> ESPAH_STAT_RET_CODE;
+	switch(ret_code) {
+		case  ESPAH_STAT_OK:
+		case  ESPAH_STAT_SOFT_TTL:
+			DPRINTF(ESPAH_DBG, "%s successful\n", __FUNCTION__);
+			failed = 0;
+			break;
+
+		case  ESPAH_STAT_HARD_TTL:
+			DPRINTF(ESPAH_ERR, "!ERR! Hard TTL\n");
+			break;
+		case  ESPAH_STAT_SA_INACTIVE:
+			DPRINTF(ESPAH_ERR, "!ERR! SA inactive\n");
+			break;
+		case  ESPAH_STAT_REPLAY:
+			DPRINTF(ESPAH_ERR, "!ERR! Replay\n");
+			break;
+		case  ESPAH_STAT_ICV_FAIL:
+			DPRINTF(ESPAH_ERR, "!ERR! SA ICV fail\n");
+			break;
+		case  ESPAH_STAT_SEQ_ROLL:
+			break;
+		case  ESPAH_STAT_MEM_ERROR:
+			DPRINTF(ESPAH_ERR, "!ERR! Memory error\n");
+			break;
+		case  ESPAH_STAT_VERS_ERROR:
+			DPRINTF(ESPAH_ERR, "!ERR! Vers error\n");
+			break;
+		case  ESPAH_STAT_PROT_ERROR:
+			DPRINTF(ESPAH_ERR, "!ERR! Prot error\n");
+			break;
+		case  ESPAH_STAT_PYLD_ERROR:
+			DPRINTF(ESPAH_ERR, "!ERR! Payload error\n");
+			break;
+		case  ESPAH_STAT_PAD_ERROR:
+			DPRINTF(ESPAH_ERR, "!ERR! Pad error\n");
+			break;
+	}
+
+	DPRINTF(ESPAH_DBG, "ESPAH - %s, ret_code = %d id = %i\n", __FUNCTION__, ret_code,(re->re_desc.d_status&0x00FF0000)>>16);
+
+	pe->pe_src_ddtavail += re->re_desc.d_src_ddt_size;
+	pe->pe_dst_ddtavail += re->re_desc.d_dst_ddt_size;
+
+	spin_unlock_irqrestore(&pe->pe_ringmtx,pe->pe_flags);
+	comcerto_crypto_done(sc, re->context, failed);
+	spin_lock_irqsave(&pe->pe_ringmtx,pe->pe_flags);
+	DPRINTF(ESPAH_DBG, "%s DONE\n", __FUNCTION__);
+}
+
+
+
+
+
+/*!
+ *	elp_irq_in_tasklet() ..... calls the callback function (elp_callback())
+ *	\param	arg Address/pointer to the main ESPAH structure
+ *	\retval	None
+ */
+static void elp_irq_in_tasklet(unsigned long arg)
+{
+	struct elp_softc *sc = (struct elp_softc*)arg;
+	u_int32_t stat;
+	unsigned long flags;
+	struct elp_packet_engine *pe;
+	struct elp_ringentry *re;
+	struct aead_request *areq;
+	const uint32_t stat_reg = (elp_get_device_id() == ELP_DEVICE_ID_30 ? ESPAH_IN_STAT : ESPAH_IN_FIFO_STAT);
+
+        DPRINTF(ESPAH_DBG, "ESPAH - %s\n", __FUNCTION__);
+
+	stat = elp_read(sc, stat_reg);
+
+	pe = &sc->pe[PE_INBOUND];
+
+	spin_lock_irqsave(&pe->pe_ringmtx, pe->pe_flags);
+
+	while ((stat & ESPAH_STAT_FIFO_EMPTY_MASK) == 0) {
+
+		elp_write(sc, ESPAH_IN_POP, 1);
+		stat = elp_read(sc, ESPAH_IN_STAT);
+		re = pe->pe_ring + ((stat&0x00FF0000)>>16);
+		re->re_desc.d_status = stat;
+
+		pe->pe_nqchip--;
+
+		elp_sync_cpu_scatterlist(sc, &re->re_dst);
+		elp_unmap_scatterlist(sc, &re->re_dst);
+		elp_unmap_scatterlist(sc, &re->re_src);
+		areq = (struct aead_request *) re->context;
+                areq->base.complete = (crypto_completion_t) sc->aead_request_complete2;
+
+		elp_callback(sc, pe, re);
+
+		re->re_flags = 0;
+
+		if (++(pe->pe_back) == pe->pe_ringtop)
+			pe->pe_back = pe->pe_ring;
+
+		stat = elp_read(sc, stat_reg);
+	}
+
+	spin_unlock_irqrestore(&pe->pe_ringmtx, (unsigned long) pe->pe_flags);
+
+	spin_lock_irqsave(&sc->reg_lock, flags);
+	elp_enable_int(sc, ESPAH_IRQ_INBND_STAT_EN);
+	spin_unlock_irqrestore(&sc->reg_lock, (unsigned long) flags);
+
+	DPRINTF(ESPAH_DBG, "ESPAH - DONE%s\n", __FUNCTION__);
+}
+
+
+/*!
+ *	elp_irq_out_tasklet() ... calls the callback (elp_callback())
+ *	\param	arg Address/pointer to the main ESPAH structure
+ *	\retval	None
+ */
+static void elp_irq_out_tasklet(unsigned long arg)
+{
+	struct elp_softc *sc = (struct elp_softc*)arg;
+	u_int32_t stat;
+	unsigned long flags;
+	struct elp_packet_engine *pe;
+	struct elp_ringentry *re;
+	struct aead_request *areq;
+	const uint32_t stat_reg = (elp_get_device_id() == ELP_DEVICE_ID_30 ? ESPAH_OUT_STAT : ESPAH_OUT_FIFO_STAT);
+
+	DPRINTF(ESPAH_DBG, "ESPAH - %s\n", __FUNCTION__);
+
+	stat = elp_read(sc, stat_reg);
+
+	pe = &sc->pe[PE_OUTBOUND];
+
+	spin_lock_irqsave(&(pe->pe_ringmtx), pe->pe_flags);
+
+	while ((stat & ESPAH_STAT_FIFO_EMPTY_MASK) == 0) {
+
+
+		elp_write(sc, ESPAH_OUT_POP, 1);
+		stat = elp_read(sc, ESPAH_OUT_STAT);
+		re = pe->pe_ring + ((stat&0x00FF0000)>>16);
+		re->re_desc.d_status = stat;
+
+		pe->pe_nqchip--;
+
+		elp_sync_cpu_scatterlist(sc, &re->re_dst);
+		elp_unmap_scatterlist(sc, &re->re_dst);
+		elp_unmap_scatterlist(sc, &re->re_src);
+
+		/* WA - Restore the pointer to esp_output_done as the first 2 bytes may be corrupted, reset to 0 */
+		areq = (struct aead_request *) re->context;
+                areq->base.complete = (crypto_completion_t) sc->aead_request_complete;
+
+		elp_callback(sc,pe,re);
+
+		re->re_flags = 0;
+
+		if (++(pe->pe_back) == pe->pe_ringtop)
+			pe->pe_back = pe->pe_ring;
+
+		stat = elp_read(sc, stat_reg);
+	}
+
+	spin_unlock_irqrestore(&pe->pe_ringmtx, (unsigned long) pe->pe_flags);
+
+	spin_lock_irqsave(&sc->reg_lock, flags);
+	elp_enable_int(sc, ESPAH_IRQ_OUTBND_STAT_EN);
+	spin_unlock_irqrestore(&sc->reg_lock, (unsigned long) flags);
+	DPRINTF(ESPAH_DBG, "ESPAH - DONE%s\n", __FUNCTION__);
+}
+
+
+/*!
+ *	espah_intr() is the Transcede ESPAH interrupt handler
+ *	\param	irq irq number
+ *	\param	dev_id device id which is the address/pointer to the main ESPAH structure
+ *	\retval	IRQ_HANDLED
+ */
+irqreturn_t espah_intr(int irq, void *dev_id)
+{
+	struct elp_softc *sc = (struct elp_softc *)dev_id;
+	u_int32_t stat;
+	unsigned long flags;
+
+	DPRINTF(ESPAH_DBG, "ESPAH - %s\n", __FUNCTION__);
+
+	spin_lock_irqsave(&sc->reg_lock, flags);
+
+	stat = elp_read(sc, ESPAH_INT_STAT);
+
+	if (stat) {
+
+		if (stat & ESPAH_STAT_OUTBND_STAT) {
+			elp_disable_int(sc, ESPAH_IRQ_OUTBND_STAT_EN);
+			elp_write(sc, ESPAH_INT_STAT, ESPAH_STAT_OUTBND_STAT|ESPAH_STAT_OUTBND_CMD);
+			tasklet_schedule(&sc->irq_out_tasklet);
+		}
+
+		if (stat & ESPAH_STAT_INBND_STAT) {
+			elp_disable_int(sc, ESPAH_IRQ_INBND_STAT_EN);
+			elp_write(sc, ESPAH_INT_STAT, ESPAH_STAT_INBND_STAT|ESPAH_STAT_INBND_CMD);
+			tasklet_schedule(&sc->irq_in_tasklet);
+		}
+
+	}
+	spin_unlock_irqrestore(&sc->reg_lock, flags);
+
+	return IRQ_HANDLED;
+}
+
+
+
+/*!
+ *	This function initializes the ESPAH layer.
+ *	\param	*sc Address/pointer to the main ESPAH structure
+ *	\retval	>0 if success, <0 if error
+ */
+
+#define IRAMDDT
+#define TRANSCEDE_ESPAH_IRAM_OFFSET 0x8000
+
+int elp_espah_init(struct elp_softc *sc)
+{
+	struct elp_packet_engine *pe;
+	int retval = 0;
+	int i;
+	int intr;
+	void *iram_addr;
+	dma_addr_t iram_phy_addr;
+
+	DPRINTF(ESPAH_DBG, "%s\n", __FUNCTION__);
+
+#ifdef IRAMDDT
+
+#ifdef CONFIG_MACH_M822XX
+/*
+ * 2 times
+ * src  : ELP_DDT_NUMBER * sizeof(struct elp_ddtdesc)
+ * dst  : ELP_DDT_NUMBER * sizeof(struct elp_ddtdesc)
+ * ring : ELP_PE_RING_SIZE * sizeof(struct elp_ringentry)
+ */
+	iram_addr =(void *) iram_heap_alloc(2 * (   2 * ELP_DDT_NUMBER * sizeof(struct elp_ddtdesc)
+						  + ELP_PE_RING_SIZE * sizeof(struct elp_ringentry)
+						)
+					   );
+
+	if (!iram_addr)
+		return -ENOMEM;
+#else
+	#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,0,0)
+		iram_addr =(void *) (TRANSCEDE_IRAM_BASE + TRANSCEDE_ESPAH_IRAM_OFFSET);
+	#else
+		iram_addr =(void *) (COMCERTO_IRAM_BASE  + TRANSCEDE_ESPAH_IRAM_OFFSET);
+	#endif
+#endif
+	iram_phy_addr = (dma_addr_t)iram_addr;
+
+	for(i=0;i<2;i++) {
+		pe = &sc->pe[i];
+		pe->pe_src_ddt_vma = iram_addr;
+		pe->pe_src_ddt_dma = iram_phy_addr;
+		pe->pe_src_ddtavail = ELP_DDT_NUMBER;
+
+		pe->pe_src_ddtring = (struct elp_ddtdesc*) pe->pe_src_ddt_vma;
+		pe->pe_src_ddtringtop = pe->pe_src_ddtring + ELP_DDT_NUMBER;
+		pe->pe_src_ddtfree = pe->pe_src_ddtring;
+		pe->pe_src_ddtbusy = pe->pe_src_ddtring;
+		memset(pe->pe_src_ddtring, 0, ELP_DDT_NUMBER * sizeof(struct elp_ddtdesc));
+
+		iram_addr+=ELP_DDT_NUMBER * sizeof(struct elp_ddtdesc);
+		iram_phy_addr+=ELP_DDT_NUMBER * sizeof(struct elp_ddtdesc);
+
+		pe->pe_dst_ddt_vma = iram_addr;
+		pe->pe_dst_ddt_dma = iram_phy_addr;
+		pe->pe_dst_ddtavail = ELP_DDT_NUMBER;
+
+		pe->pe_dst_ddtring = (struct elp_ddtdesc*) pe->pe_dst_ddt_vma;
+		pe->pe_dst_ddtringtop = pe->pe_dst_ddtring + ELP_DDT_NUMBER;
+		pe->pe_dst_ddtfree = pe->pe_dst_ddtring;
+		pe->pe_dst_ddtbusy = pe->pe_dst_ddtring;
+		memset(pe->pe_dst_ddtring, 0, ELP_DDT_NUMBER * sizeof(struct elp_ddtdesc));
+
+		iram_addr+=ELP_DDT_NUMBER * sizeof(struct elp_ddtdesc);
+		iram_phy_addr+=ELP_DDT_NUMBER * sizeof(struct elp_ddtdesc);
+#if 0
+		pe->pe_ring = (struct elp_ringentry *)kmalloc(ELP_PE_RING_SIZE*sizeof(struct elp_ringentry), GFP_KERNEL);
+		if (!pe->pe_ring)
+			return(-ENOMEM);
+#else
+		pe->pe_ring = iram_addr;
+		iram_addr+=ELP_PE_RING_SIZE*sizeof(struct elp_ringentry);
+		iram_phy_addr+=ELP_PE_RING_SIZE*sizeof(struct elp_ringentry);
+#endif
+		pe->pe_ringtop = pe->pe_ring + ELP_PE_RING_SIZE;
+		pe->pe_front = pe->pe_ring;
+		pe->pe_back = pe->pe_ring;
+		memset(pe->pe_ring, 0, ELP_PE_RING_SIZE*sizeof(struct elp_ringentry));
+
+		spin_lock_init(&pe->pe_ringmtx);
+
+		pe->pe_needwakeup = 0;
+	}
+#ifdef CONFIG_MACH_M822XX
+#else
+	#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,0,0)
+		printk("Transcede ESPAH: %i bytes used in IRAM\n",iram_phy_addr - TRANSCEDE_IRAM_BASE - TRANSCEDE_ESPAH_IRAM_OFFSET);
+	#else
+		printk("Transcede ESPAH: %i bytes used in IRAM\n",iram_phy_addr - COMCERTO_IRAM_BASE  - TRANSCEDE_ESPAH_IRAM_OFFSET);
+	#endif
+#endif
+
+#else
+	for(i=0; i<2; i++) {
+		pe = &sc->pe[i];
+
+                sc->srcddt_pool = dma_pool_create("ELP SRCDDT", &sc->pdev->dev, ELP_DDT_NUMBER * sizeof(struct elp_ddtdesc), 8, 64*1024);
+                if (!sc->srcddt_pool) {
+			DPRINTF(ESPAH_ERR, "!ERR! %s can't create SRCDDT DMA pool\n", __FUNCTION__);
+			goto err;       /* FIXME */
+                }
+                pe->pe_src_ddt_vma = dma_pool_alloc(sc->srcddt_pool, GFP_KERNEL, &(pe->pe_src_ddt_dma));
+                if (!pe->pe_src_ddt_vma) {
+			retval = -ENOMEM;
+			goto err;
+		}
+		pe->pe_src_ddtavail = ELP_DDT_NUMBER;
+		pe->pe_src_ddtring = (struct elp_ddtdesc*) pe->pe_src_ddt_vma;
+		memset(pe->pe_src_ddtring, 0, ELP_DDT_NUMBER * sizeof(struct elp_ddtdesc));
+
+		pe->pe_src_ddtringtop = pe->pe_src_ddtring + ELP_DDT_NUMBER;
+		pe->pe_src_ddtfree = pe->pe_src_ddtring;
+		pe->pe_src_ddtbusy = pe->pe_src_ddtring;
+
+                sc->dstddt_pool = dma_pool_create("ELP DSTDDT", &sc->pdev->dev, ELP_DDT_NUMBER * sizeof(struct elp_ddtdesc), 8, 64*1024);
+                if (!sc->dstddt_pool) {
+			DPRINTF(ESPAH_ERR, "!ERR! %s can't create DSTDDT DMA pool\n", __FUNCTION__);
+			goto err;       /* FIXME */
+                }
+                pe->pe_dst_ddt_vma = dma_pool_alloc(sc->dstddt_pool, GFP_KERNEL, &(pe->pe_dst_ddt_dma));
+                if (!pe->pe_dst_ddt_vma) {
+			retval = -ENOMEM;
+			goto err;
+		}
+		pe->pe_dst_ddtavail = ELP_DDT_NUMBER;
+		pe->pe_dst_ddtring = (struct elp_ddtdesc*) pe->pe_dst_ddt_vma;
+		memset(pe->pe_dst_ddtring, 0, ELP_DDT_NUMBER * sizeof(struct elp_ddtdesc));
+
+		pe->pe_dst_ddtringtop = pe->pe_dst_ddtring + ELP_DDT_NUMBER;
+		pe->pe_dst_ddtfree = pe->pe_dst_ddtring;
+		pe->pe_dst_ddtbusy = pe->pe_dst_ddtring;
+
+		pe->pe_ring = (struct elp_ringentry *)kmalloc(ELP_PE_RING_SIZE*sizeof(struct elp_ringentry), GFP_KERNEL);
+		if (!pe->pe_ring) {
+			retval = -ENOMEM;
+			goto err;
+		}
+
+		pe->pe_ringtop = pe->pe_ring + ELP_PE_RING_SIZE;
+		pe->pe_front = pe->pe_ring;
+		pe->pe_back = pe->pe_ring;
+
+		spin_lock_init(&pe->pe_ringmtx);
+
+		pe->pe_needwakeup = 0;
+	}
+
+#endif
+	/* Create SA DMA pool */
+	sc->sa_dma_pool = dma_pool_create("ELP SADB", &sc->pdev->dev, ELP_SA_SIZE, ELP_SA_SIZE, 0);
+	if (!sc->sa_dma_pool) {
+		DPRINTF(ESPAH_ERR, "!ERR! %s can't create SA DMA pool\n", __FUNCTION__);
+		goto err;	/* FIXME */
+	}
+
+	spin_lock_init(&sc->reg_lock);
+	tasklet_init(&sc->irq_in_tasklet, elp_irq_in_tasklet, (unsigned long)sc);
+	tasklet_init(&sc->irq_out_tasklet, elp_irq_out_tasklet, (unsigned long)sc);
+
+	/* connect the interrupt */
+	intr = request_irq(sc->espah_irq, espah_intr, IRQF_DISABLED, "Comcerto EAPE", sc);
+	if (intr)
+	{
+		printk(KERN_ERR DRV_NAME ": failed to hook irq %d\n", sc->espah_irq);
+		sc->espah_irq = -1;
+		return (1);
+	}
+
+	DPRINTF(ESPAH_ERR, "M84xxx ESPAH - connected irq %d\n", sc->espah_irq);
+
+#ifdef CONFIG_MACH_M822XX
+	// Take IPSec out of reset
+	ELP_WRITE_UINT(IPSEC_RESET, 0);
+#endif
+
+	dbg_elp_write(sc, ESPAH_OUT_OFFSET, 0);
+	dbg_elp_write(sc, ESPAH_IN_OFFSET, 0);
+
+	dbg_elp_write(sc, ESPAH_IRQ_CTRL, 0x10000);
+
+	dbg_elp_write(sc, ESPAH_INT_STAT, ESPAH_STAT_OUTBND_CMD | ESPAH_STAT_OUTBND_STAT | ESPAH_STAT_INBND_CMD | ESPAH_STAT_INBND_STAT);
+
+	/* enable interrupts */
+	elp_enable_int(sc, ESPAH_IRQ_OUTBND_STAT_EN | ESPAH_IRQ_INBND_STAT_EN | ESPAH_IRQ_GLBL_EN);
+
+	DPRINTF(ESPAH_DBG, "ESPAH - %s passed\n", __FUNCTION__);
+
+	return(0);
+
+err:
+	DPRINTF(ESPAH_ERR, "ESPAH - %s !ERR!\n", __FUNCTION__);
+	return(retval);
+}
+
+
+
+/*!
+ *	This function is called by comcerto_espah_remove() when the module is unloaded
+ *	\param	pdev pointer to the ESPAH platform_device structure
+ *	\retval	None
+ */
+void elp_espah_exit(struct platform_device *pdev)
+{
+	struct elp_softc *sc = platform_get_drvdata(pdev);
+
+	DPRINTF(ESPAH_ERR, "%s()\n", __FUNCTION__);
+	if (sc->espah_irq != -1) {
+		free_irq(sc->espah_irq, sc);
+		sc->espah_irq = -1;
+	}
+
+#if defined(CONFIG_MACH_M822XX) && defined(IRAMDDT)
+	/* Free allocated IRAM */
+	if (sc->pe[0].pe_src_ddt_vma)
+		iram_heap_free(sc->pe[0].pe_src_ddt_vma);
+#endif
+
+/* There is an issue with this code - it causes kernel oops.
+ * This code has never been called before, because the module has been embedded into kernel.
+ * The issue has revealed when we started using driver as a module.
+ * Have no time to investigate this issue.
+ */
+#if 0
+	/* Free all buffers allocated dynamically from DDR */
+	if (sc->pe[0].pe_src_ddtring)
+		kfree(sc->pe[0].pe_src_ddtring);
+	if (sc->pe[1].pe_src_ddtring)
+		kfree(sc->pe[1].pe_src_ddtring);
+
+	if (sc->pe[0].pe_dst_ddtring)
+		kfree(sc->pe[0].pe_dst_ddtring);
+	if (sc->pe[1].pe_dst_ddtring)
+		kfree(sc->pe[1].pe_dst_ddtring);
+
+	if (sc->pe[0].pe_ring)
+		kfree(sc->pe[0].pe_ring);
+	if (sc->pe[1].pe_ring)
+		kfree(sc->pe[1].pe_ring);
+
+	if(sc->sa_dma_pool)
+		dma_pool_destroy(sc->sa_dma_pool);
+
+	if(sc->srcddt_pool)
+		dma_pool_destroy(sc->srcddt_pool);
+
+	if(sc->dstddt_pool)
+		dma_pool_destroy(sc->dstddt_pool);
+#endif
+}
diff --git a/drivers/crypto/comcerto_espah.h b/drivers/crypto/comcerto_espah.h
new file mode 100644
index 0000000..52b8a9d
--- /dev/null
+++ b/drivers/crypto/comcerto_espah.h
@@ -0,0 +1,479 @@
+#ifndef _COMCERTO_ESPAH_H_
+#define _COMCERTO_ESPAH_H_
+#include "comcerto_types.h"
+
+
+#define DRV_NAME "Transcede ESP Offload Engine"
+
+
+#define ESPAH_INT_EN			0x0000
+#define ESPAH_INT_STAT			0x0004
+#define ESPAH_DMA_BURST_SZ		0x0008	/* maximum in words (8-256) */
+#define ESPAH_IV_RND			0x0010	/* value to be added to the core's IV */
+#define ESPAH_ENDIAN_CTRL		0x0018
+#define ESPAH_OUT_SRC_PTR		0x0020
+#define ESPAH_OUT_DST_PTR		0x0024
+#define ESPAH_OUT_OFFSET		0x0028	/* DDT offset to start of packet */
+#define ESPAH_OUT_ID			0x002c	/* software tag */
+#define ESPAH_OUT_SAI			0x0030
+#define ESPAH_OUT_POP			0x0038
+#define ESPAH_OUT_STAT			0x003C
+#define ESPAH_IN_SRC_PTR		0x0040
+#define ESPAH_IN_DST_PTR		0x0044
+#define ESPAH_IN_OFFSET			0x0048	/* DDT offset to start of packet */
+#define ESPAH_IN_ID			0x004C	/* software tag */
+#define ESPAH_IN_SAI			0x0050
+#define ESPAH_IN_POP			0x0058
+#define ESPAH_IN_STAT			0x005C
+
+#define ESPAH_SA_CACHE_FLUSH		0x0080
+#define ESPAH_SA_CACHE_READY		0x0084
+
+#if defined(CONFIG_MACH_M84XXX) || defined(CONFIG_MACH_M822XX)
+#define ESPAH_IRQ_CTRL			0x0080
+#define ESPAH_OUT_FIFO_STAT		0x0088
+#define ESPAH_IN_FIFO_STAT		0x008C
+#endif
+
+/* INT_EN bits */
+#define	ESPAH_IRQ_OUTBND_CMD_EN		0x1
+#define	ESPAH_IRQ_OUTBND_STAT_EN	0x2
+#define	ESPAH_IRQ_INBND_CMD_EN		0x4
+#define	ESPAH_IRQ_INBND_STAT_EN		0x8
+#define	ESPAH_IRQ_ALL_EN		(ESPAH_IRQ_OUTBND_CMD_EN|ESPAH_IRQ_OUTBND_STAT_EN|ESPAH_IRQ_INBND_CMD_EN|ESPAH_IRQ_INBND_STAT_EN)
+#define	ESPAH_IRQ_GLBL_EN		(1UL<<31)
+
+/* INT_STAT bits */
+#define	ESPAH_STAT_OUTBND_CMD		0x1
+#define	ESPAH_STAT_OUTBND_STAT		0x2
+#define	ESPAH_STAT_INBND_CMD		0x4
+#define	ESPAH_STAT_INBND_STAT		0x8
+
+#define BITMASK(pos,width)		(((1UL<<(width))-1) << (pos))
+
+/* IN_STAT and OUT_STAT */
+#define ESPAH_STAT_LENGTH		0
+#define ESPAH_STAT_LENGTH_MASK		0x0000FFFF
+#define ESPAH_STAT_RET_CODE		24
+#define ESPAH_STAT_RET_CODE_MASK	0x0F000000
+#define ESPAH_STAT_BUSY_BIT		31
+
+#define ESPAH_STAT_FIFO_FULL      	30
+#define ESPAH_STAT_FIFO_FULL_MASK 	(1UL<<30)
+#define ESPAH_STAT_FIFO_EMPTY     	31
+#define ESPAH_STAT_FIFO_EMPTY_MASK	(1UL<<31)
+
+/* IN_OFFSET and OUT_OFFSET */
+#define ESPAH_OFFSET_SRC		0
+#define ESPAH_OFFSET_DST		16
+
+/* SA flags (oxffest 0x7e) bits */
+#define ESPAH_ENABLED			0x0001
+#define ESPAH_SEQ_ROLL_ALLOWED		0x0002
+#define ESPAH_TTL_ENABLE		0x0004
+#define ESPAH_TTL_TYPE			0x0008 /* 0:byte 1:time */
+#define ESPAH_AH_MODE			0x0010 /* 0:ESP 1:AH */
+#define ESPAH_ANTI_REPLAY_ENABLE	0x0080
+#define ESPAH_COFF_EN			0x0400 /* 1:Crypto offload enable */
+#define ESPAH_COFF_MODE			0x0800 /* Crypto offload mode: 0: ECB cypher or raw hash, 1 CBC cypher or HMAC hash */
+#define ESPAH_IPV6_ENABLE		0x1000 /* 1:IPv6 SA */
+#define ESPAH_DST_OP_MODE		0x2000 /* IPv6 dest opt treatment EDN-0277 page 16 */
+#define ESPAH_EXTENDED_SEQNUM		0x4000
+
+
+
+/* STAT codes that go into the STAT_RET_CODE  of a register */
+#define ESPAH_STAT_OK  			0
+/* #define ESPAH_STAT_BUSY		1 */
+#define ESPAH_STAT_SOFT_TTL		2
+#define ESPAH_STAT_HARD_TTL		3
+#define ESPAH_STAT_SA_INACTIVE		4
+#define ESPAH_STAT_REPLAY		5
+#define ESPAH_STAT_ICV_FAIL		6
+#define ESPAH_STAT_SEQ_ROLL		7
+#define ESPAH_STAT_MEM_ERROR		8
+#define ESPAH_STAT_VERS_ERROR		9
+#define ESPAH_STAT_PROT_ERROR		10
+#define ESPAH_STAT_PYLD_ERROR		11
+#define ESPAH_STAT_PAD_ERROR		12
+#define ESPAH_STAT_DUMMY_PKT		13
+
+
+
+#define ESPAH_WRP_FIFO_DEPTH		32
+#define ELP_DDT_SIZE			2*sizeof(u_int32_t)	 // pointers/lenght
+#define ELP_DDT_NUMBER			ESPAH_WRP_FIFO_DEPTH*3	// 3 DDT (pointer/len + null/null) per entry
+#define ELP_SA_SIZE			256
+
+/*
+ * Codes
+*/
+#define ELP_HMAC_NULL			0
+#define ELP_HMAC_MD5    		1
+#define ELP_HMAC_SHA1   		2
+#define ELP_HMAC_SHA2   		3
+#define ELP_GCM64       		4
+#define ELP_GCM96       		5
+#define ELP_GCM128			6
+
+#define ELP_CIPHER_NULL 		0
+#define ELP_CIPHER_DES   		1
+#define ELP_CIPHER_3DES 		2
+#define ELP_CIPHER_AES128 		3
+#define ELP_CIPHER_AES192 		4
+#define ELP_CIPHER_AES256 		5
+#define ELP_CIPHER_AES128_GCM 		6
+#define ELP_CIPHER_AES192_GCM 		7
+#define ELP_CIPHER_AES256_GCM 		8
+#define ELP_CIPHER_AES128_CTR 		9
+#define ELP_CIPHER_AES192_CTR 		10
+#define ELP_CIPHER_AES256_CTR 		11
+
+
+#define ESPAH_ERR			0x1
+#define ESPAH_DBG			0x2
+#define ESPAH_WRN			0x4
+#define ESPAH_DUMP			0x8
+
+#define EDDUMP				0
+
+#define PDUMPWORD(t,b,s,m,e)     do {if(1){ dumpword(b,s,m);}}  while(0)
+
+#if 1
+extern volatile int *espah_debug;
+#define	DPRINTF(flags, a...)	if (*espah_debug&flags) { printk("comcerto-crypto : " a); }
+#else
+#define	DPRINTF(a...)
+#endif
+
+
+/* Hardware structure for Comcerto & Transcede ESPAH hardware (Elliptic) security association */
+typedef struct elp_sa_clp36 {
+	U32 seq;
+	U32 ext_seq;
+	U64 anti_replay_mask;
+	U8  auth_key[20];
+	U8  cipher_key[32];
+	U8  iv[16];
+	U32 spi;
+	U8  res1[12];
+	/* Note that next 4 16 bit variables do not match the elliptic doc:
+	 * Interface to elliptic is 32 bit wide and fpp is little-endian
+	 * ence when fields below are fed through the elp interface they arrive
+	 * at the offsets elp expects.
+	 */
+	U16 auth_token;
+	U16 cipher_token;
+	U16 thread_id;
+	U16 res2;
+	U32 hard_ttl_hi;
+	U32 hard_ttl_lo;
+	U32 soft_ttl_hi;
+	U32 soft_ttl_lo;
+	U16 flags;
+	U8  res3;
+	U8  algo;
+	U8  ext_auth_key[12];
+} elp_sa_clp36_t;
+
+/* This is copy-paste from C2000 project */
+typedef struct elp_sa_clp30 {
+	U32		seq;			//+4
+	U32		ext_seq;		// +0
+	U64		anti_replay_mask;	//+8
+	U32		auth_key1;		//+0x14
+	U32		auth_key0;		//+0x10
+	U32		auth_key3;		//+0x1c
+	U32		auth_key2;		//+0x18
+	U32		cipher0;		//+0x24
+	U32		auth_key4;		//+0x20
+	U32		cipher2;		//+0x2c
+	U32		cipher1;		//+0x28>
+	U32		cipher4;		//+0x34
+	U32		cipher3;		//+0x30
+	U32		cipher6;		//+0x3C
+	U32		cipher5;		//+0x38
+	union{
+		U32	CTR_Nonce;		//+0x44
+		U32	iv0;
+	};
+	U32		cipher7;		//+0x40
+	union {					//+0x4C
+		U32	ext_auth_key1;
+		U32	iv2;
+	};
+	union {					//+0x48
+		U32	ext_auth_key0;
+		U32	iv1;
+	};
+	U32		spi;			//+0x54
+	union {					//+0x50
+		U32	ext_auth_key2;
+		U32	iv3;
+	};
+	U8		pad1a[16];
+	U32		hard_ttl_hi;		//+0x6c
+	U8		pad1b[4];		//+0x68
+	U32		soft_ttl_hi;		//+0x74
+	U32		hard_ttl_lo;		//+0x70
+	U16		flags;			//+0x7c
+	U8		res3;
+	U8		algo;
+	U32		soft_ttl_lo;		//+0x78
+} elp_sa_clp30_t;
+
+typedef union elp_sa {
+	struct elp_sa_clp36 clp36;
+	struct elp_sa_clp30 clp30;
+} elp_sa_t;
+
+#define	ELP_MAX_PART			8 /* Maximum scatter/gather depth */
+#define ELP_MAX_ARAM_SA			8
+#define ELP_PE_RING_SIZE		32
+#define ELP_MAX_ESPAH_OVERHEAD		8+16+18+12 /* ESP + IV + max pad + 2 + 12 */
+
+#define	ELP_SESSION(sid)		( (sid) & 0x0fffffff)
+#define	ELP_SID(id, sesn)		(((id+1) << 28) | ((sesn) & 0x0fffffff))
+
+#define ELP_SESMODE_CRYPTO_CIPHER	0x00000001
+#define ELP_SESMODE_CRYPTO_HASH		0x00000002
+#define ELP_SESMODE_CRYPTO_HMAC		0x00000004
+#define ELP_SESMODE_CRYPTO_MASK		0x00000007
+#define ELP_SESMODE_ENCRYPT		0x00000010
+#define ELP_SESMODE_EBC			0x00000020
+#define ELP_SESMODE_CBC			0x00000040
+#define ELP_SESMODE_CTR			0x00000060
+
+#define ELP_SESDIR_INBOUND		0
+#define ELP_SESDIR_OUTBOUND		1
+
+
+
+
+
+/*
+ * Cryptographic operand state.  One of these exists for each
+ * source and destination operand passed in from the crypto
+ * subsystem.  When possible source and destination operands
+ * refer to the same memory.  More often they are distinct.
+ * We track the virtual address of each operand as well as
+ * where each is mapped for DMA.
+ */
+struct elp_operand {
+
+	struct scatterlist *sg;
+	int sg_len;
+
+	struct {
+		dma_addr_t ds_addr;
+		int ds_len;
+	} segs[ELP_MAX_PART];
+
+	int nsegs;
+};
+
+/*
+ * Scatter/Gather data descriptor table.
+ */
+struct elp_ddtdesc {
+	volatile u_int32_t frag_addr;
+	volatile u_int32_t frag_size;
+};
+
+
+struct elp_desc {
+	u_int32_t d_src_ddt_addr;
+	u_int32_t d_dst_ddt_addr;
+	u_int32_t d_src_ddt_size;	 /* number of fragments (2 if flat) */
+	u_int32_t d_dst_ddt_size;	 /* number of fragments (2 if flat) */
+	u_int32_t d_sa_addr;
+	u_int32_t d_status;
+};
+
+struct elp_ringentry {
+	struct elp_desc re_desc;	/* command descriptor */
+	struct cryptop *re_crp;		/* crypto operation */
+
+	struct elp_operand re_src;	/* source operand */
+	struct elp_operand re_dst;	/* destination operand */
+
+	int re_flags;
+
+	void *context;
+};
+
+
+#define	re_src_skb	re_src.u.skb
+#define	re_src_io	re_src.u.io
+#define	re_src_map	re_src.map
+#define	re_src_nsegs	re_src.nsegs
+#define	re_src_segs	re_src.segs
+#define	re_src_mapsize	re_src.mapsize
+
+#define	re_dst_skb	re_dst.u.skb
+#define	re_dst_io	re_dst.u.io
+#define	re_dst_map	re_dst.map
+#define	re_dst_nsegs	re_dst.nsegs
+#define	re_dst_segs	re_dst.segs
+#define	re_dst_mapsize	re_dst.mapsize
+
+
+struct elp_packet_engine{
+
+	spinlock_t pe_ringmtx;			/* Packet Engine ring lock */
+
+	unsigned long pe_flags;
+
+	struct elp_ringentry *pe_ring;		/* PE ring */
+	struct elp_ringentry *pe_ringtop;	/* PE ring top */
+	struct elp_ringentry *pe_front;		/* next free entry */
+	struct elp_ringentry *pe_back;		/* next pending entry */
+
+	dma_addr_t pe_src_ddt_dma;		/* DDT area */
+	void *pe_src_ddt_vma;
+	int pe_src_ddtavail;
+	struct elp_ddtdesc *pe_src_ddtring;	/* source DDT ring */
+	struct elp_ddtdesc *pe_src_ddtringtop;
+	struct elp_ddtdesc *pe_src_ddtfree;
+	struct elp_ddtdesc *pe_src_ddtbusy;
+
+
+	dma_addr_t pe_dst_ddt_dma;		/* DDT area */
+	void *pe_dst_ddt_vma;
+	int pe_dst_ddtavail;
+	struct elp_ddtdesc *pe_dst_ddtring;	/* Inbound destination DDT ring */
+	struct elp_ddtdesc *pe_dst_ddtringtop;
+	struct elp_ddtdesc *pe_dst_ddtfree;
+	struct elp_ddtdesc *pe_dst_ddtbusy;
+
+	int pe_needwakeup;			/* notify crypto layer */
+	int pe_nqchip;				/* # passed to chip */
+};
+
+
+#define PE_INBOUND	0
+#define PE_OUTBOUND	1
+
+
+struct elp_softc {
+	struct platform_device *pdev;
+
+	u32 espah_base_addr;
+	u32 espah_irq;
+
+	u32 sc_irq;
+	u32 sc_cid;			/* crypto tag */
+
+	struct elp_packet_engine pe[2];
+
+	struct dma_pool *sa_dma_pool;
+        struct dma_pool *srcddt_pool;
+        struct dma_pool *dstddt_pool;
+
+	dma_addr_t *sa_dma_addr;
+
+	struct dma_pool *sc_sa_ddr_dma;		/* additional SA in external DDR */
+	struct tasklet_struct irq_in_tasklet;
+	struct tasklet_struct irq_out_tasklet;
+
+	spinlock_t reg_lock;
+
+	crypto_completion_t aead_request_complete; /* WA to store/restore the esp s/w callback */
+	crypto_completion_t aead_request_complete2; /* WA to store/restore the esp s/w callback */
+};
+
+
+struct elp_stats {
+	u_int64_t st_ibytes;
+	u_int64_t st_obytes;
+	u_int32_t st_ipackets;
+	u_int32_t st_opackets;
+	u_int32_t st_invalid;		/* invalid argument */
+	u_int32_t st_badsession;	/* invalid session id */
+	u_int32_t st_badflags;		/* flags indicate !(mbuf | uio) */
+	u_int32_t st_inpefull;		/* inbound packet engine full */
+	u_int32_t st_outpefull;		/* outbound packet engine full */
+	u_int32_t st_maxqchip;
+	u_int32_t st_nodesc;
+};
+
+typedef enum elp_device_id {
+	ELP_DEVICE_ID_30,
+	ELP_DEVICE_ID_36,
+} elp_device_id_t;
+
+/* ---------------------------------------- */
+
+elp_device_id_t elp_get_device_id(void);
+struct elp_operand * elp_map_scatterlist(struct elp_softc *sc, struct elp_operand *buf, struct scatterlist *sg, unsigned nbytes, struct scatterlist *sg_ip);
+
+extern int elp_espah_init(struct elp_softc *sc);
+extern void elp_espah_exit(struct platform_device *pdev);
+
+irqreturn_t espah_intr(int irq, void *dev_id);
+
+
+extern int send_data_to_acrypto_hw(struct elp_softc *sc, int direction, struct scatterlist *scatter_src, int scatter_src_len, struct scatterlist *scatter_dst, int scatter_dst_len, dma_addr_t sa_dma_addr_phys, void *context);
+
+
+void comcerto_crypto_done(struct elp_softc *sc, void *context, int err);
+
+/* ---------------------------------------- */
+
+#define ELP_READ_UINT(mem)		     *(volatile U32 *)(mem)
+#define ELP_WRITE_UINT(mem, val)	     (*(volatile U32 *)(mem)=(U32)(val))
+
+
+static inline void elp_write(struct elp_softc *sc, u32 reg_offset, u32 reg_val)
+{
+	ELP_WRITE_UINT(sc->espah_base_addr + reg_offset, reg_val);
+}
+
+static inline u32 elp_read(struct elp_softc *sc, u32 reg_offset)
+{
+	u32 reg_val;
+	reg_val = ELP_READ_UINT(sc->espah_base_addr + reg_offset);
+	return reg_val;
+}
+
+static inline void dbg_elp_write(struct elp_softc *sc, u32 reg_offset, u32 reg_val)
+{
+	ELP_WRITE_UINT(sc->espah_base_addr + reg_offset, reg_val);
+}
+
+static inline u32 dbg_elp_read(struct elp_softc *sc, u32 reg_offset)
+{
+	u32 reg_val;
+	reg_val = ELP_READ_UINT(sc->espah_base_addr + reg_offset);
+	return reg_val;
+}
+
+#define ESPAH_IRQ_CTRL_SET_STAT_CNT(n) ((n) << ESPAH_IRQ_CTRL_STAT_CNT)
+
+#define ESPAH_IRQ_CTRL_CMD_CNT    0
+#define ESPAH_IRQ_CTRL_STAT_CNT   16
+
+static inline void elp_enable_int(struct elp_softc *sc, u32 mask)
+{
+	u32 reg_val;
+	reg_val = dbg_elp_read(sc, ESPAH_INT_EN);
+	reg_val |= mask;
+	dbg_elp_write(sc,  ESPAH_INT_EN, reg_val);
+
+/*	dbg_elp_read(sc, ESPAH_INT_EN); */
+}
+
+
+static inline void elp_disable_int(struct elp_softc *sc, u32 mask)
+{
+	u32 reg_val;
+	reg_val = dbg_elp_read(sc, ESPAH_INT_EN);
+	reg_val &= ~mask;
+	dbg_elp_write(sc,  ESPAH_INT_EN, reg_val);
+
+/*	dbg_elp_read(sc, ESPAH_INT_EN); */
+}
+
+
+
+#endif
diff --git a/drivers/crypto/comcerto_types.h b/drivers/crypto/comcerto_types.h
new file mode 100644
index 0000000..0aab574
--- /dev/null
+++ b/drivers/crypto/comcerto_types.h
@@ -0,0 +1,46 @@
+#ifndef _COMCERTO_TYPES_H_
+#define _COMCERTO_TYPES_H_
+
+#include <linux/kernel.h>	/* printk() */
+#include <linux/slab.h>		/* kmalloc() */
+#include <linux/fs.h>		/* everything... */
+#include <linux/errno.h>	/* error codes */
+#include <linux/types.h>	/* size_t */
+#include <linux/proc_fs.h>
+#include <linux/fcntl.h>	/* O_ACCMODE */
+#include <linux/seq_file.h>
+#include <linux/cdev.h>
+#include <linux/delay.h>	/* udelay */
+#include <linux/sched.h>
+#include <linux/kdev_t.h>
+#include <linux/mm.h>
+#include <linux/ioport.h>
+#include <linux/interrupt.h>
+#include <linux/workqueue.h>
+#include <linux/poll.h>
+#include <linux/wait.h>		/* wait queue */
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/init.h>
+
+#include <asm/system.h>		/* cli(), *_flags, mb() */
+#include <asm/uaccess.h>	/* copy_*_user */
+#include <asm/io.h>		/* memcpy_fromio */
+#include <mach/syslib.h>
+
+#define USE_IT(x) ((x)=(x))
+
+typedef unsigned int   UINT;
+#if 0
+typedef unsigned char U8;	/* unsigned 8-bit  integer */
+typedef unsigned short U16;	/* unsigned 16-bit integer */
+typedef unsigned long U32;	/* unsigned 32-bit integer */
+typedef unsigned long long U64;	/* unsigned 64-bit integer */
+typedef signed char S8;		/* 8-bit  signed integer */
+typedef signed short S16;	/* 16-bit signed integer */
+typedef signed int S32;		/* 32-bit signed integer */
+typedef signed long long S64;	/* 64 bit signed integer */
+#endif
+typedef char C8;		/* just normal char for compatability with sysytem functions */
+
+#endif
diff --git a/drivers/gpio/Makefile b/drivers/gpio/Makefile
index b605f8e..dafbd1d 100644
--- a/drivers/gpio/Makefile
+++ b/drivers/gpio/Makefile
@@ -48,3 +48,4 @@ obj-$(CONFIG_GPIO_VX855)	+= vx855_gpio.o
 obj-$(CONFIG_GPIO_ML_IOH)	+= ml_ioh_gpio.o
 obj-$(CONFIG_AB8500_GPIO)       += ab8500-gpio.o
 obj-$(CONFIG_GPIO_TPS65910)	+= tps65910-gpio.o
+obj-$(CONFIG_MACH_M822XX)   += gpio-t2200.o
diff --git a/drivers/gpio/gpio-t2200.c b/drivers/gpio/gpio-t2200.c
new file mode 100644
index 0000000..5e04127
--- /dev/null
+++ b/drivers/gpio/gpio-t2200.c
@@ -0,0 +1,211 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+#include <linux/platform_device.h>
+#include <linux/spinlock.h>
+#include <linux/gpio.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <asm/io.h>
+#include <mach/gpio.h>
+#include <mach/clkrst.h>
+
+#define DRV_NAME "t2200-gpio"
+static const unsigned long gpio_timeout_us = 100;
+static const unsigned long gpio_lock = TRANSCEDE_SEMAARMCTRL(TRANSCEDE_SYSLOCKID_GPIO);
+static unsigned long gpio_timeout_tick;
+
+struct t2200_gpio_chip {
+	struct gpio_chip chip;
+};
+
+static inline int __gpio_lock(unsigned long * pflag)
+{
+	unsigned long flags, lock_failed;
+	unsigned long timeout = get_tick() + gpio_timeout_tick;
+
+	raw_local_irq_save(flags);
+	while ((lock_failed = readl(gpio_lock)) && time_before(get_tick(), timeout))
+		;						/* wait for the lock */
+
+	if (lock_failed) {
+		printk(KERN_ERR "error: Re-inherited GPIO lock.\n");
+		*pflag = flags;
+		return -1;
+	}
+	*pflag = flags;
+	return 0;
+}
+
+static inline void __gpio_unlock(unsigned long flags)
+{
+	writel(0, TRANSCEDE_SEMAARMCTRL(TRANSCEDE_SYSLOCKID_GPIO));
+	raw_local_irq_restore(flags);
+}
+
+static int t2200_is_gpio_rsvd(unsigned offset)
+{
+	if (offset >= TRANSCEDE_GPIO_NR_GPIOS)
+		return -EINVAL;
+
+	return ((t2200_gpio_pin_stat.t2200_gpio_pins >> offset) & 0x1) ? 1 : 0;
+}
+
+static int t2200_gpio_get(struct gpio_chip *chip, unsigned offset)
+{
+	unsigned long flags;
+	unsigned long data;
+
+	if (offset >= TRANSCEDE_GPIO_NR_GPIOS)
+		return -EINVAL;
+
+	if(__gpio_lock(&flags) == -1)
+	{
+		printk(KERN_ERR "t2200_gpio_get: get GPIO lock failed.\n");
+		raw_local_irq_restore(flags);
+		return -EINVAL;
+	}
+
+	if (__raw_readl(TRANSCEDE_GPIO_OE_REG) & (0x1 << offset)) {
+		/* output */
+		data = __raw_readl(TRANSCEDE_GPIO_OUTPUT_REG) & (0x1 << offset);
+	} else {
+		/* input */
+		data = __raw_readl(TRANSCEDE_GPIO_INPUT_REG) & (0x1 << offset);
+	}
+
+	__gpio_unlock(flags);
+
+	return data;
+}
+
+static inline void __t2200_gpio_set(struct gpio_chip *chip, unsigned offset, int value)
+{
+	unsigned long data;
+
+	if (offset >= TRANSCEDE_GPIO_NR_GPIOS)
+		return;
+
+	data = __raw_readl(TRANSCEDE_GPIO_OUTPUT_REG);
+
+	if (value)
+		data |= (1 << offset);
+	else
+		data &= ~(1 << offset);
+
+	__raw_writel(data, TRANSCEDE_GPIO_OUTPUT_REG);
+}
+
+static void t2200_gpio_set(struct gpio_chip *chip, unsigned offset, int value)
+{
+	unsigned long flags;
+
+	if (offset >= TRANSCEDE_GPIO_NR_GPIOS)
+		return;
+
+	if( __gpio_lock(&flags)== -1)
+	{
+		  printk(KERN_ERR "t2200_gpio_set: get GPIO lock failed.\n");
+		  raw_local_irq_restore(flags);
+		  return;
+	}
+
+	__t2200_gpio_set(chip, offset, value);
+
+	__gpio_unlock(flags);
+}
+
+static int t2200_direction_input(struct gpio_chip *chip, unsigned offset)
+{
+	unsigned long flags;
+
+	if (offset >= TRANSCEDE_GPIO_NR_GPIOS)
+		return -EINVAL;
+
+
+	if (__gpio_lock(&flags)== -1)
+	{
+		printk(KERN_ERR "t2200_direction_input: get GPIO lock failed.\n");
+		raw_local_irq_restore(flags);
+		return -EINVAL;
+	}
+
+	__raw_writel(__raw_readl(TRANSCEDE_GPIO_OE_REG) & ~(0x1 << offset), TRANSCEDE_GPIO_OE_REG);
+
+	__gpio_unlock(flags);
+
+	return 0;
+}
+
+static int t2200_direction_output(struct gpio_chip *chip, unsigned offset, int value)
+{
+	unsigned long flags;
+
+	if (offset >= TRANSCEDE_GPIO_NR_GPIOS)
+		return -EINVAL;
+
+	if ( __gpio_lock(&flags)== -1)
+	{
+		raw_local_irq_restore(flags);
+		printk(KERN_ERR "t2200_direction_output: get GPIO lock failed.\n");
+		return -EINVAL;
+	}
+
+	__raw_writel(__raw_readl(TRANSCEDE_GPIO_OE_REG) | (0x1 << offset), TRANSCEDE_GPIO_OE_REG);
+	__t2200_gpio_set(chip, offset, value);
+
+	__gpio_unlock(flags);
+
+	return 0;
+}
+
+static struct t2200_gpio_chip t2200_gpio_chip = {
+	.chip = {
+		.label			= DRV_NAME,
+		.owner			= THIS_MODULE,
+		.direction_input	= t2200_direction_input,
+		.direction_output	= t2200_direction_output,
+		.set			= t2200_gpio_set,
+		.get			= t2200_gpio_get,
+		.base			= 0,
+		.ngpio			= TRANSCEDE_GPIO_NR_GPIOS,
+	},
+};
+
+static int __init t2200_init_gpio(void)
+{
+	int ret = gpiochip_add(&t2200_gpio_chip.chip);
+
+	if (ret) {
+		printk(KERN_ERR "T2200/T3300 GPIO registration failed: %d\n", ret);
+		return -1;
+	}
+
+	gpio_timeout_tick = ((ClkRstGetFreq(CR_SYS_AXI) / 1000UL) * gpio_timeout_us) / 1000UL;
+
+	return 0;
+}
+pure_initcall(t2200_init_gpio);
+
+MODULE_AUTHOR("Intel");
+MODULE_DESCRIPTION("T2200/T3300 GPIO driver");
+MODULE_LICENSE("GPL");
+MODULE_ALIAS(DRV_NAME);
diff --git a/drivers/i2c/busses/Kconfig b/drivers/i2c/busses/Kconfig
index 646068e..6aa9e7b 100644
--- a/drivers/i2c/busses/Kconfig
+++ b/drivers/i2c/busses/Kconfig
@@ -641,6 +641,16 @@ config I2C_TEGRA
 	  If you say yes to this option, support will be included for the
 	  I2C controller embedded in NVIDIA Tegra SOCs
 
+config I2C_TRANSCEDE
+	tristate "Transcede I2C interface"
+	depends on I2C && ARCH_TRANSCEDE
+	help
+	  If you say yes to this option, support will be included for the
+	  Transcede I2C interface.
+
+	  This driver can also be built as a module.  If so, the module
+	  will be called i2c-transcede.
+
 config I2C_VERSATILE
 	tristate "ARM Versatile/Realview I2C bus support"
 	depends on ARCH_VERSATILE || ARCH_REALVIEW || ARCH_VEXPRESS
diff --git a/drivers/i2c/busses/Makefile b/drivers/i2c/busses/Makefile
index e6cf294..605fe39 100644
--- a/drivers/i2c/busses/Makefile
+++ b/drivers/i2c/busses/Makefile
@@ -62,6 +62,7 @@ obj-$(CONFIG_I2C_SH_MOBILE)	+= i2c-sh_mobile.o
 obj-$(CONFIG_I2C_SIMTEC)	+= i2c-simtec.o
 obj-$(CONFIG_I2C_STU300)	+= i2c-stu300.o
 obj-$(CONFIG_I2C_TEGRA)		+= i2c-tegra.o
+obj-$(CONFIG_I2C_TRANSCEDE)	+= i2c-transcede.o
 obj-$(CONFIG_I2C_VERSATILE)	+= i2c-versatile.o
 obj-$(CONFIG_I2C_OCTEON)	+= i2c-octeon.o
 obj-$(CONFIG_I2C_XILINX)	+= i2c-xiic.o
diff --git a/drivers/i2c/busses/i2c-transcede.c b/drivers/i2c/busses/i2c-transcede.c
new file mode 100644
index 0000000..f4c7600
--- /dev/null
+++ b/drivers/i2c/busses/i2c-transcede.c
@@ -0,0 +1,666 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+#include <linux/i2c.h>
+#include <linux/interrupt.h>
+#include <linux/delay.h>
+#include <linux/platform_device.h>
+#include <linux/slab.h>
+#include <asm/io.h>
+#include <mach/i2c.h>
+#include <mach/irqs.h>
+
+
+MODULE_AUTHOR("Intel");
+MODULE_DESCRIPTION("T2K I2C bus driver");
+MODULE_LICENSE("GPL");
+
+
+#define SPEED_HIGH_KHZ		3400
+#define SPEED_FULL_KHZ		400
+#define SPEED_NORMAL_KHZ	100
+
+static int force_poll = 0;
+module_param(force_poll, bool, S_IRUGO);
+MODULE_PARM_DESC(force_poll, "Force polling mode: 0=interrupt mode, polling mode otherwise");
+
+static int speed = 0;
+module_param(speed, int, S_IRUGO);
+MODULE_PARM_DESC(speed, "I2C speed: 0=standard, 1=fast, 2=high speed");
+
+
+struct transcede_i2c
+{
+	struct i2c_adapter	*adapter;
+	struct device		*dev;
+	unsigned long		membase;
+	struct resource		*io;
+	int			irq;
+	u32			speed_khz;
+
+	wait_queue_head_t	wait;
+	struct i2c_msg		*msg;
+	int			msg_state;
+	int			msg_status;	/* < 0: error, == 0: success, > 0: message in progress */
+	int			msg_len;
+	int			msg_retries;
+};
+
+
+#define REG_ADDR(i2c, offset)		((i2c)->membase + (offset))
+#define RD_REG(i2c, offset)		__raw_readb(REG_ADDR(i2c, offset))
+#define WR_REG(i2c, offset, byte)	__raw_writeb(byte, REG_ADDR(i2c, offset))
+#define RD_DATA(i2c)			RD_REG(i2c, TRANSCEDE_I2C_DATA)
+#define WR_DATA(i2c, byte)		WR_REG(i2c, TRANSCEDE_I2C_DATA, byte)
+#define RD_CNTR(i2c)			RD_REG(i2c, TRANSCEDE_I2C_CNTR)
+#define WR_CNTR(i2c, byte)		WR_REG(i2c, TRANSCEDE_I2C_CNTR, byte)
+#define RD_STAT(i2c)			RD_REG(i2c, TRANSCEDE_I2C_STAT)
+#define WR_CCRFS(i2c, byte)		WR_REG(i2c, TRANSCEDE_I2C_CCRFS, byte)
+#define WR_CCRH(i2c, byte)		WR_REG(i2c, TRANSCEDE_I2C_CCRH, byte)
+#define WR_RESET(i2c, byte)		WR_REG(i2c, TRANSCEDE_I2C_RESET, byte)
+
+
+enum
+{
+	TR_IDLE = 0,
+	TR_START_ACK,
+	TR_ADDR_ACK,
+	TR_DATA_ACK,
+	RX_DATA_NACK,
+};
+
+
+static u8 transcede_i2c_calculate_dividers(struct transcede_i2c *i2c)
+{
+	int m, n, hz, speed_hz;
+	int saved_n, saved_m, saved_hz;
+	u8 dividers;
+
+	speed_hz = i2c->speed_khz*1000;
+	saved_hz = saved_n = saved_m = 0;
+
+	for (m = 0; m < 16; m++) {
+		for (n = 0; n < 8; n++) {
+			hz = TRANSCEDE_AHBCLK_HZ / ((1 << n) * (m + 1) * 10);
+			if (!saved_hz || abs(speed_hz - hz) < abs(speed_hz - saved_hz)) {
+				saved_n = n;
+				saved_m = m;
+				saved_hz = hz;
+			}
+		}
+	}
+
+	dividers = (saved_m << 3) | saved_n;
+
+	dev_dbg(i2c->dev, "%s: speed=%dkHz, M=%d, N=%d, dividers=0x%02x\n", __FUNCTION__,
+		saved_hz/1000, saved_m, saved_n, dividers);
+
+	return dividers;
+}
+
+/*
+ * Returns the timeout (in jiffies) for the given message.
+ */
+static int transcede_i2c_calculate_timeout(struct transcede_i2c *i2c, struct i2c_msg *msg)
+{
+	int timeout;
+
+	/* if no timeout was specified, calculate it */
+	if (i2c->adapter->timeout <= 0) {
+		if (i2c->irq >= 0) {
+			/* for the interrupt mode calculate timeout for 'full' message */
+			timeout = ((int)msg->len)*10;	/* convert approx. to bits */
+			timeout /= i2c->speed_khz;	/* convert to bits per ms (note of kHz scale) */
+			timeout += timeout >> 1;	/* add 50% */
+			timeout = timeout*HZ/1000;	/* convert to jiffies */
+			if (timeout < HZ/5)		/* at least 200ms */
+				timeout = HZ/5;
+		} else
+			timeout = HZ;			/* 1 second for the polling mode */
+	} else
+		timeout = i2c->adapter->timeout;
+
+	return timeout;
+}
+
+/*
+ * Initialize I2C core. Zero CNTR and DATA, try RESET. Short
+ * busy wait and check core status. After that set dividers for
+ * choosen speed.
+ */
+static void transcede_i2c_reset(struct transcede_i2c *i2c)
+{
+	u8 status, dividers;
+
+	dev_dbg(i2c->dev, "%s\n", __FUNCTION__);
+
+	WR_CNTR(i2c, 0);
+	WR_DATA(i2c, 0);
+	WR_RESET(i2c, 1);
+
+	udelay(10);
+
+	status = RD_STAT(i2c);
+	if (status != STAT_NO_RELEVANT_INFO)
+		dev_warn(i2c->dev, "%s: unexpected status after reset: 0x%02x\n", __FUNCTION__, status);
+
+	/* dividers should be placed in CCRH for high-sped mode and in CCRFS for standard/full modes */
+	dividers = transcede_i2c_calculate_dividers(i2c);
+	if (i2c->speed_khz == SPEED_HIGH_KHZ)
+		WR_CCRH(i2c, dividers);
+	else
+		WR_CCRFS(i2c, dividers);
+}
+
+static inline void transcede_i2c_message_complete(struct transcede_i2c *i2c, int status)
+{
+	i2c->msg_status = status;
+	WR_CNTR(i2c, CNTR_STP);
+}
+
+static inline int transcede_i2c_message_in_progress(struct transcede_i2c *i2c)
+{
+	return i2c->msg_status > 0;
+}
+
+/*
+ * Wait event. This function sleeps in polling mode, in interrupt
+ * mode it enables IRQ from I2C core and exits immediately.
+ */
+static int transcede_i2c_wait(struct transcede_i2c *i2c, u8 cntr)
+{
+	cntr &= ~(CNTR_IFLG | CNTR_IEN);	/* clear both IFLG and IEN */
+
+	if (i2c->irq < 0) {
+		ulong jiffies_mark = jiffies + transcede_i2c_calculate_timeout(i2c, i2c->msg);
+
+		WR_CNTR(i2c, cntr);
+		while ((RD_CNTR(i2c) & CNTR_IFLG) == 0) {
+			if (need_resched())
+				schedule();
+
+			if (time_after(jiffies, jiffies_mark)) {
+				dev_dbg(i2c->dev, "%s: polling transfer timeout\n", __FUNCTION__);
+				transcede_i2c_message_complete(i2c, -ETIME);
+				transcede_i2c_reset(i2c);
+				break;
+			}
+		}
+	} else {
+		/* enable interrupt */
+		WR_CNTR(i2c, cntr | CNTR_IEN);
+	}
+
+	return 0;
+}
+
+static void transcede_i2c_state_idle(struct transcede_i2c *i2c, u8 *cntr)
+{
+	if (unlikely(i2c->msg->flags & I2C_M_NOSTART)) {
+		i2c->msg_state = TR_ADDR_ACK;
+	} else {
+		*cntr = CNTR_STP|CNTR_STA;	/* SPT|STA to auto recover from bus error state transparently at the start of the transfer */
+		i2c->msg_state = TR_START_ACK;
+	}
+}
+
+static void transcede_i2c_state_start_ack(struct transcede_i2c *i2c, u8 *cntr)
+{
+	u8 status, addr;
+
+	*cntr = 0;	/* zero IFLG, IEN (for the interrupt mode it will be enabled in wait function) */
+
+	status = RD_STAT(i2c);
+
+	if (status == STAT_START || status == STAT_START_REPEATED) {
+		i2c->msg_state = TR_ADDR_ACK;
+
+		addr = i2c->msg->addr << 1;
+		if (i2c->msg->flags & I2C_M_RD)
+			addr |= 1;
+		if (i2c->msg->flags & I2C_M_REV_DIR_ADDR)
+			addr ^= 1;		/* invert RW bit if it's requested */
+
+		WR_DATA(i2c, addr);		/* write address and read/write bit */
+	} else {
+		dev_dbg(i2c->dev, "%s: unexpected state (%#x) on start phase, %s\n",
+			__FUNCTION__, status, i2c->msg_retries > 1 ? "retrying":"aborting");
+
+		if (--i2c->msg_retries < 0)
+			transcede_i2c_message_complete(i2c, -1);
+		else
+			transcede_i2c_state_idle(i2c, cntr);
+	}
+}
+
+static void transcede_i2c_rx(struct transcede_i2c *i2c)
+{
+	u8 status, cntr = 0;
+
+restart:
+	switch (i2c->msg_state) {
+	case TR_IDLE:
+		transcede_i2c_state_idle(i2c, &cntr);
+		if (unlikely(i2c->msg->flags & I2C_M_NOSTART))
+			goto restart;	/* needed to avoid event loss in interrupt mode */
+		break;
+
+	case TR_START_ACK:
+		transcede_i2c_state_start_ack(i2c, &cntr);
+		break;
+
+	case TR_ADDR_ACK:
+		if (unlikely(i2c->msg->flags & I2C_M_NOSTART)) {
+			/* we can enter this state if skip start/addr flag is set, so fake good ack */
+			status = STAT_ADDR_RD_ACK;
+		} else {
+			status = RD_STAT(i2c);
+			/* check whether we should ignore NACK */
+			if (status == STAT_DATA_RD_NACK && (i2c->msg->flags & I2C_M_IGNORE_NAK))
+				status = STAT_DATA_RD_ACK;
+		}
+
+		if (likely(status == STAT_ADDR_RD_ACK)) {
+			/* start reception phase - wait until data is ready and loop in RX_DATA_ACK state
+			 * until we read all the data, sending ACK after each byte (but the last)
+			 */
+			i2c->msg_len = 0;
+			if (i2c->msg->len > 1) {
+				i2c->msg_state = TR_DATA_ACK;
+				cntr = CNTR_AAK;
+			} else if (i2c->msg->len == 1) {
+				i2c->msg_state = RX_DATA_NACK;
+			} else {	/* nothing to receive, send STOP and signal success */
+				transcede_i2c_message_complete(i2c, 0);
+			}
+		} else {
+			dev_dbg(i2c->dev, "%s: unexpected state (%#x) on address phase, %s\n",
+				__FUNCTION__, status, i2c->msg_retries > 1 ? "retrying":"aborting");
+
+			if (--i2c->msg_retries < 0)
+				transcede_i2c_message_complete(i2c, -1);
+			else
+				transcede_i2c_state_idle(i2c, &cntr);
+		}
+		break;
+
+	case TR_DATA_ACK:
+		status = RD_STAT(i2c);
+
+		if (likely(status == STAT_DATA_RD_ACK)) {
+			i2c->msg->buf[i2c->msg_len++] = RD_DATA(i2c);
+
+			if (likely(i2c->msg->len - i2c->msg_len > 1)) {
+				cntr = CNTR_AAK;
+			} else {
+				i2c->msg_state = RX_DATA_NACK;
+				/* NACK should be transmitted on the last byte */
+			}
+		} else {
+			dev_dbg(i2c->dev, "%s: unexpected state (%#x) on read phase\n", __FUNCTION__, status);
+			transcede_i2c_message_complete(i2c, -1);
+		}
+		break;
+
+	case RX_DATA_NACK:
+		status = RD_STAT(i2c);
+		if (likely(status == STAT_DATA_RD_NACK)) {
+			i2c->msg->buf[i2c->msg_len++] = RD_DATA(i2c);
+			transcede_i2c_message_complete(i2c, 0);
+		} else {
+			dev_dbg(i2c->dev, "%s: unexpected state (%#x) on finishing read phase\n", __FUNCTION__, status);
+			transcede_i2c_message_complete(i2c, -1);
+		}
+	}
+
+	/* no wait if we completed message */
+	if (transcede_i2c_message_in_progress(i2c))
+		transcede_i2c_wait(i2c, cntr);
+}
+
+static void transcede_i2c_tx(struct transcede_i2c *i2c)
+{
+	u8 status, cntr = 0;
+
+restart:
+	switch (i2c->msg_state) {
+	case TR_IDLE:
+		transcede_i2c_state_idle(i2c, &cntr);
+		if (unlikely(i2c->msg->flags & I2C_M_NOSTART))
+			goto restart;	/* needed to avoid event loss in interrupt mode */
+		break;
+
+	case TR_START_ACK:
+		transcede_i2c_state_start_ack(i2c, &cntr);
+		break;
+
+	case TR_ADDR_ACK:
+		if (unlikely(i2c->msg->flags & I2C_M_NOSTART)) {
+			/* we can enter this state if skip start/addr flag is set, so fake good ack */
+			status = STAT_ADDR_WR_ACK;
+		} else {
+			status = RD_STAT(i2c);
+			if (status == STAT_DATA_WR_NACK && (i2c->msg->flags & I2C_M_IGNORE_NAK))
+				status = STAT_DATA_WR_ACK;
+		}
+
+		if (likely(status == STAT_ADDR_WR_ACK)) {
+			/* start reception phase - wait until data is ready and loop in TX_DATA_ACK state
+			 * until we read all the data, sending ACK after each byte (but the last)
+			 */
+			i2c->msg_state = TR_DATA_ACK;
+			i2c->msg_len = 0;
+			if (likely(i2c->msg->len != 0)) {
+				WR_DATA(i2c, i2c->msg->buf[i2c->msg_len++]);
+			} else {
+				/* nothing to transmit, send STOP and signal success */
+				transcede_i2c_message_complete(i2c, 0);
+			}
+		} else {
+			dev_dbg(i2c->dev, "%s: unexpected state (%#x) on address phase, %s\n",
+				__FUNCTION__, status, i2c->msg_retries > 1 ? "retrying":"aborting");
+
+			if (--i2c->msg_retries < 0)
+				transcede_i2c_message_complete(i2c, -1);
+			else
+				transcede_i2c_state_idle(i2c, &cntr);
+		}
+		break;
+
+	case TR_DATA_ACK:
+		status = RD_STAT(i2c);
+		if (status == STAT_DATA_WR_NACK && (i2c->msg->flags & I2C_M_IGNORE_NAK))
+			status = STAT_DATA_WR_ACK;
+
+		if (likely(status == STAT_DATA_WR_ACK)) {
+			if (i2c->msg->len > i2c->msg_len)
+				WR_DATA(i2c, i2c->msg->buf[i2c->msg_len++]);
+			else
+				transcede_i2c_message_complete(i2c, 0);
+		} else {
+			dev_dbg(i2c->dev, "%s: unexpected state (%#x) on read data phase\n", __FUNCTION__, status);
+			transcede_i2c_message_complete(i2c, -1);
+		}
+		break;
+	}
+
+	if (transcede_i2c_message_in_progress(i2c))
+		transcede_i2c_wait(i2c, cntr);
+}
+
+static irqreturn_t transcede_i2c_interrupt(int irq, void *dev_id)
+{
+	struct transcede_i2c *i2c = dev_id;
+
+	if (!(RD_CNTR(i2c) & CNTR_IFLG))
+		goto none;
+
+	/* IRQ enable/disable logic is hidden in state handlers, all we need is to wake
+	 * process when message completed.
+	 */
+	if (i2c->msg->flags & I2C_M_RD)
+		transcede_i2c_rx(i2c);
+	else
+		transcede_i2c_tx(i2c);
+
+	if (!transcede_i2c_message_in_progress(i2c)) {
+		WR_CNTR(i2c, RD_CNTR(i2c) & ~CNTR_IEN);	/* disable interrupt unconditionally */
+		wake_up(&i2c->wait);
+	}
+
+	return IRQ_HANDLED;
+none:
+	return IRQ_NONE;
+}
+
+static void transcede_i2c_message_process(struct transcede_i2c *i2c, struct i2c_msg *msg)
+{
+	i2c->msg = msg;
+	i2c->msg_state = TR_IDLE;
+	i2c->msg_status = 1;
+	i2c->msg_retries = i2c->adapter->retries;
+
+polling_mode:
+	if (msg->flags & I2C_M_RD)
+		transcede_i2c_rx(i2c);
+	else
+		transcede_i2c_tx(i2c);
+
+	if (i2c->irq < 0) {
+		if (i2c->msg != NULL)
+			goto polling_mode;
+	} else {
+		int timeout, res;
+		ulong flags;
+
+		timeout = transcede_i2c_calculate_timeout(i2c, msg);
+
+		res = wait_event_timeout(i2c->wait, i2c->msg_status <= 0, timeout);
+
+		local_irq_save(flags);
+
+		/* check if we timed out and set respective error codes */
+		if (res == 0) {
+			if (transcede_i2c_message_in_progress(i2c)) {
+				dev_dbg(i2c->dev, "%s: interrupt transfer timeout\n", __FUNCTION__);
+				transcede_i2c_message_complete(i2c, -ETIME);
+				transcede_i2c_reset(i2c);
+			}
+		}
+
+		local_irq_restore(flags);
+	}
+}
+
+/*
+ * Generic master transfer entrypoint.
+ * Returns the number of processed messages or error value
+ */
+static int transcede_i2c_master_xfer(struct i2c_adapter *adapter, struct i2c_msg msgs[], int num)
+{
+	struct transcede_i2c *i2c = i2c_get_adapdata(adapter);
+	int i;
+
+	dev_dbg(i2c->dev, "%s: %d messages to process\n", __FUNCTION__, num);
+
+	for (i = 0; i < num; i++) {
+		dev_dbg(i2c->dev, "%s: message #%d: addr=%#x, flags=%#x, len=%u\n", __FUNCTION__,
+			i, msgs[i].addr, msgs[i].flags, msgs[i].len);
+
+		transcede_i2c_message_process(i2c, &msgs[i]);
+
+		if (i2c->msg_status < 0) {
+			dev_dbg(i2c->dev, "%s: transfer failed on message #%d (addr=%#x, flags=%#x, len=%u)\n",
+				__FUNCTION__, i, msgs[i].addr, msgs[i].flags, msgs[i].len);
+			break;
+		}
+	}
+
+	if (i2c->msg_status == -1)
+		i2c->msg_status = -EIO;
+
+	if (i2c->msg_status == 0)
+		i2c->msg_status = num;
+
+	return i2c->msg_status;
+}
+
+static u32 transcede_i2c_functionality(struct i2c_adapter *adap)
+{
+	return (I2C_FUNC_I2C | I2C_FUNC_SMBUS_EMUL);
+}
+
+static struct i2c_algorithm transcede_i2c_algo = {
+	.master_xfer	= transcede_i2c_master_xfer,
+	.functionality	= transcede_i2c_functionality,
+};
+
+static struct i2c_adapter transcede_i2c_adapter = {
+	.owner		= THIS_MODULE,
+	.algo		= &transcede_i2c_algo,
+	.name		= "transcede_i2c",
+	.timeout	= 0,	/* <= zero means that we calculate timeout in run-time, can be changed with ioctl call */
+	.retries	= 0,	/* no retries by default - let the user decide what's the best, can be changed with ioctl call */
+};
+
+static int transcede_i2c_probe(struct platform_device *pdev)
+{
+	struct transcede_i2c *i2c;
+	struct resource *irq;
+	int res = -1;
+
+	dev_dbg(&pdev->dev, "%s\n", __FUNCTION__);
+
+	i2c = kzalloc(sizeof(*i2c), GFP_KERNEL);
+	if (i2c == NULL) {
+		dev_err(&pdev->dev, "%s: failed allocate memory\n", __FUNCTION__);
+		res = -ENOMEM;
+		goto err0;
+	}
+
+	i2c->adapter = &transcede_i2c_adapter;
+	i2c->adapter->dev.parent = &pdev->dev;
+	i2c->dev = &pdev->dev;
+
+	init_waitqueue_head(&i2c->wait);
+
+	platform_set_drvdata(pdev, i2c);
+	i2c_set_adapdata(&transcede_i2c_adapter, i2c);
+
+	i2c->io = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (i2c->io == NULL) {
+		dev_err(&pdev->dev, "%s: no IO region specified\n", __FUNCTION__);
+		res = -ENOENT;
+		goto err1;
+	}
+
+	/* io-remaped in arch/arm/mach-transcede/transcede-xxx.c */
+	i2c->membase = i2c->io->start;
+
+	irq = platform_get_resource(pdev, IORESOURCE_IRQ, 0);
+	if (irq == NULL && !force_poll) {
+		dev_warn(i2c->dev, "%s: no IRQ specified in resources, polling mode forced\n", __FUNCTION__);
+		force_poll = 1;
+		i2c->irq = -1;
+	}
+
+	if (speed == 0) {
+		i2c->speed_khz = SPEED_NORMAL_KHZ;
+	} else if (speed == 1) {
+		i2c->speed_khz = SPEED_FULL_KHZ;
+	} else if (speed == 2) {
+		i2c->speed_khz = SPEED_HIGH_KHZ;
+	} else {
+		dev_err(i2c->dev, "%s: invalid 'speed' module option provided (%d, must be 0,1,2 for normal/full/high modes)\n",
+			__FUNCTION__, speed);
+		goto err1;
+	}
+
+	transcede_i2c_reset(i2c);
+
+	if (!force_poll) {
+		i2c->irq = irq->start;
+
+		res = request_irq(i2c->irq, transcede_i2c_interrupt, IRQF_SHARED, "I2C", i2c);
+		if (res < 0) {
+			dev_warn(i2c->dev, "%s: failed to request IRQ%d, polling mode forced\n", __FUNCTION__, i2c->irq);
+			force_poll = 1;
+			i2c->irq = -1;
+		}
+	} else
+		i2c->irq = -1;
+
+	if (i2c_add_numbered_adapter(&transcede_i2c_adapter) != 0) {
+		dev_err(i2c->dev, "%s: failed to add I2C adapter\n", __FUNCTION__);
+		goto err2;
+	}
+
+	dev_dbg(&pdev->dev, "%s: I2C adapter registered\n", __FUNCTION__);
+
+	return 0;
+
+err2:
+	if (i2c->irq >= 0)
+		free_irq(i2c->irq, i2c);
+
+err1:
+	kfree(i2c);
+
+err0:
+	return res;
+}
+
+static int transcede_i2c_remove(struct platform_device *pdev)
+{
+	struct transcede_i2c *i2c = platform_get_drvdata(pdev);
+
+	dev_dbg(i2c->dev, "%s\n", __FUNCTION__);
+
+	platform_set_drvdata(pdev, NULL);
+
+	i2c_del_adapter(i2c->adapter);
+
+	if (i2c->irq >= 0)
+		free_irq(i2c->irq, i2c);
+
+	kfree(i2c);
+
+	return 0;
+}
+
+static struct platform_driver transcede_i2c_driver = {
+	.driver = {
+		.name	= "transcede_i2c",
+		.owner	= THIS_MODULE,
+	},
+	.probe	= transcede_i2c_probe,
+	.remove	= transcede_i2c_remove,
+};
+
+static int __init transcede_i2c_init(void)
+{
+	printk(KERN_DEBUG "i2c: module loaded\n");
+
+	if (platform_driver_register(&transcede_i2c_driver)) {
+		printk(KERN_ERR "i2c: failed to register platform device\n");
+		return -1;
+	}
+#ifdef CONFIG_MACH_M84XXX
+	printk(KERN_INFO "i2c: enable I2C pins instead of GPIO[27,28]\n");
+	writel(readl(TRANSCEDE_GPIO_PIN_SELECT) & ~(3 << 27), TRANSCEDE_GPIO_PIN_SELECT);
+#endif
+#ifdef CONFIG_MACH_M822XX
+	printk(KERN_INFO "i2c: enable I2C pins instead of GPIO[16,17]\n");
+	writel(readl(TRANSCEDE_GPIO_PIN_SELECT1) & ~0x0F, TRANSCEDE_GPIO_PIN_SELECT1);
+#endif
+
+	return 0;
+}
+
+static void __exit transcede_i2c_exit(void)
+{
+	printk(KERN_DEBUG "%s: module unloaded\n", __FUNCTION__);
+
+	platform_driver_unregister(&transcede_i2c_driver);
+}
+
+module_init(transcede_i2c_init);
+module_exit(transcede_i2c_exit);
diff --git a/drivers/misc/Kconfig b/drivers/misc/Kconfig
index 1704061..a63c9e4 100644
--- a/drivers/misc/Kconfig
+++ b/drivers/misc/Kconfig
@@ -527,6 +527,13 @@ config PCH_PHUB
 	  To compile this driver as a module, choose M here: the module will
 	  be called pch_phub.
 
+config TRANSCEDE_USIM_SUPPORT
+        depends on MACH_M822XX
+        bool "USIM Smart Card support"
+        default n
+        help
+                Say y if you need to buld kernel for USIM.
+
 source "drivers/misc/c2port/Kconfig"
 source "drivers/misc/eeprom/Kconfig"
 source "drivers/misc/cb710/Kconfig"
diff --git a/drivers/misc/Makefile b/drivers/misc/Makefile
index d456181..407ea16 100644
--- a/drivers/misc/Makefile
+++ b/drivers/misc/Makefile
@@ -47,3 +47,4 @@ obj-$(CONFIG_AB8500_PWM)	+= ab8500-pwm.o
 obj-y				+= lis3lv02d/
 obj-y				+= carma/
 obj-$(CONFIG_HWLAT_DETECTOR)	+= hwlat_detector.o
+obj-$(CONFIG_TRANSCEDE_USIM_SUPPORT)	+= transcede_usim.o transcede_usim_t0_handler.o transcede_usim_t1_handler.o
diff --git a/drivers/misc/transcede_usim.c b/drivers/misc/transcede_usim.c
new file mode 100644
index 0000000..24d8073
--- /dev/null
+++ b/drivers/misc/transcede_usim.c
@@ -0,0 +1,1019 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+/*!
+ * \file transcede_usim.c
+ * \brief Main file for Transcede USIM kernel driver.
+ *
+ * This file implements the core functionality of the Trancede USIM  kernel
+ * driver.  This file is responsible for opening and closing of USIM contexts
+ * and the transactions that take place between driver and smart card.
+ */
+
+#include <asm/uaccess.h>
+#include <asm/errno.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/miscdevice.h>
+#include <linux/fs.h>
+#include <linux/err.h>
+#include <linux/debugfs.h>
+#include <linux/poll.h>
+#include <linux/slab.h>
+#include <linux/io.h>
+#include <linux/interrupt.h>
+#include <linux/sched.h>
+#include <linux/platform_device.h>
+#include <linux/mindspeed/transcede_usim.h>
+#include "transcede_usim_regs.h"
+#include <linux/spinlock.h>
+#include <linux/delay.h>
+
+#define USIM_HEADER_SIZE ( 5 )
+#define USIMCKTICKS_IN_1_US 140
+
+static int
+transcede_usim_open( struct inode *inode,
+                     struct file *filp );
+static int
+transcede_usim_release( struct inode *inode,
+                        struct file *filp );
+
+static signed long
+transcede_usim_ioctl( struct file *filp,
+                      unsigned int cmd,
+                      unsigned long arg );
+
+static unsigned int
+transcede_usim_poll( struct file *filp,
+                     struct poll_table_struct *pollt );
+
+static irqreturn_t
+transcede_usim_irq( int irq,
+                    void *dev );
+
+extern DWORD
+libusim_t0_exchange_data( PUCHAR pRequest,
+                          DWORD RequestLen,
+                          PUCHAR pReply,
+                          PDWORD pReplyLen );
+
+extern DWORD
+libusim_t1_exchange_data( PUCHAR pRequest,
+                          DWORD RequestLen,
+                          PUCHAR pReply,
+                          PDWORD pReplyLen );
+
+/*!
+ * File operations structure for the driver. All file operations for the
+ * driver and transports will be in this structure and if the operation needs
+ * to be rerouted to a transport module, this will happen in these functions.
+ */
+static struct file_operations transcede_usim_fops = {
+    .owner	    = THIS_MODULE,
+    .open           = transcede_usim_open,
+    .release        = transcede_usim_release,
+    .unlocked_ioctl = transcede_usim_ioctl,
+    .poll           = transcede_usim_poll,
+};
+
+/*!
+ * \brief Structure to store devices in the system and any other data private
+ * to the driver.
+ */
+struct transcede_usim_core_t
+{
+    /*! The miscdevice used for the Linux device registration. */
+    struct miscdevice miscdev;
+
+    /*! Flag that ensures only one driver instance is opened */
+    atomic_t             open_driver_instances;
+
+    /*! Wait queue for threads. */
+    wait_queue_head_t           waitq;
+
+    /*! Virtual base address to USIM hardware registers */
+    void __iomem *base;
+
+    /*! Physical base address to USIM hardware registers */
+    unsigned long base_phys;
+
+    /*! Range of addresses */
+    unsigned base_len;
+
+    /*! USIM IRQ */
+    int irq;
+
+    /*! Interrupt register 1 */
+    unsigned interrupt_register1;
+
+    /*! Bits to be cleared from interrupt register 1 */
+    uint32_t clear_interrupt_register1;
+
+    /*! Powered up flag */
+    unsigned is_card_powered_up;
+
+    /*!  Control access to thre interrupt registers in this structure */
+    spinlock_t lock;
+};
+
+/*!
+ * The miscdevice for the driver.
+ */
+static struct transcede_usim_core_t usimdev = {
+    .miscdev    =  {
+        .fops   = &transcede_usim_fops,
+        .name   = "usim0",
+        .minor  = MISC_DYNAMIC_MINOR,
+   },
+};
+
+/************************************************************************************
+ * Internal functions
+ ***********************************************************************************/
+
+/*!
+ * Clear interrupt bits from registers
+ * @param int_clear_mask Bits to clear
+ * @return Returns zero on success, negative on failure.
+ *
+ * The purpose of this function is to clear the bits set in the interrupt registier 1
+ * that have been processed. This is acheived by collecting the bits in a mask and
+ * allowing the IRQ to clear them on the next interrupt.
+ */
+static void
+transcede_usim_clear_interrupt_bits( uint32_t int_clear_mask )
+{
+    spin_lock_bh( &usimdev.lock );
+    usimdev.clear_interrupt_register1 |= int_clear_mask;
+    spin_unlock_bh( &usimdev.lock );
+}
+
+/*!
+ * Initialise start card controller so it can detect the presence of a card
+ * @param f Smart card divider frequency
+ * @param d Bit rate adjustment
+ * @return Returns zero on success, negative on failure.
+ */
+static void
+transcede_usim_controller_init( unsigned f,
+                                unsigned d )
+{
+    uint32_t scc_divisor;
+    uint32_t baud_divisor;
+
+    pr_debug( "transcede_usim_controller_init" );
+
+    /* Expansion bus MHz ticks per uS divded by 8 and rounded up to nearest 4 */
+    scc_divisor = ( USIMCKTICKS_IN_1_US / 8)-1;
+    if (((USIMCKTICKS_IN_1_US/2) % 4) != 0)
+        ++scc_divisor;
+    writel( scc_divisor, usimdev.base +ADR_CGSCDIV);
+    pr_debug( "CGSC divisor=%u", scc_divisor );
+
+    /* Calculate Baud Rate Divisor based on smart card divider and bit rate adjustment factor
+       (f/d) */
+    baud_divisor = ( ( scc_divisor + 1 ) * ( f / d ) ) -1;
+
+    /* Apply H/W adjustment required - if baud_divisor is an exact integer subract 1 more */
+    if ( ( ( baud_divisor + 1) * d ) == ( ( scc_divisor + 1 ) * f ) )
+     --baud_divisor;
+
+    writel( baud_divisor, usimdev.base +ADR_CGBITDIV);
+    pr_debug( "CGBIT divisor=%u", baud_divisor );
+
+    /* Character LeadEdge to Character LeadEdge maximum waiting time - in terms of ETUs) */
+    writel( 0x2580, usimdev.base +ADR_C2CLIM );  /* 9600 ETUs */
+    pr_debug( "C2CLIM=%u", 0x2580 );
+
+    /* Automatic Vpp Handling. Enable automatic handling of DSCVPPEN and DSCVPPPP signals
+       during activation and deactivation sequence. */
+    writel( USIM_PAD_AUTOADEAVPP, usimdev.base +ADR_SCPADS);
+}
+
+/*!
+ * Open a new instance of the driver.
+ *
+ * @param inode The inode of the device file.
+ * @param filp The context of the open instance. The private_data member can
+ * be used to store driver information for this context.
+ * @return Returns zero on success, negative on failure.
+ */
+static int
+transcede_usim_open( struct inode *inode,
+                     struct file *filp )
+{
+    pr_debug( "transcede_usim_open" );
+
+    if ( atomic_inc_return( &usimdev.open_driver_instances ) != 1 )
+        return -ENXIO;
+
+    transcede_usim_controller_init( 372, 1 );
+
+    /* Initialise smart card detect interrupt as card must be detectable after channel
+       has been opened */
+    writel( USIM_INT1_SCINS, usimdev.base +ADR_INTEN );
+    writel( 0, usimdev.base +ADR_INTEN2 );
+
+    /* Enable interrupts */
+    writel( USIM_CR1_GINTEN, usimdev.base +ADR_CTRL1 );
+    return 0;
+}
+
+/*!
+ * Close an instance of the driver.
+ *
+ * @param inode The inode of the device file.
+ * @param filp The context of the open instance.
+ * @return Returns zero on success, negative on failure.
+ */
+static int
+transcede_usim_release( struct inode *inode,
+                        struct file *filp )
+{
+    pr_debug( "transcede_usim_release ");
+
+    if ( atomic_dec_return( &usimdev.open_driver_instances ) != 0 )
+        return -ENXIO;
+
+    return 0;
+}
+
+/*!
+ * Poll for an smard card detect interrupt.
+ *
+ * This function is intended to be call with and without a timeout. If called
+ * without a timeout it will report on the current status of card detect.
+ * If called with a timeout it will ignore the current card detect status and wait
+ * for the next card detect interrupt.
+ *
+ * @param filp The file descriptor being polled.
+ * @param pollt The poll table for the request.
+ * @return Returns the mask POLLIN on card detection status change
+ */
+static unsigned int
+transcede_usim_poll( struct file *filp,
+                     struct poll_table_struct *pollt )
+{
+    int ret=0;
+
+    pr_debug( "transcede_usim_poll" );
+
+    poll_wait( filp, &usimdev.waitq, pollt );
+
+    if ( usimdev.interrupt_register1 & USIM_INT1_SCINS )
+    {
+        transcede_usim_clear_interrupt_bits( USIM_INT1_SCINS );
+        ret = POLLIN;
+    }
+
+    return ret;
+}
+
+/*!
+ * This ISR will be called when the Trancede generates a USIM interrupt. It
+ * checks the cause(s) of the interrupts and sets the appropriate flags in
+ * the core strucuture. It then wakes up any sleepoing flags (including any
+ * threads waiting to detect a smart card).
+ *
+ * @param irq The irq that has been raised.
+ * @param dev The Transcede USIM device that has raised the interrupt.
+ * @return Returns IRQ_HANDLED on success.
+ */
+static irqreturn_t
+transcede_usim_irq( int irq,
+                    void *dev )
+{
+    uint32_t new_interrupts;
+
+    pr_debug( "transcede_usim_irq" );
+
+    if ( ( struct transcede_usim_core_t * )dev != &usimdev )
+        return IRQ_NONE;   // It was not for us
+
+    new_interrupts = readl( usimdev.base + ADR_INTSTAT );
+    usimdev.interrupt_register1 = new_interrupts |
+        ( usimdev.interrupt_register1 & ~(usimdev.clear_interrupt_register1 ) );
+    writel( new_interrupts, usimdev.base + ADR_INTSTAT );
+
+    wake_up_interruptible( &usimdev.waitq );
+
+    return IRQ_HANDLED;
+}
+
+/*!
+ * This function will perform a cold or warm start up
+ *
+ * @param warm_start Flag indiciating the type of power up required
+ */
+static void
+trancede_usim_power_up( unsigned warm_start )
+{
+    uint32_t reg_value = USIM_INT1_C2CFULL |
+                         USIM_INT1_ATRFAIL |
+                         USIM_INT1_ATRDONE |
+                         USIM_INT1_SCINS |
+                         USIM_INT1_SCACT |
+                         USIM_INT1_SCDEACT |
+                         USIM_INT1_TXPERR |
+                         USIM_INT1_RXDONE |
+                         USIM_INT1_RXPERR;
+
+    pr_debug( "trancede_usim_power_up" );
+
+    writel( reg_value, usimdev.base + ADR_INTEN );
+
+    /* Enable Tx and Rx FIFOs */
+    reg_value = USIM_CR1_PECH2FIFO  | USIM_CR1_TXEN | USIM_CR1_RXEN  | USIM_CR1_TS2FIFO |
+                USIM_CR1_ATRSTFLUSH | USIM_CR1_GINTEN;
+    writel( reg_value, usimdev.base + ADR_CTRL1 );
+
+    /* Enable auto scanning of 1.8 and 3.3V cards */
+    reg_value = readl( usimdev.base + ADR_CTRL2 )
+          | USIM_CR2_VCC18 | USIM_CR2_VCC33;
+    writel( reg_value, usimdev.base + ADR_CTRL2 );
+
+    /* Activate interface */
+    if ( !warm_start )
+    {
+        pr_debug( "trancede_usim_power_up - cold reset" );
+        reg_value |= USIM_CR2_ACT;
+    }
+    else
+    {
+        pr_debug( "trancede_usim_power_up - warm reset" );
+        reg_value |= USIM_CR2_WARMRST;
+    }
+    writel( reg_value, usimdev.base + ADR_CTRL2 );
+}
+
+/*!
+ * Perform the specified read and write requested
+ *
+ * @param tx_buffer Bytes to send
+ * @param tx_length Number of bytes to send
+ * @param rx_buffer Buffer for bytes to receive
+ * @param rx_length Maximum number of bytes to receive
+ * @return Actual bytes received
+ */
+extern int libusim_read_write( const PUCHAR tx_buffer,
+                    const DWORD tx_length,
+                    PUCHAR rx_buffer,
+                    PDWORD rx_length)
+{
+    const uint32_t rx_count = *rx_length;
+    unsigned bytes;
+
+    pr_debug( "libusim_read_write tx length=%lu rx length=%u", tx_length, rx_count );
+
+    pr_debug("Clear C2CFULL interrupt before sending");
+    writel(USIM_INT1_C2CFULL, usimdev.base + ADR_INTSTAT);
+    usimdev.interrupt_register1 &= ~(USIM_INT1_C2CFULL);
+
+    /* Write requested data */
+    for(bytes = 0; bytes < tx_length; ++bytes)
+      writel( tx_buffer[ bytes ], usimdev.base + ADR_FIFODATA );
+
+    if ( usimdev.interrupt_register1 & USIM_INT1_TXPERR )
+    {
+        pr_err("TX parity error\n");
+        return 1;
+    }
+    pr_debug( "Sent %lu bytes\n", tx_length );
+
+    *rx_length = 0;
+    while( *rx_length < rx_count )
+    {
+        /* Wait for interrupt */
+        wait_event_interruptible( usimdev.waitq,
+            usimdev.interrupt_register1 & ( USIM_INT1_RXDONE | USIM_INT1_C2CFULL ) );
+
+        if ( usimdev.interrupt_register1 & USIM_INT1_C2CFULL )
+        {
+            pr_debug( "Timeout waiting for card. Received=%lu bytes", *rx_length );
+            return 1;
+        }
+
+        if (  usimdev.interrupt_register1 & USIM_INT1_RXDONE )
+        {
+            uint32_t bytes_this_interrupt = readl( usimdev.base + ADR_RXFIFOCNT );
+
+            for( bytes = 0; ( bytes < bytes_this_interrupt ) &&
+                            ( bytes < MAX_ATR_SIZE); ++bytes )
+                rx_buffer[ *rx_length +bytes ] = readl( usimdev.base + ADR_FIFODATA );
+
+            *rx_length += bytes_this_interrupt;
+        }
+    }
+
+    pr_debug( "Received %lu bytes", *rx_length );
+    return 0;
+}
+
+/************************************************************************************************
+ * kernel API
+ ***********************************************************************************************/
+
+/*!
+ * Perform the ICC presence request
+ *
+ * @param user_req Pointer to the structure passed from userspace and
+ * returned updated
+ * @return 0 on success, non-zero on failure
+ */
+DWORD
+transcede_usim_icc_presence( void )
+{
+    uint32_t card_present_mask = readl( usimdev.base + ADR_SCPADS ) & USIM_PAD_SCPRESENT;
+    DWORD is_card_present = card_present_mask ? 1 : 0;
+
+    pr_debug( "transcede_usim: icc_presence - Card_presence=%lu", is_card_present );
+    return is_card_present;
+}
+EXPORT_SYMBOL( transcede_usim_icc_presence );
+
+/*!
+ * Get the requested capability from the smart card
+ *
+ * @param user_req Pointer to the structure passed from userspace and
+ * updated with return data
+ * @return 0 on success, non-zero on failure
+ */
+static signed long
+transcede_usim_get_capabilities( struct transcede_usim_ioc_capabilities *req )
+{
+    pr_debug( "transcede_usim: get_capabilities - Unsupported operation" );
+    return 1;
+}
+EXPORT_SYMBOL( transcede_usim_get_capabilities );
+
+/*!
+ * Set the requested capability on the smart card
+ *
+ * @param user_req Pointer to the structure passed from userspace
+ * @return 0 on success, non-zero on failure
+ */
+static signed long
+transcede_usim_set_capabilities( struct transcede_usim_ioc_capabilities *req )
+{
+    pr_debug( "transcede_usim: set_capabilities - Unsupported operation" );
+    return 1;
+}
+EXPORT_SYMBOL( transcede_usim_set_capabilities );
+
+/*!
+ * Perform the requested power operation on the smart card
+ *
+ * @param user_req Pointer to the structure passed from userspace and
+ * returned updated
+ * @return 0 on success, non-zero on failure
+ */
+signed long
+transcede_usim_power_icc( struct transcede_usim_ioc_power_icc *req )
+{
+    int ret=0;
+    DWORD is_card_present = transcede_usim_icc_presence( );
+
+    pr_debug( "transcede_usim: power_icc" );
+
+    switch( req->action )
+    {
+        unsigned bytes;
+        uint32_t control_reg;
+
+        case IFD_POWER_UP: /* Fall through intentional */
+        case IFD_RESET:
+            if ( !is_card_present )
+            {
+                pr_debug( "transcede_usim: Card not present" );
+                ret = 1;
+                break;
+            }
+
+            if ( ( ( req->action == IFD_POWER_UP ) &&  usimdev.is_card_powered_up ) ||
+                 ( ( req->action == IFD_RESET ) && !usimdev.is_card_powered_up ) )
+            {
+                pr_warn( "transcede_usim: Invalid power operation" );
+                ret = 1;
+                break;
+            }
+
+            trancede_usim_power_up( usimdev.is_card_powered_up );
+
+            req->atr_length = 0;
+
+            /* Wait for card to generate an ATR interrupt */
+            wait_event_interruptible( usimdev.waitq, usimdev.interrupt_register1 &
+                ( USIM_INT1_ATRFAIL | USIM_INT1_ATRDONE ) );
+
+            if ( usimdev.interrupt_register1 & USIM_INT1_ATRFAIL )
+            {
+                pr_err("transcede_usim: No ATR responce");
+                ret = 1;
+                break;
+            }
+
+            if ( !( usimdev.interrupt_register1 & USIM_INT1_ATRDONE ) )
+            {
+                pr_err("transcede_usim: No ATR complete interrupt");
+                ret = 1;
+                break;
+            }
+
+            /* Read ATR */
+            control_reg = readl( usimdev.base + ADR_FIFOCTRL );
+            if ( control_reg & USIM_FIFO_RXFIEMPTY )
+            {
+                pr_err("transcede_usim: ATR FIFO empty");
+                ret = 1;
+                break;
+            }
+
+            if ( control_reg & USIM_FIFO_RXFIFULL )
+            {
+                pr_err("transcede_usim: ATR FIFO overrun");
+                ret = 1;
+                break;
+            }
+
+            req->atr_length = readl( usimdev.base + ADR_RXFIFOCNT );
+            for( bytes = 0; bytes < req->atr_length && bytes < MAX_ATR_SIZE; ++bytes )
+                req->atr[ bytes] = readl( usimdev.base + ADR_FIFODATA );
+
+            usimdev.is_card_powered_up = 1;
+            transcede_usim_clear_interrupt_bits( USIM_INT1_ATRFAIL | USIM_INT1_ATRDONE );
+            break;
+
+        case IFD_POWER_DOWN:
+            if ( !usimdev.is_card_powered_up )
+            {
+                pr_debug( "transcede_usim: Card is not powered up" );
+                ret = 1;
+                break;
+            }
+
+            /* Deactivate UICC/smart card and wait for interrupt */
+            control_reg = readl( usimdev.base + ADR_CTRL2 ) | USIM_CR2_DEACT;
+            writel( control_reg, usimdev.base + ADR_CTRL2 );
+
+            wait_event_interruptible( usimdev.waitq,
+                usimdev.interrupt_register1 & USIM_INT1_SCDEACT );
+            usimdev.is_card_powered_up = 0;
+            transcede_usim_clear_interrupt_bits( USIM_INT1_SCDEACT );
+            break;
+
+        default:
+            pr_debug("transcede_usim: Power actton not recognised");
+            req->atr_length = 0;
+            ret = 1;
+    }
+
+    return ret;
+};
+EXPORT_SYMBOL( transcede_usim_power_icc );
+
+signed long
+transcede_usim_control( struct transcede_usim_ioc_control req[2] )
+{
+    pr_debug( "transcede_usim: control - Unsupported operation" );
+    return 1;
+}
+EXPORT_SYMBOL( transcede_usim_control );
+
+/*!
+ * Set the requested protocol parametewrs on the smart card
+ *
+ * @param user_req Pointer to the structure passed from userspace
+ * @return 0 on success, non-zero on failure
+ */
+signed long
+transcede_usim_set_protocol_params( struct transcede_usim_ioc_set_protocol_params *req )
+{
+    int ret;
+    unsigned n;
+
+    DWORD tx_length = 2;
+    DWORD rx_length;
+    UCHAR tx_buffer[6]; /* 2 header bytes, 3 PTS bytes and a check sum */
+    UCHAR rx_buffer[6]; /* On success the card should echo back the command */
+    DWORD is_card_present = transcede_usim_icc_presence( );
+
+    pr_debug( "transcede_usim: set_protocol_params" );
+
+    if ( !is_card_present )
+    {
+        pr_debug( "Card not present" );
+        return 1;
+    }
+
+    if ( !usimdev.is_card_powered_up )
+    {
+        pr_debug( "Card not powered up" );
+        return 1;
+    }
+
+    /* Set PPSS to the mandatory 0xFF value */
+    tx_buffer[0] = 0xFF;
+
+    /* PPS0 byte */
+    switch(req->protocol)
+    {
+        case 0:
+            tx_buffer[1] = ((UCHAR)(req->flags << 4)) + 0x00;
+            break;
+
+        case 1:
+            tx_buffer[1] = ((UCHAR)(req->flags << 4)) + 0x01;
+            break;
+
+        default:
+            pr_warn("protocol not supported\n");
+            return IFD_PROTOCOL_NOT_SUPPORTED;
+    }
+
+    if ( req->flags &
+         ( ~( IFD_NEGOTIATE_PTS1 | IFD_NEGOTIATE_PTS2 | IFD_NEGOTIATE_PTS3 ) ) )
+    {
+        pr_err("Invalid flags\n");
+        return 1;
+    }
+
+    /* Add the optional PPS1 PPS2 and PPS3 bytes */
+    if ( req->flags & IFD_NEGOTIATE_PTS1 )
+    {
+        tx_buffer[tx_length] = req->PTS1;
+        ++tx_length;
+    }
+    if ( req->flags & IFD_NEGOTIATE_PTS2 )
+    {
+        tx_buffer[tx_length] = req->PTS2;
+        ++tx_length;
+    }
+    if ( req->flags & IFD_NEGOTIATE_PTS3 )
+    {
+        tx_buffer[tx_length] = req->PTS3;
+        ++tx_length;
+    }
+
+    /* Now add the PCK (checksum character) */
+    tx_buffer[tx_length] = 0x00;
+
+    for(n = 0; n < tx_length; ++n)
+    {
+        pr_debug("PTS[%u]=0x%x\n", n, tx_buffer[n]);
+        tx_buffer[tx_length] ^= tx_buffer[n];
+    }
+    pr_debug("PTS Checksum=0x%x\n", tx_buffer[tx_length]);
+    ++tx_length;
+
+    /* Now wait for response */
+    rx_length = tx_length;
+    ret = libusim_read_write(tx_buffer, tx_length, rx_buffer, &rx_length);
+    if (ret != rx_length)
+    {
+        pr_err("Communication fail. Sent=%lu bytes, received=%lu bytes\n",
+            tx_length, rx_length);
+      ret = 1;
+    }
+    else
+    {
+        if (memcmp(rx_buffer, tx_buffer, tx_length))
+        {
+            pr_err("Match fail comparing sent and received characters\n");
+            ret = 1;
+        }
+        else
+        {
+            pr_debug("Successful comparison of sent and received characters\n");
+            ret = 0;
+        }
+    }
+
+    for(n = 0; ((n < tx_length) && (n < rx_length)); ++n)
+    {
+        pr_debug("TxBuffer[%u]=0x%02x -> RxBuffer[%u]=0x%02x\n",
+            n, tx_buffer[n], n, rx_buffer[n]);
+    }
+
+    return ret;
+}
+EXPORT_SYMBOL( transcede_usim_set_protocol_params );
+
+/*!
+ * Perform the requested transaction with the smart card
+ *
+ * @param user_req Pointer to the structure passed from userspace and
+ * updated
+ * @return 0 on success, non-zero on failure
+ */
+signed long
+transcede_usim_transmit_to_icc( struct transcede_usim_ioc_transmit_to_icc req[2] )
+{
+    int ret = 0;
+
+    pr_debug( "transcede_usim: transmit_to_icc - protocol=T%lu", req[0].protocol );
+
+    switch( req[0].protocol )
+    {
+        case 0:
+            ret = libusim_t0_exchange_data( req[0].buffer,
+                                            req[0].length,
+                                            req[1].buffer,
+                                            &req[1].length );
+            req[1].protocol = req[0].protocol;
+            break;
+
+        case 1:
+            ret = libusim_t1_exchange_data( req[0].buffer,
+                                            req[0].length,
+                                            req[1].buffer,
+                                            &req[1].length );
+            req[1].protocol = req[0].protocol;
+            break;
+
+        default:
+            ret = 1;
+    }
+
+    return ret;
+}
+EXPORT_SYMBOL( transcede_usim_transmit_to_icc );
+
+/*!
+ * Handle an ioctl request for the driver. If this is a transport instance
+ * then the ioctl request should be rerouted to the appropriate transport
+ * module.
+ *
+ * @param inode The inode of the device file.
+ * @param filp The file context for this instance.
+ * @param cmd The ioctl type.
+ * @param arg The argument for the ioctl. This may be casted to the
+ * appropriate type depending on the ioctl type.
+ */
+static signed long
+transcede_usim_ioctl( struct file *filp,
+              unsigned int cmd,
+              unsigned long arg )
+{
+    int ret = -ENOTTY;
+
+    switch ( cmd )
+    {
+        case TRANSCEDE_USIM_IOC_GET_CAPABILITIES:
+        {
+            struct transcede_usim_ioc_capabilities req;
+
+            ret = copy_from_user( &req, ( struct transcede_usim_ioc_capabilities * )arg,
+                sizeof( struct transcede_usim_ioc_capabilities_header ) );
+
+            if ( !ret )
+              ret = transcede_usim_get_capabilities( &req );
+
+            if ( !ret )
+              ret = copy_to_user( ( struct transcede_usim_ioc_capabilities * )arg,
+                  &req + sizeof( struct transcede_usim_ioc_capabilities_header ),
+                  sizeof( struct transcede_usim_ioc_capabilities_body ) );
+            break;
+        }
+
+        case TRANSCEDE_USIM_IOC_SET_CAPABILITIES:
+        {
+            struct transcede_usim_ioc_capabilities req;
+
+            ret = copy_from_user( &req, ( struct transcede_usim_ioc_capabilities * )arg,
+                sizeof( struct transcede_usim_ioc_capabilities ) );
+
+            if ( !ret )
+                ret = transcede_usim_set_capabilities( &req );
+            break;
+        }
+
+        case TRANSCEDE_USIM_IOC_SET_PROTOCOL_PARAMS:
+        {
+            struct transcede_usim_ioc_set_protocol_params req;
+
+            ret = copy_from_user( &req, ( struct transcede_usim_ioc_set_protocol_params * )arg,
+                sizeof( struct transcede_usim_ioc_set_protocol_params ) );
+
+            if ( !ret )
+                ret = transcede_usim_set_protocol_params( &req );
+            break;
+        }
+
+        case TRANSCEDE_USIM_IOC_TRANSMIT_TO_ICC:
+        {
+            struct transcede_usim_ioc_transmit_to_icc req[2];
+
+            ret = copy_from_user( &req[0],
+                ( struct transcede_usim_ioc_transmit_to_icc * )arg,
+                sizeof( struct transcede_usim_ioc_transmit_to_icc ) );
+
+            if ( !ret )
+                ret = transcede_usim_transmit_to_icc( req );
+
+            if ( !ret )
+                ret = copy_to_user(
+                    ( struct transcede_usim_ioc_transmit_to_icc * )arg,
+                    &req[1],
+                    sizeof( struct transcede_usim_ioc_transmit_to_icc ) );
+            break;
+        }
+
+        case  TRANSCEDE_USIM_IOC_POWER_ICC:
+        {
+            struct transcede_usim_ioc_power_icc req;
+
+            ret = copy_from_user( &req, (DWORD __user *)arg, sizeof( DWORD ) );
+            if ( !ret )
+                ret = transcede_usim_power_icc( &req );
+
+            if ( !ret )
+                ret = copy_to_user( (struct transcede_usim_ioc_power_icc * )arg, &req,
+                    sizeof( struct transcede_usim_ioc_power_icc ) );
+            break;
+        }
+
+	case TRANSCEDE_USIM_IOC_CONTROL:
+        {
+            struct transcede_usim_ioc_control req[2];
+
+            ret = copy_from_user( &req[0], ( struct transcede_usim_ioc_control * )arg,
+              sizeof( struct transcede_usim_ioc_control ) );
+
+            if ( !ret )
+                ret = transcede_usim_control( req );
+
+            if ( !ret )
+                ret = copy_to_user( ( struct transcede_usim_ioc_control * )arg, &req[1],
+                    sizeof( struct transcede_usim_ioc_control ) );
+            break;
+        }
+
+        case TRANSCEDE_USIM_IOC_ICC_PRESENCE:
+            {
+                DWORD is_card_present = transcede_usim_icc_presence( );
+                ret = copy_to_user( (DWORD __user *)arg, &is_card_present, sizeof( DWORD ) );
+            }
+            break;
+
+        default:
+            pr_warn( "transcede_usim: invalid ioctl cmd (%u)", cmd );
+            break;
+    }
+
+    return ret;
+}
+
+/************************************************************************************
+ * Platform functions
+ ***********************************************************************************/
+
+/*!
+ * Probe method for the Transcede platform driver. This function is responsible
+ * for allocating all of the resources required.
+ *
+ * @param pdev The platform device that has been probed.
+ * @return Returns zero on success, negative on failure.
+ */
+static int transcede_usim_probe( struct platform_device *pdev )
+{
+    struct resource *res;
+    struct resource *r;
+    int ret = -EINVAL;
+
+    pr_debug( "transcede_usim: probe" );
+
+    res = platform_get_resource( pdev, IORESOURCE_IRQ, 0 );
+    if ( !res )
+        goto out;
+
+    usimdev.irq = res->start;
+
+    res = platform_get_resource( pdev, IORESOURCE_MEM, 0 );
+    if ( !res )
+        goto out;
+
+    usimdev.base_phys = res->start;
+    usimdev.base_len = ( res->end - res->start ) + 1;
+
+    ret = -EBUSY;
+    r = request_mem_region( usimdev.base_phys, usimdev.base_len, pdev->name );
+    if ( !r )
+        goto out;
+
+    usimdev.base = ioremap( usimdev.base_phys, usimdev.base_len );
+    if ( !ret )
+    {
+        release_resource( r );
+        goto out;
+    }
+
+    ret = request_irq( usimdev.irq,  transcede_usim_irq, IRQF_DISABLED, pdev->name,
+         &usimdev );
+
+    device_init_wakeup( &pdev->dev, 1 );
+
+out:
+    return ret;
+}
+
+/*!
+ * Remove method for the Transcede USIM platform driver. This method is called when
+ * the platform driver is removed and must release all resources the driver has
+ * been using.
+ *
+ * @param pdev The platform device being remove.
+ * @return Returns zero on success, negative on failure.
+ */
+static int
+transcede_usim_remove( struct platform_device *pdev )
+{
+    pr_debug( "transcede_usim_remove" );
+
+    free_irq( usimdev.irq, &usimdev );
+    iounmap( usimdev.base );
+    release_mem_region( usimdev.base_phys, usimdev.base_len );
+
+    return 0;
+}
+
+/*! The Transcede USIM platform driver.  */
+static struct platform_driver transcede_usim_driver =
+{
+    .probe      = transcede_usim_probe,
+    .remove     = transcede_usim_remove,
+    .driver     = {
+        .name   = "transcede_usim",
+    },
+};
+
+/*!
+ * Transcede_usim module initialisation. This function registers the misc device
+ * and performs any initialisation necessary to allow users to use the
+ * services provided.
+ *
+ * @return Returns zero on success, negative on failure.
+ */
+static int
+transcede_usim_init( void )
+{
+    int ret = misc_register( &usimdev.miscdev );
+
+    pr_debug( "transcede_usim_init" );
+
+    if ( ret )
+    {
+        printk( KERN_ERR "failed to register transcede_usim miscdevice\n" );
+        goto out;
+    }
+
+    ret = platform_driver_register( &transcede_usim_driver );
+
+    if ( ret )
+    {
+        printk( KERN_ERR "failed to register driver\n");
+        goto out;
+    }
+
+    init_waitqueue_head( &usimdev.waitq );
+    spin_lock_init( &usimdev.lock );
+
+out:
+    return ret;
+}
+
+/*!
+ * Module exit function for transcede_usim. This function releases any resources
+ * and performs any cleanup necessary.
+ */
+static void
+transcede_usim_exit( void )
+{
+    pr_debug( "transcede_usim_exit" );
+
+    (void)misc_deregister( &usimdev.miscdev );
+    (void)platform_driver_unregister( &transcede_usim_driver );
+}
+
+module_init( transcede_usim_init );
+module_exit( transcede_usim_exit );
+MODULE_AUTHOR( "Andrew Watkins" );
+MODULE_LICENSE( "GPL" );
diff --git a/drivers/misc/transcede_usim_regs.h b/drivers/misc/transcede_usim_regs.h
new file mode 100644
index 0000000..b8910fc
--- /dev/null
+++ b/drivers/misc/transcede_usim_regs.h
@@ -0,0 +1,699 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+/*!
+ * \file usim.h
+ * \brief List of Transcede USIM registers offset addresses and bit allocations
+ *
+ * This file contains all the Transcede USIM registers offset addresses and bit
+ * allocations
+ */
+
+#ifndef __TRANSCEDE_USIM_REGS_H__
+#define __TRANSCEDE_USIM_REGS_H__
+
+//
+// USIM block defines
+//
+#define USIM_BASE_ADDRESS 0x00000000
+
+#define ADR_CTRL1          (USIM_BASE_ADDRESS + 0x000)
+#define ADR_CTRL2          (USIM_BASE_ADDRESS + 0x004)
+#define ADR_SCPADS         (USIM_BASE_ADDRESS + 0x008)
+#define ADR_INTEN          (USIM_BASE_ADDRESS + 0x00C)
+#define ADR_INTSTAT        (USIM_BASE_ADDRESS + 0x010)
+#define ADR_FIFOCTRL       (USIM_BASE_ADDRESS + 0x014)
+#define ADR_FIFOCNT        (USIM_BASE_ADDRESS + 0x018)
+#define ADR_RXFIFOTH       (USIM_BASE_ADDRESS + 0x01C)
+#define ADR_FIFOLIMIT      (USIM_BASE_ADDRESS + 0x01C)
+#define ADR_REPEAT         (USIM_BASE_ADDRESS + 0x020)
+#define ADR_CGSCDIV        (USIM_BASE_ADDRESS + 0x024)
+#define ADR_CGBITDIV       (USIM_BASE_ADDRESS + 0x028)
+#define ADR_SCGT           (USIM_BASE_ADDRESS + 0x02C)
+#define ADR_ADEATIME       (USIM_BASE_ADDRESS + 0x030)
+#define ADR_LOWRSTTIME     (USIM_BASE_ADDRESS + 0x034)
+#define ADR_ATRSTARTLIMIT  (USIM_BASE_ADDRESS + 0x038)
+#define ADR_C2CLIM         (USIM_BASE_ADDRESS + 0x03C)
+#define ADR_INTEN2         (USIM_BASE_ADDRESS + 0x040)
+#define ADR_INTSTAT2       (USIM_BASE_ADDRESS + 0x044)
+#define ADR_TXFIFOTH       (USIM_BASE_ADDRESS + 0x048)
+#define ADR_TXFIFOCNT      (USIM_BASE_ADDRESS + 0x04C)
+#define ADR_RXFIFOCNT      (USIM_BASE_ADDRESS + 0x050)
+
+#define ADR_FIFODATA       (USIM_BASE_ADDRESS + 0x200)
+
+#define FIFODEPTH   0x8
+
+//
+// Control Register 1 bits
+//
+
+/** @brief Bit: 0, Name: INVLEV, Access: Read/write, Description: Inverse Bit Level.
+ *
+ * When high, inverse level convention is used (A=1, Z=0).
+ */
+#define USIM_CR1_INVLEV (1 << 0)
+
+/** @brief Bit: 1, Name: INVORD, Access: Read/write, Description: Inverse Bit Ordering.
+ *
+ * When high, inverse bit ordering convention (msb-lsb) is used.
+ */
+#define USIM_CR1_INVORD (1 << 1)
+
+/** @brief Bit: 2, Name: PECH2FIFO, Access: Read/write, Description: Character With Wrong Parity to FIFO.
+ *
+ * Enables storage of the characters received with wrong parity in RX FIFO.
+ */
+#define USIM_CR1_PECH2FIFO (1 << 2)
+
+/** @brief Bit: 6, Name: CLKSTOP, Access: Read/write, Description: Clock Stop.
+ *
+ * When this bit is asserted and the smart card I/O line is in Z state,
+ * the SCR core stops driving of the smart card clock signal after the
+ * CLKSTOPDELAY time expires.
+ *
+ * The smart card clock is restarted immediately after
+ * the CLKSTOP signal is deasserted.
+ *
+ * New character transmission can be started after CLKSTARTDELAY time.
+ *
+ * The expiration of both times is signaled by the CLKSTOPRUN bit in the Interrupt registers.
+ */
+#define USIM_CR1_CLKSTOP (1 << 6)
+
+/** @brief Bit: 7, Name: CLKSTOPVAL, Access: Read/write, Description: Clock Stop Value.
+ *
+ * The value of the scclk output during the clock stop state.
+ */
+#define USIM_CR1_CLKSTOPVAL (1 << 7)
+
+/** @brief Bit: 8, Name: TXEN, Access: Read/write, Description: Transmission enable.
+ *  When enabled the characters are read from TX FIFO and transmitted through UART to the Smart Card.
+ */
+#define USIM_CR1_TXEN (1 << 8)
+
+/** @brief Bit: 9, Name: RXEN, Access: Read/write, Description: Receiving enable.
+ *
+ * When enabled the characters sent by the Smart Card are received by the UART
+ * and stored in RX FIFO. Receiving is internally disabled while a
+ * transmission is in progress.
+ */
+#define USIM_CR1_RXEN (1 << 9)
+
+/** @brief Bit: 10, Name: TS2FIFO, Access: Read/write, Description: TS to FIFO.
+ *
+ * Enables to store the first ATR character TS in RX FIFO.
+ *
+ * During ideal card session there is no necessity to store TS character,
+ * so it can be disabled.
+ */
+#define USIM_CR1_TS2FIFO (1 << 10)
+
+/** @brief Bit: 11, Name: T0T1, Access: Read/write, Description: T0/T1 Protocol.
+ *
+ * Controls the using of T=0 or T=1 protocol.
+ * No character repeating is used when T=1 protocol is selected.
+ * The Character Guardtime (minimum delay between the leading edges of
+ * two consecutive characters) is reduced to 11 ETU when T=1 protocol
+ * is used and Guardtime value N = 255.
+ */
+#define USIM_CR1_T0T1 (1 << 11)
+
+/** @brief Bit: 12, Name: ATRSTFLUSH, Access: Read/write, Description: ATR Start Flush FIFO.
+ *
+ * When enabled, both FIFOs are flushed before the ATR is started.
+ */
+#define USIM_CR1_ATRSTFLUSH (1 << 12)
+
+/** @brief Bit: 15, Name: GINTEN, Access: Read/write, Description: Global Interrupt Enable.
+ *
+ * When high, INTERRUPT output assertion is enabled.
+ */
+#define USIM_CR1_GINTEN (1 << 15)
+
+//
+// Control Register 2 bits
+//
+/** @brief Bit: 0, Name: reserved, Access: Read, Description: Reserved bits are hard-wired to zero.
+ */
+#define USIM_CR2_RESERVED_0 (1 << 0)
+
+/** @brief Bit: 1, Name: reserved, Access: Read, Description: Reserved bits are hard-wired to zero.
+ */
+#define USIM_CR2_RESERVED_1 (1 << 1)
+
+/** @brief Bit: 2, Name: WARMRST, Access: Write, Description: Warm Reset Command.
+ *
+ * Writing 1 to this bit initializes Warm Reset of the Smart Card.
+ * This bit is always read as 0.
+ */
+#define USIM_CR2_WARMRST (1 << 2)
+
+/** @brief Bit: 3, Name: ACT, Access: Read/write, Description: Activation.
+ *
+ * Setting of this bit initializes the activation sequence.
+ * When the activation is finished, the ACT bit is automatically cleared.
+ */
+#define USIM_CR2_ACT (1 << 3)
+
+/** @brief Bit: 4, Name: DEACT, Access: Read/write, Description: Deactivation.
+ *
+ * Setting of this bit initializes the deactivation sequence.
+ * When the deactivation is finished, the DEACT bit is automatically cleared.
+ */
+#define USIM_CR2_DEACT (1 << 4)
+
+/** @brief Bit: 5, Name: VCC18, Access: Read/write, Description: Control 1.8V Smart Card Vcc.
+ *
+ * Setting of this bit allows selection of 1.8V Vcc for Smart Card session (Class C).
+ * After the selection of operating class is completed, this bit is in 1 if
+ * this class was selected. Default value after reset is 0.
+ */
+#define USIM_CR2_VCC18 (1 << 5)
+
+/** @brief Bit: 6, Name: VCC33, Access: Read/write, Description: Control 3V Smart Card Vcc.
+ *
+ * Setting of this bit allows selection of 3V Vcc for Smart Card session (Class B).
+ * After the selection of operating class is completed, this bit is in 1 if
+ * this class was selected. Default value after reset is 0.
+ */
+#define USIM_CR2_VCC33 (1 << 6)
+
+/** @brief Bit: 7, Name: VCC50, Access: Read/write, Description: Control 5V Smart Card Vcc.
+ *
+ * Setting of this bit allows selection of 5V Vcc for Smart Card session (Class A).
+ * After the selection of operating class is completed, this bit is in 1 if
+ * this class was selected. Default value after reset is 0. See section 6.2.
+ */
+#define USIM_CR2_VCC50 (1 << 7)
+
+//
+// Smart Card Pad Register (SCPADS) bits
+//
+/** @brief Bit: 0, Name: DIRACCPADS, Access: Read/write, Description: Direct Access To Smart Card Pads.
+ *
+ * When high, it disables a serial interface functionality and enables direct
+ * control of the smart card pads using following 4 bits.
+ */
+#define USIM_PAD_DIRACCPADS (1 << 0)
+
+/** @brief Bit: 1, Name: DSCIO, Access: Read/write, Description: Direct Smart Card Input/Output.
+ *
+ * When DIRACCPADS = 1, the DSCIO bit provides direct access to SCIO open draing I/O pads.
+ */
+#define USIM_PAD_DSCIO (1 << 1)
+
+/** @brief Bit: 2, Name: DSCCLK, Access: Read/write, Description: Direct Smart Card Clock.
+ *
+ * When DIRACCPADS = 1, the DSCCLK bit provides direct access to SCCLK output.
+ */
+#define USIM_PAD_DSCCLK (1 << 2)
+
+/** @brief Bit: 3, Name: DSCRST, Access: Read/write, Description: Direct Smart Card Reset.
+ *
+ * When DIRACCPADS = 1, the DSCRST bit provides direct access to SCRST output.
+ */
+#define USIM_PAD_DSCRST (1 << 3)
+
+/** @brief Bit: 4, Name: DSCVCC, Access: Read/write, Description: Direct Smart Card Vcc.
+ *
+ * When DIRACCPADS = 1, the DSCVCC bit provides direct access to SCVCCx outputs.
+ * The appropriate SCVCC18, SCVCC33 and SCVCC50 outputs are driven according to
+ * state of bits VCC18, VCC33 and VCC50 in CTRL2 register.
+ */
+#define USIM_PAD_DSCVCC (1 << 4)
+
+/** @brief Bit: 5, Name: AUTOADEAVPP, Access: Read/write, Description: Automatic Vpp Handling.
+ * When high, it enables automatic handling of DSCVPPEN and DSCVPPPP signals
+ * during activation and deactivation sequence.
+ */
+#define USIM_PAD_AUTOADEAVPP (1 << 5)
+
+/** @brief Bit: 6, Name: DSCVPPEN, Access: Read/write, Description: Direct Smart Card Vpp Enable.
+ *
+ * It provides direct access to SCVPPEN output.
+ */
+#define USIM_PAD_DSCVPPEN (1 << 6)
+
+/** @brief Bit: 7, Name: DSCVPPPP, Access: Read/write, Description: Direct Smart Card Vpp Pause/Prog.
+ *
+ * It provides direct access to SCVPPPP output.
+ */
+#define USIM_PAD_DSCVPPPP (1 << 7)
+
+/** @brief Bit: 8, Name: DSCFCB, Access: Read/write, Description: Direct Smart Card Function Code Bit
+ *
+ * It provides direct access to SCFCB output.
+ */
+#define USIM_PAD_DSCFCB (1 << 8)
+
+/** @brief Bit: 9, Name: SCPRESENT, Access: Read, Description: Smart Card presented.
+ *
+ * This bit is set to 1 when the SCDETECT input is active at least for SCDETECTTIME.
+ */
+#define USIM_PAD_SCPRESENT (1 << 9)
+
+//
+// Interrupt bits for interrupt 1 enable and status registers
+//
+/** @brief Bit: 0, Name: TXFIDONE, Access: Read/write, Description: TX FIFO Done Interrupt.
+ *
+ * When enabled, this interrupt is asserted after all bytes from TX FIFO were transferred to the Smart Card.
+ */
+#define USIM_INT1_TXFIDONE (1 << 0)
+
+/** @brief Bit: 1, Name: TXFIEMPTY, Access: Read/write, Description: TX FIFO Empty Interrupt.
+ *
+ * When enabled, this interrupt is asserted if the TX FIFO is emptied out.
+ */
+#define USIM_INT1_TXFIEMPTY (1 << 1)
+
+/** @brief Bit: 2, Name: RXFIFULL, Access: Read/write, Description: RX FIFO Full Interrupt.
+ *
+ * When enabled, this interrupt is asserted if the RX FIFO is filled up.
+ */
+#define USIM_INT1_RXFIFULL (1 << 2)
+
+/** @brief Bit: 3, Name: CLKSTOPRUN, Access: Read/write, Description: Smart Card Clock Stop Interrupt.
+ *
+ * When enabled, this interrupt is asserted in two cases:
+ *  - 1. When the smart card clock is stopped.
+ *  - 2. When the new character transfer can be started after the clock restart.
+ *
+ * To distinguish between the two interrupt cases, we recommend reading the bit CLKSTOP in CTRL1 reg. .
+ */
+#define USIM_INT1_CLKSTOPRUN (1 << 3)
+
+/** @brief Bit: 4, Name: TXDONE, Access: Read/write, Description: Transmission Done Interrupt.
+ *
+ * When enabled, this interrupt is asserted after one character was transmitted to the Smart Card.
+ */
+#define USIM_INT1_TXDONE (1 << 4)
+
+/** @brief Bit: 5, Name: RXDONE, Access: Read/write, Description: Reception Done Interrupt.
+ *
+ * When enabled, this interrupt is asserted after a character was received from the Smart Card.
+ */
+#define USIM_INT1_RXDONE (1 << 5)
+
+/** @brief Bit: 6, Name: TXPERR, Access: Read/write, Description: Transmission Parity Error Interrupt.
+ *
+ * When enabled, this interrupt is asserted if the Smart Card signals wrong
+ * character parity during the guardtime after the character transmission
+ * was repeated TXREPEAT-times.
+ */
+#define USIM_INT1_TXPERR (1 << 6)
+
+/** @brief Bit: 7, Name: RXPERR, Access: Read/write, Description: Reception Parity Error Interrupt.
+ *
+ * When enabled, this interrupt is asserted after the character with wrong
+ * parity was received when the number of repeated receptions exceeds
+ * RXREPEAT value or T=1 protocol is used.
+ */
+#define USIM_INT1_RXPERR (1 << 7)
+
+/** @brief Bit: 8, Name: C2CFULL, Access: Read/write, Description: Two Consecutive Characters Limit Interrupt.
+ *
+ * When enabled, this interrupt is asserted if the time between two
+ * consecutive characters, transmitted between the Smart Card and the
+ * Reader in both directions, is equal the Two Characters Delay Limit
+ * The C2CFULL interrupt is internally enabled from the ATR start to
+ * the deactivation or ATR restart initialization.
+ *
+ * It is recommended to use this counter to detect unresponsive Smart Cards.
+ */
+#define USIM_INT1_C2CFULL (1 << 8)
+
+/** @brief Bit: 9, Name: RXTHRESHOLD, Access: Read/write, Description: RX FIFO Threshold Interrupt.
+ *
+ * When enabled, this interrupt is asserted if the number of bytes in
+ * RX FIFO is equal or exceeds the RX FIFO threshold.
+ */
+#define USIM_INT1_RXTHRESHOLD (1 << 9)
+
+/** @brief Bit: 10, Name: ATRFAIL, Access: Read/write, Description: ATR Fail Interrupt.
+ *
+ * When enabled, this interrupt is asserted if the ATR sequence fails.
+ */
+#define USIM_INT1_ATRFAIL (1 << 10)
+
+/** @brief Bit: 11, Name: ATRDONE, Access: Read/write, Description: ATR Done Interrupt.
+ *
+ * When enabled, this interrupt is asserted after the ATR sequence is successfully completed.
+ */
+#define USIM_INT1_ATRDONE (1 << 11)
+
+/** @brief Bit: 12, Name: SCREM, Access: Read/write, Description: Smart Card Removed Interrupt.
+ * When enabled, this interrupt is asserted after the smart card removal.
+ */
+#define USIM_INT1_SCREM (1 << 12)
+
+/** @brief Bit: 13, Name: SCINS, Access: Read/write, Description: Smart Card Inserted Interrupt.
+ *
+ * When enabled, this interrupt is asserted after the smart card insertion.
+ */
+#define USIM_INT1_SCINS (1 << 13)
+
+/** @brief Bit: 14, Name: SCACT, Access: Read/write, Description: Smart Card Activation Interrupt.
+ *
+ * When enabled, this interrupt is asserted after the Smart Card activation sequence is complete.
+ */
+#define USIM_INT1_SCACT (1 << 14)
+
+/** @brief Bit: 15, Name: SCDEACT, Access: Read/write, Description: Smart Card Deactivation Interrupt.
+ * When enabled, this interrupt is asserted after the Smart Card deactivation sequence is complete.
+ */
+#define USIM_INT1_SCDEACT (1 << 15)
+
+//
+// Interrupt bits for interrupt 2 enable and status registers
+//
+
+/** @brief Bit: 0, Name: TXTRHESHOLD, Access: Read/write, Description: TX FIFO Threshold Interrupt.
+ *
+ * When enabled, this interrupt is asserted if the number of bytes
+ * in TX FIFO is equal o less than the TX FIFO threshold.
+ */
+#define USIM_INT2_TXTHRESHOLD (1 << 0)
+
+//
+// FIFO control register bits
+//
+/** @brief Bit: 0, Name: TXFIEMPTY, Access: Read, Description: TX FIFO Empty.
+ */
+#define USIM_FIFO_TXFIEMPTY (1 << 0)
+
+/** @brief Bit: 1, Name: TXFIFULL, Access: Read, Description: TX FIFO Full.
+ */
+#define USIM_FIFO_TXFIFULL (1 << 1)
+
+/** @brief Bit: 2, Name: TXFIFLUSH, Access: Write, Description: Flush TX FIFO.
+ *
+ * TX FIFO is flushed, when 1 is written to this bit.
+ */
+#define USIM_FIFO_TXFIFLUSH (1 << 2)
+
+/** @brief Bit: 8, Name: RXFIEMPTY, Access: Read, Description: RX FIFO Empty
+ */
+#define USIM_FIFO_RXFIEMPTY (1 << 8)
+
+/** @brief Bit: 9, Name: RXFIFULL, Access: Read, Description: RX FIFO Full.
+ */
+#define USIM_FIFO_RXFIFULL (1 << 9)
+
+/** @brief Bit: 10, Name: RXFIFLUSH, Access: Write, Description: Flush RX FIFO.
+ *
+ * RX FIFO is flushed, when 1 is written to this bit.
+ */
+#define USIM_FIFO_RXFIFLUSH (1 << 10)
+
+/** @brief USIM controller register map as structure
+ *
+ * NOTE: All registers are actually 16 or 8 bits, but
+ * all 16 bit registers are aligned on 32 bit boundaries
+ * and the system will automatically ensure proper reads
+ * and writes using 32 bit values (most significant /
+ * unused bits are truncated on write
+ * and set to zero on read)
+ */
+typedef struct
+{
+    /** @brief Offset 0x00, 16 bits Control Register 1 (CTRL1)
+     *
+     * This is a 16-bit, read/write register that provides the control
+     * of the Smart Card Reader interface
+     *
+     * See defines section for bit definitions.
+     */
+    uint32_t ControlRegister1;
+
+    /** @brief Offset 0x04, 16 bits Control Register 2 (CTRL1)
+     *
+     * This is a 16-bit, read/write register that provides the control
+     * of the Smart Card Reader interface
+     *
+     * See defines section for bit definitions.
+     */
+    uint32_t ControlRegister2;
+
+    /** @brief Offset 0x08, 16 bits Smart Card Pads Register (SCPADS)
+     *
+     * This is a 16-bit, read/write register that provides direct access
+     * to Smart Card pads without serial interface assistance.
+     *
+     * You can use this register feature with synchronous and any other
+     * non-ISO 7816 and non-EMV cards.
+     *
+     * This feature can also be used in initial board bringup to toggle
+     * output lines from software to validate proper board routing
+     * from chip to card socket.
+     */
+    uint32_t SmartCardPadsRegister;
+
+    /** @brief Offset 0x0C, 16 bits Interrupt Enable Register 1 (INTEN1)
+     *
+     * This is a 16-bit read/write register that enables assertion
+     * of each interrupt bit individually in the
+     * first Interrupt Status Register.
+     *
+     * See defines section for bit definitions (bit positions are common
+     * with interrupt status register 1).
+     */
+    uint32_t InterruptEnableRegister1;
+
+    /** @brief Offset 0x10  16 bits Interrupt Status Register 1 (INTSTAT1)
+     *
+     * This 16-bit read/clear register provides information about the state
+     * of each interrupt bit as enabled by Interrrupt Enable Register 1.
+     *
+     * You can clear the register bits individually
+     * by writing '1' to a bit you intend to clear.
+     *
+     * If the global interrupt is enabled in the GINTEN bit in
+     * Control Register 1, then each interrupt bit assertion in
+     * the Interrupt Status Register invokes the global interrupt.
+     *
+     * See defines section for bit definitions (bit positions are common
+     * with interrupt enable register 1).
+     */
+    uint32_t InterruptStatusRegister1;
+
+    /** @brief Offset 0x14, 16 bits FIFO Control Register (FIFOCTRL)
+     *
+     * This is a 16-bit read/write register that provides the control
+     * and status of RX and TX FIFO.
+     *
+     * See defines section for bit definitions.
+     */
+    uint32_t FifoControlRegister;
+
+    /** @brief Offset 0x18,  8 bits Legacy TX FIFO Counter, Offset 0x19  8 bits Legacy RX FIFO Counter
+     *
+     * @par Legacy TX FIFO counter
+     * This is an 8-bit, read-only register that provides the lower part
+     * of the 16 bit TX FIFO Counter. It is equal to TX FIFO Counter up to
+     * value 255. All values above 255 are read as 255. It is recommended
+     * to use the 16-bit TX FIFO Counter instead of this register.
+     *
+     * @par Legacy RX FIFO counter
+     * This is an 8-bit, read-only register that provides the lower part
+     * of the 16 bit RX FIFO Counter. It is equal to RX FIFO Counter up to
+     * value 255. All values above 255 are read as 255. It is recommended
+     * to use the 16-bit RX FIFO Counter instead of this register.
+     */
+    uint32_t  LegacyTxRxFifoCounters;
+
+    /** @brief Offset 0x1C, 16 bits RX FIFO Threshold
+     *
+     * This is a 16-bit, read/write register that sets the interrupt threshold.
+     * The interrupt is asserted when the number of bytes it receives is
+     * equal to, or exceeds the threshold.
+     */
+    uint32_t RxFifoThreshold;
+
+    /** @brief Offset 0x20, 8 bits, 4 bits TX Repeat register, 4 bits RX repeat register
+     *
+     * RX Repeat: This is a 4-bit, read/write register that specifies
+     * the number of attempts to request character re-transmission
+     * after wrong parity was detected. The re-transmission of the character
+     * is requested using the error signal during the guardtime.
+     *
+     * TX Repeat: This is a 4-bit, read/write register that specifies the number
+     * of attempts to re-transmit the character after the Smart Card signals
+     * the wrong parity during the guardtime.
+     */
+    uint32_t TxRxRepeat;
+
+    /** @brief Offset 0x24, 16 bits Smart Card Clock Divisor (SCCDIV)
+     *
+     * This is a 16-bit, read/write register that defines the divisor
+     * value used to generate the Smart Card Clock from the system clock.
+     *
+     * Divisor Calculation info:
+     *  - CCLK frequency =  CLK_freq/(2*(SCCDIV+1)
+     *  - SCCDIV         = (CLK_freq/SCClk_freq/2) - 1;
+     *  - SCCLK target frequency
+     *     - 1 to 5 MHz under class A cards
+     *     - 1 to 4 MHz under class B cards
+     *     - Example:
+     *         - CLK_freq   = 150MHz       -- Example System Clock
+     *         - SCCLK_freq = 3.947MHz     -- Required SmartCard Clock frequency
+     *         - SCCDIV     = 150MHz/3.947MHz/2 - 1 = 18 (0x12 hex)
+     *
+     */
+    uint32_t SmartCardClockDivisor;
+
+    /** @brief Offset 0x28, 16 bits Baud Clock Divisor (BAUDDIV)
+     *
+     * This is a 16-bit, read/write register that defines a divisor
+     * value used to generate the Baud Clock impulses from the
+     * system clock.
+     *
+     * Divisor Calculation info:
+     *   - BAUD frequency = CLK_freq/(2*(BAUDDIV+1)
+     *   - Initial BAUDDIV value should be in accordance with BAUD_freq = SCClk_freq/372
+     *   - BAUDDIV = ((372*CLK_freq)/(2*SCClk_freq))-1
+     *   - Example:
+     *      - CLK_freq   = 150MHz       -- Example System Clock
+     *      - SCCLK_freq = 3.947MHz     -- SmartCard Clock frequency set by SCCDIV register
+     *      - BAUDDIV    = (372*150MHz)/(2*3.947MHz)-1 = 7068 (0x1B9C hex)
+     *
+     */
+    uint32_t BaudClockDivisor;
+
+    /** @brief Offset 0x2C, 8 bits Smart Card Guardtime
+     *
+     * This is an 8-bit, read/write register that sets a delay at the
+     * end of each character transmitted from the Smart Card Reader
+     * to the Smart Card. The value is in Elementary Time Units (ETU).
+     * The parity error is besides signaled during the guardtime.
+     */
+    uint32_t SmartCardGuardtime;
+
+    /** @brief Offset 0x30, 16 bits  Activation / Deactivation Time (ADEATIME)
+     *
+     * This is a 16-bit, read/write register that sets the duration
+     * of each part of the activation and deactivation sequence.
+     * The value is in Smart Card Clock Cycles.
+     *
+     * Bits (7:0) of this register are hard-wired to zero.
+     */
+    uint32_t ActivationDeactivationTime;
+
+    /** @brief Offset 0x34, 16 bits Reset Duration (LOWRSTTIME)
+     *
+     * This is a 16-bit read/write register that sets the duration
+     * of the smart card reset sequence. This value is same for the
+     * cold and warm reset. The value is in terms of smart card clock
+     * cycles.
+     *
+     * Bits (7:0) of this register are hard-wired to zero.
+     */
+    uint32_t ResetDuration;
+
+    /** @brief Offset 0x38, 16 bits ATR Start Limit (ATRSTARTLIMIT)
+     *
+     * This is a 16-bit read/write register that defines the maximum time
+     * between the rising edge of the SCRSTN signal and the start of
+     * ATR response. The value is in terms of smart card clock cycles.
+     *
+     * Bits (7:0) of this register are hard-wired to zero.
+     */
+    uint32_t AtrStartLimit;
+
+    /** @brief Offset 0x3C, 16 bits Two Characters Delay Limit (C2CLIM)
+     *
+     * This is a 16-bit, read/write register that sets the maximum time
+     * between the leading edges of two, consecutive characters.
+     * The value is in ETUs.
+     */
+    uint32_t TwoCharactersDelayLimit;
+
+    /** @brief Offset 0x40, 16 bits Interrupt Enable Register 2 (INTEN2)
+     *
+     * This is a 16-bit read/write register that enables assertion
+     * of each interrupt bit individually in the
+     * second Interrupt Status Register.
+     *
+     * See defines section for bit definitions (bit positions are common
+     * with the second interrupt status register).
+     */
+    uint32_t InterruptEnableRegister2;
+
+    /** @brief Offset 0x44  16 bits Interrupt Status Register 2 (INTSTAT2)
+     *
+     * This 16-bit read/clear register provides information about the state
+     * of each interrupt bit as enabled by Interrrupt Enable Register 2.
+     *
+     * You can clear the register bits individually
+     * by writing '1' to a bit you intend to clear.
+     *
+     * If the global interrupt is enabled in the GINTEN bit in
+     * Control Register 1, then each interrupt bit assertion in
+     * the Interrupt Status Register invokes the global interrupt.
+     *
+     * See defines section for bit definitions (bit positions are common
+     * with the second interrupt enable register).
+     */
+    uint32_t InterruptStatusRegister2;
+
+    /** @brief Offset 0x48 16 bits TX FIFO Threshold
+     *
+     * This is a 16-bit, read/write register that sets the interrupt threshold.
+     * The interrupt is asserted when the number of bytes in TX FIFO
+     * is equal or less than the threshold.
+     */
+    uint32_t TxFifoThreshold;
+
+    /** @brief Offset 0x4C 16 bits TX FIFO Counter
+     *
+     * This is a 16-bit, read-only register that provides the number of bytes
+     * stored in the TX FIFO.
+     */
+    uint32_t TxFifoCounter;
+
+    /** @brief Offset 0x50 16 bits RX FIFO Counter
+     *
+     * This is a 16-bit, read-only register that provides the number of bytes
+     * stored in the RX FIFO.
+     */
+    uint32_t RxFifoCounter;
+
+    /* End of main registers, filler area until FIFO data starting @ 0x200 */
+
+    /** @brief Offsets 0x54-0x1FC Not used, reserved for possible future use */
+    uint32_t Reserved0x54[107];
+
+    /** Offsets 0x200-0x3FF: FIFO
+     *
+     * This is an 8-bit, read/write register that provides access to the
+     * receive and transmit FIFO buffers.
+     *
+     *   - The TX FIFO is accessed during the APB write transfer.
+     *   - The RX FIFO is accessed during the APB read transfer.
+     *   - All read/write accesses at address range offsets
+     *     200h-3ffh are redirected to the FIFO.
+     */
+    uint8_t  FifoData[0x200];
+
+} USIMREGS, *PUSIMREGS;
+
+#endif //__TRANSCEDE_USIM_REGS_H__
diff --git a/drivers/misc/transcede_usim_t0_handler.c b/drivers/misc/transcede_usim_t0_handler.c
new file mode 100644
index 0000000..bca96da
--- /dev/null
+++ b/drivers/misc/transcede_usim_t0_handler.c
@@ -0,0 +1,171 @@
+/*****************************************************************
+/
+/ File   :   T0Hndlr.c
+/ Author :   David Corcoran
+/ Date   :   October 15, 1999
+/ Purpose:   This provides a T=0 handler.
+/            See http://www.linuxnet.com for more information.
+/ License:   See file LICENSE
+/
+******************************************************************/
+
+#include <asm/uaccess.h>
+#include <linux/debugfs.h>
+#include <linux/io.h>
+#include <linux/delay.h>
+#include <linux/mindspeed/pcscdefines.h>
+
+extern int
+libusim_read_write( const PUCHAR tx_buffer,
+                    const DWORD tx_length,
+                    PUCHAR rx_buffer,
+                    PDWORD rx_length);
+
+extern DWORD
+libusim_t0_exchange_data( PUCHAR pRequest,
+                          DWORD RequestLen,
+                          PUCHAR pReply,
+                          PDWORD pReplyLen )
+{
+
+  int rv;
+  int bytesWritten, bytesToWrite;
+  int totalBytesToWrite;
+  int isoApduCase=0;
+  unsigned long rspSize;
+  unsigned char rsp[MAX_RESPONSE_SIZE];
+  unsigned char cmd[MAX_RESPONSE_SIZE];
+
+  /* Determine the type of APDU to be sent */
+
+  if ( (RequestLen == 5) && (pRequest[4] == 0x00) ) {
+    isoApduCase = 1;  /* ISO 7816 Case 1 Transaction */
+  } else if ( (RequestLen == 5) && (pRequest[4] > 0x00) ) {
+    isoApduCase = 2;  /* ISO 7816 Case 2 Transaction */
+  } else if ( (RequestLen > 5) && (pRequest[4] > 0x00) ) {
+    isoApduCase = 3;  /* ISO 7816 Case 3 Transaction */
+  } else if ( (RequestLen > 5) && ((RequestLen) ==
+              (pRequest[4] + 5 + 1)) ) {
+    isoApduCase = 4;  /* ISO 7816 Case 3 Transaction */
+  }
+
+  bytesWritten      = 0;
+  totalBytesToWrite = RequestLen;
+
+  bytesToWrite = 5;
+
+  do {
+    rspSize = 1;  /* Only get 1 byte from the card */
+
+    memcpy( cmd, &pRequest[bytesWritten], bytesToWrite );
+
+    rv = libusim_read_write( cmd, bytesToWrite, rsp, &rspSize );
+
+    if ( rv != 0 ) {
+      *pReplyLen = 0;
+      return rv;
+    }
+
+    bytesWritten      += bytesToWrite;
+    totalBytesToWrite -= bytesToWrite;
+
+    if (rsp[0] == 0x60) {
+      mdelay(50);    /* Sleep for a bit     */
+      bytesToWrite = 0; /* Just read this time */
+      continue;
+
+    } else if ( rsp[0] == cmd[1] ) {
+      if ( isoApduCase == 1 ) {
+        rspSize = 2;
+        rv = libusim_read_write( cmd, 0, rsp, &rspSize );
+
+        if (rv != 0) {
+          *pReplyLen = 0;
+          return rv;
+        }
+
+        *pReplyLen = 2;
+        memcpy( pReply, rsp, rspSize );
+        return rv;
+
+      } else if ( isoApduCase == 2 ) {
+        rspSize = pRequest[4] + 2;
+        rv = libusim_read_write( cmd, 0, rsp, &rspSize );
+
+        if (rv != 0) {
+          *pReplyLen = 0;
+          return rv;
+        }
+
+        *pReplyLen = rspSize;
+        memcpy( pReply, rsp, rspSize );
+        return rv;
+
+      } else if ( isoApduCase == 3 ) {
+        rspSize = 2;
+        pr_debug("Length to send is %d\n", totalBytesToWrite);
+        memcpy( cmd, &pRequest[bytesWritten], totalBytesToWrite );
+        rv = libusim_read_write( cmd, totalBytesToWrite, rsp, &rspSize );
+
+        if (rv != 0) {
+          *pReplyLen = 0;
+          return rv;
+        }
+
+        *pReplyLen = 2;
+        memcpy( pReply, rsp, rspSize );
+        return rv;
+      } else if ( isoApduCase == 4 ) {
+        /* Stupid ass ISO 7816 brain dead transaction */
+      } else {
+        /* We have a problem here.... */
+        *pReplyLen = 0;
+        return 1;
+      }
+
+   } else if ( (rsp[0] == ~cmd[1]) ||
+               (rsp[0] == ~(cmd[1] + 1)) ) {
+
+      if ( isoApduCase == 1 ) {
+         /* This will never happen */
+         *pReplyLen = 0;
+         return 1;
+       } else if ( isoApduCase == 2 ) {
+         /* we can only receive the next byte */
+         bytesToWrite = 0;
+         continue;
+       } else if ( isoApduCase == 3 ) {
+         /* we can only send the next byte */
+         bytesToWrite = 1;
+         continue;
+       } else if ( isoApduCase == 4 ) {
+        /* ISO 7816 case 4 transaction */
+       }
+
+   } else if (( rsp[0] >= 0x61 && rsp[0] <= 0x6F ) ||
+             ( rsp[0] >= 0x90 && rsp[0] <= 0x9F )) {
+
+     pReply[0] = rsp[0];
+
+     rspSize = 1;
+     rv = libusim_read_write( cmd, 0, rsp, &rspSize );
+
+     if (rv != 0) {
+        *pReplyLen = 0;
+        return rv;
+     }
+
+     *pReplyLen = 2;
+     pReply[1] = rsp[0];
+
+     return rv;
+   } else {
+     /* Serious problem here */
+     return 1;
+   }
+
+  } while (1);
+
+
+  return 0;
+}
diff --git a/drivers/misc/transcede_usim_t1_handler.c b/drivers/misc/transcede_usim_t1_handler.c
new file mode 100644
index 0000000..52cd9fb
--- /dev/null
+++ b/drivers/misc/transcede_usim_t1_handler.c
@@ -0,0 +1,336 @@
+/******************************************************************
+
+            Title  : T1Hndlr.c
+            Package: Linux Smartcard Reader Driver
+            Author : David Corcoran
+            Date   : 10/15/99
+            Purpose: This handles the T=1 protocol.
+            LICENSE: See LICENSE
+
+********************************************************************/
+
+#include <asm/uaccess.h>
+#include <linux/debugfs.h>
+#include <linux/io.h>
+#include <linux/delay.h>
+#include <linux/mindspeed/pcscdefines.h>
+
+/* This can actually be read from the ATR but since many
+   smartcard manufacturers still can't figure out what it
+   means to be ISO compliant I will just rely on a small
+   number.  ( Most cards have a 32 byte IFSD_SIZE )
+*/
+
+#define MAX_IFSD_SIZE  20
+#define BWT_MULTIPLIER  3
+
+#define T1_SBLOCK_SUCCESS      0x200
+#define T1_SBLOCK_WTXREQUEST   0x201
+#define T1_RBLOCK_SUCCESS      0x210
+#define T1_IBLOCK_SUCCESS      0x220
+#define T1_ERROR_PARITY        0x230
+#define T1_ERROR_OTHER         0x240
+#define T1_INVALID_BLOCK       0x250
+
+extern int
+libusim_read_write( const PUCHAR tx_buffer,
+                    const DWORD tx_length,
+                    PUCHAR rx_buffer,
+                    PDWORD rx_length);
+
+
+static UCHAR
+T1CalculateLRC( PUCHAR pucBuffer,
+                DWORD dwLength )
+{
+  UCHAR ucXSum;
+  short i;
+
+  ucXSum = 0;
+  for ( i=0; i<dwLength; i++ ) {
+    ucXSum ^= pucBuffer[i];
+  }
+  return ucXSum;
+}
+
+static DWORD
+T1_GetResponseType( PUCHAR pucRBuffer,
+                    DWORD dwLength )
+{
+  /* Checks to see what type of block it is */
+
+  if ( (pucRBuffer[1] & 0x80)&&(pucRBuffer[1] & 0x40) ) {
+    /* S Block Found */
+    pr_debug("S Block Found\n");
+
+    if ( pucRBuffer[1] & 0x03 ) {
+    pr_debug("WTX Request Made\n");
+    return T1_SBLOCK_WTXREQUEST;
+    } else if ( pucRBuffer[1] & 0x04 ) {
+    pr_debug("Vpp Error Response Made\n");
+    } else if ( pucRBuffer[1] & 0x02 ) {
+    pr_debug("ABORT Request Made\n");
+    } else if ( pucRBuffer[1] & 0x01 ) {
+    pr_debug("IFS Request Made\n");
+    } else if ( pucRBuffer[1] == 0xC0 ) {
+    pr_debug("RESYNCH Request Made\n");
+    } else {
+    pr_debug("Unknown Request Made\n");
+    }
+
+    return T1_SBLOCK_SUCCESS;
+
+  } else if ( pucRBuffer[1] & 0x80 ) {
+    /* R Block Found */
+    pr_debug("R Block Found\n");
+    if ( pucRBuffer[1] & 0x10 ) {
+      if ( pucRBuffer[1] & 0x01 ) {
+	return T1_ERROR_PARITY;
+      }
+
+      if ( pucRBuffer[1] & 0x02 ) {
+	return T1_ERROR_OTHER;
+      }
+
+    } else {
+      return T1_RBLOCK_SUCCESS;
+    }
+
+  } else if ( (pucRBuffer[1] & 0x80) == 0 ) {
+    /* I Block Found */
+    pr_debug("I Block Found\n");
+    return T1_IBLOCK_SUCCESS;
+
+  } else {
+    return T1_INVALID_BLOCK;
+  }
+
+  return T1_ERROR_OTHER; // should never get here
+}
+
+static DWORD
+T1_Transaction( PUCHAR pRequest,
+                DWORD RequestLen,
+                PUCHAR pReply,
+                PDWORD pReplyLen )
+{
+  DWORD rv;
+  UCHAR ucTxBuffer[MAX_RESPONSE_SIZE];
+  UCHAR ucResponse[MAX_RESPONSE_SIZE];
+  DWORD ulRxLength;
+
+  memcpy( ucTxBuffer, pRequest, RequestLen );
+
+  /* Receive the basic T=1 Nad, PCB, Len */
+  ulRxLength = 3;
+  rv = libusim_read_write( ucTxBuffer, RequestLen, ucResponse, &ulRxLength );
+
+  if ( rv == 0 ) {
+
+    memcpy( pReply, ucResponse, ulRxLength );
+
+    ulRxLength = ucResponse[2] + 1;
+    rv = libusim_read_write( ucTxBuffer, 0, ucResponse, &ulRxLength );
+
+    memcpy( &pReply[3], ucResponse, ulRxLength);
+    *pReplyLen = ulRxLength + 3;
+
+  } else {
+    *pReplyLen = 0;
+  }
+
+  return rv;
+}
+
+static DWORD
+T1_WTXResponse( UCHAR ucWtx,
+                PUCHAR ucRBuffer)
+{
+
+  DWORD rv;
+  DWORD dwRBufferLen;
+  UCHAR ucTBuffer[MAX_RESPONSE_SIZE];
+
+  ucTBuffer[0] = 0x00;          /* NAD - Addressing not used */
+  ucTBuffer[1] = 0xE3;          /* PCB - WTX Response        */
+  ucTBuffer[2] = 1;             /* LEN - 1                   */
+  ucTBuffer[3] = ucWtx;         /* WTX Value                 */
+  ucTBuffer[4] = T1CalculateLRC( ucTBuffer, 4 );
+  dwRBufferLen = MAX_RESPONSE_SIZE;
+
+  //Adm_SetWWT( ucWtx*BWT_MULTIPLIER );
+
+  rv = T1_Transaction( ucTBuffer, 5, ucRBuffer, &dwRBufferLen );
+
+  return rv;
+}
+
+extern DWORD
+libusim_t1_exchange_data( PUCHAR pucTBuffer,
+                          DWORD dwTLength,
+                          PUCHAR pucRBuffer,
+                          PDWORD dwRLength )
+{
+  DWORD rv = 0, tv;
+  UCHAR ucTBuffer[MAX_RESPONSE_SIZE];
+  UCHAR ucRBuffer[MAX_RESPONSE_SIZE];
+  UCHAR ucRTemp[MAX_RESPONSE_SIZE];
+  DWORD dwCycles, dwRemaining, dwOffset;
+  DWORD dwRBufferLen, dwRCount=0;
+  int i;
+  static UCHAR ucSChainNum  = 0;
+  static UCHAR ucRChainNum  = 1;
+
+
+    /* Problem: Command may have a return that exceeds the
+       IFSD Length.
+
+       Solution: Wait for ICC's I-block transmission with
+       the MORE bit set and send appropriate R-blocks.
+    */
+
+  if ( dwTLength + 4 < MAX_IFSD_SIZE ) {
+
+  retryblocka:
+
+    ucTBuffer[0] = 0x00;          /* NAD - Addressing not used */
+    ucTBuffer[1] = 0x00;          /* PCB - No chaining yet     */
+    ucTBuffer[2] = dwTLength;     /* LEN - Size of the APDU    */
+    ucTBuffer[1] |= (ucSChainNum%2)?0x40:0x00;  /* N(R)          */
+    memcpy( &ucTBuffer[3], pucTBuffer, dwTLength );
+    ucTBuffer[3 + dwTLength] = T1CalculateLRC( ucTBuffer, dwTLength + 3 );
+    dwRBufferLen             = sizeof(ucRBuffer);
+
+    rv = T1_Transaction( ucTBuffer, dwTLength+4, ucRBuffer, &dwRBufferLen );
+
+    tv = T1_GetResponseType( ucRBuffer, dwRBufferLen );
+
+    switch( tv ) {
+      UCHAR ucWtx;
+
+      case T1_SBLOCK_WTXREQUEST:
+        ucWtx = ucRBuffer[3];
+        T1_WTXResponse( ucWtx, ucRBuffer );
+        break;
+      case T1_ERROR_OTHER:
+        ucSChainNum += 1;
+        goto retryblocka;
+        break;
+      default:
+        break;
+    }
+
+    /* Copy the response from the DATA field */
+     if ( ucRBuffer[2] > 0 && rv == 0 ) {
+      memcpy( ucRTemp, &ucRBuffer[3], ucRBuffer[2] );
+      dwRCount = ucRBuffer[2];
+    }
+
+     ucSChainNum += 1;
+
+    /* Start of loop which checks the More Data bit after every transfer
+       to determine whether or not another R-block must be sent
+    */
+
+    if ( ucRBuffer[1] & 0x20 ) {     /* PCB More Data Bit */
+
+      do {
+
+	/* Create the R-block */
+	ucTBuffer[0] = 0x00;                            /* DF Nad   */
+	ucTBuffer[1] = (ucRChainNum%2)?0x90:0x80;        /* N(R)     */
+	ucTBuffer[2] = 0x00;                            /* Len  = 0 */
+	ucTBuffer[3] = T1CalculateLRC( ucTBuffer, 3 );  /* Lrc      */
+	dwRBufferLen = sizeof(ucRBuffer);
+
+	rv = T1_Transaction( ucTBuffer, 4, ucRBuffer, &dwRBufferLen );
+
+	if (rv != 0) {
+	  return 1;
+	}
+
+	memcpy( &ucRTemp[dwRCount], &ucRBuffer[3], ucRBuffer[2] );
+	dwRCount   += ucRBuffer[2];
+	ucRChainNum += 1;
+
+      } while ( ucRBuffer[1] & 0x20 );
+
+    }
+
+    pr_debug("Full T=1 Response Data APDU: ");
+    for (i=0; i < dwRCount; i++) {
+      pr_debug("%x ", ucRTemp[i]);
+    } pr_debug("\n");
+
+    /* Problem: Command is larger than the IFSD Length
+       This is a large outgoing command which
+       will have a return of 2 status bytes.
+
+       Solution: Use I-block chaining and wait for ICC
+       R-block responses
+    */
+
+  } else {
+
+    dwCycles     = dwTLength / MAX_IFSD_SIZE;
+    dwRemaining  = dwTLength % MAX_IFSD_SIZE;
+    dwRBufferLen = sizeof(ucRBuffer);
+
+    for ( i=0; i < dwCycles; i++ ) {
+
+    retryblockb:
+
+      ucTBuffer[0]  = 0x00;          /* NAD - Addressing not used */
+      ucTBuffer[1]  = 0x20;          /* PCB - Chaining used       */
+      ucTBuffer[2]  = MAX_IFSD_SIZE; /* LEN - Size of the APDU    */
+      ucTBuffer[1] |= (ucSChainNum%2)?0x40:0x00;  /* N(R)          */
+      dwOffset      = MAX_IFSD_SIZE*i;            /* Buffer Offset */
+      memcpy( &ucTBuffer[3], &pucTBuffer[dwOffset], MAX_IFSD_SIZE );
+      ucTBuffer[3 + MAX_IFSD_SIZE] = T1CalculateLRC( ucTBuffer,
+						     MAX_IFSD_SIZE + 3 );
+
+      T1_Transaction( ucTBuffer, 4+MAX_IFSD_SIZE, ucRBuffer, &dwRBufferLen );
+
+      tv = T1_GetResponseType( ucRBuffer, dwRBufferLen );
+
+      switch( tv ) {
+        UCHAR ucWtx;
+
+        case T1_SBLOCK_WTXREQUEST:
+          ucWtx = ucRBuffer[3];
+          T1_WTXResponse( ucWtx, ucRBuffer );
+          break;
+        case T1_ERROR_OTHER:
+          ucSChainNum += 1;
+          goto retryblockb;
+          break;
+        default:
+          break;
+      }
+
+      ucSChainNum += 1;
+    }
+
+
+    ucTBuffer[0]  = 0x00;          /* NAD - Addressing not used */
+    ucTBuffer[1]  = 0x00;          /* PCB - Chaining done       */
+    ucTBuffer[2]  = dwRemaining;   /* LEN - Size of the APDU    */
+    ucTBuffer[1] |= (ucSChainNum%2)?0x40:0x00;  /* N(R)          */
+    dwOffset      = MAX_IFSD_SIZE*i;           /* Buffer Offset */
+    memcpy( &ucTBuffer[3], &pucTBuffer[dwOffset], dwRemaining );
+    ucTBuffer[3 + MAX_IFSD_SIZE] = T1CalculateLRC( ucTBuffer,
+						   dwRemaining + 3 );
+
+    T1_Transaction( ucTBuffer, dwRemaining+4, ucRBuffer, &dwRBufferLen );
+
+    if ( ucRBuffer[2] > 0 && rv == 0 ) {
+      memcpy( ucRTemp, &ucRBuffer[3], ucRBuffer[2] );
+      dwRCount = ucRBuffer[2];
+    }
+  }
+
+  *dwRLength = dwRCount;
+  memcpy( pucRBuffer, ucRTemp, dwRCount );
+
+  return rv;
+}
diff --git a/drivers/net/Kconfig b/drivers/net/Kconfig
index 0ee5518..6f78b5a 100644
--- a/drivers/net/Kconfig
+++ b/drivers/net/Kconfig
@@ -222,6 +222,8 @@ menuconfig NET_ETHERNET
 
 if NET_ETHERNET
 
+source "drivers/net/transcede/Kconfig"
+
 config MACB
 	tristate "Atmel MACB support"
 	depends on HAVE_NET_MACB
diff --git a/drivers/net/Makefile b/drivers/net/Makefile
index d5ce011..3a4a64c 100644
--- a/drivers/net/Makefile
+++ b/drivers/net/Makefile
@@ -307,3 +307,6 @@ obj-$(CONFIG_CAIF) += caif/
 obj-$(CONFIG_OCTEON_MGMT_ETHERNET) += octeon/
 obj-$(CONFIG_PCH_GBE) += pch_gbe/
 obj-$(CONFIG_TILE_NET) += tile/
+obj-$(CONFIG_MACH_M84XXX) += transcede/
+obj-$(CONFIG_MACH_M822XX) += transcede/
+
diff --git a/drivers/net/transcede/Kconfig b/drivers/net/transcede/Kconfig
new file mode 100644
index 0000000..214a2ae
--- /dev/null
+++ b/drivers/net/transcede/Kconfig
@@ -0,0 +1,33 @@
+config TRANSCEDE_ETH
+	tristate "Transcede 4000 Ethernet Driver"
+	depends on MACH_M84XXX
+	default y
+
+config TRANSCEDE_2200_ETH
+	tristate "Transcede 2200 Ethernet Driver"
+	depends on MACH_M822XX
+	default y
+
+config TRANSCEDE_ETH_CPRI
+	tristate "Transcede Virtual CPRI Driver"
+	depends on MACH_M84XXX
+	default y
+
+config TRANSCEDE_VED_3300
+        tristate "Transcede Virtual Ethernet Driver for T3300"
+        depends on MACH_M822XX
+        default y
+
+config TRANSCEDE_ETH_QOSCOM
+	tristate "Transcede QoS hardware configuration driver"
+	depends on MACH_M84XXX
+	default n
+	help
+		This option enables the QoS block configuration driver
+
+config TRANSCEDE_4000_ETH_ADM_BLOCK
+	bool "Enable Transcede 4000 Admittance block"
+	depends on TRANSCEDE_ETH_QOSCOM
+	default n
+	help
+		This configuration enables the Transcede 4000 Admittance block.
diff --git a/drivers/net/transcede/Makefile b/drivers/net/transcede/Makefile
new file mode 100644
index 0000000..607b2ee
--- /dev/null
+++ b/drivers/net/transcede/Makefile
@@ -0,0 +1,7 @@
+obj-$(CONFIG_TRANSCEDE_ETH) += c4000_eth.o c4000_gemac.o transcede_gem_AL.o \
+                        transcede_mii.o transcede_ethtool.o
+obj-$(CONFIG_TRANSCEDE_2200_ETH) += t2200_eth.o transcede_gem_AL.o \
+                        transcede_mii.o transcede_ethtool.o t3300_reth.o
+obj-$(CONFIG_TRANSCEDE_ETH_CPRI) += c4000_cpri.o user_ipi.o user_ipi.o
+obj-$(CONFIG_TRANSCEDE_VED_3300) +=  t3300_ved.o t3300_smi.o
+obj-$(CONFIG_TRANSCEDE_ETH_QOSCOM) += qoscom.o
diff --git a/drivers/net/transcede/api.h b/drivers/net/transcede/api.h
new file mode 100644
index 0000000..ca49bd5
--- /dev/null
+++ b/drivers/net/transcede/api.h
@@ -0,0 +1,50 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef _API_H_
+#define _API_H_
+
+#define LOWER_IF 0x8000
+#define UPPER_IF 1
+#define HOST_IF  2
+#define CPRI_IF  31
+
+
+struct api_hdr_t
+{
+       unsigned int control;
+       unsigned int length;
+       unsigned short type;
+       unsigned short srcID;
+       unsigned short dstID;
+       unsigned short msgID;
+       unsigned char  payload[1];
+};
+
+struct boot_param_t
+{
+       unsigned short paramID;
+       unsigned short length;
+       char  data[1];
+};
+
+#endif /* _API_H_ */
\ No newline at end of file
diff --git a/drivers/net/transcede/c4000_cpri.c b/drivers/net/transcede/c4000_cpri.c
new file mode 100644
index 0000000..f47d192
--- /dev/null
+++ b/drivers/net/transcede/c4000_cpri.c
@@ -0,0 +1,331 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <net/ip.h>
+#include <net/sock.h>
+#include "c4000_cpri.h"
+#include "user_ipi.h"
+#include "api.h"
+
+char ctrl_mac[6] = { 0x00, 0x11, 0x22, 0x33, 0x44, 0x77 };
+char radio_mac[6] = { 0x00, 0x11, 0x22, 0x33, 0x44, 0x66 };
+
+#define PKT_BUF_SZ (1540)	/* Size of each rx buffer */
+#define IPI_IRQ (32+7)
+
+int cpri_state = 0;
+
+static int cpri_xmit(struct sk_buff *skb, struct net_device *dev);
+static struct net_device_stats *cpri_get_stats(struct net_device *dev);
+
+int device_open(struct net_device *dev);
+int device_release(struct net_device *dev);
+static int cpri_write_packet(struct sk_buff *skb, struct net_device *dev);
+
+static int cpri_poll(struct napi_struct *napi, int budget);
+static int stop_cpri(void);
+static struct net_device_stats *cpri_get_stats(struct net_device *dev);
+
+struct net_device *ctrl_dev;
+
+#define DEBUG
+
+/***************************************************/
+static int cpri_init(void)
+{
+        struct net_device *dev;
+        struct cpri_priv *priv;
+
+        dev = alloc_etherdev(sizeof(struct cpri_priv));
+
+        /* Initialize the device structure. */
+        priv = netdev_priv(dev);
+        if (!priv)
+                return -ENOMEM;
+
+        memset(priv, 0, sizeof(struct cpri_priv));
+
+        spin_lock_init(&priv->lock);
+        // init private section
+        priv->state = 0; /* closed */
+        priv->dev = dev;
+
+        /* Fill in device structure with ethernet-generic values. */
+        dev->priv = priv;
+        dev->open = device_open;
+        dev->stop = device_release;
+	dev->get_stats = cpri_get_stats;
+        dev->hard_start_xmit = cpri_xmit;
+
+        netif_napi_add(dev, &priv->napi, cpri_poll, 64);
+
+//	dev->flags &= ~IFF_MULTICAST;
+        dev->flags |= IFF_NOARP;
+        memcpy(dev->dev_addr, ctrl_mac, 6);
+
+        ctrl_dev = dev;
+        return 0;
+}
+
+/***************************************************/
+static int cpri_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+        struct cpri_priv *priv = (struct cpri_priv *)dev->priv;
+        unsigned long flags;
+        int rc;
+
+        DEBUG("cpri_xmit");
+        spin_lock_irqsave(&priv->lock, flags);
+
+        rc = cpri_write_packet(skb, dev);
+        if (rc) {
+                priv->stats.tx_dropped++;
+        } else {
+                priv->stats.tx_packets++;
+                priv->stats.tx_bytes += skb->len;
+                dev->trans_start = jiffies;	/* save the timestamp */
+        }
+        spin_unlock_irqrestore(&priv->lock, flags);
+        return 0;
+}
+
+
+
+static irqreturn_t lowarm_interrupt(int irq, void *context)
+{
+        struct net_device *dev = context;
+        struct cpri_priv *priv = dev->priv;
+
+        if (netif_rx_schedule_prep(dev, &priv->napi)) {
+                __netif_rx_schedule(dev, &priv->napi);
+        }
+        return IRQ_HANDLED;
+}
+
+/***************************************************/
+int device_open(struct net_device *dev)
+{
+        struct cpri_priv *priv = dev->priv;
+        int rc = 0;
+        if (priv->state == 1)
+                return rc;
+
+        napi_enable(&priv->napi);
+        netif_wake_queue(dev);
+        priv->state = 1; /* open */
+
+        if ((rc = request_irq(IPI_IRQ, lowarm_interrupt,
+                              IRQF_DISABLED | IRQF_SHARED, "c4000_cpri", dev))) {
+                printk(KERN_ALERT "%s: error %d requesting irq %d\n",
+                       __FUNCTION__, rc, IPI_IRQ);
+                return -ENODEV;
+        }
+
+        return rc;
+}
+
+/***************************************************/
+int device_release(struct net_device *dev)
+{
+        struct cpri_priv *priv = dev->priv;
+
+        /* do nothing if interface is already down */
+        if (priv->state == 1) {
+                napi_disable(&priv->napi);
+                netif_stop_queue(dev);	/* can't transmit any more */
+
+                stop_cpri();
+                priv->state = 0;
+                free_irq(IPI_IRQ, dev);
+        }
+        return 0;
+}
+
+/***************************************************/
+static struct net_device_stats *cpri_get_stats(struct net_device *dev) {
+        return &(((struct cpri_priv *)dev->priv)->stats);
+}
+
+/***************************************************/
+int __init cpri_init_module(void)
+{
+        int rc;
+
+        user_ipi_init();
+        cpri_init();
+
+        /* Find a name for this unit */
+        rc = dev_alloc_name(&ctrl_dev[0], CTRL);
+        if (rc < 0) {
+                printk(KERN_ERR "cpri_init_module: failed to alloc device name %s\n", CTRL);
+                return rc;
+        }
+
+        rc = register_netdev(&ctrl_dev[0]);
+        if (rc) {
+                printk(KERN_ERR "cpri_init_module: failed to register dev %s\n", CTRL);
+                return rc;
+
+        }
+
+
+        return 0;
+}
+
+/***************************************************/
+static void __exit cpri_cleanup_module(void)
+{
+        unregister_netdev(&ctrl_dev[0]);
+        kfree(ctrl_dev[0].priv);
+        user_ipi_destroy();
+}
+
+
+/***************************************************/
+static int cpri_read_packet(struct net_device *dev, void *data_addr, int length)
+{
+        struct cpri_priv *priv = (struct cpri_priv *)dev->priv;
+        struct sk_buff *skb;
+//        struct api_hdr_t *hdr = data_addr;
+//        struct boot_param_t *param = (struct boot_param_t*)hdr->payload;
+
+        DEBUG("cpri_read_packet");
+        skb = dev_alloc_skb(PKT_BUF_SZ);
+
+        if (skb) {
+                skb_reserve(skb, NET_IP_ALIGN);
+                memcpy(skb->data, data_addr, length);
+                skb->dev = dev;
+                skb_put(skb, length);
+                skb->protocol = eth_type_trans(skb, dev);
+
+                if (unlikely(skb->tail > skb->end))
+                        skb_over_panic(skb, length, current_text_addr());
+
+                if ((netif_receive_skb(skb))) {
+                        priv->stats.rx_dropped++;
+                }
+
+                priv->stats.rx_packets++;
+                priv->stats.rx_bytes += length;
+                dev->last_rx = jiffies;
+
+        } else {
+                priv->stats.rx_dropped++;
+                return 0;
+        }
+        return 1;
+}
+
+/***************************************************/
+static int cpri_poll(struct napi_struct *napi, int budget)
+{
+        struct cpri_priv *priv = container_of(napi, struct cpri_priv, napi);
+        struct net_device *dev = priv->dev;
+        struct api_hdr_t *msg = NULL;
+        int done, length;
+
+        msg = read_message_id(CPRI_IF);
+	if ( msg ) {
+		struct boot_param_t *param = (struct boot_param_t*)msg->payload;
+
+		length = sizeof(struct api_hdr_t) + msg->length;
+		done = cpri_read_packet(dev, param->data, length);
+		free_rx_message( msg );
+	}
+        netif_rx_complete(dev, napi);
+        return done;
+}
+
+static void memcpy8(char *dst, char *src, int len)
+{
+    while (len--)
+	*dst++ = *src++;
+}
+
+/***************************************************/
+static int cpri_write_packet(struct sk_buff *skb, struct net_device *dev)
+{
+        struct api_hdr_t *hdr;
+        struct boot_param_t *param;
+        struct cpri_priv *priv = (struct cpri_priv *)dev->priv;
+
+	printk("cpri_write_packet\n");
+
+        hdr = alloc_tx_message();
+//        printk("alloc_tx_message %x\n", hdr);
+	if ( hdr ) {
+	    param = (struct boot_param_t*)hdr->payload;
+	    hdr->control = 0x10000000;
+	    hdr->type = 2;
+	    hdr->srcID = UPPER_IF;
+	    hdr->dstID = CPRI_IF;
+	    hdr->msgID = 1;
+
+	    param->paramID = 3;
+	    param->length = skb->len;
+	    hdr->length = param->length + 4;
+
+	    memcpy8(param->data, skb->data, skb->len);
+            memcpy8(param->data, radio_mac, 6);
+	    //param->data[0] = 0x00;
+	    //param->data[1] = 0x11;
+	    //param->data[2] = 0x22;
+	    //param->data[3] = 0x33;
+	    //param->data[4] = 0x44;
+	    //param->data[5] = 0x66;
+
+	    param->data[20] = 0;	// flags / frag
+	    param->data[21] = 0;
+	    param->data[24] = 0;	// ip checksum
+	    param->data[25] = 0;
+	    param->data[40] = 0;	// udp checksum
+	    param->data[41] = 0;
+	    param->data[18] = 0xab;
+	    param->data[19] = 0xcd;
+
+	    send_message(hdr);
+	}
+
+        dev_kfree_skb(skb);
+
+        return 0;
+}
+
+/***************************************************/
+static int stop_cpri(void)
+{
+        struct net_device *dev;
+        struct cpri_priv *priv;
+
+        dev = &ctrl_dev[0];
+        priv = (struct cpri_priv *)dev->priv;
+
+        return 0;
+}
+
+module_init(cpri_init_module);
+module_exit(cpri_cleanup_module);
diff --git a/drivers/net/transcede/c4000_cpri.h b/drivers/net/transcede/c4000_cpri.h
new file mode 100644
index 0000000..15ccd77
--- /dev/null
+++ b/drivers/net/transcede/c4000_cpri.h
@@ -0,0 +1,48 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef _TRANSCEDE_VED_H
+#define _TRANSCEDE_VED_H
+
+#include <linux/netdevice.h>	/* struct device, and other headers */
+
+
+#define VED_SKB_POOL_SIZE	512
+#define VED_TIMEOUT 5		/* In jiffies */
+#define CTRL	"cpri"
+
+
+/*
+ * This structure is private to each device. It is used to pass
+ * packets in and out, so there is place for a packet
+ */
+
+struct cpri_priv {
+	struct net_device_stats stats;
+	spinlock_t lock;
+	struct	napi_struct napi;
+	int state;
+	struct net_device *dev;
+};
+
+
+#endif /* _TRANSCEDE_VED_H */
diff --git a/drivers/net/transcede/c4000_eth.c b/drivers/net/transcede/c4000_eth.c
new file mode 100644
index 0000000..9ca7ad9
--- /dev/null
+++ b/drivers/net/transcede/c4000_eth.c
@@ -0,0 +1,2433 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+  *
+  * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+  *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+  *
+  * You should have received a copy of the GNU General Public License
+  * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+  */
+
+#include <linux/kernel.h>
+#include <linux/interrupt.h>
+#include <linux/dma-mapping.h>
+#include <linux/dmapool.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/mii.h>
+#include <linux/phy.h>
+#include <linux/platform_device.h>
+
+#include <net/ip.h>
+#include <net/sock.h>
+
+#include <asm/io.h>
+#include <asm/irq.h>
+#include <asm/delay.h>
+#include <asm/mach/irq.h>
+
+#include <mach/hardware.h>
+#include <mach/memory.h>
+
+#if defined(CONFIG_INET_IPSEC_OFFLOAD) || defined(CONFIG_INET6_IPSEC_OFFLOAD)
+#include <net/xfrm.h>
+#endif
+
+#include "c4000_eth.h"
+#include "transcede_mii.h"
+
+extern struct transcede_eth_platform_data transcede_gem0_pdata;
+extern struct transcede_eth_platform_data transcede_gem1_pdata;
+extern struct ethtool_ops              c4k_ethtool_ops;
+
+/* Local defines */
+
+#define PKT_HEADROOM    2       /**< Fixed headroom for receive to align data past MAC header to 32 bits */
+#define PKT_BUF_SZ      1536    /**< Fixed buffer size to allow for normal max size ethernet packet plus some extra */
+#define ETH_ALIGN       128     /**< Data alignment to force buffers on a 128 byte boundary */
+#define GEM_HW_CHECKSUM         /**< Flag to always enable Hardware L3 & L4 checksum option of GEM device, regardless of device options */
+#define GEM_SCH_ENABLED         /**< Flag to use GEM scheduler block instead of legacy transmit queuing/processing */
+
+#ifdef CONFIG_TRANSCEDE_4000_ETH_ADM_BLOCK
+#define GEM_ADM_ENABLED         /**< Flag to use GEM admittance block instead of legacy receive processing */
+#endif
+
+
+/* Optimization options */
+
+#if 1
+#define GEM_RX_CHECKSUM_OFFLOAD_ENABLED
+#endif
+
+#if 1
+#define GEM_TX_CHECKSUM_OFFLOAD_ENABLED
+#endif
+
+#if 0
+#define GEM_FORCE_RX_PROMISCUOUS_MODE
+#endif
+
+
+/* Local Function prototypes and external function references */
+
+/* Local function prototypes */
+static void c4k_eth_release_buffers(struct net_device *dev);
+static int  c4k_eth_init_buffers   (struct net_device *dev);
+irqreturn_t c4k_eth_rx_interrupt   (int irq, void *dev_id);
+static void c4k_adjust_link        (struct net_device *dev);
+int         c4k_set_tx_csum        (struct net_device *dev, uint32_t data);
+static u32  c4k_ring_to_phys       (struct net_device *dev, volatile u32* va);
+int c4k_eth_free_tx_packets(struct net_device *dev);
+
+/* External function prototypes */
+extern int  c4k_emac_checksum      (struct eth_c4k_priv *priv, u32 status, u8 *ip_summed);
+extern void c4k_gemac_setduplex    (struct net_device *dev, int duplex);
+extern void c4k_gemac_setspeed     (struct net_device *dev, int speed);
+extern void c4k_init_sysfs         (struct net_device *dev);
+extern int  c4k_gemac_init         (struct net_device *dev, uint irq_mask_ena, uint irq_mask_dis);
+
+struct net_device * t4000_dev_ptr[2] = {NULL,NULL};
+static struct hrtimer t4000_hrtimer[2];
+static uint t4000_eth_rx_latency = 0;
+static uint GEM_IRQ_RX_DONE_FLAG = GEM_IRQ_RX_DONE;
+static uint RX_INT_FLAG = RX_INT;
+
+#ifdef CONFIG_GLOBAL_POLLING
+static void t4000_hrtimer_callback(unsigned long id)
+#else  /* !CONFIG_GLOBAL_POLLING */
+static enum hrtimer_restart t4000_hrtimer_callback( struct hrtimer *timer )
+#endif /* CONFIG_GLOBAL_POLLING */
+{
+    struct eth_c4k_priv * priv;
+    struct tRXdesc * ThisRXdesc; /**< Current Receive descriptor pointer */
+    struct napi_struct *  napi;
+
+#ifndef CONFIG_GLOBAL_POLLING
+    uint id = 0;
+    if (timer == &t4000_hrtimer[1])
+    {
+	id = 1;
+    }
+#endif /* !CONFIG_GLOBAL_POLLING */
+    
+    priv = netdev_priv(t4000_dev_ptr[id]);
+    napi = &priv->napi;
+    
+    // here we need to check do we need to schedule pooling or to 
+    // just restart the time to check RX packets later 
+    // (if at this moment we do not have any packets) 
+    
+    ThisRXdesc = priv->RxBase + priv->RxtocleanIndex;
+    
+    // if no any packets
+    if ((ThisRXdesc->rx_extstatus & GEMRX_OWN)==0)
+    {
+#ifdef CONFIG_GLOBAL_POLLING
+	    return;
+#else  /* !CONFIG_GLOBAL_POLLING */
+	ktime_t ktime;
+	ktime = ktime_set( 0, t4000_eth_rx_latency * 1000);
+	hrtimer_forward(timer, ktime_get(), ktime);
+	return HRTIMER_RESTART;
+#endif /* CONFIG_GLOBAL_POLLING */
+    }
+    
+    // at this moment we detected that we have something to receive
+    // so we need to schedule the polling process
+    // the timer will be rescheduled later after polling
+    
+    if (likely(napi_schedule_prep(napi))) {
+        __napi_schedule(napi);
+    }
+    
+#ifndef CONFIG_GLOBAL_POLLING
+    return HRTIMER_NORESTART;
+#endif /* !CONFIG_GLOBAL_POLLING */
+}
+
+int t4000_init_timer(struct net_device * pdev)
+{
+    struct eth_c4k_priv * priv = netdev_priv(pdev);
+
+    if (t4000_eth_rx_latency == 0)
+	return 0;
+
+    hrtimer_init( &t4000_hrtimer[priv->id], CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+    t4000_hrtimer[priv->id].function = &t4000_hrtimer_callback;
+    
+    return 0;
+}
+
+int t4000_start_timer(struct net_device * pdev)
+{
+    struct eth_c4k_priv * priv = netdev_priv(pdev);
+#ifndef CONFIG_GLOBAL_POLLING
+    ktime_t ktime;
+#endif /* !CONFIG_GLOBAL_POLLING */
+
+    if (t4000_eth_rx_latency == 0)
+	return 0;
+
+#ifdef CONFIG_GLOBAL_POLLING
+    priv->periodic_task = periodic_task_add(0, t4000_eth_rx_latency, t4000_hrtimer_callback, priv->id, "ethRx");
+#else  /* !CONFIG_GLOBAL_POLLING */
+    ktime = ktime_set( 0, t4000_eth_rx_latency * 1000);
+    hrtimer_start( &t4000_hrtimer[priv->id], ktime, HRTIMER_MODE_REL);
+#endif /* CONFIG_GLOBAL_POLLING */
+
+    return 0;
+}
+
+int t4000_stop_timer(struct net_device * pdev)
+{
+    struct eth_c4k_priv * priv = netdev_priv(pdev);
+    int err = 0;
+
+    if (t4000_eth_rx_latency == 0)
+	return 0;
+
+#ifdef CONFIG_GLOBAL_POLLING
+    if (priv->periodic_task) {
+	    err = periodic_task_remove(priv->periodic_task);
+	    priv->periodic_task = NULL;
+    }
+#else  /* !CONFIG_GLOBAL_POLLING */
+    err = hrtimer_cancel( &t4000_hrtimer[priv->id]);
+#endif /* CONFIG_GLOBAL_POLLING */
+
+    return err;
+}
+
+//#ifdef GEM_ETHDRV_TEMP_DEBUG
+#if 1
+#define MAX_SKB_DATA_DUMP 32
+static void print_skb_info(struct sk_buff * skb)
+{
+	unsigned char * pTemp;
+	int             i;
+
+	if (skb == NULL)
+		return;
+
+	printk(KERN_INFO "sk_buff:%p head:%p data:%p tail:%p end:%p len:%u truesize:%u\n",
+		(void *)virt_to_phys(skb),
+		(void *)virt_to_phys(skb->head),
+		(void *)virt_to_phys(skb->data),
+		(void *)virt_to_phys(skb->tail),
+		(void *)virt_to_phys(skb->end),
+		skb->len,
+		skb->truesize
+		);
+	if ((skb->data == NULL) || (skb->len == 0))
+		return;
+
+	pTemp = (unsigned char *)skb->data;
+
+	for (i=0; ((i< skb->len) && (i < MAX_SKB_DATA_DUMP)) ; i++,pTemp++)
+	{
+		printk("%02x ",*pTemp);
+		if ((i & 31) == 31)
+		{
+			printk("\n");
+		}
+	}
+	if ((i & 31) != 0)
+	{
+		printk("\n");
+	}
+}
+#endif
+
+//External Function removed (not currently used):
+//extern irqreturn_t c4k_gemac_interrupt(int irq, void *dev_id);
+
+/**
+ * c4k_eth_ioctl - c4k ioctl interface (only PHY functions supported)
+ */
+static int c4k_eth_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+
+	if (!priv->phydev)
+		return -ENODEV;
+
+	if ((cmd == SIOCGMIIPHY) || (cmd == SIOCGMIIREG) || (cmd == SIOCSMIIREG))
+		return phy_mii_ioctl(priv->phydev, ifr, cmd);
+
+	return -EOPNOTSUPP;
+}
+
+/** 
+ * Function to return the current value of interface
+ * mode the GEM device is running based on current
+ * MII mode read from priv->einfo->mii_config
+ *
+ * @return
+ * Returns 0 if GEM_SW_CONF bit false in priv->einfo->gemac_mode,
+ * otherwise returns one of the following:
+ *   - PHY_INTERFACE_MODE_GMII,
+ *   - PHY_INTERFACE_MODE_RGMII,
+ *   - PHY_INTERFACE_MODE_RMII or
+ *   - PHY_INTERFACE_MODE_MII
+ */
+static phy_interface_t c4k_get_interface(struct net_device *dev)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+	u32 gemcore_config = readl(priv->baseaddr + GEM_CFG);
+	phy_interface_t mode = PHY_INTERFACE_MODE_RGMII;
+
+	if (gemcore_config & 1) {
+		/* mode set by software */
+		if (gemcore_config & 0x0E)
+			return PHY_INTERFACE_MODE_SGMII;
+	} else {
+		/* mode set by pins */
+		if (gemcore_config & 0x40)
+			return PHY_INTERFACE_MODE_SGMII;
+	}
+
+	return mode;
+}
+
+/**
+ * Function to initialize driver's PHY state, and attach to the PHY.
+ *
+ * @return
+ * Returns 0 on success, returns PTR_ERR(phydev) if
+ * unable to connect to the PHY device.
+ */
+static int c4k_phy_start(struct net_device *dev)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+	uint gigabit_support =
+		priv->einfo->phy_flags & GEMAC_PHY_1000 ?
+		(SUPPORTED_1000baseT_Full | SUPPORTED_1000baseT_Half): 0;
+	struct phy_device *phydev;
+	char phy_id[MII_BUS_ID_SIZE];
+	phy_interface_t interface;
+
+	priv->oldlink   = 0;
+	priv->oldspeed  = 0;
+	priv->oldduplex = -1;
+
+	snprintf(phy_id, sizeof(phy_id), PHY_ID_FMT, priv->einfo->bus_id, priv->einfo->phy_id);
+
+	printk(KERN_INFO "PHY start: %s\n", phy_id);
+
+	interface = c4k_get_interface(dev);
+
+	priv->oldlink   = 0;
+	priv->oldspeed  = 0;
+	priv->oldduplex = -1;
+
+	phydev = phy_connect(dev, phy_id, &c4k_adjust_link, 0, interface);
+
+	if (IS_ERR(phydev))
+	{
+		printk(KERN_ERR "%s: Could not attach to PHY\n", dev->name);
+		return PTR_ERR(phydev);
+	}
+
+	/* Remove any features not supported by the controller */
+	phydev->supported |= (TRANSCEDE_SUPPORTED | gigabit_support);
+	phydev->advertising = phydev->supported;
+
+	priv->phydev = phydev;
+
+	return 0;
+}
+
+/** Function to stop the PHY device for this Ethernet device */
+static void c4k_phy_stop(struct net_device *dev)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+
+	phy_disconnect(priv->phydev);
+	priv->phydev = NULL;
+}
+
+/**
+ * Function to start the queues of the ethernet
+ */
+void c4k_eth_start_queues(struct net_device *dev, int queue)
+{
+	if(queue == 1)
+		netif_wake_queue(dev);
+	else
+		netif_tx_start_all_queues(dev);
+}
+
+/**
+ * Function to stop the queues of the ethernet
+ */
+void c4k_eth_stop_queues(struct net_device *dev, int queue)
+{
+	if(queue == 1)
+		netif_stop_queue(dev);
+	else
+		netif_tx_stop_all_queues(dev);
+}
+
+//HL. 08/31. check carefully the following sequence.
+/**
+ * Function to bring the controller up and running
+ *
+ * @see c4k_eth_init_buffers
+ * @see phy_start
+ * @see gem_enable_rx
+ * @see gem_enable_tx
+ * @see c4k_ring_to_phys
+ */
+int c4k_eth_start(struct net_device *dev)
+{
+	struct eth_c4k_priv * priv = netdev_priv(dev);
+	u32                   u32temp;
+
+	dev_dbg(&dev->dev, "%s\n", __func__);
+
+	if(priv->tx_queue_num == 1)
+		printk (KERN_INFO "Single queue mode\n");
+	else
+		printk (KERN_INFO "Multi queue (%d) mode\n", priv->tx_queue_num);
+
+	if (c4k_eth_init_buffers(dev)) {
+		dev_err(&dev->dev, "could not allocate buffer descriptors!\n");
+		return -ENOMEM;
+	}
+
+	if (priv->phydev)
+		phy_start(priv->phydev);
+
+#ifdef GEM_SCH_ENABLED
+	/* Enable scheduler block for tx */
+
+	dev_info(&dev->dev, "Enabling GEM Scheduler Block\n");
+
+	/* Enable Scheduler in the DMA configuration */
+	u32temp  = readl(priv->baseaddr + GEM_IP + GEM_DMA_CONFIG);  /* Get current DMA value (scheduler should be disabled) */
+	u32temp |= GEM_DMA_SCHEDULER_ENB;                            /* Set DMA configuration register scheduler enable bit (bit 31) */
+	writel(u32temp,  priv->baseaddr + GEM_IP + GEM_DMA_CONFIG);  /* Write out the value to the register */
+
+	/* Set scheduler control */
+	writel(0x3,      priv->baseaddr + GEM_SCH_BLOCK + SCH_CONTROL);
+
+	/* This code is used to initialize gemac #1, unless we support this in u-boot
+	 * It has to be removed, once u-boot supports 2 gemac interfaces
+	 */
+	*(volatile u32*)(0xfe17e010) = 0x86180F04;
+        *(volatile u32*)(0xfe0de010) = 0x86180F04;
+
+#ifdef GEM_ADM_ENABLED
+	dev_info(&dev->dev, "Enabling GEM Admittance Block\n");
+
+	//clean up queue
+	while (readl(priv->baseaddr + GEM_ADM_BLOCK + ADM_QUEUEDEPTH))
+		writel(0,                priv->baseaddr + GEM_ADM_BLOCK + ADM_PKTDQ);
+
+	/* Start the controller */
+	gem_set_rx_offset(&priv->gemdev, PKT_HEADROOM);
+
+	writel(priv->RxRingSize - 2,     priv->baseaddr + GEM_ADM_BLOCK + ADM_QFULLTHR);
+
+	// setup Rx coalescing
+	writel(priv->rx_coal_time * 125, priv->baseaddr + GEM_ADM_BLOCK + ADM_BATCHINTRTIMERINIT);
+	writel(priv->rx_coal_count,      priv->baseaddr + GEM_ADM_BLOCK + ADM_BATCHINTRPKTTHRES);
+
+	writel(0x84210030,               priv->baseaddr + GEM_ADM_BLOCK + ADM_CNFG);
+	writel(0x000000aa,               priv->baseaddr + GEM_ADM_BLOCK + ADM_DECAYTIMER);
+
+	writel(0,                        priv->baseaddr + GEM_ADM_BLOCK + ADM_BATCHINTRPKTCNT);
+	writel(0x7F,                     priv->baseaddr + GEM_ADM_BLOCK + ADM_CONTROL);
+#endif
+	/* Enable TX and RX */
+
+#ifdef GEM_FORCE_RX_PROMISCUOUS_MODE
+	gem_enable_copy_all(&priv->gemdev);
+#endif
+
+	gem_enable_rx(&priv->gemdev);
+	gem_enable_tx(&priv->gemdev);
+
+	printk("fe0de010 <-> %08X\n", *(volatile u32*)(0xfe0de010));
+#else
+
+	/* Start the controller */
+	// no need for this. HL.
+	//	gem_set_rx_offset(&priv->gemdev, PKT_HEADROOM);
+
+	gem_enable_rx(&priv->gemdev);
+
+#endif /* ifdef GEM_SCH_ENABLED */
+
+	return 0;
+}
+
+/**
+ * Function to stop the ethernet device
+ * @see gem_abort_tx
+ * @see gem_disable_rx
+ * @see c4k_eth_release_buffers
+ */
+void c4k_eth_stop(struct net_device *dev)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+	unsigned long flags;
+
+	dev_dbg(&dev->dev, "%s\n", __func__);
+
+	/* Lock it down */
+	spin_lock_irqsave(&priv->txlock, flags);
+
+	gem_abort_tx(  &priv->gemdev);
+	gem_disable_rx(&priv->gemdev);
+
+	spin_unlock_irqrestore(&priv->txlock, flags);
+
+	if (priv->phydev)
+		phy_stop(priv->phydev);
+
+	c4k_eth_release_buffers(dev);
+}
+
+/** Function to open the ethernet device */
+int c4k_eth_open(struct net_device *dev)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+	int rc;
+	struct transcede_tx_q *this_tx_queue;
+	int q_idx;
+
+	dev_dbg(&dev->dev, "%s\n", __func__);
+
+        t4000_dev_ptr[priv->id] = dev;
+	t4000_init_timer(dev);
+
+	if (t4000_eth_rx_latency != 0) {
+	    printk ("Eth%d dev(idx=%d) is opened by the system in polling mode, resolution is %d "
+#ifdef CONFIG_GLOBAL_POLLING
+	            "(ms)\n",
+#else  /* !CONFIG_GLOBAL_POLLING */
+	            "(us)\n",
+#endif	/* CONFIG_GLOBAL_POLLING */
+		    priv->id+1, priv->id, t4000_eth_rx_latency);
+	            }
+	else {
+	    printk ("Eth%d dev(idx=%d) is opened by the system in IRQ mode\n",
+		    priv->id+1, priv->id);
+	}
+
+	dev->irq = priv->phys_rx_int;
+
+	/* Clear out TX and RX descriptor and socket buffer pointer rings */
+	priv->RxRingSize = MAX_RX_DESC_NT;
+
+	memset(priv->RxSkbRing, 0, MAX_RX_DESC_NT * sizeof(struct sk_buff *));
+	dev_dbg(&dev->dev,"%s %s: Init queue (%d)\n", dev->name, __func__, priv->tx_queue_num);
+
+	for(q_idx = 0; q_idx < priv->tx_queue_num; q_idx++)
+	{
+		/*For each TX queue allocate the ring buffer. set the size and the associated queue base */
+		this_tx_queue = &(priv->tx_queue[q_idx]);
+		switch(q_idx)
+		{
+			case 0:
+				if(priv->tx_queue_num == 1)
+				{
+					this_tx_queue->TxRingSize        = MAX_TX_DESC_NT;
+					this_tx_queue->TxMaxRingSize     = MAX_TX_DESC_NT;
+					this_tx_queue->tx_gem_queue_base = GEM_QUEUE_BASE0;
+				}
+				else
+				{
+					this_tx_queue->TxRingSize        = MAX_TX_DESC_QUEUE_0;
+					this_tx_queue->TxMaxRingSize     = MAX_TX_DESC_QUEUE_0;
+					this_tx_queue->tx_gem_queue_base = GEM_QUEUE_BASE0;
+				}
+				break;
+
+			case 1:
+				this_tx_queue->TxRingSize        = MAX_TX_DESC_QUEUE_1;
+				this_tx_queue->TxMaxRingSize     = MAX_TX_DESC_QUEUE_1;
+				this_tx_queue->tx_gem_queue_base = GEM_QUEUE_BASE1;
+				break;
+			case 2:
+				this_tx_queue->TxRingSize        = MAX_TX_DESC_QUEUE_2;
+				this_tx_queue->TxMaxRingSize     = MAX_TX_DESC_QUEUE_2;
+				this_tx_queue->tx_gem_queue_base = GEM_QUEUE_BASE2;
+				break;
+			case 3:
+				this_tx_queue->TxRingSize        = MAX_TX_DESC_QUEUE_3;
+				this_tx_queue->TxMaxRingSize     = MAX_TX_DESC_QUEUE_3;
+				this_tx_queue->tx_gem_queue_base = GEM_QUEUE_BASE3;
+				break;
+			case 4:
+				this_tx_queue->TxRingSize        = MAX_TX_DESC_QUEUE_4;
+				this_tx_queue->TxMaxRingSize     = MAX_TX_DESC_QUEUE_4;
+				this_tx_queue->tx_gem_queue_base = GEM_QUEUE_BASE4;
+				break;
+			case 5:
+				this_tx_queue->TxRingSize        = MAX_TX_DESC_QUEUE_5;
+				this_tx_queue->TxMaxRingSize     = MAX_TX_DESC_QUEUE_5;
+				this_tx_queue->tx_gem_queue_base = GEM_QUEUE_BASE5;
+				break;
+			case 6:
+				this_tx_queue->TxRingSize        = MAX_TX_DESC_QUEUE_6;
+				this_tx_queue->TxMaxRingSize     = MAX_TX_DESC_QUEUE_6;
+				this_tx_queue->tx_gem_queue_base = GEM_QUEUE_BASE6;
+				break;
+			case 7:
+				this_tx_queue->TxRingSize        = MAX_TX_DESC_QUEUE_7;
+				this_tx_queue->TxMaxRingSize     = MAX_TX_DESC_QUEUE_7;
+				this_tx_queue->tx_gem_queue_base = GEM_QUEUE_BASE7;
+				break;
+		}
+
+		this_tx_queue->TxSkbRing = kmalloc(this_tx_queue->TxMaxRingSize * sizeof(struct sk_buff *), GFP_ATOMIC);
+		if (this_tx_queue->TxSkbRing == NULL)
+		{
+			/*FIXME Free already allocated buffers*/
+			printk(KERN_ERR "%s %s: Can't allocate skb ring buffer\n", dev->name, __func__);
+			rc = -1;
+			goto err0;
+		}
+
+		memset(this_tx_queue->TxSkbRing, 0, this_tx_queue->TxMaxRingSize * sizeof(struct sk_buff *));
+		printk(KERN_DEBUG "%s %s: Init queue (%d) TxRingSize = %d TxMaxRingSize = %d TxSkbRing = %x queue_base %x\n",
+				dev->name, __func__,
+				q_idx,
+				this_tx_queue->TxRingSize,
+				this_tx_queue->TxMaxRingSize,
+				(unsigned int)this_tx_queue->TxSkbRing,
+				(unsigned int)this_tx_queue->tx_gem_queue_base);
+
+		this_tx_queue->Txtosend = 0;
+		this_tx_queue->Txdone = 0;
+		this_tx_queue->Txavail = this_tx_queue->TxRingSize - 1; // always keep one as marker;
+	}
+
+	/* Setup MAC in initialized and disabled mode */
+	/* To enable RX IRQ only if polling time value is zero (which means non-polling mode) */
+	c4k_gemac_init(dev, (t4000_eth_rx_latency == 0) ? (GEM_IRQ_RX_DONE) : 0, 0);
+
+	if (!is_valid_ether_addr(dev->dev_addr)) {
+		dev_err(&dev->dev, "invalid MAC address\n");
+		rc = -EADDRNOTAVAIL;
+		goto err0;
+	}
+
+#ifdef GEM_RX_CHECKSUM_OFFLOAD_ENABLED
+	//
+	// Enable receive checksum offload
+	//
+	gem_enable_rx_checksum_offload(&priv->gemdev);
+#endif
+
+#ifdef GEM_TX_CHECKSUM_OFFLOAD_ENABLED
+	//
+	// Enable receive checksum offload
+	//
+	gem_enable_tx_checksum_offload(&priv->gemdev);
+#endif
+	/* Setup GEM MAC address */
+	gem_add_arc_entry(&priv->gemdev, dev->dev_addr);
+
+	/* Setup PHY as long as the "NO PHY" flag is not set */
+	if (!(priv->einfo->phy_flags & GEMAC_NO_PHY)) {
+		rc = c4k_phy_start(dev);
+		if (rc) {
+			dev_err(&dev->dev, "failed to get phy, error %d\n", rc);
+			goto err0;
+		}
+	}
+
+	/* Start up and enable the Ethernet function */
+	rc = c4k_eth_start(dev);
+
+	if (unlikely(rc)) {
+		dev_err(&dev->dev, "failed to start the device, error %d\n", rc);
+		goto err1;
+	}
+
+	/* Enable napi to inform Linux we are ready to go */
+	napi_enable(&priv->napi);
+
+	/* Wakeup the interface to inform Linux we are ready to send and receive
+	 * Ethernet frames
+	 */
+	c4k_eth_start_queues(dev, priv->tx_queue_num);
+
+	/* Request an interrupt service from Linux for the RX interrupt function */
+	rc = request_irq(dev->irq, c4k_eth_rx_interrupt, IRQF_DISABLED, dev->name, dev);
+	if (unlikely(rc)) {
+		dev_err(&dev->dev, "request IRQ%d failed, error %d\n", dev->irq, rc);
+		goto err2;
+	}
+
+        t4000_start_timer(dev);
+
+	/* If we are here, then the Ethernet device is now open and functioning */
+	return rc;
+
+
+err2:
+	napi_disable(&priv->napi);
+
+	c4k_eth_stop_queues(dev, priv->tx_queue_num);
+
+	c4k_eth_stop(dev);
+
+err1:
+	/* Disconnect from the PHY */
+	if (!(priv->einfo->phy_flags & GEMAC_NO_PHY))
+		c4k_phy_stop(dev);
+
+err0:
+	return rc;
+}
+
+/**
+ * Function to close the ethernet device
+ */
+int c4k_eth_close(struct net_device *dev)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+
+	dev_dbg(&dev->dev, "%s\n", __func__);
+
+        t4000_stop_timer(dev);
+	t4000_dev_ptr[priv->id] = NULL;
+
+	napi_disable(&priv->napi);
+
+	free_irq(dev->irq, dev);
+
+	c4k_eth_stop_queues(dev, priv->tx_queue_num);
+
+	c4k_eth_stop(dev);
+
+	if (!(priv->einfo->phy_flags & GEMAC_NO_PHY))
+		c4k_phy_stop(dev);
+
+	return 0;
+}
+
+/**
+ * Function to setup local MAC address for specified ethernet device
+ */
+static int c4k_set_mac_address(struct net_device *dev, void *addr)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+	struct sockaddr *sa = addr;
+
+	if (!is_valid_ether_addr(sa->sa_data))
+		return -EADDRNOTAVAIL;
+
+	memcpy(dev->dev_addr, sa->sa_data, ETH_ALEN);
+
+	gem_add_arc_entry(&priv->gemdev, dev->dev_addr);
+
+	return 0;
+}
+
+/**
+ * Function to setup promiscuous, broadcast, multicast and loopback
+ * options for Ethernet device
+ */
+static void c4k_eth_set_multi(struct net_device *dev)
+{
+	return;
+#if 0
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+	struct dev_mc_list  *mcptr;
+	GEM_DEVICE *gemdev = &priv->gemdev;
+	u8          *addr_start = NULL; /**< start of address */
+	MAC_ADDR    hash_addr;          /**< hash register structure */
+	u8          temp1,temp2,temp3,temp4,temp5,temp6,temp7,temp8;
+	int         i, result;          /**< index into hash register to set.. */
+
+	/* Enable promiscuous mode (receive all ethernet frames) if requested
+	 * (normally it is not)
+	 */
+
+
+	if (unlikely(dev->flags & IFF_PROMISC))
+	{
+
+		if (netif_msg_drv(priv))
+			printk(KERN_INFO "%s : Entering promiscuous mode.\n", dev->name);
+
+		gem_enable_copy_all(gemdev);
+	}
+	else
+	{
+
+		gem_disable_copy_all(gemdev);
+	}
+
+	/* Enable broadcast frame reception if requested (normally it is) */
+	if (likely(dev->flags & IFF_BROADCAST))
+	{
+
+		gem_allow_broadcast(gemdev);
+	}
+	else
+	{
+
+		if (netif_msg_drv(priv))
+			printk(KERN_INFO "%s: disabling broadcast frame reception.\n", dev->name);
+
+		gem_no_broadcast(gemdev);
+	}
+
+	/* Enable all multicast frames to be received if requested */
+	if (dev->flags & IFF_ALLMULTI)
+	{
+
+
+		/* Set the hash to rx all multicast frames if IFF_ALLMULTI requested */
+		hash_addr.bottom = 0xFFFFFFFF;
+		hash_addr.top    = 0xFFFFFFFF;
+		gem_set_hash(gemdev, &hash_addr);
+		gem_enable_multicast(gemdev);
+
+	} 	
+	/* Setup multicast hash table if multicast address count > 0 */
+	else if (dev->mc_count > 0)
+	{
+
+
+		/* Multicast address count > 0, setup and enable multicast addresses */
+		/* Clear the hash table: */
+		hash_addr.bottom = 0;
+		hash_addr.top    = 0;
+		gem_set_hash(gemdev, &hash_addr);
+
+		/* Get the multicast list pointer */
+		mcptr = dev->mc_list;
+
+		/* Loop through the list to generate hash table entries: */
+		for (i = 0; i < dev->mc_count; i++) {
+			addr_start = (unsigned char *)&mcptr->dmi_addr;
+
+			if (netif_msg_drv(priv))
+				printk(KERN_DEBUG "%s: adding multicast address %X:%X:%X:%X:%X:%X to gem filter\n",
+					dev->name,
+					addr_start[0], addr_start[1], addr_start[2],
+					addr_start[3], addr_start[4], addr_start[5]);
+
+			temp1 =   addr_start[0] & 0x3F;
+			temp2 = ((addr_start[0] & 0xC0) >> 6) | ((addr_start[1] & 0x0F) << 2);
+			temp3 = ((addr_start[1] & 0xF0) >> 4) | ((addr_start[2] & 0x03) << 4);
+			temp4 =  (addr_start[2] & 0xFC) >> 2;
+			temp5 =   addr_start[3] & 0x3F;
+			temp6 = ((addr_start[3] & 0xC0) >> 6) | ((addr_start[4] & 0x0F) << 2);
+			temp7 = ((addr_start[4] & 0xF0) >> 4) | ((addr_start[5] & 0x03) << 4);
+			temp8 = ((addr_start[5] & 0xFC) >> 2);
+
+			result = temp1 ^ temp2 ^ temp3 ^ temp4 ^ temp5 ^ temp6 ^ temp7 ^ temp8;
+
+			if (result >= GEM_HASH_REG_BITS) {
+				break;
+			} else {
+				if (result < 32) {
+					hash_addr.bottom |= (1 << result);
+				} else {
+					hash_addr.top |= (1 << (result - 32));
+				}
+			}
+
+			mcptr = mcptr->next;
+		}
+		/* Setup the hash table and enable multicast: */
+		gem_set_hash(gemdev, &hash_addr);
+		gem_enable_multicast(gemdev);
+	}
+
+	/* Setup Ethernet GEM internal loopback if requested */
+	if (dev->flags & IFF_LOOPBACK)
+	{
+
+		gem_set_loop(gemdev, LB_LOCAL);
+	}
+
+	return;
+#endif
+}
+
+
+/** Function to return the pointer to the ethernet statistics for this device */
+static struct net_device_stats *c4k_eth_get_stats(struct net_device *dev)
+{
+	struct eth_c4k_priv * priv     = netdev_priv(dev);
+	GEM_DEVICE *          gemdev   = &priv->gemdev;
+	struct gem_reg *      regs     = gemdev->registers;
+	struct gem_stats *    reg_stat = &regs->stats;
+	u32                   u;
+
+	// Get selected statistics from GEM statistics registers
+	// NOTE: When GEM statistics are read, this will also
+	// clear them back to zero inside the GEM device
+	// for those statistics (and only those statistics) read
+
+	priv->stats.rx_packets += reg_stat->frames_rx;     /* total packets received	*/
+	priv->stats.tx_packets += reg_stat->frames_tx;     /* total packets transmitted	*/
+	priv->stats.rx_bytes   += reg_stat->octets_rx_bot; /* total bytes received 	*/
+	priv->stats.tx_bytes   += reg_stat->octets_tx_bot; /* total bytes transmitted	*/
+
+	priv->stats.multicast  += reg_stat->multicast_rx;
+
+	u = reg_stat->length_check_errors;
+	priv->stats.rx_length_errors  += u;
+	priv->stats.rx_errors         += u; /* bad packets received		*/
+
+	u = reg_stat->fcs_errors;
+	priv->stats.rx_crc_errors     += u; /* recved pkt with crc error	*/
+	priv->stats.rx_errors         += u;
+
+	u = reg_stat->rx_orun;
+	priv->stats.rx_fifo_errors    += u; /* recv'r fifo overrun		*/
+	priv->stats.rx_errors         += u;
+
+	u = reg_stat->align_errors;
+	priv->stats.rx_frame_errors   += u; /* recv'd frame alignment error */
+	priv->stats.rx_errors         += u;
+
+	u = reg_stat->tx_urun;
+	priv->stats.tx_fifo_errors    += u;
+	priv->stats.tx_errors         += u; /* packet transmit problems	*/
+
+	u = reg_stat->crs_errors;
+	priv->stats.tx_carrier_errors += u;
+	priv->stats.tx_errors         += u;
+	
+	return &priv->stats;
+}
+
+/** Function to change the maximum transmit unit (MTU) size */
+static int c4k_eth_change_mtu(struct net_device *dev, int new_mtu)
+{
+#if 1
+	//MTU size change currently not supported, always return code
+	return 0;
+#else
+	//
+	//Previous code to support MTU size changes, including
+	//Jumbo frame support:
+	//
+	int tempsize, tempval;
+	struct eth_c4k_priv *priv = (struct eth_c4k_priv *)dev->priv;
+	int oldsize = priv->rx_buffer_size;
+	int frame_size = new_mtu + ETH_HLEN;
+
+	//frame_size += priv->padding;
+
+	if ((frame_size < 64) || (frame_size > JUMBO_FRAME_SIZE))
+	{
+		if (netif_msg_drv(priv))
+			printk(KERN_ERR "%s: Invalid MTU setting\n",
+					dev->name);
+		return -EINVAL;
+	}
+
+	/* Only stop and start the controller if it isn't already
+	 * stopped, and we changed something
+	 */
+	if ((oldsize != tempsize) && (dev->flags & IFF_UP))
+		stop_gfar(dev);
+
+	priv->rx_buffer_size = tempsize;
+
+	dev->mtu = new_mtu;
+
+	/* If the mtu is larger than the max size for standard
+	 * ethernet frames (ie, a jumbo frame), setup Jumbo frames
+	 */
+	gem_enable_rx_jmb()
+
+	if ((oldsize != tempsize) && (dev->flags & IFF_UP))
+		startup_gfar(dev);
+
+	return 0;
+#endif
+}
+
+/**
+ * Function to release all transmit and receive socket buffers and
+ * cleanup/initialize transmit and receive desciptors/queues
+ * for a given device
+ *
+ * AKB: Recoded transmit release to release the data based
+ * on what is in the socket buffer rather than the descriptor
+ *
+ */
+static void c4k_eth_release_buffers(struct net_device *dev)
+{
+	struct eth_c4k_priv * priv = netdev_priv(dev);
+	struct tRXdesc *      ThisRXdesc;
+	struct tTXdesc *      ThisTXdesc;
+	int                   i;
+	struct sk_buff *      skb;
+	u32                   length;
+	struct transcede_tx_q *this_tx_queue;
+	int q_idx;
+
+	if (netif_msg_drv(priv))
+		printk (KERN_INFO "%s: %s\n", __func__, dev->name);
+
+	/* Go through all RX descriptors, umap DMA, release all all receive
+	 * socket buffers, set RX descriptor data pointers to NULL
+	 * and set all socket ring buffer entries to NULL
+	 */
+	spin_lock(&priv->rxlock);
+	for (i = 0; i < priv->RxRingSize; i++)
+	{
+		/* Test if socket buffer pointer NULL, if so, skip release */
+		if(priv->RxSkbRing[i])
+		{
+			ThisRXdesc                = &(priv->RxBase[i]); // AKB fixed 2010-09-28, was priv->RxBase + i
+			dma_unmap_single(&dev->dev, ThisRXdesc->rx_data, PKT_BUF_SZ, DMA_FROM_DEVICE); //just be safe
+			ThisRXdesc->rx_data       = (u32)NULL;
+			ThisRXdesc->rx_extstatus |= GEMRX_OWN;
+			dev_kfree_skb(priv->RxSkbRing[i]);
+			priv->RxSkbRing[i]        = NULL;
+
+#ifdef GEM_ADM_ENABLED
+			length = (ThisRXdesc->rx_status & RX_STA_LEN_MASK) >> RX_STA_LEN_POS;
+			if (ThisRXdesc->rx_extstatus & GEMRX_OWN)
+				writel(length, priv->baseaddr + GEM_ADM_BLOCK + ADM_PKTDQ);
+#endif
+		}
+	}
+	spin_unlock(&priv->rxlock);
+
+	/* Go through all TX descriptors, unmap DMA, release all transmit
+	 * socket buffers and set all socket buffer ring entries to NULL
+	 * all descriptors
+	 */
+	for(q_idx = 0; q_idx < priv->tx_queue_num; q_idx++)
+	{
+		this_tx_queue = &(priv->tx_queue[q_idx]);
+		spin_lock(&this_tx_queue->txlock);
+		ThisTXdesc = this_tx_queue->TxBase;
+		for (i = 0; i < this_tx_queue->TxRingSize; i++)
+		{
+			/* Get current descriptor data length and socket buffer pointer (if any) */
+			length = ThisTXdesc->txctl & GEMTX_LENGTH_MASK;
+			skb = this_tx_queue->TxSkbRing[i];
+			if ((length != 0) && (ThisTXdesc->txdata))
+			{
+				/* Non-zero length in descriptor, give it back to the Linux CPU */
+				dma_unmap_single(&dev->dev, ThisTXdesc->txdata, length, DMA_TO_DEVICE);
+				/* Cleanup the TX descriptor */
+				ThisTXdesc->txdata = 0;                       /* Clear pointer */
+				ThisTXdesc->txctl &= ~(GEMTX_LENGTH_MASK);    /* Clear length  */
+				ThisTXdesc->txctl |=   GEMTX_USED_MASK;       /* Set as USED   */
+			}
+			/* Get current socket buffer pointer (if any) */
+			skb    = this_tx_queue->TxSkbRing[i];
+			if(skb)
+			{
+				dev_kfree_skb(skb);
+				this_tx_queue->TxSkbRing[i] = NULL;
+			}
+			/* Bump TX descriptor pointer */
+			ThisTXdesc++;
+		}
+		spin_unlock(&this_tx_queue->txlock);
+
+		/*Free the tx ring buffer*/
+		kfree(this_tx_queue->TxSkbRing);
+	}
+}
+
+/**
+ * Function to map a given IRAM virtual address to the physical address
+ * of the IRAM base address
+ */
+static u32 c4k_ring_to_phys(struct net_device *dev, volatile u32* va)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+
+	/* Calculate address to use by:
+	 *  1) Taking passed address
+	 *  2) Get relative address by Subtracting Base virtual address of IRAM base used by the driver
+	 *  3) Adding IRAM base physical address to relative address to get physical address
+	 */
+	return (u32)(priv->IRAM_baseaddr_pa + ((u32)va - (u32)priv->IRAM_baseaddr_v));
+}
+
+/**
+ * Function to initialize TX and RX buffer for C4K ethernet device
+ *
+ * @return Returns 0 if success, returns 1 if unable to allocate memory
+ */
+static int c4k_eth_init_buffers(struct net_device *dev)
+{
+	struct eth_c4k_priv * priv = netdev_priv(dev);
+	struct tRXdesc *      ThisRXdesc;
+	struct tTXdesc *      ThisTXdesc;
+	struct sk_buff *      skb;
+	int                   i;
+	struct transcede_tx_q *this_tx_queue;
+	int q_idx;
+	struct tTXdesc *this_tx_base;
+
+	dev_dbg(&dev->dev, "%s\n", __func__);
+
+	priv->RxBase = priv->IRAM_baseaddr_v;
+	memset(priv->RxBase, 0, priv->RxRingSize * sizeof(struct tRXdesc));
+
+	ThisRXdesc = priv->RxBase;
+	for (i = 0; i < priv->RxRingSize; i++) {
+		int offset;
+		if(!(skb = dev_alloc_skb(PKT_BUF_SZ + ETH_ALIGN))) {
+			if (unlikely(netif_msg_drv(priv)))
+				dev_err(&dev->dev, "skb allocation failed in %s\n", __func__);
+			goto err;
+		}
+
+		if (i < 4)
+			printk (KERN_INFO "%s: Init: Socket buffer %u & data 0x%08X 0x%08X\n",
+			        dev->name,
+			        i,
+			        (u32)skb,
+			        (u32)skb->data
+			       );
+
+		priv->RxSkbRing[i] = skb;
+
+		offset = ETH_ALIGN - ((u32)skb->data & (ETH_ALIGN-1));
+		skb_reserve(skb, offset);
+
+		if (i < 4)
+			printk (KERN_INFO "%s: Init: Socket buffer %u & data 0x%08X 0x%08X\n",
+			        dev->name,
+			        i,
+			        (u32)skb,
+			        (u32)skb->data
+			       );
+
+		ThisRXdesc->rx_data    = dma_map_single(&dev->dev, skb->data, PKT_BUF_SZ, DMA_FROM_DEVICE);
+		/* Clear least significant 2 bits of address to get 32 bit based
+		 * address for GEM (GEM offset put in RX offset value in GEM
+		 * register
+		 */
+		ThisRXdesc->rx_data &= 0xFFFFFFFC;
+		ThisRXdesc->rx_status |= RX_INT_FLAG;
+		ThisRXdesc++;
+	}
+
+	/* Move pointer back by 1 and set the WRAP bit on the last entry */
+	ThisRXdesc--;
+	ThisRXdesc->rx_status |= GEMRX_WRAP;
+
+	writel(c4k_ring_to_phys(dev, (u32*) priv->RxBase), priv->baseaddr + GEM_IP + GEM_RX_QPTR);
+	/* Init RX  ring control variables */
+	priv->RxtocleanIndex = 0;
+	priv->RxtofillIndex  = 0;
+
+	/* Init Tx ring */
+	priv->TxBase = (struct tTXdesc *) (priv->RxBase + priv->RxRingSize);
+	this_tx_base = priv->TxBase;
+
+	for(q_idx = 0; q_idx < priv->tx_queue_num; q_idx++)
+	{
+		this_tx_queue = &(priv->tx_queue[q_idx]);
+		this_tx_queue->TxBase = this_tx_base;
+		ThisTXdesc = this_tx_queue->TxBase;
+		memset(this_tx_base, 0, this_tx_queue->TxMaxRingSize * sizeof(struct tTXdesc));
+		printk (KERN_DEBUG "%s: %s Queue %d TxBase %x size %d \n", dev->name,  __func__, q_idx,
+					(unsigned int) this_tx_queue->TxBase, this_tx_queue->TxMaxRingSize);
+
+		/* Initialize the TX descriptor ring */
+		for (i = 0; i < this_tx_queue->TxRingSize; i++)
+		{
+			ThisTXdesc->txdata =  0;                  /* Initialize data pointer to NULL */
+			ThisTXdesc->txctl  = (0|GEMTX_USED_MASK); /* Initialize TX control to GEMTX_USED_MASK */
+			ThisTXdesc++;
+		}
+		this_tx_base = ThisTXdesc;
+
+		/* Move pointer back by 1 and set the WRAP bit on the last entry */
+		ThisTXdesc--;
+		ThisTXdesc->txctl |= GEMTX_WRAP;
+
+
+		/* Setup the base pointer to the TX descriptor ring in the GEM device */
+		writel(c4k_ring_to_phys(dev, (u32*) this_tx_queue->TxBase), priv->baseaddr + GEM_IP + this_tx_queue->tx_gem_queue_base);
+	}
+	/* Always returns OK (0) */
+	return 0;
+
+err:
+	c4k_eth_release_buffers(dev);
+
+	return -ENOMEM;
+}
+
+
+/**
+ * Function to handle the hardware specifics of transmitting a single packet
+ * based on pointer to device and pointer to socket buffer passed as parameters
+ */
+//static
+int c4k_hardware_send_packet(struct net_device *dev, struct sk_buff *skb, struct transcede_tx_q *this_tx_queue)
+{
+	struct eth_c4k_priv * priv = netdev_priv(dev);
+/*	GEM_DEVICE *          gemdev = &priv->gemdev; */
+	struct tTXdesc *      ThisTXdesc;      /**< Base Transmit descriptor */
+	struct tTXdesc *      CurrentTXdesc;   /**< Current Transmit descriptor: For handling multiple TX descriptors */
+	u32                   CurrentLength;   /**< Current Transmit length:     For handling multiple TX descriptors */
+	u32                   RemainingLength; /**< Remaining Transmit length:   For handling multiple TX descriptros */
+	u8*                   CurrentTXpointer;/**< Current data pointer to TX:  For handling multiple TX descriptors */
+	int                   err = 0;
+	u32                   txctl;
+	u32		      queue_index;
+
+	queue_index = (this_tx_queue - &priv->tx_queue[0]) & 7;
+
+	// Option to print message on every transmit frame if interface messages
+	// enabled is commented out.  Be careful if you uncomment this out
+	// as this is called on every transmit ethernet frame
+	//if (netif_msg_intr(priv))
+	//	printk(KERN_DEBUG "\n%s %s: txavail %d txtosend %d txdone %d\n", dev->name, __func__, priv->Txavail, priv->Txtosend, priv->Txdone);
+
+#ifdef GEM_ETHDRV_TEMP_DEBUG
+	//AKB TEMP DEBUG:
+	printk(KERN_NOTICE "\nTX:a:%u p:%p l:%u s:%u d:%u\n",this_tx_queue->Txavail, skb->data, skb->len, this_tx_queue->Txtosend, priv->Txdone);
+#endif
+	/* Before proceeding, first make sure that there is at least two
+	 * transmit queue entries available
+	 */
+	if (unlikely(this_tx_queue->Txavail <= 1))
+	{
+		//AKB TEMP DEBUG:
+		printk(KERN_ERR "\nTA:%u p:%p l:%u\n",this_tx_queue->Txavail, skb->data, skb->len);
+
+		err = -ENOMEM;
+		goto err;
+	}
+
+	/* There is at least two transmit queue entries available: */
+	/* Make sure we have exclusive use of this socket buffer, if not,
+	 * exit
+	 */
+	if (unlikely((skb = skb_unshare(skb, GFP_ATOMIC)) == NULL))
+	{
+		//AKB TEMP DEBUG:
+		printk(KERN_ERR "\nUS\n");
+
+		priv->stats.tx_dropped++;
+		goto err;
+	}
+
+	/* Get pointer to current avaiable transmit queue entry */
+	ThisTXdesc = this_tx_queue->TxBase + this_tx_queue->Txtosend;
+#if 0
+	/* Setup DMA */
+	/* Call dma_map_single, this is used by the linux kernel to
+	 * map a single buffer for streaming DMA and to
+	 * ensure that any data held in the cache is appropriately discarded
+	 * or written back.  The device owns the memory once this call has
+	 * completed.
+	 */
+	ThisTXdesc->txdata = dma_map_single(&dev->dev, skb->data, skb->len, DMA_TO_DEVICE);
+
+	/* A call to dma_sync_single_for_devie is required to allow
+	 * the device to have access to the buffer
+	 */
+	dma_sync_single_for_device(&dev->dev, ThisTXdesc->txdata, skb->len, DMA_TO_DEVICE);
+
+#endif
+	/* Begin code for C4K issue with TX DMA crossing a 1K boundary
+	 * Get data pointer from socket buffer and length and see
+	 * if it crosses a 1K boundary or not
+	 */
+	/* Init length and pointer fields for loop below */
+	RemainingLength  = skb->len;
+	CurrentTXpointer = skb->data;
+	CurrentTXdesc    = ThisTXdesc;
+
+	/* Loop while there is still data to send */
+	while (RemainingLength != 0)
+	{
+		/* There is still data to send */
+		/* For C4K 1K boundary issue workaround, get amount of data we can
+		 * send without crossing a 1K boundary
+		 */
+		CurrentLength = 1024UL -((u32)CurrentTXpointer & 0x000003FFUL);
+		if (CurrentLength >= (u32)RemainingLength)
+		{
+			/* Amount remaining can fit the rest of the packet
+			 * without crossing a 1K boundary
+			 */
+			CurrentLength   = RemainingLength;
+			RemainingLength = 0;
+			//AKB TEMP DEBUG:
+			//if (CurrentLength != skb->len)
+			//	printk(KERN_NOTICE "\nX2 p:%p l:%04u c:%04u r:%04u\n",CurrentTXpointer,skb->len,CurrentLength,RemainingLength);
+		}
+		else
+		{
+			/* Amount remaining will cross a 1K boundary, Send
+			 * only up to the 1K boundary   CurrentLength
+			 * will send data from the current pointer up to
+			 * but not crossing the 1K boundary
+			 */
+			RemainingLength -= CurrentLength;
+			//AKB TEMP DEBUG:
+			//printk(KERN_NOTICE "\nX1 p:%p l:%04u c:%04u r:%04u\n",skb->data,skb->len,CurrentLength,RemainingLength);
+		}
+		/* Setup DMA */
+		/* Call dma_map_single, this is used by the linux kernel to
+		 * map a single buffer for streaming DMA and to
+		 * ensure that any data held in the cache is appropriately discarded
+		 * or written back.  The device owns the memory once this call has
+		 * completed.
+		 */
+
+#ifdef CONFIG_IPSEC_DMA_MAP_HACK_TX
+		if (unlikely(skb->dma_mapped_data)) {
+			if (likely(CurrentTXpointer < skb->dma_mapped_data && CurrentTXpointer + CurrentLength >= skb->dma_mapped_data)) {
+				/* The buffer which we currently need to map is not completely unmapped.
+				 * So we need to map only what was previously unmapped.
+				 */
+				CurrentTXdesc->txdata = dma_map_single(&dev->dev, CurrentTXpointer, skb->dma_mapped_data - CurrentTXpointer, DMA_TO_DEVICE);
+				goto dma_mapped;
+			} else if (CurrentTXpointer >= skb->dma_mapped_data) {
+				/* The buffer which we currently need to map is already mapped */
+				CurrentTXdesc->txdata = dma_map_single(&dev->dev, CurrentTXpointer, 0, DMA_TO_DEVICE);
+				goto dma_mapped;
+			} 
+		}
+#endif
+		CurrentTXdesc->txdata = dma_map_single(&dev->dev, CurrentTXpointer, CurrentLength, DMA_TO_DEVICE);
+
+dma_mapped:
+		/* A call to dma_sync_single_for_devie is required to allow
+		 * the device to have access to the buffer
+		 */
+//		dma_sync_single_for_device(&dev->dev, CurrentTXdesc->txdata, CurrentLength, DMA_TO_DEVICE);
+#ifdef GEM_ETHDRV_TEMP_DEBUG
+		// AKB TEMP FOR DEBUG
+		printk(KERN_NOTICE "\nDM p:%p\n",CurrentTXdesc->txdata);
+#endif
+
+		/* Setup transmit control word for transmit descriptor */
+
+		/* Setup base bit values (in all descriptors) */
+		txctl =
+			CurrentLength | /* Length of data to transmit */
+			GEMTX_FCS;      /* FCS generation enabled */
+
+		/* Test if last (or only) descriptor, if so, set last bit */
+		if (RemainingLength == 0)
+		{
+			txctl |= GEMTX_LAST; /* Last of only descriptor for this transmit packet */
+			/* Save pointer to socket buffer structure for later release
+			 * on completion of transmit.  The socket buffer pointer
+			 * is always put into the same index as the last descriptor
+			 * to make sure the DMA is fully completed prior to
+			 * returning the buffer back to the system.
+			 */
+			this_tx_queue->TxSkbRing[this_tx_queue->Txtosend] = skb;
+		}
+		else
+		{
+			/* Not the last or only descriptor, set this
+			 * Socket buffer pointer entry to 0 as we only
+			 * want to set the socket pointer on the entry
+			 * with the last descriptor or only ring
+			 * entry
+			 */
+			this_tx_queue->TxSkbRing[this_tx_queue->Txtosend] = 0;
+		}
+
+		/* If Layer 3 and 4 checksum offload enabled and requested, set
+		 * those flags as well
+		 */
+		if ((priv->flags & TX_CSUM_OFFLOAD_ENABLED) && (skb->ip_summed == CHECKSUM_PARTIAL))
+		{
+		    txctl |= GEMTX_L4_CSUM | GEMTX_L3_CSUM;
+		}
+
+#ifdef GEM_TX_CHECKSUM_OFFLOAD_ENABLED
+		/* Code to always force Layer 3 and Layer 4 checksum generation */
+		txctl |= GEM_DMA_TX_L3_L4_ADDED;  // (3 << 25)
+#endif
+
+#if defined(CONFIG_INET_IPSEC_OFFLOAD) || defined(CONFIG_INET6_IPSEC_OFFLOAD)
+		/* Test if socket buffer has requested IPSEC offload (acceleration) */
+		/* AKB NOTE: UNSURE HOW TO HANDLE THIS FOR NEW C4K WORKAROUND
+		 * WHERE WE SPLIT THE DESCRIPTOR POINTERS ACROSS 1K BOUNDARIES
+		 * IF NECESSARY.  WILL HAVE TO DESIGN/CODE/UPDATE
+		 * THIS SECTION OF CODE FOR IPSEC AT A LATER TIME.
+		 */
+		if (skb->ipsec_offload)
+		{
+			/* IPSEC offload has been requested: */
+			if (skb->sp)
+			{
+				int i;
+				struct tTXextdesc *ThisTXextdesc = priv->TxextBase + this_tx_queue->Txtosend;
+				u16 *sah = ThisTXextdesc->SAhandle;
+
+				*(u64*)ThisTXextdesc = 0;
+				for (i = skb->sp->len - 1; i >= 0; i--)
+				{
+					struct xfrm_state *x = skb->sp->xvec[i];
+					*sah++ = x->handle;
+				}
+				txctl |= GEMTX_EXPT_IPSEC_OUT;
+			}
+			else
+			{
+				txctl |= GEMTX_EXPT_IPSEC_IN;
+			}
+		}
+#endif /* IPSEC offload */
+
+		/* Test if end of the descriptor ring, if so, then set the
+		 * wrap bit.  Either way, more the descriptor ring
+		 * to next entry and decrement total number of
+		 * free entries.
+		 */
+		if (unlikely((++(this_tx_queue->Txtosend)) >= this_tx_queue->TxRingSize))
+		{
+			/* End of descriptor ring, set WRAP bit and move
+			 * index back to zero
+			 */
+			txctl |= GEMTX_WRAP;
+			this_tx_queue->Txtosend = 0;
+		}
+		this_tx_queue->Txavail--; /* Decrement number of Transmit descriptor slots available */
+
+		/* Transmit descriptor control data bits all determined, setup the actual
+		 * control field and data pointer in the descriptor itself
+		 */
+		CurrentTXdesc->txctl  = txctl;
+
+#ifdef GEM_ETHDRV_TEMP_DEBUG
+		if (CurrentLength != skb->len)
+			printk(KERN_NOTICE "\nX3 s:%p l:%04u c:%04u r:%04u a:%04u s:%04u d:%04u tp:%8.8X tc:%8.8X\n",
+			    CurrentTXpointer,
+			    skb->len,
+			    CurrentLength,
+			    RemainingLength,
+			    this_tx_queue->Txavail,
+			    this_tx_queue->Txtosend,
+			    priv->Txdone,
+			    CurrentTXdesc->txdata,
+			    CurrentTXdesc->txctl
+			    );
+#endif
+		/* Move the pointer to the next amount of data to prepare to send (if any)
+		 * and move the descriptor pointer
+		 */
+		CurrentTXpointer += CurrentLength;
+		CurrentTXdesc     = this_tx_queue->TxBase + this_tx_queue->Txtosend;
+	} /* End while loop, loop until remaining length is zero */
+
+
+
+#ifdef GEM_SCH_ENABLED
+	writel((skb->len | queue_index << 16), priv->baseaddr + GEM_SCH_BLOCK + SCH_PACKET_QUEUED);
+#else
+	writel(c4k_ring_to_phys(dev, (u32)ThisTXdesc), priv->baseaddr +  GEM_IP + GEM_QUEUE_BASE0);
+//	if (ThisTXdesc != CurrentTXdesc)
+//	{
+//		/* AKB: Seeing if GEM requires 2 writes on multiple descriptors */
+//		/* 2 buffer sequence */
+//		writel(c4k_ring_to_phys(dev, (u32)CurrentTXdesc), priv->baseaddr +  GEM_IP + GEM_QUEUE_BASE0);
+//	}
+	gem_start_tx(gemdev);
+#endif
+
+
+	/* Bump transmit statistics */
+	priv->stats.tx_packets++;
+	priv->stats.tx_bytes += skb->len;
+	// AKB TEMP DEBUG:
+	//printk("\nTX p:%p l:%u s:%u a:%u d:%u\n",skb->data,skb->len,priv->Txtosend,priv->Txavail,priv->Txdone);
+
+err:
+	return(err);
+}
+
+/**
+ * Entry function to transmit a single packet based on pointer to device
+ * and pointer to socket buffer passed as parameters.
+ * The function first tries to lock the TX function
+ * using spin_trylock(&priv->txlock) and if sucessfull
+ * then calls c4k_hardware_send_packet which takes care
+ * of the hardware specific functions.
+ *
+ * @return
+ * Returns:
+ *   - NETDEV_TX_OK:      Packet queued for transmission OK
+ *   - NETDEV_TX_LOCKED:  Unable to unlock transmit using spin_trylock
+ *   - NETDEV_TX_BUSY:    -ENOMEM error (hard error) returned from c4k_hardware_send_packet
+ */
+//
+// AKB TEMP REMOVED STATIC FOR DEBUG
+//static
+int c4k_eth_send_packet(struct sk_buff *skb, struct net_device *dev)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+	unsigned long flags;
+	int result;
+	struct netdev_queue *txq;
+	struct transcede_tx_q *this_tx_queue;
+
+	this_tx_queue = &(priv->tx_queue[skb->queue_mapping]);
+	txq = netdev_get_tx_queue(dev, skb->queue_mapping);
+
+	if (unlikely(!spin_trylock_irqsave(&this_tx_queue->txlock, flags)))
+	{
+		/* Collision - tell upper layer to requeue */
+		if (netif_msg_tx_err(priv))
+			printk(KERN_DEBUG "%s %s: TX collision on queue %d\n", dev->name, __func__, skb->queue_mapping);
+
+		return NETDEV_TX_LOCKED;
+	}
+
+	result = c4k_hardware_send_packet(dev, skb, this_tx_queue);
+
+	switch (result) {
+	case -ENOSPC:
+		/* We queued the skb, but now we're out of space. */
+		// AKB TEMP: ALWAYS PRINT ERROR MESSAGES if (netif_msg_tx_err(priv))
+			printk(KERN_NOTICE "%s %s: no space for tx\n", dev->name, __func__);
+		/* Stop upper layer from sending us additional packets */
+		netif_tx_stop_queue(txq);
+
+		break;
+
+	case -ENOMEM:
+		/* This is a hard error - log it. */
+		// AKB TEMP: ALWAYS PRINT ERROR MESSAGES if (netif_msg_tx_err(priv))
+			printk(KERN_ERR "%s %s: out of tx resources, returning skb\n", dev->name, __func__);
+
+		netif_tx_stop_queue(txq);
+		spin_unlock_irqrestore(&this_tx_queue->txlock, flags);
+
+		return NETDEV_TX_BUSY;
+
+	default:
+		break;
+	}
+
+	dev->trans_start = jiffies;
+	spin_unlock_irqrestore(&this_tx_queue->txlock, flags);
+
+	c4k_eth_free_tx_packets(dev); //dd
+	return NETDEV_TX_OK;
+}
+
+/** 
+ * Function to replenish the RX queue (RX ring) of socket buffers
+ * and to setup for additional reception for previously used
+ * slots
+ *
+ * @return
+ * Returns
+ *   - 0      : OK
+ *   - -EAGAIN: Unable to get a new receive socket buffer
+ */
+//static
+int c4k_eth_rx_refill(struct net_device *dev)
+{
+	struct eth_c4k_priv * priv = netdev_priv(dev);
+	int                   rtf;        /**< RX to fill  current index  */
+	int                   rtc;        /**< RX to clean current index */
+	int                   ret = 0;    /**< Return code, assumes OK */
+	struct tRXdesc *      ThisRXdesc; /**< Current Receive descriptor pointer */
+	struct sk_buff *      skb;        /**< Current socket buffer pointer */
+	int offset;
+
+
+	/* Check if anything to do (nothing to do if "to fill"
+	 * index is equal to "to clean" index)
+	 */
+	if (priv->RxtofillIndex != priv->RxtocleanIndex)
+	{
+		rtf = priv->RxtofillIndex;
+		rtc = priv->RxtocleanIndex;
+		/* Loop through descriptor table until indeces are equal */
+		while (rtc != rtf)
+		{
+			/* Get pointer to current descriptor: */
+			ThisRXdesc = priv->RxBase + rtf;
+
+			/* Request a new socket buffer from the system */
+			skb = dev_alloc_skb(PKT_BUF_SZ + ETH_ALIGN);
+			/* Test if we go a buffer OK */
+			if (likely(skb))
+			{
+				/* We got a new socket buffer OK, set the pointer to the
+				 * socket buffer in the current index of the receive
+				 * socket buffer ring
+				 */
+				priv->RxSkbRing[rtf] = skb;
+
+				/* Align the data field up as required: */
+				offset = ETH_ALIGN - ((u32)skb->data & (ETH_ALIGN-1));
+				skb_reserve(skb, offset);
+
+				/* Setup the DMA for this socket buffer */
+				ThisRXdesc->rx_data= dma_map_single(&dev->dev, skb->data, PKT_BUF_SZ, DMA_FROM_DEVICE);
+
+				/* Clear least significant 2 bits of address to get 32 bit based
+				 * address for GEM (GEM offset put in RX offset value in GEM
+				 * register
+				 */
+				ThisRXdesc->rx_data &= 0xFFFFFFFC;
+
+				/* Setup the descriptor status fields: */
+				*(volatile u32*) &ThisRXdesc->rx_extstatus = 0;
+				*(volatile u32*) &ThisRXdesc->rx_status    = 0 | RX_INT_FLAG;
+
+				/* Setup the Wrap bit if this is the last descriptor in the ring */
+				if(unlikely(ThisRXdesc == (priv->RxBase + priv->RxRingSize - 1)))
+					*(volatile u32*) &ThisRXdesc->rx_status |= GEMRX_WRAP;
+
+			}
+			else
+			{
+				/* Oops, we didn't get a socket buffer, will have to try
+				 * again later.  Indicate low on memory by setting
+				 * return code to -EAGAIN and exit the loop
+				 */
+				// AKB TEMP: ALWAYS PRINT ERROR MESSAGES if (unlikely(netif_msg_rx_err(priv)))
+					printk(KERN_ERR "%s %s: low on mem\n", dev->name, __func__);
+
+				ret = -EAGAIN;
+				break;
+			}
+			/* Set next return to fill index and keep looping until done */
+			if (++rtf >= priv->RxRingSize)
+				rtf = 0;
+		}
+		/* All done looping, update the receive "to fill" index */
+		priv->RxtofillIndex = rtf;
+	}
+
+	return (ret);
+}
+
+//static
+int c4k_eth_rx_packet(struct net_device *dev, unsigned int *work_done,
+	unsigned int work_to_do)
+{
+	struct eth_c4k_priv * priv = netdev_priv(dev);
+	struct tRXdesc *      ThisRXdesc;
+	int                   rtc;
+	u32                   rx_data;
+	u32                   rx_status;
+	u32                   rx_extstatus;
+	int                   length;
+	struct sk_buff *      skb;
+	int                   ret = 0;
+	u8                    ip_summed;
+
+	//if (netif_msg_intr(priv))
+	//	printk(KERN_DEBUG "%s: %s\n", dev->name, __func__);
+
+	rtc = priv->RxtocleanIndex;
+
+	// Loop through the receive descriptors
+	while (1)
+	{
+		// loop through the receive descriptors
+		ThisRXdesc   = priv->RxBase + rtc;
+		rx_extstatus = ThisRXdesc->rx_extstatus;
+
+		if (unlikely(*work_done >= work_to_do))
+		{
+			ret = -EAGAIN;
+
+		//	if (netif_msg_intr(priv))
+		//	{
+		//		printk (KERN_DEBUG "%s %s: FDesc %#lx, RxtocleanIndex %d\n", dev->name, __func__, (unsigned long)ThisRXdesc, rtc);
+		//		printk (KERN_DEBUG "%s %s: work_done %d, work_to_do %d\n", dev->name, __func__, *work_done, work_to_do);
+		//	}
+
+			goto done_exit;
+		}
+
+		if (!(rx_extstatus & GEMRX_OWN)) {
+//			if (unlikely(netif_msg_intr(priv)))
+	//			printk(KERN_DEBUG "%s %s: done, FDesc %#lx, RxtocleanIndex %d\n", dev->name, __func__, (unsigned long)ThisRXdesc, rtc);
+			break;
+		}
+
+//		dma_sync_single_for_cpu(&dev->dev, ThisRXdesc->rx_data, PKT_BUF_SZ , DMA_FROM_DEVICE);
+
+		rx_data   = ThisRXdesc->rx_data;
+		rx_status = ThisRXdesc->rx_status;
+		length    = (rx_status & RX_STA_LEN_MASK) >> RX_STA_LEN_POS;
+
+#ifdef GEM_ADM_ENABLED
+		writel(length, priv->baseaddr + GEM_ADM_BLOCK + ADM_PKTDQ);
+#endif
+
+		dma_unmap_single(&dev->dev,
+		                 ThisRXdesc->rx_data,
+		                 length+2,
+		                 DMA_FROM_DEVICE
+		                );
+
+		skb = priv->RxSkbRing[rtc];
+		if (likely(!(rx_status & RX_CHECK_ERROR) && !c4k_emac_checksum(priv, rx_extstatus, &ip_summed)))
+		{
+			/* RX Ethernet frame is OK: */
+#if defined(CONFIG_INET_IPSEC_OFFLOAD) || defined(CONFIG_INET6_IPSEC_OFFLOAD)
+			if (rx_extstatus & RX_IPSEC_IN) {
+				struct sec_path *sp;
+				struct xfrm_state *x;
+				u16 *sah = (u16*)&ThisRXdesc->pad;
+				int i = 0;
+
+				sp = secpath_dup(skb->sp);
+
+				if (!sp) {
+					kfree_skb(skb);
+					goto pkt_drop;
+				}
+
+				skb->sp = sp;
+				while ((*sah)) {
+					if ((i > 3) || ((x = xfrm_state_lookup_byhandle(*sah++)) == NULL)) {
+						kfree_skb(skb);
+						goto pkt_drop;
+					}
+
+					sp->xvec[i++] = x;
+					if (!x->curlft.use_time)
+						x->curlft.use_time = (unsigned long)xtime.tv_sec;
+				}
+
+				sp->len = i;
+			}
+#endif
+			/* Update socket buffer fields */
+			skb->dev       = dev;
+			skb_put(skb, length);
+			skb->protocol  = eth_type_trans(skb, dev);
+#if 0
+#ifdef GEM_RX_CHECKSUM_OFFLOAD_ENABLED
+			//
+			// Checksum offload is enabled, check fields
+			// Check descriptor status for IP checksum fields
+			//
+			if (rx_status & GEM_RX_DESCR_TYPE_ID_OR_CHKSUM_INFO_MASK)
+			{
+				// Non-zero checksum field, TCP, UDP and/or
+				// IP checksum was already validated as OK
+				// by the GEM.  Mark as Checksum UNNECESSARY
+				skb->ip_summed = CHECKSUM_UNNECESSARY;
+#if 0
+				// DEBUG PRINT, HAVE VALIDATED THAT THIS CODE IS EXECUTING :-)
+				printk(KERN_INFO "U%u\n",length);
+#endif
+			}
+			else
+			{
+				//
+				// This frame's CRC/FCS was OK, but packet
+				// was not in TCP, UDP or IP format.
+				// Mark as checksum NONE
+				//
+				skb->ip_summed = CHECKSUM_NONE;
+			}
+#else
+			//
+			// Checksum offload is disabled, always set
+			// socket buffer ip_summed to NONE
+			// Up to upper level SW to take care of all checksums
+			// if needed
+			//
+			skb->ip_summed = CHECKSUM_NONE;
+#endif
+
+#else
+			// ORIGINAL T4000 CODE, ASSUME IT IS OK
+			skb->ip_summed = ip_summed;
+#if 0
+			// TEMP FOR DEBUG
+			if (ip_summed == CHECKSUM_UNNECESSARY)
+				// DEBUG PRINT
+				printk(KERN_INFO "U%u\n",length);
+
+#endif
+#endif
+
+#if 0
+			// TEMP DEBUG, PRINT RECEIVE INFO EVERY TIME RX to clean IS 0
+			// TO GET SAMPLE OF RX BUFFERS RECEIVED
+			if (rtc == 0)
+			{
+				printk(KERN_INFO "RX rtc 0 l:%u d:%08X\n",
+					length,
+					ThisRXdesc->rx_data
+					);
+				print_skb_info(skb);
+			}
+#endif
+
+			/* Pass received socket buffer to upper layer */
+			netif_receive_skb(skb);
+
+			/* Update receive queue and statistics */
+			priv->RxSkbRing[rtc]  = NULL;
+			priv->stats.rx_packets++;
+			priv->stats.rx_bytes += length;
+			dev->last_rx          = jiffies;
+			(*work_done)++;
+		}
+		else
+		{
+			/* Receive Ethernet frame has an error */
+			dev_kfree_skb(skb);	// no one will touch this anymore, just free it.
+			priv->stats.rx_errors++;
+		}
+
+		if (++rtc >= priv->RxRingSize)
+			rtc = 0;
+	}
+
+done_exit:
+	priv->RxtocleanIndex = rtc;
+	return ret;
+}
+
+/**
+ * Function to scan the transmit queue and determine which
+ * descriptors have completed transmission.
+ * For those descriptors that have completed transmission
+ * the transmit socket buffer is freed, the desciptors
+ * are setup to allow for a future ethernet frame to be
+ * sent and the transmit queue (TX ring) control variables
+ * are updated.
+ *
+ * @param dev  Pointer to struct net_device with device driver information
+ * @return     Returns number of Transmit descriptors cleaned (buffers released)
+ *
+ * @todo
+ * This code currently does not check any of the error bits after parsing
+ * through the descriptors to free the descriptors up and return
+ * any socket buffers back to the system.  It would be better
+ * if this code also checked the error status and incremented
+ * error statistics.  NOTE: There are hardware statistics stored
+ * in the chip as well, so if this is needed, it should only be
+ * done for those errors not counted by the chip itself.
+ *
+ */
+//static
+int c4k_eth_free_tx_packets(struct net_device *dev)
+{
+	struct eth_c4k_priv * priv =  netdev_priv(dev); /**< Private data pointer */
+	struct tTXdesc *      ThisTXdesc;               /**< Current TX descriptor */
+	struct tTXdesc *      NextTXdesc;               /**< Next    TX descriptor */
+	u32                   NextTXindex;
+	int                   tx_cleaned = 0;           /**< Transmit cleaned counter */
+	struct sk_buff *      skb;                      /**< Pointer to current socket buffer */
+	u32                   length;                   /**< length extracted from TX descriptor */
+	int                   LastBitPending = 0;       /**< Variable to keep processing multiple descriptors until LAST bit seen */
+
+	struct transcede_tx_q *this_tx_queue;
+	int q_idx;
+	/* Print debug message on function entry if enabled: */
+	//if (unlikely(netif_msg_intr(priv)))
+	//	printk(KERN_DEBUG "%s: %s\n", dev->name, __func__);
+
+
+	/*
+	 * Loop through transmit queue (TX ring) from last done position
+	 * until bits indicate there is a transmit in process
+	 */
+	for(q_idx = 0; q_idx < priv->tx_queue_num; q_idx++)
+	{
+		this_tx_queue = &(priv->tx_queue[q_idx]);
+		/* Spin lock on transmit lock */
+		spin_lock(&this_tx_queue->txlock);
+		while (1)
+		{
+			/* Get current starting point (base plus "done") */
+			ThisTXdesc  =  this_tx_queue->TxBase + this_tx_queue->Txdone;
+			NextTXindex = (this_tx_queue->Txdone + 1)  &  (this_tx_queue->TxRingSize - 1);
+			NextTXdesc  =  this_tx_queue->TxBase + NextTXindex;
+
+			length      = ThisTXdesc->txctl & GEMTX_LENGTH_MASK;
+
+#ifdef GEM_ETHDRV_TEMP_DEBUG
+			// AKB TEMP DEBUG:
+			if ((this_tx_queue->Txdone != this_tx_queue->Txtosend) && (length >= 500))
+			{
+				printk(KERN_NOTICE "\nFR: d:%u s:%u tc:%8.8X tp:%8.8X tb:%p nc:%8.8X np:%8.8X nb:%p\n",
+					this_tx_queue->Txdone,
+					this_tx_queue->Txtosend,
+					ThisTXdesc->txctl,
+					ThisTXdesc->txdata,
+					this_tx_queue->TxSkbRing[this_tx_queue->Txdone],
+					NextTXdesc->txctl,
+					NextTXdesc->txdata,
+					priv->TxSkbRing[NextTXindex]
+					);
+			}
+#endif
+			/*
+			* Test current TX descriptor if transmission has completed
+			* A transmission is completed when the first descriptor's USED
+			* bit is set.  Note that for transmissions where there
+			* is more than one descriptor per ethernet frame sent,
+			* only the first descriptor's USED bit is set.
+			* To process all the buffers for a set, we must release
+			* all descriptors in the set until the last bit is seen
+			* length of this descriptor is non-zero
+			*/
+			if (    (LastBitPending | (ThisTXdesc->txctl & GEMTX_USED_MASK))   /* Test USED or LAST bit set */
+			&& (length != 0)         /* Test Length non-zero */
+			&& (ThisTXdesc->txdata)  /* Make sure descriptor pointer is not NULL */
+			)
+			{
+				/* First return memory for this descriptor back to
+				* Linux CPU by performing an unmap operation
+				*/
+				dma_unmap_single(&dev->dev, ThisTXdesc->txdata, length, DMA_TO_DEVICE);
+				/*
+				* Test if LAST bit is set, if not, then this was a frame
+				* transmitted using more than descriptor.  If not,
+				* then this we are either at the end of a multi-descriptor sequence for the
+				* tranmitted frame or there was only one descriptor for the frame
+				* to begin with.
+				*/
+				if (ThisTXdesc->txctl & GEMTX_LAST)
+				{
+					/* Last bit is set */
+					LastBitPending = 0;
+
+					/* The index of the last or only descriptor of a sequence is also used
+					* to store the socket buffer pointer in the socket buffer
+					* ring. Get the pointer to the current socket buffer pointer
+					* and test if it is non-zero
+					*/
+					if((skb = this_tx_queue->TxSkbRing[this_tx_queue->Txdone]))
+					{
+						/*
+						* Socket buffer pointer is non-zero, stop the DMA and release the
+						* socket buffer back to the kernel
+						*/
+						dev_kfree_skb(skb);
+						/* AKB: Trying moving TX cleaned to here to ony
+						* return number if socket buffer returned, not just
+						* on intermediate descriptors
+						*/
+						tx_cleaned++;
+					}
+					else
+					{
+						/*
+						* Socket buffer pointer is NULL for a transmitted packet, this
+						* should not happen.  Print error message and exit
+						*/
+						printk(KERN_ERR "%s: %s encountered errors while freeing tx descriptor, socket buffer NULL\n", dev->name, __func__);
+						spin_unlock(&this_tx_queue->txlock);
+						return (0);
+					}
+				}
+				else
+				{
+					/* Last bit is not set for a sequence where the USED bit
+					* was set at least once.  This means this is a multi-descriptor
+					* frame sequence and we need to keep freeing descriptors'
+					* data until the last bit is seen.
+					*/
+					LastBitPending = 1;  /* Set LAST bit is still pending */
+					skb            = 0;  /* Clear temporary socket buffer pointer */
+				}
+				/*
+				* AKB Added:
+				* Now that socket buffer is returned, setup TX descriptor
+				* to allow for another packet to be transmitted.
+				*/
+				ThisTXdesc->txdata = 0;                   /* Clear pointer */
+				ThisTXdesc->txctl  = (GEMTX_FCS|GEMTX_USED_MASK); /* Clear length and control information */
+
+				/* Set current socket buffer ring entry to NULL,
+				* bump the TX avaiable count and cleaned count
+				* and set "done" entry to next entry based on the Ring size
+				*/
+				this_tx_queue->TxSkbRing[this_tx_queue->Txdone] = NULL;
+				this_tx_queue->Txavail++;
+				//
+				// AKB: Moved counter to socket return above ...
+				//tx_cleaned ++;
+				//
+				if (++this_tx_queue->Txdone >= this_tx_queue->TxRingSize)
+				this_tx_queue->Txdone = 0;
+#ifdef GEM_ETHDRV_TEMP_DEBUG
+					//AKB TEMP DEBUG
+					if (skb)
+						if (skb->len > 512)
+							printk(KERN_NOTICE "\nFR p:%p l:%u d:%u a:%u c:%u\n",skb->data,skb->len,this_tx_queue->Txdone,this_tx_queue->Txavail,tx_cleaned);
+#endif
+			}
+			else
+			{
+				/*
+				* Current pointer is not a completed packet, nothing to
+				* do, so exit the loop
+				*/
+				break;
+			}
+		} // while (1)
+
+		if (unlikely(this_tx_queue->Txavail && __netif_subqueue_stopped(dev, q_idx)))
+		{
+			//AKB TEMP: PRINT ALL DEBUG MESSAGES if (unlikely(netif_msg_intr(priv)))
+				printk(KERN_DEBUG "%s  c1K_eth_free_tx_packets :  netif_wake_queue \n", dev->name);
+			netif_wake_subqueue(dev, q_idx);
+		}
+		spin_unlock(&this_tx_queue->txlock);
+	}
+
+	return(tx_cleaned);
+}
+
+
+/**
+ * Function that is setup polled on a periodic and frequent basis.
+ *
+ * @par
+ * This function first calls c4k_eth_free_tx_packets to
+ * inspect the transmit queue for completed packets
+ * and if any are completed, the socket buffer is released
+ * and the queue is updated.
+ *
+ * @par
+ * It next goes to check if any receive packets are available
+ * and process the receive queue accordingly
+ *
+ * @return
+ * Returns number of packets received or zero
+ * if no packets are received on this call to the poll function
+ */
+static int c4k_eth_poll(struct napi_struct *napi, int budget)
+{
+	struct eth_c4k_priv * priv   = container_of(napi, struct eth_c4k_priv, napi);
+	struct net_device *   dev    = priv->dev;
+	GEM_DEVICE *          gemdev = &priv->gemdev;
+
+	unsigned int          work_to_do = budget;
+	unsigned int          work_done  = 0;
+	int                   rc;
+	int                   tx_cleaned;
+
+	/* Free up any completed transmitted socket buffers and descriptors
+	 * in the transmit queue (TX ring)
+	 */
+	tx_cleaned = c4k_eth_free_tx_packets(dev);
+
+	//if (unlikely(netif_msg_intr(priv)))
+	//	printk(KERN_DEBUG "%s: %s %d tx descriptors freed\n", dev->name, __func__, tx_cleaned);
+
+	/* Process the receive buffer queue (RX ring) for any current packets */
+	c4k_eth_rx_packet(dev, &work_done, work_to_do);
+
+	/* Refill the receive queue's socket buffers as necessary */
+	rc = c4k_eth_rx_refill(dev);
+
+	//if (unlikely(netif_msg_intr(priv)))
+	//	printk(KERN_DEBUG "%s %s: work_done %d\n", dev->name, __func__, work_done);
+
+	if ((!tx_cleaned && !work_done && !rc)) {
+		napi_complete(napi);
+		//if (unlikely(netif_msg_intr(priv)))
+		//	printk(KERN_DEBUG "%s %s: re-enable irqs\n", dev->name, __func__);
+
+
+#ifdef GEM_ADM_ENABLED
+		/* clear batch counter and timer interrupt status */
+		writel(0x3, priv->baseaddr + GEM_ADM_BLOCK + ADM_BATCHINTRSTAT);
+#endif
+		// basically re-enable interrupt
+		//re-enable irq for this port(exit polling)
+		gem_enable_irq(gemdev, 	GEM_IRQ_RX_DONE_FLAG);
+
+#ifndef CONFIG_GLOBAL_POLLING
+                t4000_start_timer(dev);
+#endif /* !CONFIG_GLOBAL_POLLING */
+	}
+	else
+		work_done = budget;
+
+	return work_done;		//return received packet count
+}
+
+irqreturn_t c4k_eth_rx_interrupt(int irq, void *dev_id)
+{
+	struct net_device *dev = (struct net_device *)dev_id;
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+	GEM_DEVICE *gemdev = &priv->gemdev;
+	struct napi_struct *napi = &priv->napi;
+
+	unsigned irq_stat;
+
+	//if (unlikely(netif_msg_intr(priv)))
+	//	printk(KERN_DEBUG "%s %s: irq %d\n", dev->name, __func__, irq);
+	if (napi_schedule_prep(napi)) {
+		gem_mask_irq(gemdev, GEM_IRQ_RX_DONE_FLAG);
+		__napi_schedule(napi);
+	} else if (netif_running(dev)) {
+		gem_mask_irq(gemdev, GEM_IRQ_RX_DONE_FLAG);
+		printk(KERN_ERR "%s %s: bug! interrupt while in poll\n", dev->name, __func__);
+	}
+
+	irq_stat = gem_get_irq_stat(gemdev);
+	gem_set_irq_stat(gemdev, irq_stat);
+	return IRQ_HANDLED;
+}
+
+static const struct net_device_ops t4k_netdev_ops = {
+	.ndo_open               = c4k_eth_open,
+	.ndo_start_xmit		= c4k_eth_send_packet,
+	.ndo_stop               = c4k_eth_close,
+	.ndo_do_ioctl		= c4k_eth_ioctl,
+	.ndo_get_stats          = c4k_eth_get_stats,
+	.ndo_change_mtu         = c4k_eth_change_mtu,
+	.ndo_set_multicast_list = c4k_eth_set_multi,
+	.ndo_set_mac_address    = c4k_set_mac_address,
+};
+
+static int c4k_eth_probe(struct platform_device *pdev)
+{
+	struct net_device *dev = NULL;
+	struct eth_c4k_priv *priv = NULL;
+	GEM_DEVICE *gemdev;
+	struct transcede_eth_platform_data *einfo;
+	struct resource *r;
+	int idx, q_idx;
+	int err;
+
+	dev_dbg(&pdev->dev, "%s\n", __func__);
+
+	einfo = (struct transcede_eth_platform_data *) pdev->dev.platform_data;
+	if (!einfo) {
+		dev_err(&pdev->dev, "no additional platform data\n");
+		err = -ENODEV;
+		goto err0;
+	}
+
+	if(TRANSCEDE_TX_QUEUE_NUM == 1)
+		dev = alloc_etherdev(sizeof (*priv));
+	else
+		dev = alloc_etherdev_mqs(sizeof (*priv), TRANSCEDE_TX_QUEUE_NUM , TRANSCEDE_RX_QUEUE_NUM);
+	if (!dev) {
+		dev_err(&pdev->dev, "device allocation failed\n");
+		err = -ENOMEM;
+		goto err0;
+	}
+
+	priv = netdev_priv(dev);
+	priv->dev = dev;
+	gemdev = &priv->gemdev;
+
+	priv->id = pdev->id;
+	priv->einfo = einfo;
+	priv->phys_rx_int = platform_get_irq_byname(pdev, "irq");
+	if (priv->phys_rx_int < 0) {
+		dev_err(&pdev->dev, "no 'irq' number in platform data\n");
+		err = -EINVAL;
+		goto err1;
+	}
+
+	r = platform_get_resource_byname(pdev, IORESOURCE_MEM, "gemac");
+	if (!r) {
+		dev_err(&pdev->dev, "'gemac' base address in plaform data\n");
+		err = -EINVAL;
+		goto err1;
+	}
+
+	priv->baseaddr = __io_address(r->start);
+
+	/* Kernel may need to access registers before device is fully opened */
+	gemdev->gemac_baseaddr = priv->baseaddr;
+	gemdev->registers = priv->baseaddr + GEM_IP;
+
+	r = platform_get_resource_byname(pdev, IORESOURCE_MEM, "descriptors");
+	if (!r) {
+		dev_err(&pdev->dev, "no 'descriptors' address in platform data\n");
+		err = -EINVAL;
+		goto err1;
+	}
+
+	/* NOTE: assuming IRAM address passed via platform data, 1:1 static mapping */
+	priv->IRAM_baseaddr_pa = (void*) r->start;
+	priv->IRAM_baseaddr_v  = (void*) r->start;
+	dev_info(&pdev->dev, "0x%08X descriptors base for %u/%u rx/tx descriptors\n", r->start, MAX_RX_DESC_NT, MAX_TX_DESC_NT);
+	
+	priv->tx_queue_num = TRANSCEDE_TX_QUEUE_NUM;
+	for(q_idx = 0; q_idx < priv->tx_queue_num; q_idx++)
+	{
+		spin_lock_init(&priv->tx_queue[q_idx].txlock);
+		priv->tx_queue[q_idx].dev = dev;
+	}
+
+	spin_lock_init(&priv->rxlock);
+	spin_lock_init(&priv->txlock);
+
+	platform_set_drvdata(pdev, dev);
+
+	/* Copy the station address into the dev structure, */
+	memcpy(dev->dev_addr, einfo->mac_addr, MAC_ADDR_LEN);
+
+	err = dev_alloc_name(dev, einfo->name);
+	if (err < 0) {
+		dev_err(&pdev->dev, "cannot allocate net device name %s, aborting\n", einfo->name);
+		err = -EINVAL;
+		goto err2;
+	}
+	SET_NETDEV_DEV(dev, &pdev->dev);
+
+	/* Fill in the dev structure (mostly with our function pointers
+	 * to common device driver functions
+	 */
+	dev->netdev_ops		= &t4k_netdev_ops;
+	dev->mtu                = 1500;
+
+#ifdef GEM_TX_CHECKSUM_OFFLOAD_ENABLED
+	dev->features |= NETIF_F_HW_CSUM; // GEM in T4000/T3000 can support TCP, UDP, IPv4 and IPv6 checksums
+#endif
+	/* Setup c4k_eth_poll to be run on a regular basis for device polling operations
+	 * (see function above)
+	 */
+	netif_napi_add(dev, &priv->napi, c4k_eth_poll, 64);
+
+	dev->ethtool_ops = &c4k_ethtool_ops;
+
+	priv->flags = 0;
+
+#ifdef GEM_TX_CHECKSUM_OFFLOAD_ENABLED
+	priv->flags |= TX_CSUM_OFFLOAD_ENABLED;
+#endif
+
+#ifdef GEM_RX_CHECKSUM_OFFLOAD_ENABLED
+	priv->flags |= RX_CSUM_OFFLOAD_ENABLED;
+#endif
+
+//ifdef GEM_HW_CHECKSUM // define once supported
+//	priv->flags = RX_CSUM_OFFLOAD_ENABLED | TX_CSUM_OFFLOAD_ENABLED;
+//else
+//	priv->flags = 0;
+//endif
+
+	/* Enable most messages by default */
+	priv->msg_enable = NETIF_MSG_IFUP
+	                 | NETIF_MSG_IFDOWN
+	                 | NETIF_MSG_LINK
+	                 | NETIF_MSG_PROBE
+	                 | NETIF_MSG_INTR;
+
+	/* Register device driver with the system */
+	err = register_netdev(dev);
+	if (err) {
+		printk(KERN_ERR "%s: cannot register net device, aborting.\n", dev->name);
+		goto err2;
+	}
+
+	priv->RxRingSize = MAX_RX_DESC_NT;
+	priv->rx_coal_count = DEFAULT_RX_COAL_PKTS;
+	priv->rx_coal_time = DEFAULT_RX_COAL_TIME;
+
+#ifdef GEM_ADM_ENABLED
+	/* account for average queue depth lag relative to the instant queue depth
+	 * this is, in the worst case, given by the average window weigth, set to 8 bellow
+	 * keep 6 entries for reserved traffic */
+	/* Most of these are overwriten by qosapp later on */
+	writel(priv->RxRingSize - 2, priv->baseaddr + GEM_ADM_BLOCK + ADM_QFULLTHR);
+	writel((priv->RxRingSize - 8 - 6) << 8, priv->baseaddr + GEM_ADM_BLOCK + ADM_QDROPMAXTHR);
+	writel((priv->RxRingSize - 8 - 6 - 6) << 8, priv->baseaddr + GEM_ADM_BLOCK + ADM_QDROPMINTHR);
+#endif
+
+	/* Print out the device info */
+	printk(KERN_INFO "%s: ", dev->name);
+	for (idx = 0; idx < 6; idx++)
+		printk("%02x%c", dev->dev_addr[idx], idx == 5 ? '\n' : ':');
+
+	return 0;
+
+err2:
+	platform_set_drvdata(pdev, NULL);
+
+err1:
+	free_netdev(dev);
+err0:
+	return err;
+}
+
+static void c4k_adjust_link(struct net_device *dev)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+
+	unsigned long flags;
+	struct phy_device *phydev = priv->phydev;
+	int new_state = 0;
+
+	if (unlikely(netif_msg_drv(priv) && net_ratelimit()))
+		printk(KERN_DEBUG "%s c4k_adjust_link\n", dev->name);
+
+	spin_lock_irqsave(&priv->txlock, flags);
+	if (phydev->link) {
+		/* Now we make sure that we can be in full duplex mode.
+		 * If not, we operate in half-duplex mode. */
+		if (phydev->duplex != priv->oldduplex) {
+			new_state = 1;
+			c4k_gemac_setduplex(dev, phydev->duplex);
+			priv->oldduplex = phydev->duplex;
+
+		}
+
+		if (phydev->speed != priv->oldspeed) {
+			new_state = 1;
+			c4k_gemac_setspeed(dev, phydev->speed);
+			priv->oldspeed = phydev->speed;
+		}
+
+		if (!priv->oldlink) {
+			new_state = 1;
+			priv->oldlink = 1;
+			netif_start_queue(dev);
+		}
+
+	} else if (priv->oldlink) {
+		new_state       = 1;
+		priv->oldlink   = 0;
+		priv->oldspeed  = 0;
+		priv->oldduplex = -1;
+	}
+
+	if (new_state && netif_msg_link(priv))
+		phy_print_status(phydev);
+
+	spin_unlock_irqrestore(&priv->txlock, flags);
+}
+
+static int c4k_eth_remove(struct platform_device *pdev)
+{
+	struct net_device *dev = platform_get_drvdata(pdev);
+
+	unregister_netdevice(dev);
+
+	platform_set_drvdata(pdev, NULL);
+
+	/* Free the device, then exit */
+	free_netdev(dev);
+
+	return 0;
+}
+
+
+/** Structure for a device driver */
+static struct platform_driver c4k_eth_driver = {
+	.probe = c4k_eth_probe,
+	.remove = c4k_eth_remove,
+	.driver	= {
+		.name = "c4000-eth",
+	},
+};
+
+/**
+ *	This is a kernel command line parameter. The format is as following:
+ *	hwaddress=<interface name>,<mac address>,<interface name>,<mac address> ....
+ *	This parameter is mainly use when mounting the root filesytem over NFS.
+ *	For all the other cases, the MAC address will be given through the ifconfig application
+ *	i.e: ifconfig <interface name> hw ether <mac address>
+ */
+static int __init hwaddress_setup(char *str)
+{
+	int             index = 0;
+	int             i     = 0;
+	unsigned char * pchar = (unsigned char*) str;
+
+	for (i = 0; i < 2; i++) {
+		if (strncmp(pchar, "eth1", 4) == 0) {
+			pchar = strpbrk(pchar,",");
+			++pchar;
+			index = 0;
+
+			while (pchar && (index < ETH_ALEN)) {
+				if (pchar) {
+					unsigned char tmp = simple_strtol(pchar, NULL, 16);
+					transcede_gem0_pdata.mac_addr [index++] = (unsigned char)tmp;
+					pchar +=3;
+				}
+			}
+		} else if (strncmp(pchar, "eth2", 4) == 0) {
+			pchar = strpbrk(pchar,",");
+			++pchar;
+			index = 0;
+
+			while (pchar && (index < ETH_ALEN)) {
+				if (pchar) {
+					unsigned char tmp = simple_strtol(pchar, NULL, 16);
+					transcede_gem1_pdata.mac_addr [index++] = (unsigned char)tmp;
+					pchar +=3;
+				}
+			}
+		}
+	}
+
+	return 1;
+}
+
+__setup("hwaddress=", hwaddress_setup);
+
+static int __init ethpoll_setup(char *str)
+{
+    t4000_eth_rx_latency = simple_strtol(str, NULL, 10);
+
+#ifdef CONFIG_GLOBAL_POLLING
+    /* us -> ms with round up */
+    t4000_eth_rx_latency = (1000 - 1 + t4000_eth_rx_latency) / 1000;
+#endif	/* CONFIG_GLOBAL_POLLING */
+
+    if (t4000_eth_rx_latency != 0)
+    {
+	GEM_IRQ_RX_DONE_FLAG = 0;
+	RX_INT_FLAG=0;
+    }
+    return 1;
+}
+
+__setup("ethpoll=", ethpoll_setup);
+
+/** Standard linux module initialization function for C4K ethernet driver */
+static int __init c4k_eth_init(void)
+{
+	return platform_driver_register(&c4k_eth_driver);
+}
+
+/** Standard linux module exit function for C4K ethernet driver */
+static void __exit c4k_eth_exit(void)
+{
+	platform_driver_unregister(&c4k_eth_driver);
+}
+
+module_init(c4k_eth_init);
+module_exit(c4k_eth_exit);
+
+/* eof c4000_eth.c */
diff --git a/drivers/net/transcede/c4000_eth.h b/drivers/net/transcede/c4000_eth.h
new file mode 100644
index 0000000..819c3d6
--- /dev/null
+++ b/drivers/net/transcede/c4000_eth.h
@@ -0,0 +1,651 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+  *
+  * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+  *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+  *
+  * You should have received a copy of the GNU General Public License
+  * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef _C4000_ETH_H
+#define _C4000_ETH_H
+
+#include <linux/netdevice.h>
+#include <linux/mii.h>
+#include <linux/ethtool.h>
+#include "transcede_gemac.h"
+
+#define GEMAC_VERSION_STRING	"1.0"
+
+#if defined(CONFIG_COMCERTO_GEMAC_0) && defined(CONFIG_COMCERTO_GEMAC_1)
+/* for both ports enabled half the number of descriptors,
+ * note that RX descriptor is twice as big as TX (16 bytes vs 8 bytes)
+ */
+#define MAX_RX_DESC_NT      680		/**< Maximum number of Receive  descriptors */
+#define MAX_TX_DESC_NT      680		/**< Maximum number of Transmit descriptors */
+
+#else
+
+#define MAX_RX_DESC_NT      1360
+#define MAX_TX_DESC_NT      1360
+
+#endif
+
+#define DEFAULT_RX_COAL_TIME	500  /* us*/
+#define DEFAULT_RX_COAL_PKTS	8
+
+#define TRANSCEDE_TX_QUEUE_NUM	8
+#define TRANSCEDE_RX_QUEUE_NUM	1
+
+#define MAX_TX_DESC_QUEUE_0	(MAX_TX_DESC_NT*85 / 680)
+#define MAX_TX_DESC_QUEUE_1	(MAX_TX_DESC_NT*85 / 680)
+#define MAX_TX_DESC_QUEUE_2	(MAX_TX_DESC_NT*85 / 680)
+#define MAX_TX_DESC_QUEUE_3	(MAX_TX_DESC_NT*85 / 680)
+#define MAX_TX_DESC_QUEUE_4	(MAX_TX_DESC_NT*85 / 680)
+#define MAX_TX_DESC_QUEUE_5	(MAX_TX_DESC_NT*85 / 680)
+#define MAX_TX_DESC_QUEUE_6	(MAX_TX_DESC_NT*85 / 680)
+#define MAX_TX_DESC_QUEUE_7	(MAX_TX_DESC_NT*85 / 680)
+
+/** GEM receive descriptor data structure */
+struct tRXdesc 
+{
+	volatile u32 rx_data;          /**< Physical pointer to data to receive into */
+	volatile u32 rx_status;        /**< Receive descriptor status */
+	volatile u32 rx_extstatus;     /**< Receive descriptor IPSEC EXT status */
+	volatile u32 pad;              /**< 32 bits of pad, reserved for future use */
+};
+
+
+
+// gemac rx status, offset 0x4
+#define RX_STA_BCAST		(1UL<<31)
+#define RX_STA_MCAST		(1<<30)
+#define RX_STA_UM		(1<<29)
+#define GEMRX_WRAP 	(1<<28)
+#define RX_MAC_MATCH_FLAG 	(0x4<<25)
+#define RX_MAC_MATCH_NUM_MASK (0x3<<25)
+#define RX_MAC_MATCH_POS 	25
+#define RX_INT 				(1<<24)		/**< generate an interrupt when set */
+//#define RX_IPSEC_OUT		(1<<23)
+//#define RX_IPSEC_IN	 		(1<<22)
+#define RX_STA_VLAN 		(1<<21)
+#define RX_STA_VLAN_802p 	(1<<20)
+#define RX_STA_VLAN_PRI_MASK	(7<<17)
+#define RX_STA_VLAN_PRI_POS	17
+#define RX_STA_VLAN_CFI		(1<<16)
+#define RX_STA_EOF		(1<<15)
+#define RX_STA_SOF		(1<<14)
+#define RX_STA_PACKET		(RX_STA_SOF|RX_STA_EOF)
+#define RX_STA_CRCERR		(1<<13)
+#define RX_STA_LEN_MASK		0x1fff
+#define RX_STA_LEN_POS	0
+#define	RX_CHECK_ERROR		RX_STA_CRCERR
+
+// gemac rx extended status, offset 0x8
+#define RX_STA_L4OFF_MASK	(0xff<<24)
+#define RX_STA_L4OFF_POS	24
+#define RX_STA_L3OFF_MASK	(0xff<<16)
+#define RX_STA_L3OFF_POS	16
+#define GEMRX_OWN	(1<<15)
+
+#define RX_STA_L3_CKSUM	(1<<11)
+#define RX_STA_L3_GOOD		(1<<12)
+#define RX_STA_L4_CKSUM	(1<<13)
+#define RX_STA_L4_GOOD		(1<<14)
+
+#define RX_STA_TCP		(1<<9)
+#define RX_STA_UDP		(1<<8)
+#define RX_STA_IPV6		(1<<7)
+#define RX_STA_IPV4		(1<<6)
+#define RX_STA_PPPOE		(1<<5)
+#define RX_STA_WILLHANDLE	( RX_STA_IPV6 | RX_STA_IPV4)
+#define RX_STA_QinQ		(1<<4)
+#define RX_STA_TYPEID_MATCH_FLAG (0x8 << 0)
+#define RX_STA_TYPEID		(0x7 << 0)
+#define RX_STA_TYPEID_POS	0
+
+/** GEM Transmit descriptor structure */
+struct tTXdesc
+{
+	volatile u32 txdata;            /**< Physical pointer of data to transmit */
+	union
+	{
+		volatile u32 txctl;     /**< Transmit control for prior to transmit */
+		volatile u32 txstatus;  /**< Transmit status  after completion of transmit */
+	} ;
+};
+
+
+// offset 0x4
+#define GEMTX_USED_MASK 	(1UL<<31)
+#define GEMTX_WRAP		(1UL<<30)
+#define GEMTX_IE		(1UL<<29)
+#define GEMTX_FRAME_CORRUPTION (1UL<<28)
+#define GEMTX_FRAME_LATE_COLISSION (1UL<<27)
+#define GEMTX_OFFSET_MASK 	0xff
+#define GEMTX_OFFSET_SHIFT 	16
+#define GEMTX_LENGTH_MASK	0x1fff
+#define GEMTX_LENGTH_SHIFT	0
+#define GEMTX_LENGTH_MAX	0x1fff
+#define	GEMTX_L3_CSUM		(1UL<<26)
+#define	GEMTX_L4_CSUM		(1UL<<25)
+#define	GEMTX_FCS		(1UL<<24)
+#define	GEMTX_LAST		(1UL<<15)
+
+
+/////////////////////////////////////////////////////////////////////////
+// copied from c1k driver, no details yet
+// Exception path bits re-definition
+// bits 27/28/29 are used to sepcify QoS bits between CSP and FPP
+/////////////////////////////////////////////////////////////////////////
+#define GEMTX_EXPT_QOS_SHIFT	27
+#define GEMTX_EXPT_QOS_MASK	(7UL << GEMTX_EXPT_QOS_SHIFT)
+//bits 13/14 are used to identify IPsec packets -> usage of ext descriptors array
+#define GEMTX_EXPT_IPSEC_IN	(1UL<<13)
+#define GEMTX_EXPT_IPSEC_OUT	(1UL<<14)
+
+
+/** Expt extended descriptor (IPsec) */
+struct tTXextdesc
+{
+	volatile u16 SAhandle[2];
+	u32          RSVD;
+};
+
+/** Private driver flag for indicating GEMAC RX checksum offload is enabled */
+#define RX_CSUM_OFFLOAD_ENABLED		(1 << 0)  /* Receive  L3/L4 checksum offload feature enabled */
+
+/** Private driver flag for indicating GEMAC TX checksum offload is enabled */
+#define TX_CSUM_OFFLOAD_ENABLED		(1 << 1)  /* Transmit L3/L4 checksum offload feature enabled */
+
+/***********************************************************************
+**               Misc. useful Ethrnet contstants                       *
+************************************************************************/
+
+/* AKB: IEEE 1588v2 Precision timing protocol mis. constants */
+
+/* Event messages (TX & RX timestamping required, UDP port 0x13F)*/
+#define SYNC_MESSAGE                  0x0
+#define DELAY_REQ_MESSAGE             0x1
+#define PDELAY_REQ_MESSAGE            0x2
+#define PDELAY_RESP_MESSAGE           0x3
+
+ /* Reserved 0x4-0x7 */
+
+ /* General messages (No timestamping necessary, UDP port 0x140) */
+ #define FOLLOWUP_MESSAGE             0x8
+ #define DELAY_RESP_MESSAGE           0x9
+ #define PDELAY_RESP_FOLLOWUP_MESSAGE 0xA
+ #define ANNOUNCE_MESSAGE             0xB
+ #define SIGNALING_MESSAGE            0xC
+ #define MANAGEMENT_MESSAGE           0xD
+
+/* Common Ethernet Ethertypes */
+#define ETHERTYPE_IPV4                  0x0800  /**< @brief Internet Protocol version 4 */
+#define ETHERTYPE_ARP                   0x0806  /**< @brief IP Address Resolution Protocol */
+#define ETHERTYPE_IPV6                  0x86DD  /**< @brief Internet Protocol version 6 */
+#define ETHERTYPE_VLAN                  0x8100  /**< @brief VLAN 802.1Q */
+#define ETHERTYPE_PTP                   0x88F7  /**< @brief Precision Timing Protocol Ethernet direct encapsulation */
+#define ETHERTYPE_EXPERIMENTAL_1        0x88B5  /**< @brief IEEE 802 local experimental Ethertype #1 */
+#define ETHERTYPE_EXPERIMENTAL_2        0x88B6  /**< @brief IEEE 802 local experimental Ethertype #2 */
+
+#define BYTE_REVERSE_ETHERTYPE(ethertype) ((UINT16) (((ethertype & 0xFF)<<8) | (ethertype>>8)))
+
+#define MAC_ADDRESS_SIZE                6
+#define MAC_ADDRESSES_FIELD_SIZE        12
+#define ETHERTYPE_SIZE                  2
+
+// AKB: Begin copied from ethdrv.h
+
+// Network Control Register offset 0x000
+
+#define GEM_NETCTRL_TX_LB               (1 << 0)    /**< Transmit loopback */
+#define GEM_NETCTRL_RX_LB               (1 << 1)    /**< Receive  loopback */
+#define GEM_NETCTRL_RX_ENB              (1 << 2)    /**< Receive  enable */
+#define GEM_NETCTRL_TX_ENB              (1 << 3)    /**< Transmit enable */
+#define GEM_NETCTRL_MDIO_ENB            (1 << 4)    /**< MDIO     enable */
+#define GEM_NETCTRL_CLR_STAT            (1 << 5)    /**< Clear statistics */
+#define GEM_NETCTRL_INC_STAT            (1 << 6)    /**< Increment all statistics register by one    (test only) */
+#define GEM_NETCTRL_STAT_WR_ENB         (1 << 7)    /**< Allow write access for statistics registers (test only) */
+#define GEM_NETCTRL_START_TX            (1 << 9)    /**< Start transmission (1 starts TX) */
+#define GEM_NETCTRL_HALT_TX             (1 << 10)   /**< Halt  transmission after current TX frame if any (1 stops  TX) */
+#define GEM_NETCTRL_TX_PAUSE            (1 << 11)   /**< Transmit a single pause frame */
+#define GEM_NETCTRL_TX_ZERO_PAUSE       (1 << 12)   /**< Transmit a single pause frame with zero quantum */
+#define GEM_NETCTRL_SNAPSHOT            (1 << 13)   /**< Record current statistics in snapshot registers, clear stats registers */
+#define GEM_NETCTRL_READ_SNAPSHOT       (1 << 14)   /**< Read Snapshot (1) or current (0) statsistics register */
+#define GEM_NETCTRL_STORE_TIMESTAMP     (1 << 15)   /**< Setting this bit stores RX timestamps to CRC field */
+
+// Network Configuration Register offset 0x004
+
+#define GEM_NETCFG_100BM                (1 << 0)    /**< Speed - set to logic one to indicate 100Mbps */
+#define GEM_NETCFG_FULL_DUPLEX          (1 << 1)    /**< Full duplex */
+#define GEM_NETCFG_DISCARD_NON_VLAN     (1 << 2)    /**< Discard non-VLAN frames */
+#define GEM_NETCFG_JUMBO_ENB            (1 << 3)    /**< Jumbo frames */
+#define GEM_NETCFG_COPY_ALL             (1 << 4)    /**< Copy all frames */
+#define GEM_NETCFG_NO_BROADCAST         (1 << 5)    /**< No broadcast */
+#define GEM_NETCFG_MULTICAST_ENB        (1 << 6)    /**< Multicast hash enable */
+#define GEM_NETCFG_UNICAST_ENB          (1 << 7)    /**< Unicast hash enable */
+#define GEM_NETCFG_1536_ENB             (1 << 8)    /**< Receive 1536 byte frames */
+#define GEM_NETCFG_EXT_ADDR_ENB         (1 << 9)    /**< External address match enable */
+#define GEM_NETCFG_1GB_MODE             (1 << 10)   /**< setting this bit configures the GEM for 1000 Mbps operation */
+#define GEM_NETCFG_TBI_ENB              (1 << 11)   /**< 1: TBI enabled, GMII/MII disabled */
+#define GEM_NETCFG_PAUSE_ENB            (1 << 13)   /**< Pause enable */
+#define GEM_NETCFG_RX_OFFS_MASK         (3 << 14)   /**< Receive buffer offset */
+#define GEM_NETCFG_VALID_L3_CHKS        (1 << 14)   /**< Validate L3 checksum */
+#define GEM_NETCFG_DROP_BAD_L3          (1 << 15)   /**< Drop packet in case of bad check */
+#define GEM_NETCFG_NO_LEN_ERR           (1 << 16)   /**< Length field error frame discard */
+#define GEM_NETCFG_FCS_REMOVE           (1 << 17)   /**< FCS remove */
+#define GEM_NETCFG_VALID_L4_CHKS        (1 << 21)   /**< Validate L4 checksum */
+#define GEM_NETCFG_DROP_BAD_L4          (1 << 22)   /**< Drop packet in case of bad check */
+#define GEM_NETCFG_DONOT_COPY_PAUSE     (1 << 23)
+#define GEM_NETCFG_CALC_CHKS_ENB        (1 << 24)   /**< Receive checksum offload enable */
+#define GEM_NETCFG_RX_HALF_ENB          (1 << 25)   /**< Enable frames to be received in half-duplex mode while transmitting */
+#define GEM_NETCFG_IGNORE_RX_FCS        (1 << 26)   /**< Ignore RX FCS - when set frames with FCS/CRC errors will not be rejected */
+#define GEM_NETCFG_SGMII_MODE           (1 << 27)   /**< SGMII mode enable */
+#define GEM_NETCFG_IPG                  (1 << 28)   /**< IPG stretch enable */
+#define GEM_NETCFG_BAD_PREAMBLE         (1 << 29)   /**< Receive bad preamble */
+#define GEM_NETCFG_IGNORE_RX_ERR        (1 << 30)   /**< Ignore RX_ER when RX_DV is low */
+#define GEM_NETCFG_UNIDIR               (1UL << 31) /**< Uni-direction-enable */
+
+// DMA Configuration Register
+
+#define GEM_DMA_BURST_SINGLE            (1 << 0)    /**< 00001: Always use SINGLE AHB bursts */
+#define GEM_DMA_BURST_INCR4             (1 << 2)    /**< 001xx: Attempt to use INCR4 AHB bursts */
+#define GEM_DMA_BURST_INCR8             (1 << 3)    /**< 01xxx: Attempt to use INCR8 AHB bursts */
+#define GEM_DMA_BURST_INCR16            (1 << 4)    /**< 1xxxx: Attempt to use INCR16 AHB bursts */
+#define GEM_DMA_BIG_ENDIAN              (1 << 7)    /**< AHB endian swap mode enable */
+#define GEM_DMA_RX_PB_1K                (0 << 8)    /**< 00: Do not use top three address bits (1 Kb) */
+#define GEM_DMA_RX_PB_2K                (1 << 8)    /**< 01: Do not use top two address bits (2 Kb) */
+#define GEM_DMA_RX_PB_4K                (2 << 8)    /**< 10: Do not use top address bit (4 Kb) */
+#define GEM_DMA_RX_PB_8K                (3 << 8)    /**< 11: Use full configured addressable space (8 Kb) */
+#define GEM_DMA_TX_PB_2K                (0 << 10)   /**< 0: Do not use top address bit (2 Kb) */
+#define GEM_DMA_TX_PB_4K                (1 << 10)   /**< 1: Use full configured addressable space (4 Kb) */
+#define GEM_DMA_CHK_SUM_ENB             (1 << 11)   /**< Transmitter IP, TCP and UDP checksum generation offload enable */
+#define GEM_DMA_ADM_MGR_ENB             (1 << 12)   /**< Enable Admittance Manager */
+#define GEM_DMA_RX_64                   (1 << 16)   /**< DMA receive  64 bytes */
+#define GEM_DMA_RX_128                  (2 << 16)   /**< DMA receive 128 bytes */
+#define GEM_DMA_RX_192                  (3 << 16)   /**< DMA receive 192 bytes */
+#define GEM_DMA_RX_256                  (4 << 16)   /**< DMA receive 256 bytes */
+#define GEM_DMA_RX_320                  (5 << 16)   /**< DMA receive 320 bytes */
+#define GEM_DMA_RX_384                  (6 << 16)   /**< DMA receive 384 bytes */
+#define GEM_DMA_RX_448                  (7 << 16)   /**< DMA receive 448 bytes */
+#define GEM_DMA_RX_512                  (8 << 16)   /**< DMA receive 512 bytes */
+#define GEM_DMA_DEF_RX_SIZE(size)       ((((size)   >>  6) & 0xFF) << 16) /**< Macro to define (set) DMA receive size */
+#define GEM_DMA_GET_RX_SIZE(dmareg)     ((((dmareg) >> 16) & 0xFF) *  64) /**< Macro to get (read)   DMA receive size */
+#define GEM_DMA_BREAK_BURST_EARLY       (1 << 24)   /**< Break Bursts Early on EOP access and when about to cross 1k boundary */
+#define GEM_DMA_RX_SW_ALLOC             (1 << 25)   /**< Enable RX DMA Software Buffer Allocation Mode */
+#define GEM_DMA_TX_SW_ALLOC             (1 << 26)   /**< Enable TX DMA Software Buffer Allocation Mode */
+#define GEM_DMA_START_BURST_16B         (1 << 27)   /**< Start Bursts on 16 byte boundary enable */
+#define GEM_DMA_SCHEDULER_ENB           (1UL << 31) /**< DMA Scheduler block enabled */
+
+// Transmit Status Register
+
+#define GEM_TX_STAT_USED_WAS_READ       (1 << 0)    /**< Used bit read */
+#define GEM_TX_STAT_COLLISION           (1 << 1)    /**< Collision occurred */
+#define GEM_TX_STAT_RETRY_EXCEEDED      (1 << 2)    /**< Retry limit exceeded */
+#define GEM_TX_STAT_GO                  (1 << 3)    /**< Transmit go, if high transmit is active */
+#define GEM_TX_STAT_AHB_ERR             (1 << 4)    /**< Transmit frame corruption due to AHB error */
+#define GEM_TX_STAT_COMPLETE            (1 << 5)    /**< Transmit complete - set when a frame has been transmitted */
+#define GEM_TX_STAT_UNDERRUN            (1 << 6)    /**< Transmit under run */
+#define GEM_TX_STAT_LATE_COLLISION      (1 << 7)    /**< Late collision occurred */
+#define GEM_TX_STAT_HRESP_ERR           (1 << 8)    /**< Hresp not OK - set when the DMA block sees hresp not OK */
+
+#define GEM_TX_ERROR_MASK               (0                          |\
+                                        GEM_TX_STAT_COLLISION       |\
+                                        GEM_TX_STAT_RETRY_EXCEEDED  |\
+                                        GEM_TX_STAT_AHB_ERR         |\
+                                        GEM_TX_STAT_UNDERRUN        |\
+                                        GEM_TX_STAT_LATE_COLLISION  |\
+                                        GEM_TX_STAT_HRESP_ERR       |\
+                                        0)
+
+
+// Receive Status Register
+
+#define GEM_RX_STAT_NO_BUF              (1 << 0)    /**< Buffer not available */
+#define GEM_RX_STAT_FRM_RECV            (1 << 1)    /**< Frame received       */
+#define GEM_RX_STAT_OVERRUN             (1 << 2)    /**< Receive overrun      */
+#define GEM_RX_STAT_HRESP_ERR           (1 << 3)    /**< Hresp not OK         */
+
+#define GEM_RX_ERROR_MASK               (0                          |\
+                                        GEM_RX_STAT_NO_BUF          |\
+                                        GEM_RX_STAT_OVERRUN         |\
+                                        GEM_RX_STAT_HRESP_ERR       |\
+                                        0)
+
+//
+// IRQ ENABLE, IRQ DISABLE, IRQ STATUS
+//
+// GEM base interrupts
+#define GEM_INT_BIT_PHY_COMPLETE            0       /**< @brief Management frame sent - the PHY maintenance register has completed its operation */
+#define GEM_INT_BIT_RX_COMPLETE             1       /**< @brief Receive complete - a frame has been stored in memory */
+#define GEM_INT_BIT_RX_USED_BIT_READ        2       /**< @brief RX used bit read - set when a receive buffer descriptor is read with its used bit set */
+#define GEM_INT_BIT_TX_USED_BIT_READ        3       /**< @brief TX used bit read - set when a transmit buffer descriptor is read with its used bit set */
+#define GEM_INT_BIT_TX_UNDERRUN             4       /**< @brief Transmit under run */
+#define GEM_INT_BIT_RETRY_EXCEEDED          5       /**< @brief Retry limit exceeded or late collision - transmit error */
+#define GEM_INT_BIT_AHB_ERR                 6       /**< @brief Transmit frame corruption due to AHB error */
+#define GEM_INT_BIT_TX_COMPLETE             7       /**< @brief Transmit complete */
+#define GEM_INT_BIT_LINK_CHANGE             9       /**< @brief Link change */
+#define GEM_INT_BIT_RX_OVERRUN              10      /**< @brief Receive overrun */
+#define GEM_INT_BIT_HRESP_ERR               11      /**< @brief Hresp not OK */
+#define GEM_INT_BIT_RX_PAUSE_FRM            12      /**< @brief Pause frame with non-zero pause quantum received */
+#define GEM_INT_BIT_PAUSE_ZERO              13      /**< @brief Pause time zero */
+#define GEM_INT_BIT_TX_PAUSE_FRM            14      /**< @brief Pause frame transmitted */
+#define GEM_INT_BIT_EXTERNAL                15      /**< @brief External interrupt */
+#define GEM_INT_BIT_PCS_COPLETE             16      /**< @brief PCS auto-negotiation complete */
+#define GEM_INT_BIT_PCS_LINK_RECEIVED       17      /**< @brief PCS link partner page received */
+
+// Timestamp interrupts for PTP/IEEE-1588
+#define GEM_INT_BIT_TSU_RX_PTP_DLY_REQ      18      /**< @brief PTP delay_request   frame received */
+#define GEM_INT_BIT_TSU_RX_PTP_SYNC_FRM     19      /**< @brief PTP sync            frame received */
+#define GEM_INT_BIT_TSU_TX_SYNC_FRM         20      /**< @brief PTP sync            frame transmitted */
+#define GEM_INT_BIT_TSU_TX_PDLY_REQ         21      /**< @brief PTP pdelay_request  frame transmitted */
+#define GEM_INT_BIT_TSU_RX_PTP_PDLY_REQ     22      /**< @brief PTP pdelay_request  frame received */
+#define GEM_INT_BIT_TSU_RX_PTP_PDLY_RESP    23      /**< @brief PTP pdelay_response frame received */
+#define GEM_INT_BIT_TSU_TX_PTP_DLY_REQ      24      /**< @brief PTP delay_request   frame transmitted */
+#define GEM_INT_BIT_TSU_TX_PDLY_RESP        25      /**< @brief PTP pdelay_response frame transmitted */
+#define GEM_INT_BIT_TSU_SECONDS_INCREMENT   26      /**< @brief PTP Seconds counter has incremented */
+
+#define GEM_INT_PHY_COMPLETE            (1 << 0)    /**< @brief Management frame sent - the PHY maintenance register has completed its operation */
+#define GEM_INT_RX_COMPLETE             (1 << 1)    /**< @brief Receive complete - a frame has been stored in memory */
+#define GEM_INT_RX_USED_BIT_READ        (1 << 2)    /**< @brief RX used bit read - set when a receive buffer descriptor is read with its used bit set */
+#define GEM_INT_TX_USED_BIT_READ        (1 << 3)    /**< @brief TX used bit read - set when a transmit buffer descriptor is read with its used bit set */
+#define GEM_INT_TX_UNDERRUN             (1 << 4)    /**< @brief Transmit under run */
+#define GEM_INT_RETRY_EXCEEDED          (1 << 5)    /**< @brief Retry limit exceeded or late collision - transmit error */
+#define GEM_INT_BUS_ERR                 (1 << 6)    /**< @brief Transmit frame corruption due to AHB error */
+#define GEM_INT_TX_COMPLETE             (1 << 7)    /**< @brief Transmit complete */
+#define GEM_INT_NOT_USED_8              (1 << 8)    /**< @brief Not used, reserved for possible future use */
+#define GEM_INT_LINK_CHANGE             (1 << 9)    /**< @brief Link change */
+#define GEM_INT_RX_OVERRUN              (1 << 10)   /**< @brief Receive overrun */
+#define GEM_INT_HRESP_ERR               (1 << 11)   /**< @brief Hresp not OK */
+#define GEM_INT_RX_PAUSE_FRM            (1 << 12)   /**< @brief Pause frame with non-zero pause quantum received */
+#define GEM_INT_PAUSE_ZERO              (1 << 13)   /**< @brief Pause time zero */
+#define GEM_INT_TX_PAUSE_FRM            (1 << 14)   /**< @brief Pause frame transmitted */
+#define GEM_INT_EXTERNAL                (1 << 15)   /**< @brief External interrupt */
+#define GEM_INT_PCS_COMPLETE            (1 << 16)   /**< @brief PCS auto-negotiation complete */
+#define GEM_INT_PCS_LINK_RECEIVED       (1 << 17)   /**< @brief PCS link partner page received */
+
+#define GEM_INT_TSU_RX_PTP_DLY_REQ      (1 << 18)   /**< @brief PTP delay_request   frame received */
+#define GEM_INT_TSU_RX_PTP_SYNC_FRM     (1 << 19)   /**< @brief PTP sync frame received */
+#define GEM_INT_TSU_TX_PTP_DLY_REQ      (1 << 20)   /**< @brief PTP delay_request   frame transmitted */
+#define GEM_INT_TSU_TX_PTP_SYNC_FRM     (1 << 21)   /**< @brief PTP sync            frame transmitted */
+#define GEM_INT_TSU_RX_PTP_PDLY_REQ     (1 << 22)   /**< @brief PTP pdelay_request  frame received */
+#define GEM_INT_TSU_RX_PTP_PDLY_RESP    (1 << 23)   /**< @brief PTP pdelay_response frame received */
+#define GEM_INT_TSU_TX_PTP_PDLY_REQ     (1 << 24)   /**< @brief PTP pdelay_request  frame transmitted */
+#define GEM_INT_TSU_TX_PTP_PDLY_RESP    (1 << 25)   /**< @brief PTP pdelay_response frame transmitted */
+#define GEM_INT_TSU_SECONDS_INCREMENT   (1 << 26)   /**< @brief PTP Seconds counter has incremented */
+
+
+#define GEM_CFG_MODE_SEL_PIN            (0 << 0)
+#define GEM_CFG_MODE_SEL_GEM            (1 << 0)
+#define GEM_CFG_MODE_GEM_MASK           (7 << 1)
+#define GEM_CFG_MODE_GEM_MII            (0 << 1)
+#define GEM_CFG_MODE_GEM_GMII           (1 << 1)
+#define GEM_CFG_MODE_GEM_RMII           (2 << 1)
+#define GEM_CFG_MODE_GEM_RGMII          (3 << 1)
+#define GEM_CFG_MODE_GEM_SGMII          (4 << 1)
+#define GEM_CFG_MODE_GEM_SMII           (6 << 1)
+#define GEM_CFG_MODE_GEM_TBI            (7 << 1)
+
+#define GEM_CFG_MODE_PIN_MASK           (7 << 4)
+#define GEM_CFG_MODE_PIN_MII            (0 << 4)
+#define GEM_CFG_MODE_PIN_GMII           (1 << 4)
+#define GEM_CFG_MODE_PIN_RMII           (2 << 4)
+#define GEM_CFG_MODE_PIN_RGMII          (3 << 4)
+#define GEM_CFG_MODE_PIN_SGMII          (4 << 4)
+#define GEM_CFG_MODE_PIN_SMII           (6 << 4)
+#define GEM_CFG_MODE_PIN_TBI            (7 << 4)
+
+#define GEM_CFG_DUPLEX_SEL_PHY          (0 << 8)
+#define GEM_CFG_DUPLEX_SEL_GEM          (1 << 8)
+#define GEM_CFG_DUPLEX_GEM_HALF         (0 << 9)
+#define GEM_CFG_DUPLEX_GEM_FULL         (1 << 9)
+#define GEM_CFG_DUPLEX_PHY_HALF         (0 << 10)
+#define GEM_CFG_DUPLEX_PHY_FULL         (1 << 10)
+#define GEM_CFG_SPEED_SEL_PHY           (0 << 11)
+#define GEM_CFG_SPEED_SEL_GEM           (1 << 11)
+#define GEM_CFG_SPEED_MASK              (3 << 12)
+#define GEM_CFG_SPEED_GEM_10M           (0 << 12)
+#define GEM_CFG_SPEED_GEM_100M          (1 << 12)
+#define GEM_CFG_SPEED_GEM_1G            (2 << 12)
+#define GEM_CFG_SPEED_PHY_10M           (0 << 14)
+#define GEM_CFG_SPEED_PHY_100M          (1 << 14)
+#define GEM_CFG_SPEED_PHY_1G            (2 << 14)
+#define GEM_CFG_PHY_LINK_DOWN           (0 << 16)
+#define GEM_CFG_PHY_LINK_UP             (1 << 16)
+#define GEM_CFG_GEM_LOOPBACK            (1 << 17)
+
+/******************************************************************************
+*       Definition of the GEM 1588 timer and PTP control macros               *
+*******************************************************************************/
+
+#define GEM_1588_NANSECONDS_MASK        0x3FFFFFFF  /**< @brief Mask for all nanosecond based registers */
+#define GEM_1588_GET_NANOSECONDS(reg)   (*((UINT32*)reg) & GEM_1588_NANOSECONDS_MASK)
+#define GEM_1588_GET_SECONDS(reg)       (*((UINT32*)reg))
+
+// 1588 Timer Adjust register: 0x1D8
+#define GEM_1588_TMR_ADJUST_ADD         (0UL << 31)
+#define GEM_1588_TMR_ADJUST_SUBTRACT    (1UL << 31)
+#define GEM_1588_TMR_ADJUST_RSVD        (0UL << 30)
+
+// 1588 Timer Increment register: 0x1DC
+#define GEM_1588_TME_INCR_RSVD_MASK     (0xFF << 24)
+#define GEM_1588_TMR_INCR_ALT_NUM_MASK  (0xFF << 16)
+#define GEM_1588_TMR_INCR_ALT_NSECS     (0xFF <<  8)
+#define GEM_1588_TMR_INCR_NSEC_COUNT    (0xFF)
+
+#define GEM_1588_TMR_INCR_SET(altnum, altnsecs, nsecs) ((altnum   << 16) |\
+                                                        (altnsecs <<  8) |\
+                                                        nsecs             \
+                                                       )
+
+#define NANOSECONDS_PER_SECOND 1000000000UL
+#define SECONDS_AND_NANOSECONDS_TO_NANOSECONDS(secs,nsecs) ((((UINT64)(secs)) * NANOSECONDS_PER_SECOND) + nsecs)
+
+
+/******************************************************************************
+*       Definition of the GEM DMA buffer descriptor and macros                *
+*******************************************************************************/
+
+// RX status flags, WORD-1 of the buffer descriptor
+
+#define GEM_DMA_RX_BROADCAST_BIT            (1UL << 31)
+#define GEM_DMA_RX_MULTICAST_BIT            (1 << 30)
+#define GEM_DMA_RX_UNICAST_BIT              (1 << 29)
+#define GEM_DMA_RX_WRAP_BIT                 (1 << 28)
+#define GEM_DMA_RX_SPEC_ADDR_MASK           (7 << 25)
+#define GEM_DMA_RX_IRQ_ENB                  (1 << 24) /**< Generate an interrupt after this descriptor is populated */
+#define GEM_DMA_RX_VLAN_BIT                 (1 << 21)
+#define GEM_DMA_RX_PRIOR_BIT                (1 << 20)
+#define GEM_DMA_RX_VLAN_PRIOR_MASK          (7 << 17)
+#define GEM_DMA_RX_CFI_BIT                  (1 << 16)
+#define GEM_DMA_RX_EOF_BIT                  (1 << 15) /**< end of frame bit */
+#define GEM_DMA_RX_SOF_BIT                  (1 << 14) /**< start of frame bit */
+#define GEM_DMA_RX_CRC_STAT                 (1 << 13)
+#define GEM_DMA_RX_LEN_MASK                 (0x01FFF)
+
+// RX status flags, WORD-2 of the buffer descriptor
+
+#define GEM_DMA_RX_L4_OFFS_MASK             (0xFFUL << 24)
+#define GEM_DMA_RX_L3_OFFS_MASK             (0xFFUL << 16)
+#define GEM_DMA_RX_READY_BIT                (1 << 15)
+#define GEM_DMA_RX_L4_TCP                   (1 << 9)
+#define GEM_DMA_RX_L4_UDP                   (1 << 8)
+#define GEM_DMA_RX_L3_IPV6                  (1 << 7)
+#define GEM_DMA_RX_L3_IPV4                  (1 << 6)
+#define GEM_DMA_RX_PPPoE                    (1 << 5)
+#define GEM_DMA_RX_STACKED_VLAN             (1 << 4)
+#define GEM_DMA_RX_TYPE_MASK                (0xFF  )
+
+
+// access to the WORD-1 bits of the RX buffer descriptor
+
+#define GEM_DMA_RX_SET_ADDR(d, addr)        ((d)->BPtr = (addr))
+
+#define GEM_DMA_RX_SET_WRAP(d)              ((d)->Descr    |=   GEM_DMA_RX_WRAP_BIT)
+#define GEM_DMA_RX_IS_WRAP(d)               ((d)->Descr    &    GEM_DMA_RX_WRAP_BIT)
+#define GEM_DMA_RX_IS_READY(d)              ((d)->ExDescr  &    GEM_DMA_RX_READY_BIT)
+#define GEM_DMA_RX_SET_READY(d)             ((d)->ExDescr  &=  ~GEM_DMA_RX_READY_BIT)
+
+#define GEM_DMA_RX_GET_SPEC_ADDR(d)         (((d)->Descr   &    GEM_DMA_RX_SPEC_ADDR_MASK) >> 25)
+#define GEM_DMA_RX_GET_TYPE_ID(d)           (((d)->ExDescr &    GEM_DMA_RX_TYPE_MASK))
+#define GEM_DMA_RX_IS_EOF(d)                (((d)->Descr   &    GEM_DMA_RX_EOF_BIT))
+#define GEM_DMA_RX_IS_SOF(d)                (((d)->Descr   &    GEM_DMA_RX_SOF_BIT))
+#define GEM_DMA_RX_CLR_EOF(d)               (((d)->Descr   &=  ~GEM_DMA_RX_EOF_BIT))
+#define GEM_DMA_RX_CLR_SOF(d)               (((d)->Descr   &=  ~GEM_DMA_RX_SOF_BIT))
+#define GEM_DMA_RX_CLR_EOF_SOF(d)           (((d)->Descr   &= ~(GEM_DMA_RX_SOF_BIT | GEM_DMA_RX_EOF_BIT)))
+#define GEM_DMA_RX_GET_LEN(d)               (((d)->Descr   &    GEM_DMA_RX_LEN_MASK))
+#define GEM_DMA_RX_CLR_STATUS(d)            (d)->Descr = ((d)->Descr & GEM_DMA_RX_WRAP_BIT); (d)->ExDescr = 0;
+
+// TX status flags, WORD-1 of the buffer descriptor
+
+#define GEM_DMA_TX_USED_BIT                 (1UL << 31) /**< Used - must be zero for the GEM to read data to the transmit buffer */
+#define GEM_DMA_TX_WRAP                     (1 << 30)   /**< Wrap - marks last descriptor in transmit buffer descriptor list */
+#define GEM_DMA_TX_RETRY_EXCEEDED           (1 << 29)   /**< Retry limit exceeded, transmit error detected */
+#define GEM_DMA_TX_IRQ_ENB                  (1 << 29)   /**< Global TX Interrupts Enable */
+#define GEM_DMA_TX_FRM_CORRUPT              (1 << 28)   /**< Transmit Frame Corruption */
+#define GEM_DMA_TX_LATE_COLLISION           (1 << 27)   /**< Late collision, transmit error detected */
+#define GEM_DMA_TX_NO_L3_L4                 (0 << 25)   /**< Do Not Generate L3 or L4 header checksums */
+#define GEM_DMA_TX_L3_ADDED                 (2 << 25)   /**< Generate L3 checksum */
+#define GEM_DMA_TX_L4_ADDED                 (1 << 25)   /**< Generate L4 checksum */
+#define GEM_DMA_TX_L3_L4_ADDED              (3 << 25)   /**< Generate both L3 and L4 checksums */
+#define GEM_DMA_TX_CRC                      (1 << 24)   /**< Generate CRC for frame */
+#define GEM_DMA_TX_OFFS_MASK                (0xFF << 16)/**< Buffer Offset OR TCP/IP checksum generation status */
+#define GEM_DMA_TX_LAST_BUF                 (1 << 15)   /**< Last buffer */
+#define GEM_DMA_TX_LEN_MASK                 (0x01FFF)   /**< Length of buffer [12:0] */
+
+// access to the WORD-1 bits of the TX buffer descriptor
+
+#define GEM_DMA_TX_IS_USED(d)               ( (d)->Descr   &     GEM_DMA_TX_USED_BIT)
+#define GEM_DMA_TX_SET_USED(d)              ( (d)->Descr   |=    GEM_DMA_TX_USED_BIT)
+#define GEM_DMA_TX_IS_SCHEDULED(d)          (!((d)->Descr  &     GEM_DMA_TX_USED_BIT))
+#define GEM_DMA_TX_CLR_USED(d)              ( (d)->Descr   &=   ~GEM_DMA_TX_USED_BIT)
+#define GEM_DMA_TX_SET_WRAP(d)              ( (d)->Descr   |=    GEM_DMA_TX_WRAP)
+#define GEM_DMA_TX_IS_WRAP(d)               ( (d)->Descr   &     GEM_DMA_TX_WRAP)
+#define GEM_DMA_TX_CHKS_RES(d)              ( ((d)->Descr  >> 16) & 0xFF)
+#define GEM_DMA_TX_SET_LAST(d)              ( (d)->Descr   |=    GEM_DMA_TX_LAST_BUF)
+#define GEM_DMA_TX_IS_LAST(d)               ( (d)->Descr   &     GEM_DMA_TX_LAST_BUF)
+#define GEM_DMA_TX_CLR_LAST(d)              ( (d)->Descr   &=   ~GEM_DMA_TX_LAST_BUF)
+#define GEM_DMA_TX_SET_LEN(d, l)            ( (d)->BPtr = ((l) & GEM_DMA_TX_LEN_MASK))
+
+#define GEM_DMA_TX_IS_ERR(d)                ((d)->Descr & (0                    |\
+                                            GEM_DMA_TX_RETRY_EXCEEDED           |\
+                                            GEM_DMA_TX_LATE_COLLISION           |\
+                                            GEM_DMA_TX_FRM_CORRUPT              |\
+                                            GEM_DMA_TX_LATE_COLLISION           |\
+                                            0))
+
+
+struct transcede_tx_q {
+	spinlock_t txlock __attribute__ ((aligned (SMP_CACHE_BYTES)));
+	struct tTXdesc		*TxBase;			/**< Base pointer to transmit descriptor ring */
+	struct tTXextdesc	*TxextBase;			/**< Pointer to IPSEC EXT descriptor ring */
+
+	struct sk_buff		**TxSkbRing;			/**< Transmit socket buffer ring */
+	u32			TxRingSize;			/**< Transmit ring size */
+	u32			TxMaxRingSize;			/**< Transmit ring max size */
+	u32			Txtosend;			/**< Index for transmit ring current "to send" index */
+	u32			Txdone;
+	int			Txavail;			/**< Number of TX ring slots currently available for use */
+	struct net_device	*dev;				/**< Pointer to standard linux device driver control strucutre */
+	u32			tx_gem_queue_base;
+};
+
+// AKB: End copied from ethdrv.h
+
+/** C4K Ethernet private control data for device driver */
+struct eth_c4k_priv
+{
+	struct tRXdesc		*RxBase;                     /**< Base Pointer to receive descriptor ring */
+	struct sk_buff		*RxSkbRing[MAX_RX_DESC_NT];  /**< Receive socket buffer ring */
+	u32			RxRingSize;                 /**< Size of receive ring */
+	u32			RxtocleanIndex;             /**< Index for receive ring needed to clean */
+	u32			RxtofillIndex;              /**< Index for receive ring to fill (with socket buffers) */
+	spinlock_t		rxlock;                     /**< Receive lock for calls to spin lock */
+	int			rx_coal_count;
+	int			rx_coal_time;
+
+	spinlock_t		txlock;                     /**< Transmit lock for calls to spin lock */
+	struct tTXdesc		*TxBase;                     /**< Base pointer to transmit descriptor ring */
+	struct tTXextdesc	*TxextBase;                  /**< Pointer to IPSEC EXT descriptor ring */
+
+	u32			tx_queue_num;
+	struct transcede_tx_q	tx_queue[TRANSCEDE_TX_QUEUE_NUM];
+
+	struct net_device_stats	stats;                 /**< Structure of standard linux device driver statistics */
+	struct net_device 	*dev;                   /**< Pointer to standard linux device driver control strucutre */
+	int			id;
+
+	struct transcede_eth_platform_data *einfo;
+
+	int			phys_rx_int;
+
+
+	void			*baseaddr;
+	void			*IRAM_baseaddr_v;
+	void			*IRAM_baseaddr_pa;	
+	GEM_DEVICE		gemdev; // used for gem AL.c
+	struct napi_struct 	napi;
+
+#ifdef CONFIG_GLOBAL_POLLING
+	void *periodic_task;
+#endif	/* CONFIG_GLOBAL_POLLING */
+
+	/* PHY stuff */
+	struct phy_device * phydev;
+	int                 oldspeed;
+	int                 oldduplex;
+	int                 oldlink;
+
+	u32 msg_enable;
+	u32 flags;
+};
+
+/* Exported functions for T4000/T3000 (also known as C4000, C4K) Ethernet driver */
+
+/**
+ * Function to bring the controller up and running
+ *
+ * @see c4k_eth_init_buffers
+ * @see phy_start
+ * @see gem_enable_rx
+ * @see gem_enable_tx
+ * @see c4k_ring_to_phys
+ */
+int c4k_eth_start(struct net_device *dev);
+
+/**
+ * Function to stop the ethernet device
+ * @see gem_abort_tx
+ * @see gem_disable_rx
+ * @see c4k_eth_release_buffers
+ */
+void c4k_eth_stop(struct net_device *dev);
+
+/**
+ * Function to start the queues of the ethernet
+ */
+void c4k_eth_start_queues(struct net_device *dev, int queue);
+
+/**
+ * Function to stop the queues of the ethernet
+ */
+void c4k_eth_stop_queues(struct net_device *dev, int queue);
+
+
+#endif /* _C4000_ETH_H */
+
+/* eof c4000_eth.h */
diff --git a/drivers/net/transcede/c4000_gemac.c b/drivers/net/transcede/c4000_gemac.c
new file mode 100644
index 0000000..3103bfc
--- /dev/null
+++ b/drivers/net/transcede/c4000_gemac.c
@@ -0,0 +1,223 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#include <asm/delay.h>
+#include <mach/hardware.h>
+#include <mach/io.h>
+
+#if defined(CONFIG_MACH_M84XXX)
+#include "c4000_eth.h"
+#else
+#include "t2200_eth.h"
+#endif
+
+#include "transcede_gemac.h"
+
+/** Local defines */
+#define GEM_SCH_ENABLED
+
+
+
+/**
+ * Initializes data structures and registers for the controller,
+ * and brings the interface up. Returns the link status, meaning
+ * that it returns success if the link is up, failure otherwise.
+ * This allows u-boot to find the first active controller.
+ */
+#ifndef CONFIG_MACH_M822XX
+static int gemac_init(struct eth_c4k_priv *priv)
+{
+	GEM_DEVICE *gemdev = &priv->gemdev;
+
+	// Reset the MAC
+	gem_reset(gemdev);
+
+	gem_disable_copy_all(gemdev);
+	gem_allow_broadcast (gemdev);
+//	gem_enable_unicast  (gemdev);
+//	gem_enable_multicast(gemdev);
+	gem_enable_fcs_rx   (gemdev);		// we do want the CRC attached.
+//	gem_disable_fcs_rx  (gemdev);
+//	gem_enable_rd_snap  (gemdev);
+
+	gem_stats_clr(gemdev);			// clear all statistics counters
+
+	// GEM will perform checksum verifications
+	if (priv->flags & RX_CSUM_OFFLOAD_ENABLED)
+		gem_enable_rx_checksum_offload(gemdev);
+	else
+		gem_disable_rx_checksum_offload(gemdev);
+
+	if (priv->flags & TX_CSUM_OFFLOAD_ENABLED)
+		gem_enable_tx_checksum_offload(gemdev);
+	else
+		gem_disable_tx_checksum_offload(gemdev);
+
+	// enable Pause frame reception
+	gem_enable_pause_rx(gemdev);
+
+	return(1);
+}
+#endif
+
+#if 0
+irqreturn_t c4k_gemac_interrupt(int irq, void *dev_id)
+{
+	struct net_device *dev = (struct net_device *)dev_id;
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+	GEM_DEVICE *gem = &priv->gemdev;
+	unsigned long statusword;
+
+	statusword = gem_get_irq_stat(gem);
+	return IRQ_HANDLED;
+}
+#endif
+
+/** Function to set port to full or half duplex */
+void c4k_gemac_setduplex(struct net_device *dev, int duplex)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+	GEM_DEVICE *gemdev = &priv->gemdev;
+
+	if (duplex == DUPLEX_HALF)
+		gem_half_duplex(gemdev);
+	else
+		gem_full_duplex(gemdev);
+}
+
+/** Function to set port's speed 10, 100 or 1000 Megabits per second */
+void c4k_gemac_setspeed(struct net_device *dev, int speed)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+	GEM_DEVICE *gemdev = &priv->gemdev;
+
+	switch (speed) {
+	case 10:
+		writel(readl(priv->baseaddr + GEM_IP + GEM_NET_CONFIG) | GEM_RX_BAD_PREAMBLE, priv->baseaddr + GEM_IP + GEM_NET_CONFIG);
+		gem_set_speed(gemdev, SPEED_10M);
+	        break;
+	case 100:
+		writel(readl(priv->baseaddr + GEM_IP + GEM_NET_CONFIG) & ~GEM_RX_BAD_PREAMBLE, priv->baseaddr + GEM_IP + GEM_NET_CONFIG);
+		gem_set_speed(gemdev, SPEED_100M);
+	        break;
+	case 1000:
+	default:
+		writel(readl(priv->baseaddr + GEM_IP + GEM_NET_CONFIG) & ~GEM_RX_BAD_PREAMBLE, priv->baseaddr + GEM_IP + GEM_NET_CONFIG);
+		gem_set_speed(gemdev, SPEED_1000M);
+	        break;
+	}
+}
+
+/**
+ * Function to put the GEM device into an initialized and disabled state.
+ *
+ * @note
+ * NOTE: Later on in the startup sequence in Linux, the device is setup
+ * to enable the receive and transmit (including the scheduler
+ * and admittance block if those blocks are to be enabled).
+ *
+ *
+ * @see readl
+ * @see writel
+ * @see gemac_init
+ * @see gem_enable_irq
+ * @return Always returns 0
+ */
+int c4k_gemac_init(struct net_device *dev, uint irq_mask_ena, uint irq_mask_dis)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+	GEM_DEVICE *gemdev = &priv->gemdev;
+/*	u32 dma_cfg;
+
+	Code commented out, as current driver init code always
+	sets up the initial value to a known state
+
+	dma_cfg = readl(priv->baseaddr + GEM_IP + GEM_DMA_CONFIG);
+*/
+#if !defined(CONFIG_MACH_M822XX)
+	gemac_init(priv);
+#endif
+
+	/*
+	if (irq_mask_ena & GEM_IRQ_RX_ORUN)
+	{
+	    gem_enable_irq(gemdev, GEM_IRQ_RX_ORUN);
+	}
+	*/
+
+	if (irq_mask_ena & GEM_IRQ_RX_DONE)
+	{
+	    gem_enable_irq(gemdev, GEM_IRQ_RX_DONE);
+	}
+
+
+	/* if 'disable' mask is non-zero value we may add 
+	   disabling of some GEM IRQs, it's applicable for GEM reinitialization
+	   when we want to keep some already set interrupts but want 
+	   to disable some of it*/
+	
+	
+
+	return 0;
+}
+
+/**
+ * Function to setup Layer 3 and Layer 4 checksum.
+ * @return 0 if OK, -1 if error (example: unsupported combination of options)
+ */
+int c4k_emac_checksum(struct eth_c4k_priv *priv, u32 status, u8 *ip_summed)
+{
+	/* Test if checksum enabled */
+	if (!(priv->flags & RX_CSUM_OFFLOAD_ENABLED))
+	{
+		*ip_summed = CHECKSUM_NONE;
+		return 0;
+	}
+#if defined(CONFIG_MACH_M84XXX)
+	/* Test if checksums checked and correct */
+	if ((status & (RX_STA_L3_CKSUM | RX_STA_L3_GOOD | RX_STA_L4_CKSUM | RX_STA_L4_GOOD)) 
+	           == (RX_STA_L3_CKSUM | RX_STA_L3_GOOD | RX_STA_L4_CKSUM | RX_STA_L4_GOOD)
+	   )
+	{
+		*ip_summed = CHECKSUM_UNNECESSARY;
+		return 0;
+	}
+
+	/* Test if there is at least one checksum failure */
+	if ((status & (RX_STA_L3_CKSUM | RX_STA_L4_CKSUM)) == (RX_STA_L3_CKSUM | RX_STA_L4_CKSUM))
+		return -1;
+
+	/* Test for L3 checksum failure */
+	if ((status & (RX_STA_L3_CKSUM | RX_STA_L3_GOOD)) == RX_STA_L3_CKSUM)
+		return -1;
+
+	/* Test if L4 checksum failure */
+	if ((status & (RX_STA_L4_CKSUM | RX_STA_L4_GOOD)) == RX_STA_L4_CKSUM)
+		return -1;
+#endif
+	/* one or both checksums were not checked */
+	*ip_summed = CHECKSUM_NONE;
+
+	return 0;
+}
+
+/* eof c4000_gemac.c */
diff --git a/drivers/net/transcede/m84xxx_adm.h b/drivers/net/transcede/m84xxx_adm.h
new file mode 100644
index 0000000..477d4f0
--- /dev/null
+++ b/drivers/net/transcede/m84xxx_adm.h
@@ -0,0 +1,606 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef _ADM_H
+#define _ADM_H
+
+#include "m84xxx_common.h"
+
+#define ADM0	0
+#define ADM1	1
+#define ADM_NUM_DROP_ZONE	4
+#define ADM_TCAM_MAX_ENTRIES	32
+#define ADM_QMAX_HEADROOM	16
+
+#define CHECK_ADM(adm)		(adm > ADM1 ? 0 : 1) /* return 0 if adm is out of range */
+
+
+extern unsigned long *FAKE_EMAC0_BASE;
+extern unsigned long *FAKE_EMAC1_BASE;
+
+/* There's one admittance block per GEM interface */
+#ifdef C1K_REG_SIM
+#warning !!!!!!!! running with fake gemac registers
+#define adm_base(adm) (get_gemac_base(adm) + 0x4000)
+#else
+#define adm_base(adm) ((adm > 0 ? AAB_XP_VADDR(EMAC1_BASE) : AAB_XP_VADDR(EMAC0_BASE)) + 0x4000)
+#endif
+/******************************** STATUS register ********************************/
+#define ADM_STATUS_REG(adm)			(volatile unsigned long *)(adm_base(adm) + 0x0000)
+
+#define ADM_STATUS_ACCEPTED_BIT			0
+#define ADM_STATUS_ACCEPTED_W			0x01
+#define ADM_GET_STATUS_ACCEPTED(adm)		(*ADM_STATUS_REG(adm) >> ADM_STATUS_ACCEPTED_BIT) & ADM_STATUS_ACCEPTED_W
+
+#define ADM_STATUS_DROPPED_BIT			1
+#define ADM_STATUS_DROPPED_W			0x01
+#define ADM_GET_STATUS_DROPPED(adm)		(*ADM_STATUS_REG(adm) >> ADM_STATUS_DROPPED_BIT) & ADM_STATUS_DROPPED_W
+
+#define ADM_STATUS_QZONE_BIT			2
+#define ADM_STATUS_QZONE_W			0x07
+#define ADM_GET_STATUS_QZONE(adm)		(*ADM_STATUS_REG(adm) >> ADM_STATUS_QZONE_BIT) & ADM_STATUS_QZONE_W
+
+#define ADM_STATUS_CLASSFIFODEPTH_BIT		5
+#define ADM_STATUS_CLASSFIFODEPTH_W		0x1F
+#define ADM_GET_STATUS_CLASSFIFODEPTH(adm)	(*ADM_STATUS_REG(adm) >> ADM_STATUS_CLASSFIFODEPTH_BIT) & ADM_STATUS_CLASSFIFODEPTH_W
+
+#define	ADM_STATUS_TCAMENTRY_BIT		10
+#define	ADM_STATUS_TCAMENTRY_W			0x1F
+#define ADM_GET_STATUS_TCAMENTRY		(*ADM_STATUS_REG(adm) >> ADM_STATUS_TCAMENTRY_BIT) & ADM_STATUS_TCAMENTRY_W
+
+#define ADM_STATUS_TCAMHIT_BIT			15
+#define ADM_STATUS_TCAMHIT_W			0x01
+#define ADM_GET_STATUS_TCAMHIT			(*ADM_STATUS_REG(adm) >> ADM_STATUS_TCAMHIT_BIT) & ADM_STATUS_TCAMHIT_W
+
+#define ADM_STATUS_PAUSESENT_BIT		16
+#define ADM_STATUS_PAUSESENT_W			0x01
+#define ADM_GET_STATUS_PAUSESENT		(*ADM_STATUS_REG(adm) >> ADM_STATUS_PAUSESENT_BIT) & ADM_STATUS_PAUSESENT_W
+
+#define ADM_STATUS_PACKETTYPE_BIT		17
+#define ADM_STATUS_PACKETTYPE_W			0x03
+#define ADM_GET_STATUS_PACKETTYPE		(*ADM_STATUS_REG(adm) >> ADM_STATUS_PACKETTYPE_BIT) & ADM_STATUS_PACKETTYPE_W
+
+#define ADM_STATUS_DROP_CONGESTION_BIT		19
+#define ADM_STATUS_DROP_CONGESTION_W		0x01
+#define ADM_GET_STATUS_DROP_CONGESTION		(*ADM_STATUS_REG(adm) >> ADM_STATUS_DROP_CONGESTION_BIT) & ADM_STATUS_DROP_CONGESTION_W
+
+#define ADM_STATUS_DROP_QFULL_BIT		20
+#define ADM_STATUS_DROP_QFULL_W			0x01
+#define ADM_GET_STATUS_DROP_QFULL		(*ADM_STATUS_REG(adm) >> ADM_STATUS_DROP_QFULL_BIT) & ADM_STATUS_DROP_QFULL_W
+
+#define ADM_STATUS_DROP_SHAPER_BIT		21
+#define ADM_STATUS_DROP_SHAPER_W		0x01
+#define ADM_GET_STATUS_DROP_SHAPER		(*ADM_STATUS_REG(adm) >> ADM_STATUS_DROP_SHAPER_BIT) & ADM_STATUS_DROP_SHAPER_W
+
+#define ADM_STATUS_DROP_ACL_BIT			22
+#define ADM_STATUS_DROP_ACL_W			0x01
+#define ADM_GET_STATUS_DROP_ACL			(*ADM_STATUS_REG(adm) >> ADM_STATUS_DROP_ACL_BIT) & ADM_STATUS_DROP_ACL_W
+
+#define ADM_STATUS_DROP_POLICER0_BIT		23
+#define ADM_STATUS_DROP_POLICER0_W		0x01
+#define ADM_GET_STATUS_DROP_POLICER0		(*ADM_STATUS_REG(adm) >> ADM_STATUS_DROP_POLICER0_BIT) & ADM_STATUS_DROP_POLICER0_W
+
+#define ADM_STATUS_DROP_POLICER1_BIT		24
+#define ADM_STATUS_DROP_POLICER1_W		0x01
+#define ADM_GET_STATUS_DROP_POLICER1		(*ADM_STATUS_REG(adm) >> ADM_STATUS_DROP_POLICER1_BIT) & ADM_STATUS_DROP_POLICER1_W
+
+#define ADM_STATUS_DROP_ERROR_BIT		25
+#define ADM_STATUS_DROP_ERROR_W			0x01
+#define ADM_GET_STATUS_DROP_ERROR		(*ADM_STATUS_REG(adm) >> ADM_STATUS_DROP_ERROR_BIT) & ADM_STATUS_DROP_ERROR_W
+
+#define ADM_STATUS_PKTSEQNUM_BIT		26
+#define ADM_STATUS_PKTSEQNUM_W			0x1F
+#define ADM_GET_STATUS_PKTSEQNUM		(*ADM_STATUS_REG(adm) >> ADM_STATUS_PKTSEQNUM_BIT) & ADM_STATUS_PKTSEQNUM_W
+
+#define ADM_STATUS_READY_BIT			31
+#define ADM_STATUS_READY_W			0x01
+#define ADM_GET_STATUS_READY(adm)		(*ADM_STATUS_REG(adm) >> ADM_STATUS_READY_BIT) & ADM_STATUS_READY_W
+
+
+/************************************ PACKET DEQUEUED register **********************************************/
+#define ADM_PKT_DEQUEUED_REG(adm)		(volatile unsigned long *)(adm_base(adm) + 0x0004)
+
+#define ADM_PKT_DEQUEUED_BIT			0
+#define ADM_PKT_DEQUEUED_W			0x3FFF
+#define ADM_GET_PKT_DEQUEUED(adm)		(*ADM_PKT_DEQUEUED_REG(adm) >> ADM_PKT_DEQUEUED_BIT) & ADM_PKT_DEQUEUED_W
+#define ADM_SET_PKT_DEQUEUED(adm,value)		*ADM_PKT_DEQUEUED_REG(adm) |= (value & ADM_PKT_DEQUEUED_BIT) << ADM_PKT_DEQUEUED_W
+
+/******************************************* CONFIGURATION register *******************************************/
+#define ADM_CONF_REG(adm)			(volatile unsigned long *)(adm_base(adm) + 0x0008)
+
+#define ADM_CONF_MODE_BIT			0
+#define ADM_CONF_MODE_W				0x01
+#define ADM_GET_CONF_MODE(adm)			(*ADM_CONF_REG(adm) >> ADM_CONF_MODE_BIT) & ADM_CONF_MODE_W
+#define ADM_SET_CONF_MODE_DISCARD(adm)		*ADM_CONF_REG(adm) &= ~(ADM_CONF_MODE_W << ADM_CONF_MODE_BIT) /* 0 = DISCARD */
+#define ADM_SET_CONF_MODE_FLOWCTRL(adm)		*ADM_CONF_REG(adm) |= (ADM_CONF_MODE_W << ADM_CONF_MODE_BIT) /* 1 = FLOWCTRL */
+
+#define ADM_CONF_ADMITERR_BIT			1
+#define ADM_CONF_ADMITERR_W			0x01
+#define ADM_GET_CONF_ADMITERR(adm)		(*ADM_CONF_REG(adm) >> ADM_CONF_ADMITERR_BIT) & ADM_CONF_ADMITERR_W
+#define ADM_SET_CONF_ADMITERR(adm)		*ADM_CONF_REG(adm) |= (ADM_CONF_ADMITERR_W << ADM_CONF_ADMITERR_BIT) /* 1 = ADMIT ERRORS PKTS */
+#define ADM_SET_CONF_DISCARDERR(adm)		*ADM_CONF_REG(adm) &= ~(ADM_CONF_ADMITERR_W << ADM_CONF_ADMITERR_BIT) /* 0 = DISCARD ERRORS PKTS*/
+
+#define ADM_CONF_WEIGTH_BIT			4
+#define ADM_CONF_WEIGTH_W			0x0F
+#define ADM_SET_CONF_WEIGTH(adm,value)		SET_REG_BITSHIFT(ADM_CONF_REG(adm), value, ADM_CONF_WEIGTH_W , ADM_CONF_WEIGTH_BIT)
+
+#define ADM_CONF_PROB0_BIT			16
+#define ADM_CONF_PROB0_W			0x0F
+#define ADM_SET_CONF_PROB0(adm,value)		SET_REG_BITSHIFT(ADM_CONF_REG(adm), value, ADM_CONF_PROB0_W , ADM_CONF_PROB0_BIT)
+
+#define ADM_CONF_PROB1_BIT			20
+#define ADM_CONF_PROB1_W			0x0F
+#define ADM_SET_CONF_PROB1(adm,value)		SET_REG_BITSHIFT(ADM_CONF_REG(adm), value, ADM_CONF_PROB1_W , ADM_CONF_PROB1_BIT)
+
+#define ADM_CONF_PROB2_BIT			24
+#define ADM_CONF_PROB2_W			0x0F
+#define ADM_SET_CONF_PROB2(adm,value)		SET_REG_BITSHIFT(ADM_CONF_REG(adm) , value, ADM_CONF_PROB2_W , ADM_CONF_PROB2_BIT)
+
+#define ADM_CONF_PROB3_BIT			28
+#define ADM_CONF_PROB3_W			0x0F
+#define ADM_SET_CONF_PROB3(adm,value)		SET_REG_BITSHIFT(ADM_CONF_REG(adm) , value, ADM_CONF_PROB3_W , ADM_CONF_PROB3_BIT)
+
+
+/****************************************** CONTROL register ********************************************/
+#define ADM_CTRL_REG(adm)			(volatile unsigned long *)(adm_base(adm) + 0x000C)
+
+#define ADM_CTRL_ADM_ENABLE_BIT			0
+#define ADM_CTRL_ADM_ENABLE_W			1
+#define ADM_GET_CTRL_ADM_ENABLE(adm)		(*ADM_CTRL_REG(adm) >> ADM_CTRL_ADM_ENABLE_BIT) & ADM_CTRL_ADM_ENABLE_W
+#define ADM_SET_CTRL_ADM_ENABLE(adm)		*ADM_CTRL_REG(adm) |= (1 << ADM_CTRL_ADM_ENABLE_BIT)
+#define ADM_SET_CTRL_ADM_DISABLE(adm)		*ADM_CTRL_REG(adm) &= ~(1 << ADM_CTRL_ADM_ENABLE_BIT)
+
+#define ADM_CTRL_TCAM_ENABLE_BIT		1
+#define ADM_CTRL_TCAM_ENABLE_W			1
+#define ADM_GET_CTRL_TCAM_ENABLE(adm)		(*ADM_CTRL_REG(adm) >> ADM_CTRL_TCAM_ENABLE_BIT) & ADM_CTRL_TCAM_ENABLE_W
+#define ADM_SET_CTRL_TCAM_ENABLE(adm)		*ADM_CTRL_REG(adm) |= (1 << ADM_CTRL_TCAM_ENABLE_BIT)
+#define ADM_SET_CTRL_TCAM_DISABLE(adm)		*ADM_CTRL_REG(adm) &= ~(1 << ADM_CTRL_TCAM_ENABLE_BIT)
+
+#define ADM_CTRL_POLICER_ENABLE_BIT		2
+#define ADM_CTRL_POLICER_ENABLE_W		1
+#define ADM_GET_CTRL_POLICER_ENABLE(adm)	(*ADM_CTRL_REG(adm) >> ADM_CTRL_POLICER_ENABLE_BIT) & ADM_CTRL_POLICER_ENABLE_W
+#define ADM_SET_CTRL_POLICER_ENABLE(adm)	*ADM_CTRL_REG(adm) |= (1 << ADM_CTRL_POLICER_ENABLE_BIT)
+#define ADM_SET_CTRL_POLICER_DISABLE(adm)	*ADM_CTRL_REG(adm) &= ~(1 << ADM_CTRL_POLICER_ENABLE_BIT)
+
+#define ADM_CTRL_SHAPER_ENABLE_BIT		3
+#define ADM_CTRL_SHAPER_ENABLE_W		1
+#define ADM_GET_CTRL_SHAPER_ENABLE(adm)		(*ADM_CTRL_REG(adm) >> ADM_CTRL_SHAPER_ENABLE_BIT) & ADM_CTRL_SHAPER_ENABLE_W
+#define ADM_SET_CTRL_SHAPER_ENABLE(adm)		*ADM_CTRL_REG(adm) |= (1 << ADM_CTRL_SHAPER_ENABLE_BIT)
+#define ADM_SET_CTRL_SHAPER_DISABLE(adm)	*ADM_CTRL_REG(adm) &= ~(1 << ADM_CTRL_SHAPER_ENABLE_BIT)
+
+#define ADM_CTRL_DISCARD_ENABLE_BIT		4
+#define ADM_CTRL_DISCARD_ENABLE_W		1
+#define ADM_GET_CTRL_DISCARD_ENABLE(adm)	(*ADM_CTRL_REG(adm) >> ADM_CTRL_DISCARD_ENABLE_BIT) & ADM_CTRL_DISCARD_ENABLE_W
+#define ADM_SET_CTRL_DISCARD_ENABLE(adm)	*ADM_CTRL_REG(adm) |= (1 << ADM_CTRL_DISCARD_ENABLE_BIT)
+#define ADM_SET_CTRL_DISCARD_DISABLE(adm)	*ADM_CTRL_REG(adm) &= ~(1 << ADM_CTRL_DISCARD_ENABLE_BIT)
+
+#define ADM_CTRL_FLOWCTRL_ENABLE_BIT		5
+#define ADM_CTRL_FLOWCTRL_ENABLE_W		1
+#define ADM_GET_CTRL_FLOWCTRL_ENABLE(adm)	(*ADM_CTRL_REG(adm) >> ADM_CTRL_FLOWCTRL_ENABLE_BIT) & ADM_CTRL_FLOWCTRL_ENABLE_W
+#define ADM_SET_CTRL_FLOWCTRL_ENABLE(adm)	*ADM_CTRL_REG(adm) |= (1 << ADM_CTRL_FLOWCTRL_ENABLE_BIT)
+#define ADM_SET_CTRL_FLOWCTRL_DISABLE(adm)	*ADM_CTRL_REG(adm) &= ~(1 << ADM_CTRL_FLOWCTRL_ENABLE_BIT)
+
+#define ADM_CTRL_STATISTICS_ENABLE_BIT		6
+#define ADM_CTRL_STATISTICS_ENABLE_W		1
+#define ADM_GET_CTRL_STATISTICS_ENABLE(adm)	(*ADM_CTRL_REG(adm) >> ADM_CTRL_STATISTICS_ENABLE_BIT) & ADM_CTRL_STATISTICS_ENABLE_W
+#define ADM_SET_CTRL_STATISTICS_ENABLE(adm)	*ADM_CTRL_REG(adm) |= (1 << ADM_CTRL_STATISTICS_ENABLE_BIT)
+#define ADM_SET_CTRL_STATISTICS_DISABLE(adm)	*ADM_CTRL_REG(adm) &= ~(1 << ADM_CTRL_STATISTICS_ENABLE_BIT)
+
+#define ADM_CTRL_SNAPSHOT_ENABLE_BIT		16
+#define ADM_CTRL_SNAPSHOT_ENABLE_W		1
+#define ADM_GET_CTRL_SNAPSHOT_ENABLE(adm)	(*ADM_CTRL_REG(adm) >> ADM_CTRL_SNAPSHOT_ENABLE_BIT) & ADM_CTRL_SNAPSHOT_ENABLE_W
+#define ADM_SET_CTRL_SNAPSHOT_ENABLE(adm)	*ADM_CTRL_REG(adm) |= (1 << ADM_CTRL_SNAPSHOT_ENABLE_BIT)
+#define ADM_SET_CTRL_SNAPSHOT_DISABLE(adm)	*ADM_CTRL_REG(adm) &= ~(1 << ADM_CTRL_SNAPSHOT_ENABLE_BIT)
+
+#define ADM_CTRL_DIS_GEM_SNAPSHOT_BIT		17
+#define ADM_CTRL_DIS_GEM_SNAPSHOT_W		1
+#define ADM_GET_CTRL_GEM_SNAPSHOT_ENABLE(adm)	(*ADM_CTRL_REG(adm) >> ADM_CTRL_DIS_GEM_SNAPSHOT_BIT) & ADM_CTRL_DIS_GEM_SNAPSHOT_W
+#define ADM_SET_CTRL_GEM_SNAPSHOT_ENABLE(adm)	*ADM_CTRL_REG(adm) |= (1 << ADM_CTRL_DIS_GEM_SNAPSHOT_BIT)
+#define ADM_SET_CTRL_GEM_SNAPSHOT_DISABLE(adm)	*ADM_CTRL_REG(adm) &= ~(1 << ADM_CTRL_DIS_GEM_SNAPSHOT_BIT)
+
+
+/* 0x4010 - 0x401C reserved*/
+
+
+/*************************************** PORT ***********************************************/
+#define ADM_PORT_BASE				0x0020
+#define adm_port(adm)				(adm_base(adm) + ADM_PORT_BASE)
+#define ADM_PORT_SHA_FRAC_RATE(adm)		(volatile unsigned long *)(adm_port(adm) + 0x00)
+#define ADM_PORT_SHA_MAX_CREDIT(adm)		(volatile unsigned long *)(adm_port(adm) + 0x04)
+#define ADM_PORT_SHA_CREDIT(adm)		(volatile unsigned long *)(adm_port(adm) + 0x08)
+#define ADM_PORT_SHA_CTRL(adm)			(volatile unsigned long *)(adm_port(adm) + 0x0C)
+#define ADM_PORT_SHA_PKT_OVER(adm)		(volatile unsigned long *)(adm_port(adm) + 0x10)
+
+#define ADM_PORT_SHA_FRAC_RATE_BIT		0
+#define ADM_PORT_SHA_FRAC_RATE_W		0xFF
+#define ADM_SET_PORT_SHA_FRAC_RATE(adm,value)	SET_REG_BITSHIFT(ADM_PORT_SHA_FRAC_RATE(adm) , value, ADM_PORT_SHA_FRAC_RATE_W , ADM_PORT_SHA_FRAC_RATE_BIT)
+#define ADM_GET_PORT_SHA_FRAC_RATE(adm)		(*ADM_PORT_SHA_FRAC_RATE(adm) >> ADM_PORT_SHA_FRAC_RATE_BIT) & ADM_PORT_SHA_FRAC_RATE_W
+
+#define ADM_PORT_SHA_MAX_CREDIT_BIT		0
+#define ADM_PORT_SHA_MAX_CREDIT_W		0x0FFFFF
+#define ADM_SET_PORT_SHA_MAX_CREDIT(adm,value)	SET_REG_BITSHIFT(ADM_PORT_SHA_MAX_CREDIT(adm) , value, ADM_PORT_SHA_MAX_CREDIT_W , ADM_PORT_SHA_MAX_CREDIT_BIT)
+#define ADM_GET_PORT_SHA_MAX_CREDIT(adm)	(*ADM_PORT_SHA_MAX_CREDIT(adm) >> ADM_PORT_SHA_MAX_CREDIT_BIT) & ADM_PORT_SHA_MAX_CREDIT_W
+
+
+#define ADM_PORT_SHA_CREDIT_BIT			0
+#define ADM_PORT_SHA_CREDIT_W			0x0FFFFF
+#define ADM_SET_PORT_SHA_CREDIT(adm,value)	SET_REG_BITSHIFT(ADM_PORT_SHA_CREDIT(adm), value, ADM_PORT_SHA_CREDIT_W , ADM_PORT_SHA_CREDIT_BIT)
+#define ADM_GET_PORT_SHA_CREDIT(adm)		(*ADM_PORT_SHA_CREDIT(adm) >> ADM_PORT_SHA_CREDIT_BIT) & ADM_PORT_SHA_CREDIT_W
+
+
+#define ADM_PORT_SHA_PKT_OVER_BIT		0
+#define ADM_PORT_SHA_PKT_OVER_W			0x0FFFFF
+#define ADM_SET_PORT_SHA_PKT_OVER(adm,value)	 SET_REG_BITSHIFT(ADM_PORT_SHA_PKT_OVER(adm) , value, ADM_PORT_SHA_PKT_OVER_W ,ADM_PORT_SHA_PKT_OVER_BIT)
+#define ADM_GET_PORT_SHA_PKT_OVER(adm)		(*ADM_PORT_SHA_PKT_OVER(adm) >> ADM_PORT_SHA_PKT_OVER_BIT) & ADM_PORT_SHA_PKT_OVER_W
+
+#define ADM_PORT_SHA_ENABLE_BIT			4
+#define ADM_PORT_SHA_ENABLE_W			1
+#define ADM_SET_PORT_SHA_ENABLE(adm,value)	SET_REG_BITSHIFT(ADM_PORT_SHA_CTRL(adm), value, ADM_PORT_SHA_ENABLE_W ,ADM_PORT_SHA_ENABLE_BIT)
+#define ADM_GET_PORT_SHA_ENABLE(adm)		(*ADM_PORT_SHA_CTRL(adm) >> ADM_PORT_SHA_ENABLE_BIT) & ADM_PORT_SHA_ENABLE_W
+
+#define ADM_PORT_SHA_CLK_BIT			0
+#define ADM_PORT_SHA_CLK_W			0x0F
+#define ADM_SET_PORT_SHA_CLOCK(adm,value)	SET_REG_BITSHIFT(ADM_PORT_SHA_CTRL(adm), value, ADM_PORT_SHA_CLK_W , ADM_PORT_SHA_CLK_BIT)
+#define ADM_GET_PORT_SHA_CLOCK(adm)		(*ADM_PORT_SHA_CTRL(adm) >> ADM_PORT_SHA_CLK_BIT) & ADM_PORT_SHA_CLK_W
+
+
+/* 0x4034 - 0x403C reserved */
+
+/*************************************** STATISTICS ******************************************/
+#define ADM_STATIS_BASE				0x0040
+#define adm_statis(adm)				(adm_base(adm) + ADM_STATIS_BASE)
+#define ADM_STATIS_PORT_BYTE_CNT(adm)		(volatile unsigned long *)(adm_statis(adm) + 0x00)
+#define ADM_STATIS_PORT_PKT_CNT(adm)		(volatile unsigned long *)(adm_statis(adm) + 0x04)
+#define ADM_STATIS_RSVD_BYTE_CNT(adm)		(volatile unsigned long *)(adm_statis(adm) + 0x08)
+#define	ADM_STATIS_RSVD_PKT_CNT(adm)		(volatile unsigned long *)(adm_statis(adm) + 0x0C)
+#define ADM_STATIS_MNGD_BYTE_CNT(adm)		(volatile unsigned long *)(adm_statis(adm) + 0x10)
+#define	ADM_STATIS_MNGD_PKT_CNT(adm)		(volatile unsigned long *)(adm_statis(adm) + 0x14)
+#define ADM_STATIS_PKT_DROP_CNT(adm)		(volatile unsigned long *)(adm_statis(adm) + 0x18)
+#define ADM_STATIS_PKT_DROP_ERROR(adm)		(volatile unsigned long *)(adm_statis(adm) + 0x1C)
+#define ADM_STATIS_PKT_DROP_DENIED(adm)		(volatile unsigned long *)(adm_statis(adm) + 0x20)
+#define ADM_STATIS_PKT_DROP_POL0(adm)		(volatile unsigned long *)(adm_statis(adm) + 0x24)
+#define ADM_STATIS_PKT_DROP_POL1(adm)		(volatile unsigned long *)(adm_statis(adm) + 0x28)
+#define ADM_STATIS_PKT_DROP_QFULL(adm)		(volatile unsigned long *)(adm_statis(adm) + 0x2C)
+#define ADM_STATIS_PKT_DROP_SHAPER		(volatile unsigned long *)(adm_statis(adm) + 0x30)
+#define ADM_STATIS_PKT_DROP_MNGD_CNGS		(volatile unsigned long *)(adm_statis(adm) + 0x34)
+#define ADM_STATIS_PKT_DROP_UNMNGD_CNGS		(volatile unsigned long *)(adm_statis(adm) + 0x38)
+
+/* 0x407C reserved */
+
+/************************************ DEPTH ***********************************************************/
+#define ADM_DEPTH_BASE				0x0080
+#define adm_depth(adm)				(adm_base(adm) + ADM_DEPTH_BASE)
+#define ADM_DEPTH_Q_DEPTH(adm)			(volatile unsigned long *)(adm_depth(adm) + 0x00)
+#define ADM_DEPTH_AVG_Q_DEPTH(adm)		(volatile unsigned long *)(adm_depth(adm) + 0x04)
+#define ADM_DEPTH_Q_FULL_THRESHOLD(adm)		(volatile unsigned long *)(adm_depth(adm) + 0x08)
+#define ADM_DEPTH_Q_DROP_MAX(adm)		(volatile unsigned long *)(adm_depth(adm) + 0x0C)
+#define ADM_DEPTH_Q_DROP_MIN(adm)		(volatile unsigned long *)(adm_depth(adm) + 0x10)
+#define ADM_DEPTH_PAUSE_INT_TIMER(adm)		(volatile unsigned long *)(adm_depth(adm) + 0x14)
+#define ADM_DEPTH_DECAY_TIMER(adm)		(volatile unsigned long *)(adm_depth(adm) + 0x18)
+#define ADM_DEPTH_DROP_RANDOM(adm)		(volatile unsigned long *)(adm_depth(adm) + 0x1C)
+
+#define ADM_DEPTH_Q_FULL_THRESHOLD_BIT		0
+#define ADM_DEPTH_Q_FULL_THRESHOLD_W		0xFFFF
+#define ADM_GET_Q_FULL_THRESHOLD(adm)	(*ADM_DEPTH_Q_FULL_THRESHOLD(adm) >>  ADM_DEPTH_Q_FULL_THRESHOLD_BIT) & ADM_DEPTH_Q_FULL_THRESHOLD_W;
+#define ADM_SET_Q_FULL_THRESHOLD(adm,value)	SET_REG_BITSHIFT(ADM_DEPTH_Q_FULL_THRESHOLD(adm), value, ADM_DEPTH_Q_FULL_THRESHOLD_W, ADM_DEPTH_Q_FULL_THRESHOLD_BIT)
+
+#define ADM_DEPTH_Q_DROP_MAX_BIT		8
+#define ADM_DEPTH_Q_DROP_MAX_W			0xFFFF
+#define ADM_SET_Q_DROP_MAX(adm,value)  SET_REG_BITSHIFT(ADM_DEPTH_Q_DROP_MAX(adm) , value, ADM_DEPTH_Q_DROP_MAX_W, ADM_DEPTH_Q_DROP_MAX_BIT)
+
+#define ADM_SET_FLOWCTRL_ON_TRESHOLD(adm,value) ADM_SET_Q_DROP_MAX(adm, value)
+
+#define ADM_DEPTH_Q_DROP_MIN_BIT		8
+#define ADM_DEPTH_Q_DROP_MIN_W			0xFFFF
+#define ADM_SET_Q_DROP_MIN(adm,value)		SET_REG_BITSHIFT(ADM_DEPTH_Q_DROP_MIN(adm), value, ADM_DEPTH_Q_DROP_MIN_W , ADM_DEPTH_Q_DROP_MIN_BIT)
+#define ADM_SET_FLOWCTRL_OFF_TRESHOLD(adm,value) ADM_SET_Q_DROP_MIN(adm, value)
+
+#define ADM_DEPTH_Q_DEPTH_BIT			0
+#define ADM_DEPTH_Q_DEPTH_W			0xFFFF
+#define ADM_SET_DEPTH_Q_DEPTH(adm,value)	SET_REG_BITSHIFT(ADM_DEPTH_Q_DEPTH(adm), value, ADM_DEPTH_Q_DEPTH_W , ADM_DEPTH_Q_DEPTH_BIT)
+
+#define ADM_DEPTH_AVG_Q_DEPTH_BIT		16
+#define ADM_DEPTH_AVG_Q_DEPTH_W			0xFFFF
+#define ADM_SET_DEPTH_AVG_Q_DEPTH(adm,value)	SET_REG_BITSHIFT(ADM_DEPTH_AVG_Q_DEPTH(adm), value, ADM_DEPTH_AVG_Q_DEPTH_W , ADM_DEPTH_AVG_Q_DEPTH_BIT)
+
+#define ADM_DEPTH_DECAY_TIMER_BIT		0
+#define ADM_DEPTH_DECAY_TIMER_W			0xFFFFFF
+#define ADM_SET_DEPTH_DECAY_TIMER(adm,value)	 SET_REG_BITSHIFT(ADM_DEPTH_DECAY_TIMER(adm), value, ADM_DEPTH_DECAY_TIMER_W , ADM_DEPTH_DECAY_TIMER_BIT)
+
+#define ADM_DEPTH_PAUSE_INT_TIMER_BIT			0
+#define ADM_DEPTH_PAUSE_INT_TIMER_W			0xFFFF
+#define ADM_SET_DEPTH_PAUSE_INT_TIMER(adm,value)	SET_REG_BITSHIFT(ADM_DEPTH_PAUSE_INT_TIMER(adm), value, ADM_DEPTH_PAUSE_INT_TIMER_W , ADM_DEPTH_PAUSE_INT_TIMER_BIT)
+
+
+/****************************************** POLICER  ***********************************************/
+#define ADM_POLICER0_BASE			0x00A0
+#define ADM_POLICER1_BASE			0x00B0
+#define ADM_POLICER_BASE(policer)		(policer > 0 ? ADM_POLICER1_BASE : ADM_POLICER0_BASE)
+#define adm_pol(adm,policer)			(adm_base(adm) + ADM_POLICER_BASE(policer))
+#define ADM_POL_FRAC_RATE(adm,policer)		(volatile unsigned long *)(adm_pol(adm,policer) + 0x00)
+#define ADM_POL_MAX_CREDIT(adm,policer)		(volatile unsigned long *)(adm_pol(adm,policer) + 0x04)
+#define ADM_POL_CREDIT(adm,policer)		(volatile unsigned long *)(adm_pol(adm,policer) + 0x08)
+#define ADM_POL_CTRL(adm,policer)		(volatile unsigned long *)(adm_pol(adm,policer) + 0x0C)
+
+#define ADM_POLICER_FRAC_RATE_BIT			0
+#define ADM_POLICER_FRAC_RATE_W				0xFF
+#define	ADM_GET_POLICER_FRAC_RATE(adm,policer)		*ADM_POL_FRAC_RATE(adm,policer)
+#define	ADM_SET_POLICER_FRAC_RATE(adm,policer,value)	SET_REG_BITSHIFT(ADM_POL_FRAC_RATE(adm,policer), value, ADM_POLICER_FRAC_RATE_W , ADM_POLICER_FRAC_RATE_BIT)
+
+#define ADM_POLICER_MAX_CREDIT_BIT			0
+#define ADM_POLICER_MAX_CREDIT_W			0x0FFFFF
+#define	ADM_GET_POLICER_MAX_CREDIT(adm,policer)		*ADM_POL_MAX_CREDIT(adm,policer)
+#define	ADM_SET_POLICER_MAX_CREDIT(adm,policer,value)	SET_REG_BITSHIFT(ADM_POL_MAX_CREDIT(adm,policer), value, ADM_POLICER_MAX_CREDIT_W , ADM_POLICER_MAX_CREDIT_BIT)
+
+#define ADM_POLICER_INIT_BURST_BIT			0
+#define ADM_POLICER_INIT_BURST_W			0x1FFFFF
+#define	ADM_GET_POLICER_INIT_BURST(adm,policer)		*ADM_POL_CREDIT(adm,policer)
+#define	ADM_SET_POLICER_INIT_BURST(adm,policer,value)	 SET_REG_BITSHIFT(ADM_POL_CREDIT(adm,policer), value, ADM_POLICER_INIT_BURST_W , ADM_POLICER_INIT_BURST_BIT)
+
+#define ADM_POLICER_ENABLE_BIT				4
+#define ADM_POLICER_ENABLE_W				0x01
+#define	ADM_GET_POLICER_ENABLE(adm,policer)		(*ADM_POL_CTRL(adm,policer) >>  ADM_POLICER_ENABLE_BIT) & ADM_POLICER_ENABLE_W
+#define	ADM_SET_POLICER_ENABLE(adm,policer,value)	 SET_REG_BITSHIFT(ADM_POL_CTRL(adm,policer), value, ADM_POLICER_ENABLE_W , ADM_POLICER_ENABLE_BIT)
+
+#define ADM_POLICER_CLOCK_BIT				0
+#define ADM_POLICER_CLOCK_W				0x0F
+#define	ADM_GET_POLICER_CLOCK(adm,policer)		(*ADM_POL_CTRL(adm,policer) >>  ADM_POLICER_CLOCK_BIT) & ADM_POLICER_CLOCK_W
+#define	ADM_SET_POLICER_CLOCK(adm,policer,value)	SET_REG_BITSHIFT(ADM_POL_CTRL(adm,policer), value, ADM_POLICER_CLOCK_W , ADM_POLICER_CLOCK_BIT)
+
+#define ADM_SET_POLICER_CONTROL(adm, policer, clock, enable) SET_REG_BITSHIFT(ADM_POL_CTRL(adm,policer), ((clock & ADM_POLICER_CLOCK_W) << ADM_POLICER_CLOCK_BIT) | ((enable & ADM_POLICER_ENABLE_W) << ADM_POLICER_ENABLE_BIT), 0x1f , 0)
+
+/****************************************** LRU ***************************************************/
+#define ADM_LRU_BASE			0x00C0
+#define adm_lru(adm)			(adm_base(adm) + ADM_LRU_BASE)
+#define ADM_LRU_ENTRY(adm)		(volatile unsigned long *)(adm_lru(adm) + 0x00)
+#define ADM_LRU_ENTRY_MASK(adm)		(volatile unsigned long *)(adm_lru(adm) + 0x04)
+#define ADM_LRU_HIT_CNT_0_3(adm)	(volatile unsigned long *)(adm_lru(adm) + 0x08)
+#define ADM_LRU_HIT_CNT_4_7(adm)	(volatile unsigned long *)(adm_lru(adm) + 0x0C)
+#define ADM_LRU_HIT_CNT_8_11(adm)	(volatile unsigned long *)(adm_lru(adm) + 0x10)
+#define ADM_LRU_HIT_CNT_12_15(adm)	(volatile unsigned long *)(adm_lru(adm) + 0x14)
+#define ADM_LRU_HIT_CNT_16_19(adm)	(volatile unsigned long *)(adm_lru(adm) + 0x18)
+#define ADM_LRU_HIT_CNT_20_23(adm)	(volatile unsigned long *)(adm_lru(adm) + 0x1C)
+#define ADM_LRU_HIT_CNT_24_27(adm)	(volatile unsigned long *)(adm_lru(adm) + 0x20)
+#define ADM_LRU_HIT_CNT_28_31(adm)	(volatile unsigned long *)(adm_lru(adm) + 0x24)
+#define ADM_LRU_TIMER(adm)		(volatile unsigned long *)(adm_lru(adm) + 0x28)
+#define ADM_LRU_PERIOD(adm)		(volatile unsigned long *)(adm_lru(adm) + 0x2C)
+#define ADM_LRU_CTRL(adm)		(volatile unsigned long *)(adm_lru(adm) + 0x30)
+
+#define ADM_GET_LRU_ENTRY_BIT		0
+#define ADM_GET_LRU_ENTRY_W		0x1F
+#define ADM_GET_LRU_ENTRY(adm)		(*ADM_LRU_ENTRY(adm) >> ADM_GET_LRU_ENTRY_BIT) & ADM_GET_LRU_ENTRY_W
+
+#define ADM_GET_LRU_ENTRY_HIT_BIT	16
+#define ADM_GET_LRU_ENTRY_HIT_W		0xFF
+#define ADM_GET_LRU_ENTRY_HIT(adm)	(*ADM_LRU_ENTRY(adm) >> ADM_GET_LRU_ENTRY_HIT_BIT) & ADM_GET_LRU_ENTRY_HIT_W
+
+#define ADM_GET_LRU_MASTER_ENABLE_BIT	31
+#define ADM_GET_LRU_MASTER_ENABLE_W	0x1
+#define ADM_SET_LRU_MASTER_DISABLE(adm)  *ADM_LRU_ENTRY(adm) |= (ADM_TCAM_VALID_W << ADM_TCAM_VALID_BIT)
+#define ADM_SET_LRU_MASTER_ENABLE(adm)   *ADM_LRU_ENTRY(adm) &= ~(ADM_TCAM_VALID_W << ADM_TCAM_VALID_BIT)
+
+#define ADM_GET_LRU_HIT(adm,index)	(*(ADM_LRU_HIT_CNT_0_3(adm) + (index >> 2))) >> ((index & 0x03) * 8)
+#define ADM_GET_LRU_HIT_GROUP(adm,index) (*(ADM_LRU_HIT_CNT_0_3(adm) + (index >> 2)))
+#define ADM_GET_LRU_HIT_GROUP_ADDR(adm,index) (ADM_LRU_HIT_CNT_0_3(adm) + (index >> 2))
+
+#define ADM_SET_LRU_ENABLE(adm,entry)	*ADM_LRU_ENTRY_MASK(adm) |= (1 << entry)
+
+#define ADM_SET_LRU_DISABLE(adm,entry)	*ADM_LRU_ENTRY_MASK(adm) &= ~(1 << entry)
+
+#define ADM_SET_LRU_PERIOD(adm,value)	*ADM_LRU_PERIOD(adm) = value
+
+#define ADM_SET_LRU_MASK(adm,value)	*ADM_LRU_ENTRY_MASK(adm) = value
+
+#define ADM_SET_LRU_STATE(adm, value)	*ADM_LRU_CTRL(adm) |= value
+
+/* 0x40F4 - 0x40FC reserved */
+
+/********************************************** BATCH ******************************************************/
+#define ADM_BATCH_BASE			0x0100
+#define adm_batch(adm)			(adm_base(adm) + ADM_BATCH_BASE)
+#define ADM_BATCH_PKT_CNT(adm)		(volatile unsigned long *)(adm_batch(adm) + 0x00)
+#define ADM_BATCH_PKT_THRESHOLD(adm)	(volatile unsigned long *)(adm_batch(adm) + 0x04)
+#define ADM_BATCH_TIMER(adm)		(volatile unsigned long *)(adm_batch(adm) + 0x08)
+#define ADM_BATCH_TIMER_INIT_VAL(adm)	(volatile unsigned long *)(adm_batch(adm) + 0x0C)
+#define ADM_BATCH_INT_STATUS(adm)	(volatile unsigned long *)(adm_batch(adm) + 0x10)
+
+/******************************************* TCAM **********************************************************/
+/* Up to 31 entries can be configured */
+#define ADM_TCAM_BASE				0x0800
+#define ADM_TCAM_ENTRY_SIZE			0x0040
+#define check_entry(entry)			(entry > (ADM_TCAM_MAX_ENTRIES - 1) ? 0 : entry) /* means upon error entry 0 is selected */
+#define adm_tcam(adm, entry)			(adm_base(adm) + ADM_TCAM_BASE + (check_entry(entry) * ADM_TCAM_ENTRY_SIZE))
+#define ADM_TCAM_VALID(adm,entry)		(volatile unsigned long *)(adm_tcam(adm,entry) + 0x00)
+#define ADM_TCAM_ETHERTYPE(adm,entry)		(volatile unsigned long *)(adm_tcam(adm,entry) + 0x04)
+#define ADM_TCAM_VLANID(adm,entry)		(volatile unsigned long *)(adm_tcam(adm,entry) + 0x08)
+#define ADM_TCAM_PPPOEPROTO(adm,entry)		(volatile unsigned long *)(adm_tcam(adm,entry) + 0x0C)
+#define ADM_TCAM_IPTOS(adm,entry)		(volatile unsigned long *)(adm_tcam(adm,entry) + 0x10)
+#define ADM_TCAM_IPPROTO(adm,entry)		(volatile unsigned long *)(adm_tcam(adm,entry) + 0x14)
+#define ADM_TCAM_IPSRC(adm,entry)		(volatile unsigned long *)(adm_tcam(adm,entry) + 0x18)
+#define ADM_TCAM_IPDST(adm,entry)		(volatile unsigned long *)(adm_tcam(adm,entry) + 0x1C)
+#define ADM_TCAM_L4_SPORT(adm,entry)		(volatile unsigned long *)(adm_tcam(adm,entry) + 0x20)
+#define ADM_TCAM_L4_DPORT(adm,entry)		(volatile unsigned long *)(adm_tcam(adm,entry) + 0x24)
+#define ADM_TCAM_MCAST_BCAST(adm,entry)		(volatile unsigned long *)(adm_tcam(adm,entry) + 0x28)
+#define ADM_TCAM_STATE(adm,entry)		(volatile unsigned long *)(adm_tcam(adm,entry) + 0x2C)
+#define ADM_TCAM_MASK_REG0(adm,entry)		(volatile unsigned long *)(adm_tcam(adm,entry) + 0x30)
+#define ADM_TCAM_MASK_REG1(adm,entry)		(volatile unsigned long *)(adm_tcam(adm,entry) + 0x34)
+#define ADM_TCAM_MASK_REG2(adm,entry)		(volatile unsigned long *)(adm_tcam(adm,entry) + 0x38)
+#define ADM_TCAM_RESERVED(adm,entry)		(volatile unsigned long *)(adm_tcam(adm,entry) + 0x3C)
+
+#define ADM_GET_TCAM_ENTRY(adm,entry)		(volatile unsigned long *)(adm_tcam(adm,entry))
+
+#define ADM_TCAM_VALID_BIT				0
+#define ADM_TCAM_VALID_W				0x01
+#define ADM_GET_TCAM_ENABLE(adm,entry)			(*ADM_TCAM_VALID(adm,entry) >> ADM_TCAM_VALID_BIT) & ADM_TCAM_VALID_W
+#define ADM_SET_TCAM_ENABLE(adm,entry,value)		SET_REG_BITSHIFT(ADM_TCAM_VALID(adm,entry), value, ADM_TCAM_VALID_W,  ADM_TCAM_VALID_BIT)
+
+#define ADM_TCAM_ETHERTYPE_BIT				0
+#define ADM_TCAM_ETHERTYPE_W				0xFFFF
+#define ADM_GET_TCAM_ETHERTYPE(adm,entry)		(*ADM_TCAM_ETHERTYPE(adm,entry) >> ADM_TCAM_ETHERTYPE_BIT) & ADM_TCAM_ETHERTYPE_W
+#define ADM_SET_TCAM_ETHERTYPE(adm,entry,value)		SET_REG_BITSHIFT(ADM_TCAM_ETHERTYPE(adm,entry), value, ADM_TCAM_ETHERTYPE_W , ADM_TCAM_ETHERTYPE_BIT)
+
+#define ADM_TCAM_VLANID_BIT				0
+#define ADM_TCAM_VLANID_W				0xFFFF
+#define ADM_GET_TCAM_VLANID(adm,entry)			(*ADM_TCAM_VLANID(adm,entry) >> ADM_TCAM_VLANID_BIT) & ADM_TCAM_VLANID_W
+#define ADM_SET_TCAM_VLANID(adm,entry,value)		SET_REG_BITSHIFT(ADM_TCAM_VLANID(adm,entry), value, ADM_TCAM_VLANID_W , ADM_TCAM_VLANID_BIT)
+
+#define	ADM_TCAM_PPPOEPROTO_IP4_BIT			0
+#define ADM_TCAM_PPPOEPROTO_IP4_W			0x01
+#define ADM_GET_TCAM_PPPOEPROTO_IP4(adm,entry)		(*ADM_TCAM_PPPOEPROTO(adm,entry) >> ADM_TCAM_PPPOEPROTO_IP4_BIT) & ADM_TCAM_PPPOEPROTO_IP4_W
+#define ADM_SET_TCAM_PPPOEPROTO_IP4(adm,entry,value) SET_REG_BITSHIFT(ADM_TCAM_PPPOEPROTO(adm,entry), value, ADM_TCAM_PPPOEPROTO_IP4_W , ADM_TCAM_PPPOEPROTO_IP4_BIT)
+
+#define	ADM_TCAM_PPPOEPROTO_IP6_BIT			1
+#define ADM_TCAM_PPPOEPROTO_IP6_W			0x01
+#define ADM_GET_TCAM_PPPOEPROTO_IP6(adm,entry)		(*ADM_TCAM_PPPOEPROTO(adm,entry) >> ADM_TCAM_PPPOEPROTO_IP6_BIT) & ADM_TCAM_PPPOEPROTO_IP6_W
+#define ADM_SET_TCAM_PPPOEPROTO_IP6(adm,entry,value)	 SET_REG_BITSHIFT(ADM_TCAM_PPPOEPROTO(adm,entry), value, ADM_TCAM_PPPOEPROTO_IP6_W, ADM_TCAM_PPPOEPROTO_IP6_BIT)
+
+#define ADM_SET_TCAM_PPPOEPROTO(adm,entry,pppoe_ipv4,pppoe_ipv6)        SET_REG_BITSHIFT(ADM_TCAM_PPPOEPROTO(adm,entry), ((pppoe_ipv4 & ADM_TCAM_PPPOEPROTO_IP4_W) << ADM_TCAM_PPPOEPROTO_IP4_BIT) | ((pppoe_ipv6 & ADM_TCAM_PPPOEPROTO_IP6_W) << ADM_TCAM_PPPOEPROTO_IP6_BIT), 0xFFFFFFFF, 0)
+
+#define	ADM_TCAM_IPTOS_BIT				0
+#define ADM_TCAM_IPTOS_W				0xFF
+#define ADM_GET_TCAM_IPTOS(adm,entry)			(*ADM_TCAM_IPTOS(adm,entry) >> ADM_TCAM_IPTOS_BIT) & ADM_TCAM_IPTOS_W
+#define ADM_SET_TCAM_IPTOS(adm,entry,value)		SET_REG_BITSHIFT(ADM_TCAM_IPTOS(adm,entry) , value, ADM_TCAM_IPTOS_W , ADM_TCAM_IPTOS_BIT)
+
+#define	ADM_TCAM_IPPROTO_BIT				0
+#define ADM_TCAM_IPPROTO_W				0xFF
+#define ADM_GET_TCAM_IPPROTO(adm,entry)			(*ADM_TCAM_IPPROTO(adm,entry) >> ADM_TCAM_IPPROTO_BIT) & ADM_TCAM_IPPROTO_W
+#define ADM_SET_TCAM_IPPROTO(adm,entry,value)		SET_REG_BITSHIFT(ADM_TCAM_IPPROTO(adm,entry) , value, ADM_TCAM_IPPROTO_W , ADM_TCAM_IPPROTO_BIT)
+
+#define ADM_GET_TCAM_IPSRC(adm,entry)			*ADM_TCAM_IPSRC(adm,entry)
+#define ADM_SET_TCAM_IPSRC(adm,entry,value)		*ADM_TCAM_IPSRC(adm,entry) = value
+
+#define ADM_GET_TCAM_IPDST(adm,entry)			*ADM_TCAM_IPDST(adm,entry)
+#define ADM_SET_TCAM_IPDST(adm,entry,value)		*ADM_TCAM_IPDST(adm,entry) = value
+
+#define	ADM_TCAM_L4_SPORT_MIN_BIT			0
+#define ADM_TCAM_L4_SPORT_MIN_W				0xFFFF
+#define ADM_GET_TCAM_L4_SPORT_MIN(adm,entry)		(*ADM_TCAM_L4_SPORT(adm,entry) >> ADM_TCAM_L4_SPORT_MIN_BIT) & ADM_TCAM_L4_SPORT_MIN_W
+#define ADM_SET_TCAM_L4_SPORT_MIN(adm,entry,value)	SET_REG_BITSHIFT(ADM_TCAM_L4_SPORT(adm,entry), value, ADM_TCAM_L4_SPORT_MIN_W , ADM_TCAM_L4_SPORT_MIN_BIT)
+
+#define	ADM_TCAM_L4_SPORT_MAX_BIT			16
+#define ADM_TCAM_L4_SPORT_MAX_W				0xFFFF
+#define ADM_GET_TCAM_L4_SPORT_MAX(adm,entry)		(*ADM_TCAM_L4_SPORT(adm,entry) >> ADM_TCAM_L4_SPORT_MAX_BIT) & ADM_TCAM_L4_SPORT_MAX_W
+#define ADM_SET_TCAM_L4_SPORT_MAX(adm,entry,value)	SET_REG_BITSHIFT(ADM_TCAM_L4_SPORT(adm,entry), value, ADM_TCAM_L4_SPORT_MAX_W , ADM_TCAM_L4_SPORT_MAX_BIT)
+
+#define ADM_SET_TCAM_L4_SPORT(adm,entry,min,max)        SET_REG_BITSHIFT(ADM_TCAM_L4_SPORT(adm,entry), ((min & ADM_TCAM_L4_SPORT_MIN_W) << ADM_TCAM_L4_SPORT_MIN_BIT) | ((max & ADM_TCAM_L4_SPORT_MAX_W) << ADM_TCAM_L4_SPORT_MAX_BIT), 0xFFFFFFFF, 0)
+
+#define	ADM_TCAM_L4_DPORT_MIN_BIT			0
+#define ADM_TCAM_L4_DPORT_MIN_W				0xFFFF
+#define ADM_GET_TCAM_L4_DPORT_MIN(adm,entry)		(*ADM_TCAM_L4_DPORT(adm,entry) >> ADM_TCAM_L4_SPORT_MIN_BIT) & ADM_TCAM_L4_DPORT_MIN_W
+#define ADM_SET_TCAM_L4_DPORT_MIN(adm,entry,value)	SET_REG_BITSHIFT(ADM_TCAM_L4_DPORT(adm,entry), value, ADM_TCAM_L4_DPORT_MIN_W , ADM_TCAM_L4_SPORT_MIN_BIT)
+
+#define	ADM_TCAM_L4_DPORT_MAX_BIT			16
+#define ADM_TCAM_L4_DPORT_MAX_W				0xFFFF
+#define ADM_GET_TCAM_L4_DPORT_MAX(adm,entry)		(*ADM_TCAM_L4_DPORT(adm,entry) >> ADM_TCAM_L4_DPORT_MAX_BIT) & ADM_TCAM_L4_DPORT_MAX_W
+#define ADM_SET_TCAM_L4_DPORT_MAX(adm,entry,value)	SET_REG_BITSHIFT(ADM_TCAM_L4_DPORT(adm,entry), value, ADM_TCAM_L4_DPORT_MAX_W , ADM_TCAM_L4_DPORT_MAX_BIT)
+
+#define ADM_SET_TCAM_L4_DPORT(adm,entry,min,max)	SET_REG_BITSHIFT(ADM_TCAM_L4_DPORT(adm,entry), ((min & ADM_TCAM_L4_DPORT_MIN_W) << ADM_TCAM_L4_DPORT_MIN_BIT) |((max & ADM_TCAM_L4_DPORT_MAX_W) << ADM_TCAM_L4_DPORT_MAX_BIT), 0xFFFFFFFF, 0)
+
+#define	ADM_TCAM_MCAST_BIT				0
+#define ADM_TCAM_MCAST_W				0x01
+#define ADM_GET_TCAM_MCAST(adm,entry)			(*ADM_TCAM_MCAST_BCAST(adm,entry) >> ADM_TCAM_MCAST_BIT) & ADM_TCAM_MCAST_W
+#define ADM_SET_TCAM_MCAST(adm,entry,value)		SET_REG_BITSHIFT(ADM_TCAM_MCAST_BCAST(adm,entry), value, ADM_TCAM_MCAST_W , ADM_TCAM_MCAST_BIT)
+
+#define ADM_TCAM_BCAST_BIT                              0
+#define ADM_TCAM_BCAST_W                                0x01
+#define ADM_GET_TCAM_BCAST(adm,entry)                   (*ADM_TCAM_MCAST_BCAST(adm,entry) >> ADM_TCAM_BCAST_BIT) & ADM_TCAM_BCAST_W
+#define ADM_SET_TCAM_BCAST(adm,entry,value)             SET_REG_BITSHIFT(ADM_TCAM_MCAST_BCAST(adm,entry), value, ADM_TCAM_BCAST_W , ADM_TCAM_BCAST_BIT)
+
+#define ADM_SET_TCAM_MCAST_BCAST(adm,entry,mcast,bcast) SET_REG_BITSHIFT(ADM_TCAM_MCAST_BCAST(adm,entry), ((mcast & ADM_TCAM_MCAST_W) << ADM_TCAM_MCAST_BIT)|((bcast & ADM_TCAM_BCAST_W) << ADM_TCAM_BCAST_BIT), 0xFFFFFFFF, 0)
+
+#define	ADM_TCAM_DROP_BIT				0
+#define ADM_TCAM_DROP_W					0x01
+#define ADM_GET_TCAM_DROP(adm,entry)			(*ADM_TCAM_STATE(adm,entry) >> ADM_TCAM_DROP_BIT) & ADM_TCAM_DROP_W
+#define ADM_SET_TCAM_DROP(adm,entry,value)		SET_REG_BITSHIFT(ADM_TCAM_STATE(adm,entry), value, ADM_TCAM_DROP_W , ADM_TCAM_DROP_BIT)
+
+#define	ADM_TCAM_POLICER_BIT				1
+#define ADM_TCAM_POLICER_W				0x03
+#define ADM_GET_TCAM_POLICER(adm,entry)			(*ADM_TCAM_STATE(adm,entry) >> ADM_TCAM_POLICER_BIT) & ADM_TCAM_POLICER_W
+#define ADM_SET_TCAM_POLICER(adm,entry,value)		SET_REG_BITSHIFT(ADM_TCAM_STATE(adm,entry) , value, ADM_TCAM_POLICER_W , ADM_TCAM_POLICER_BIT)
+
+#define	ADM_TCAM_RSVD_BIT				3
+#define ADM_TCAM_RSVD_W					0x01
+#define ADM_GET_TCAM_RSVD(adm,entry)			(*ADM_TCAM_STATE(adm,entry) >> ADM_TCAM_RSVD_BIT) & ADM_TCAM_RSVD_W
+#define ADM_SET_TCAM_RSVD(adm,entry,value)		SET_REG_BITSHIFT(ADM_TCAM_STATE(adm,entry) , value, ADM_TCAM_RSVD_W , ADM_TCAM_RSVD_BIT)
+
+#define ADM_SET_TCAM_ETHERTYPE_MASK_BIT			0
+#define ADM_SET_TCAM_ETHERTYPE_MASK_W			0xFFFF
+#define ADM_GET_TCAM_ETHERTYPE_MASK(adm,entry)		(*ADM_TCAM_MASK_REG0(adm,entry) >> ADM_SET_TCAM_ETHERTYPE_MASK_BIT) & ADM_SET_TCAM_ETHERTYPE_MASK_W
+#define ADM_SET_TCAM_ETHERTYPE_MASK(adm,entry,value)	 SET_REG_BITSHIFT(ADM_TCAM_MASK_REG0(adm,entry) , value, ADM_SET_TCAM_ETHERTYPE_MASK_W , ADM_SET_TCAM_ETHERTYPE_MASK_BIT)
+
+#define ADM_SET_TCAM_VLAN_MASK_BIT			16
+#define ADM_SET_TCAM_VLAN_MASK_W			0xFFFF
+#define ADM_GET_TCAM_VLAN_MASK(adm,entry)		(*ADM_TCAM_MASK_REG0(adm,entry) >> ADM_SET_TCAM_VLAN_MASK_BIT) & ADM_SET_TCAM_VLAN_MASK_W
+#define ADM_SET_TCAM_VLAN_MASK(adm,entry,value)		SET_REG_BITSHIFT(ADM_TCAM_MASK_REG0(adm,entry), value, ADM_SET_TCAM_VLAN_MASK_W ,ADM_SET_TCAM_VLAN_MASK_BIT)
+
+#define ADM_SET_TCAM_MASK_REG0(adm,entry,etype,vlan)    SET_REG_BITSHIFT(ADM_TCAM_MASK_REG0(adm,entry), ((etype & ADM_SET_TCAM_ETHERTYPE_MASK_W) << ADM_SET_TCAM_ETHERTYPE_MASK_BIT) | ((vlan & ADM_SET_TCAM_VLAN_MASK_W) << ADM_SET_TCAM_VLAN_MASK_BIT), 0xFFFFFFFF, 0)
+
+#define ADM_SET_TCAM_PPPOE_MASK_BIT			0
+#define ADM_SET_TCAM_PPPOE_MASK_W			0x03
+#define ADM_GET_TCAM_PPPOE_MASK(adm,entry)		(*ADM_TCAM_MASK_REG1(adm,entry) >> ADM_SET_TCAM_PPPOE_MASK_BIT) & ADM_SET_TCAM_PPPOE_MASK_W
+#define ADM_SET_TCAM_PPPOE_MASK(adm,entry,value)	SET_REG_BITSHIFT(ADM_TCAM_MASK_REG1(adm,entry), value, ADM_SET_TCAM_PPPOE_MASK_W , ADM_SET_TCAM_PPPOE_MASK_BIT)
+
+#define ADM_SET_TCAM_SPORT_MASK_BIT			2
+#define ADM_SET_TCAM_SPORT_MASK_W			0x01
+#define ADM_GET_TCAM_SPORT_MASK(adm,entry)		(*ADM_TCAM_MASK_REG1(adm,entry) >> ADM_SET_TCAM_SPORT_MASK_BIT) & ADM_SET_TCAM_SPORT_MASK_W
+#define ADM_SET_TCAM_SPORT_MASK(adm,entry,value)	SET_REG_BITSHIFT(ADM_TCAM_MASK_REG1(adm,entry), value, ADM_SET_TCAM_SPORT_MASK_W , ADM_SET_TCAM_SPORT_MASK_BIT)
+
+#define ADM_SET_TCAM_DPORT_MASK_BIT			3
+#define ADM_SET_TCAM_DPORT_MASK_W			0x01
+#define ADM_GET_TCAM_DPORT_MASK(adm,entry)		(*ADM_TCAM_MASK_REG1(adm,entry) >> ADM_SET_TCAM_DPORT_MASK_BIT) & ADM_SET_TCAM_DPORT_MASK_W
+#define ADM_SET_TCAM_DPORT_MASK(adm,entry,value)	SET_REG_BITSHIFT(ADM_TCAM_MASK_REG1(adm,entry), value, ADM_SET_TCAM_DPORT_MASK_W , ADM_SET_TCAM_DPORT_MASK_BIT)
+
+#define ADM_SET_TCAM_IPTOS_MASK_BIT			8
+#define ADM_SET_TCAM_IPTOS_MASK_W			0xFF
+#define ADM_GET_TCAM_IPTOS_MASK(adm,entry)		(*ADM_TCAM_MASK_REG1(adm,entry) >> ADM_SET_TCAM_IPTOS_MASK_BIT) & ADM_SET_TCAM_IPTOS_MASK_W
+#define ADM_SET_TCAM_IPTOS_MASK(adm,entry,value)	SET_REG_BITSHIFT(ADM_TCAM_MASK_REG1(adm,entry) , value, ADM_SET_TCAM_IPTOS_MASK_W , ADM_SET_TCAM_IPTOS_MASK_BIT)
+
+#define ADM_SET_TCAM_IPPROTO_MASK_BIT			16
+#define ADM_SET_TCAM_IPPROTO_MASK_W			0xFF
+#define ADM_GET_TCAM_IPPROTO_MASK(adm,entry)		(*ADM_TCAM_MASK_REG1(adm,entry) >> ADM_SET_TCAM_IPPROTO_MASK_BIT) & ADM_SET_TCAM_IPPROTO_MASK_W
+#define ADM_SET_TCAM_IPPROTO_MASK(adm,entry,value)	SET_REG_BITSHIFT(ADM_TCAM_MASK_REG1(adm,entry) , value, ADM_SET_TCAM_IPPROTO_MASK_W , ADM_SET_TCAM_IPPROTO_MASK_BIT)
+
+#define ADM_SET_TCAM_MCAST_MASK_BIT			24
+#define ADM_SET_TCAM_MCAST_MASK_W			0x01
+#define ADM_GET_TCAM_MCAST_MASK(adm,entry)		(*ADM_TCAM_MASK_REG1(adm,entry) >> ADM_SET_TCAM_MCAST_MASK_BIT) & ADM_SET_TCAM_MCAST_MASK_W
+#define ADM_SET_TCAM_MCAST_MASK(adm,entry,value)	SET_REG_BITSHIFT(ADM_TCAM_MASK_REG1(adm,entry) , value, ADM_SET_TCAM_MCAST_MASK_W , ADM_SET_TCAM_MCAST_MASK_BIT)
+
+#define ADM_SET_TCAM_BCAST_MASK_BIT			25
+#define ADM_SET_TCAM_BCAST_MASK_W			0x01
+#define ADM_GET_TCAM_BCAST_MASK(adm,entry)		(*ADM_TCAM_MASK_REG1(adm,entry) >> ADM_SET_TCAM_BCAST_MASK_BIT) & ADM_SET_TCAM_BCAST_MASK_W
+#define ADM_SET_TCAM_BCAST_MASK(adm,entry,value)	SET_REG_BITSHIFT(ADM_TCAM_MASK_REG1(adm,entry), value, ADM_SET_TCAM_BCAST_MASK_W , ADM_SET_TCAM_BCAST_MASK_BIT)
+
+#define ADM_SET_TCAM_MASK_REG1(adm,entry,pppoe,sport,dport,iptos,ipproto,mcast,bcast)       SET_REG_BITSHIFT(ADM_TCAM_MASK_REG1(adm,entry),((pppoe & ADM_SET_TCAM_PPPOE_MASK_W) << ADM_SET_TCAM_PPPOE_MASK_BIT) | ((sport & ADM_SET_TCAM_SPORT_MASK_W) << ADM_SET_TCAM_SPORT_MASK_BIT) | ((dport & ADM_SET_TCAM_DPORT_MASK_W) << ADM_SET_TCAM_DPORT_MASK_BIT) | ((iptos & ADM_SET_TCAM_IPTOS_MASK_W) << ADM_SET_TCAM_IPTOS_MASK_BIT) | ((ipproto & ADM_SET_TCAM_IPPROTO_MASK_W) << ADM_SET_TCAM_IPPROTO_MASK_BIT) | ((mcast & ADM_SET_TCAM_MCAST_MASK_W) << ADM_SET_TCAM_MCAST_MASK_BIT) | ((bcast & ADM_SET_TCAM_BCAST_MASK_W) << ADM_SET_TCAM_BCAST_MASK_BIT), 0xFFFFFFFF, 0)
+
+#define ADM_SET_TCAM_SADDR_MASK_BIT			0
+#define ADM_SET_TCAM_SADDR_MASK_W			0x3F
+#define ADM_GET_TCAM_SADDR_MASK(adm,entry)		(*ADM_TCAM_MASK_REG2(adm,entry) >> ADM_SET_TCAM_SADDR_MASK_BIT) & ADM_SET_TCAM_SADDR_MASK_W
+#define ADM_SET_TCAM_SADDR_MASK(adm,entry,value)	SET_REG_BITSHIFT(ADM_TCAM_MASK_REG2(adm,entry), value, ADM_SET_TCAM_SADDR_MASK_W , ADM_SET_TCAM_SADDR_MASK_BIT)
+
+#define ADM_SET_TCAM_DADDR_MASK_BIT			8
+#define ADM_SET_TCAM_DADDR_MASK_W			0x3F
+#define ADM_GET_TCAM_DADDR_MASK(adm,entry)		(*ADM_TCAM_MASK_REG2(adm, entry) >> ADM_SET_TCAM_DADDR_MASK_BIT) & ADM_SET_TCAM_DADDR_MASK_W
+#define ADM_SET_TCAM_DADDR_MASK(adm,entry,value)	SET_REG_BITSHIFT(ADM_TCAM_MASK_REG2(adm,entry), value, ADM_SET_TCAM_DADDR_MASK_W , ADM_SET_TCAM_DADDR_MASK_BIT)
+
+#define ADM_SET_TCAM_MASK_REG2(adm,entry,saddr,daddr)   SET_REG_BITSHIFT(ADM_TCAM_MASK_REG2(adm,entry), ((saddr & ADM_SET_TCAM_SADDR_MASK_W) << ADM_SET_TCAM_SADDR_MASK_BIT) | ((daddr & ADM_SET_TCAM_DADDR_MASK_W) << ADM_SET_TCAM_DADDR_MASK_BIT), 0xFFFFFFFF, 0)
+
+#endif /* _ADM_H */
diff --git a/drivers/net/transcede/m84xxx_common.h b/drivers/net/transcede/m84xxx_common.h
new file mode 100644
index 0000000..bb65a59
--- /dev/null
+++ b/drivers/net/transcede/m84xxx_common.h
@@ -0,0 +1,53 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef _COMMON_H
+#define _COMMON_H
+
+#include <asm/delay.h>
+#include <mach/hardware.h>
+
+#if !defined(CONFIG_MACH_M84XXX)
+#define  C1K_REG_SIM
+#endif
+
+#define EMAC0_BASE TRANSCEDE_GEM0
+#define EMAC1_BASE TRANSCEDE_GEM1
+
+#define REFCLK	125000000UL
+
+#ifdef C1K_REG_SIM
+unsigned long get_gemac_base(int gemac);
+#endif
+
+static inline void SET_REG_BITSHIFT(volatile unsigned long * reg, unsigned long value, unsigned long width, unsigned long shift)
+{
+	unsigned long tmp = *reg;
+	tmp &= ~(width << shift);
+	tmp |= (value & width) << shift;
+	*reg = tmp;
+	/* Prevent W/R race condition */
+	udelay(5);
+}
+
+
+#endif /* _SCH_H */
diff --git a/drivers/net/transcede/m84xxx_sch.h b/drivers/net/transcede/m84xxx_sch.h
new file mode 100644
index 0000000..9885250
--- /dev/null
+++ b/drivers/net/transcede/m84xxx_sch.h
@@ -0,0 +1,326 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef _SCH_H
+#define _SCH_H
+
+#include "m84xxx_common.h"
+
+
+extern unsigned long *FAKE_EMAC0_BASE;
+extern unsigned long *FAKE_EMAC1_BASE;
+
+/* There's one scheduler block per GEM interface */
+#ifdef C1K_REG_SIM
+#warning !!!!!!!! running with fake gemac registers
+#define sch_base(sch) (get_gemac_base(sch) + 0x8000)
+#else
+#define sch_base(sch) ((sch > 0 ? AAB_XP_VADDR(EMAC1_BASE) : AAB_XP_VADDR(EMAC0_BASE)) + 0x4000)
+#endif
+
+/******************************** STATUS register ********************************/
+#define SCH_STATUS_REG(sch)		(volatile unsigned long *)(sch_base(sch) + 0x00)
+
+#define SCH_STATUS_QEMPTY_BIT		0
+#define SCH_STATUS_QEMPTY_W		0xFF
+#define SCH_GET_STATUS_QEMPTY(sch)	(*SCH_STATUS_REG(sch) >> SCH_STATUS_QEMPTY_BIT) & SCH_STATUS_QEMPTY_W
+
+#define SCH_STATUS_QNEXT_BIT		8
+#define SCH_STATUS_QNEXT_W		0xFF
+#define SCH_GET_STATUS_QNEXT(sch)	(*SCH_STATUS_REG(sch) >> SCH_STATUS_QNEXT_BIT) & SCH_STATUS_QNEXT_W
+
+#define SCH_STATUS_Q0RDY_BIT		16
+#define SCH_STATUS_Q0RDY_W		0x01
+#define SCH_GET_STATUS_Q0RDY(sch)	(*SCH_STATUS_REG(sch) >> SCH_STATUS_Q0RDY_BIT) & SCH_STATUS_Q0RDY_W
+
+#define SCH_STATUS_Q1RDY_BIT		17
+#define SCH_STATUS_Q1RDY_W		0x01
+#define SCH_GET_STATUS_Q1RDY(sch)	(*SCH_STATUS_REG(sch) >> SCH_STATUS_Q1RDY_BIT) & SCH_STATUS_Q1RDY_W
+
+#define SCH_STATUS_Q2RDY_BIT		18
+#define SCH_STATUS_Q2RDY_W		0x01
+#define SCH_GET_STATUS_Q2RDY(sch)	(*SCH_STATUS_REG(sch) >> SCH_STATUS_Q2RDY_BIT) & SCH_STATUS_Q2RDY_W
+
+#define SCH_STATUS_Q3RDY_BIT		19
+#define SCH_STATUS_Q3RDY_W		0x01
+#define SCH_GET_STATUS_Q3RDY(sch)	(*SCH_STATUS_REG(sch) >> SCH_STATUS_Q3RDY_BIT) & SCH_STATUS_Q3RDY_W
+
+#define SCH_STATUS_Q4RDY_BIT		20
+#define SCH_STATUS_Q4RDY_W		0x01
+#define SCH_GET_STATUS_Q4RDY(sch)	(*SCH_STATUS_REG(sch) >> SCH_STATUS_Q4RDY_BIT) & SCH_STATUS_Q4RDY_W
+
+#define SCH_STATUS_Q5RDY_BIT		21
+#define SCH_STATUS_Q5RDY_W		0x01
+#define SCH_GET_STATUS_Q5RDY(sch)	(*SCH_STATUS_REG(sch) >> SCH_STATUS_Q5RDY_BIT) & SCH_STATUS_Q5RDY_W
+
+#define SCH_STATUS_BESTRDY_BIT		22
+#define SCH_STATUS_BESTRDY_W		0x01
+#define SCH_GET_STATUS_BESTRDY(sch)	(*SCH_STATUS_REG(sch) >> SCH_STATUS_BESTRDY_BIT) & SCH_STATUS_BESTRDY_W
+
+#define SCH_STATUS_DATARDY_BIT		23
+#define SCH_STATUS_DATARDY_W		0x01
+#define SCH_GET_STATUS_DATARDY(sch)	(*SCH_STATUS_REG(sch) >> SCH_STATUS_DATARDY_BIT) & SCH_STATUS_DATARDY_W
+
+#define SCH_STATUS_VIDEORDY_BIT		24
+#define SCH_STATUS_VIDEORDY_W		0x01
+#define SCH_GET_STATUS_VIDEORDY(sch)	(*SCH_STATUS_REG(sch) >> SCH_STATUS_VIDEORDY_BIT) & SCH_STATUS_VIDEORDY_W
+
+#define SCH_STATUS_VOICERDY_BIT		25
+#define SCH_STATUS_VOICERDY_W		0x01
+#define SCH_GET_STATUS_VOICERDY(sch)	(*SCH_STATUS_REG(sch) >> SCH_STATUS_VOICERDY_BIT) & SCH_STATUS_VOICERDY_W
+
+#define SCH_STATUS_PORTRDY_BIT		26
+#define SCH_STATUS_PORTRDY_W		0x01
+#define SCH_GET_STATUS_PORTRDY(sch)	(*SCH_STATUS_REG(sch) >> SCH_STATUS_PORTRDY_BIT) & SCH_STATUS_PORTRDY_W
+
+#define SCH_STATUS_QSELSM_BIT		27
+#define SCH_STATUS_QSELSM_W		0x07
+#define SCH_GET_STATUS_QSELSM(sch)	(*SCH_STATUS_REG(sch) >> SCH_STATUS_QSELSM_BIT) & SCH_STATUS_QSELSM_W
+
+#define SCH_STATUS_PKTERR_BIT		30
+#define SCH_STATUS_PKTERR_W		0x01
+#define SCH_GET_STATUS_PKTERR(sch)	(*SCH_STATUS_REG(sch) >> SCH_STATUS_PKTERR_BIT) & SCH_STATUS_PKTERR_W
+
+#define SCH_STATUS_RDY_BIT		31
+#define SCH_STATUS_RDY_W		0x01
+#define SCH_GET_STATUS_RDY(sch)		(*SCH_STATUS_REG(sch) >> SCH_STATUS_RDY_BIT) & SCH_STATUS_RDY_W
+
+
+/******************************** CONTROL register ********************************/
+#define SCH_CTRL_REG(sch)		(volatile unsigned long *)(sch_base(sch) + 0x04)
+
+#define SCH_CTRL_SHAPER_ENABLE_BIT	0
+#define SCH_SET_SHAPER_ENABLE(sch)	*SCH_CTRL_REG(sch) |= (1 << SCH_CTRL_SHAPER_ENABLE_BIT)
+#define SCH_SET_SHAPER_DISABLE(sch)	*SCH_CTRL_REG(sch) &= ~(1 << SCH_CTRL_SHAPER_ENABLE_BIT)
+
+#define SCH_CTRL_DWRR_ENABLE_BIT	1
+#define SCH_SET_DWRR_ENABLE(sch)	*SCH_CTRL_REG(sch) |= (1 << SCH_CTRL_DWRR_ENABLE_BIT)
+#define SCH_SET_DWRR_DISABLE(sch)	*SCH_CTRL_REG(sch) &= ~(1 << SCH_CTRL_DWRR_ENABLE_BIT)
+
+#define SCH_CTRL_SNAPSHOT_BIT		16
+#define SCH_SET_SNAPSHOT(sch)		*SCH_CTRL_REG(sch) |= (1 << SCH_CTRL_SNAPSHOT_BIT)
+
+#define SCH_CTRL_GEM_SNAPSHOT_BIT	17
+#define SCH_SET_GEM_SNAPSHOT(sch)	*SCH_CTRL_REG(sch) |= (1 << SCH_CTRL_GEM_SNAPSHOT_BIT)
+
+
+
+/******************************** PACKET QUEUED register ********************************/
+#define SCH_PKT_QUEUED(sch)			(volatile unsigned long *)(sch_base(sch) + 0x08)
+
+#define	SCH_PKT_QUEUED_SIZE_BIT			0
+#define SCH_PKT_QUEUED_SIZE_W			0xFFFF
+#define	SCH_PKT_QUEUED_Q_BIT			16
+#define SCH_PKT_QUEUED_Q_W			0x07
+
+#define SCH_SET_PKT_QUEUED(sch,queue,size)	*SCH_PKT_QUEUED(sch) = ((size & SCH_PKT_QUEUED_SIZE_W) << SCH_PKT_QUEUED_SIZE_BIT) | ((queue & SCH_PKT_QUEUED_Q_W) << SCH_PKT_QUEUED_Q_BIT)
+
+
+/******************************** PORT BYTE register ********************************/
+#define SCH_PORT_BYTE_CNT(sch)		(volatile unsigned long *)(sch_base(sch) + 0x0C)
+
+#define SCH_PORT_BYTE_CNT_BIT		0
+#define SCH_PORT_BYTE_CNT_W		0xFFFFFFFF
+#define SCH_GET_PORT_BYTE_CNT(sch)	(*SCH_PORT_BYTE_CNT(sch) >> SCH_PORT_BYTE_CNT_BIT) & SCH_PORT_BYTE_CNT_W
+
+/******************************** PORT PACKET COUNT register ********************************/
+#define SCH_PORT_PKT_CNT(sch)		(volatile unsigned long *)(sch_base(sch) + 0x10)
+
+#define SCH_PORT_PKT_CNT_BIT		0
+#define SCH_PORT_PKT_CNT_W		0x03FFFFFF
+#define SCH_GET_PORT_PKT_CNT(sch)	(*SCH_PORT_PKT_CNT(sch) >> SCH_PORT_PKT_CNT_BIT) & SCH_PORT_PKT_CNT_W
+
+/******************************** PACKET OVERHEAD register ********************************/
+#define SCH_PKT_OVERHEAD(sch)		(volatile unsigned long *)(sch_base(sch) + 0x14)
+
+/******************************** HW FAULT STATUS register ********************************/
+#define SCH_HW_FAULT_STATUS(sch)	(volatile unsigned long *)(sch_base(sch) + 0x18)
+
+/******************************** HW FAULT MASK register ********************************/
+#define SCH_HW_FAULT_MASK(sch)		(volatile unsigned long *)(sch_base(sch) + 0x1C)
+
+/* 0x8020 - 0x803C reserved */
+
+
+/******************************** PORT  aggregate Q0 to Q7 ********************************/
+#define SCH_PORT_BASE			0x0040
+#define sch_port(sch)			(sch_base(sch) + SCH_PORT_BASE)
+
+#define SCH_PORT_BYTE_IN_Q(sch)		(volatile unsigned long *)(sch_port(sch) + 0x00)
+#define SCH_PORT_PKT_IN_Q(sch)		(volatile unsigned long *)(sch_port(sch) + 0x04)
+#define SCH_PORT_FRAC_RATE(sch)		(volatile unsigned long *)(sch_port(sch) + 0x10)
+#define SCH_PORT_MAX_CREDIT(sch)	(volatile unsigned long *)(sch_port(sch) + 0x14)
+#define SCH_PORT_CREDIT(sch)		(volatile unsigned long *)(sch_port(sch) + 0x18)
+#define SCH_PORT_CONTROL(sch)		(volatile unsigned long *)(sch_port(sch) + 0x1C)
+
+#define SCH_PORT_BYTE_IN_Q_BIT			0
+#define SCH_PORT_BYTE_IN_Q_W			0xFFFFFFFF
+#define SCH_GET_PORT_BYTE_IN_Q(sch)		(*SCH_PORT_BYTE_IN_Q(sch) >> SCH_PORT_BYTE_IN_Q_BIT) & SCH_PORT_BYTE_IN_Q_W
+
+#define SCH_PORT_PKT_IN_Q_BIT			0
+#define SCH_PORT_PKT_IN_Q_W			0x03FFFFFF
+#define SCH_GET_PORT_PKT_IN_Q(sch)		(*SCH_PORT_PKT_IN_Q(sch) >> SCH_PORT_PKT_IN_Q_BIT) & SCH_PORT_PKT_IN_Q_W
+
+#define SCH_PORT_FRAC_RATE_BIT			0
+#define SCH_PORT_FRAC_RATE_W			0xFF
+#define	SCH_GET_PORT_FRAC_RATE(sch)		(*SCH_PORT_FRAC_RATE(sch) >> SCH_PORT_FRAC_RATE_BIT) & SCH_PORT_FRAC_RATE_W
+#define	SCH_SET_PORT_FRAC_RATE(sch,value)	SET_REG_BITSHIFT(SCH_PORT_FRAC_RATE(sch) ,value, SCH_PORT_FRAC_RATE_W , SCH_PORT_FRAC_RATE_BIT)
+
+#define SCH_PORT_MAX_CREDIT_BIT			0
+#define SCH_PORT_MAX_CREDIT_W			0x0FFFFF
+#define	SCH_GET_PORT_MAX_CREDIT(sch)		(*SCH_PORT_MAX_CREDIT(sch) >> SCH_PORT_MAX_CREDIT_BIT) & SCH_PORT_MAX_CREDIT_W
+#define	SCH_SET_PORT_MAX_CREDIT(sch,value)	SET_REG_BITSHIFT(SCH_PORT_MAX_CREDIT(sch),value, SCH_PORT_MAX_CREDIT_W , SCH_PORT_MAX_CREDIT_BIT)
+
+#define SCH_PORT_CREDIT_BIT			0
+#define SCH_PORT_CREDIT_W			0x1FFFFF
+#define	SCH_GET_PORT_INIT_BURST(sch)		(*SCH_PORT_CREDIT(sch) >> SCH_PORT_CREDIT_BIT) & SCH_PORT_CREDIT_W
+#define	SCH_SET_PORT_INIT_BURST(sch,value)	SET_REG_BITSHIFT(SCH_PORT_CREDIT(sch) ,value, SCH_PORT_CREDIT_W, SCH_PORT_CREDIT_BIT)
+
+#define SCH_SET_PORT_CONTROL_CLOCK_BIT		0
+#define SCH_SET_PORT_CONTROL_CLOCK_W		0x0F
+#define SCH_GET_PORT_CONTROL_CLOCK(sch)		(*SCH_PORT_CONTROL(sch) >> SCH_SET_PORT_CONTROL_CLOCK_BIT) & SCH_SET_PORT_CONTROL_CLOCK_W
+#define SCH_SET_PORT_CONTROL_CLOCK(sch,value)	SET_REG_BITSHIFT(SCH_PORT_CONTROL(sch) ,value,SCH_SET_PORT_CONTROL_CLOCK_W , SCH_SET_PORT_CONTROL_CLOCK_BIT)
+
+#define SCH_SET_PORT_CONTROL_ENABLE_BIT		4
+#define SCH_SET_PORT_CONTROL_ENABLE_W		0x01
+#define SCH_GET_PORT_CONTROL_ENABLE(sch)	(*SCH_PORT_CONTROL(sch) >> SCH_SET_PORT_CONTROL_ENABLE_BIT) & SCH_SET_PORT_CONTROL_ENABLE_W
+#define SCH_SET_PORT_CONTROL_ENABLE(sch,value)	SET_REG_BITSHIFT(SCH_PORT_CONTROL(sch),value,SCH_SET_PORT_CONTROL_ENABLE_W, SCH_SET_PORT_CONTROL_ENABLE_BIT)
+
+
+
+
+/******************************** GROUP  aggregate Q1 to Q5 ********************************/
+#define SCH_GROUP_BASE			0x0060
+#define sch_group(sch)			(sch_base(sch) + SCH_GROUP_BASE)
+#define SCH_GROUP_BYTE_IN_Q(sch)	(volatile unsigned long *)(sch_group(sch) + 0x00)
+#define SCH_GROUP_PKT_IN_Q(sch)		(volatile unsigned long *)(sch_group(sch) + 0x04)
+#define SCH_GROUP_FRAC_RATE(sch)	(volatile unsigned long *)(sch_group(sch) + 0x10)
+#define SCH_GROUP_MAX_CREDIT(sch)	(volatile unsigned long *)(sch_group(sch) + 0x14)
+#define SCH_GROUP_CREDIT(sch)		(volatile unsigned long *)(sch_group(sch) + 0x18)
+#define SCH_GROUP_CTRL(sch)		(volatile unsigned long *)(sch_group(sch) + 0x1C)
+
+#define SCH_GROUP_BYTE_IN_Q_BIT			0
+#define SCH_GROUP_BYTE_IN_Q_W			0xFFFFFFFF
+#define SCH_GET_GROUP_BYTE_IN_Q(sch)		(*SCH_GROUP_BYTE_IN_Q(sch) >> SCH_GROUP_BYTE_IN_Q_BIT) & SCH_GROUP_BYTE_IN_Q_W
+
+#define SCH_GROUP_PKT_IN_Q_BIT			0
+#define SCH_GROUP_PKT_IN_Q_W			0x03FFFFFF
+#define SCH_GET_GROUP_PKT_IN_Q(sch)		(*SCH_GROUP_PKT_IN_Q(sch) >> SCH_GROUP_PKT_IN_Q_BIT) & SCH_GROUP_PKT_IN_Q_W
+
+#define SCH_GROUP_FRAC_RATE_BIT			0
+#define SCH_GROUP_FRAC_RATE_W			0xFF
+#define	SCH_GET_GROUP_FRAC_RATE(sch)		(*SCH_GROUP_FRAC_RATE(sch) >> SCH_GROUP_FRAC_RATE_BIT) & SCH_GROUP_FRAC_RATE_W
+#define	SCH_SET_GROUP_FRAC_RATE(sch,value)	SET_REG_BITSHIFT(SCH_GROUP_FRAC_RATE(sch) ,value, SCH_GROUP_FRAC_RATE_W , SCH_GROUP_FRAC_RATE_BIT)
+
+#define SCH_GROUP_MAX_CREDIT_BIT		0
+#define SCH_GROUP_MAX_CREDIT_W			0x0FFFFF
+#define	SCH_GET_GROUP_MAX_CREDIT(sch)		(*SCH_GROUP_MAX_CREDIT(sch) >> SCH_GROUP_MAX_CREDIT_BIT) & SCH_GROUP_MAX_CREDIT_W
+#define	SCH_SET_GROUP_MAX_CREDIT(sch,value)	SET_REG_BITSHIFT(SCH_GROUP_MAX_CREDIT(sch) ,value, SCH_GROUP_MAX_CREDIT_W , SCH_GROUP_MAX_CREDIT_BIT)
+
+#define SCH_GROUP_CREDIT_BIT			0
+#define SCH_GROUP_CREDIT_W			0x1FFFFF
+#define	SCH_GET_GROUP_INIT_BURST(sch)		(*SCH_GROUP_CREDIT(sch) >> SCH_GROUP_CREDIT_BIT) & SCH_GROUP_CREDIT_W
+#define	SCH_SET_GROUP_INIT_BURST(sch,value)	SET_REG_BITSHIFT(SCH_GROUP_CREDIT(sch) ,value, SCH_GROUP_CREDIT_W , SCH_GROUP_CREDIT_BIT)
+
+#define SCH_SET_GROUP_CONTROL_CLOCK_BIT		0
+#define SCH_SET_GROUP_CONTROL_CLOCK_W		0x0F
+#define SCH_GET_GROUP_CONTROL_CLOCK(sch)	(*SCH_GROUP_CTRL(sch) >> SCH_SET_GROUP_CONTROL_CLOCK_BIT) & SCH_SET_GROUP_CONTROL_CLOCK_W
+#define SCH_SET_GROUP_CONTROL_CLOCK(sch,value)	SET_REG_BITSHIFT(SCH_GROUP_CTRL(sch),value, SCH_SET_GROUP_CONTROL_CLOCK_W , SCH_SET_GROUP_CONTROL_CLOCK_BIT)
+
+#define SCH_SET_GROUP_CONTROL_ENABLE_BIT	4
+#define SCH_SET_GROUP_CONTROL_ENABLE_W		0x01
+#define SCH_GET_GROUP_CONTROL_ENABLE(sch)	(*SCH_GROUP_CTRL(sch) >> SCH_SET_GROUP_CONTROL_ENABLE_BIT) & SCH_SET_GROUP_CONTROL_ENABLE_W
+#define SCH_SET_GROUP_CONTROL_ENABLE(sch,value)	SET_REG_BITSHIFT(SCH_GROUP_CTRL(sch),value, SCH_SET_GROUP_CONTROL_ENABLE_W, SCH_SET_GROUP_CONTROL_ENABLE_BIT)
+
+
+
+/******************************** QUEUES  ********************************/
+/* Up to 8 queues  can be configured */
+#define SCH_Q_BASE			0x0080
+#define SCH_Q_SIZE			0x0020
+#define check_queue(queue)		(queue > 7 ? 0 : queue) /* means upon error queue 0 is selected */
+#define check_queue1_5(queue)		(check_queue(queue) > 5 ? 1 : queue) /* means upon error queue 1 is selected */
+#define check_queue6_7(queue)		(check_queue(queue) < 6 ? 6 : queue) /* means upon error queue 6 is selected */
+#define sch_queue(sch, queue)		(sch_base(sch) + SCH_Q_BASE + (check_queue(queue) * SCH_Q_SIZE))
+#define sch_queue1_5(sch, queue)	(sch_base(sch) + SCH_Q_BASE + (check_queue1_5(queue) * SCH_Q_SIZE))
+#define sch_queue6_7(sch, queue)	(sch_base(sch) + SCH_Q_BASE + (check_queue6_7(queue) * SCH_Q_SIZE))
+
+#define SCH_Q_BYTE_IN_Q(sch,queue)	(volatile unsigned long *)(sch_queue(sch,queue) + 0x00)
+#define SCH_Q_PKT_IN_Q(sch,queue)	(volatile unsigned long *)(sch_queue(sch,queue) + 0x04)
+#define SCH_Q_IDLE(sch,queue)		(volatile unsigned long *)(sch_queue(sch,queue) + 0x08)
+
+#define SCH_Q_DWRR_WEIGHT(sch,queue)	(volatile unsigned long *)(sch_queue1_5(sch,queue) + 0x10)
+#define SCH_Q_DWRR_CREDIT(sch,queue)	(volatile unsigned long *)(sch_queue1_5(sch,queue) + 0x14)
+
+#define SCH_Q_FRAC_RATE(sch,queue)	(volatile unsigned long *)(sch_queue6_7(sch,queue) + 0x10)
+#define SCH_Q_MAX_CREDIT(sch,queue)	(volatile unsigned long *)(sch_queue6_7(sch,queue) + 0x14)
+#define SCH_Q_CREDIT(sch,queue)		(volatile unsigned long *)(sch_queue6_7(sch,queue) + 0x18)
+#define SCH_Q_CONTROL(sch,queue)	(volatile unsigned long *)(sch_queue6_7(sch,queue) + 0x1C)
+
+#define SCH_Q_BYTE_IN_Q_BIT			0
+#define SCH_Q_BYTE_IN_Q_W			0xFFFFFFFF
+#define SCH_GET_Q_BYTE_IN_Q(sch,queue)		(*SCH_Q_BYTE_IN_Q(sch,queue) >> SCH_Q_BYTE_IN_Q_BIT) & SCH_Q_BYTE_IN_Q_W
+
+#define SCH_Q_PKT_IN_Q_BIT			0
+#define SCH_Q_PKT_IN_Q_W			0x03FFFFFF
+#define SCH_GET_Q_PKT_IN_Q(sch,queue)		(*SCH_Q_PKT_IN_Q(sch,queue) >> SCH_Q_PKT_IN_Q_BIT) & SCH_Q_PKT_IN_Q_W
+
+#define SCH_Q_IDLE_BIT				0
+#define SCH_Q_IDLE_W				0xFFFFFFFF
+#define SCH_GET_Q_IDLE(sch,queue)		(*SCH_Q_IDLE(sch,queue) >> SCH_Q_IDLE_BIT) & SCH_Q_IDLE_W
+
+#define SCH_Q_DWRR_WEIGHT_BIT			0
+#define SCH_Q_DWRR_WEIGHT_W			0x0FFFFF
+#define	SCH_GET_Q_DWRR_WEIGHT(sch,queue)	(*SCH_Q_DWRR_WEIGHT(sch,queue) >> SCH_Q_DWRR_WEIGHT_BIT) & SCH_Q_DWRR_WEIGHT_W
+#define	SCH_SET_Q_DWRR_WEIGHT(sch,queue,value)	SET_REG_BITSHIFT(SCH_Q_DWRR_WEIGHT(sch,queue) ,value, SCH_Q_DWRR_WEIGHT_W , SCH_Q_DWRR_WEIGHT_BIT)
+
+#define SCH_Q_DWRR_CREDIT_BIT			0
+#define SCH_Q_DWRR_CREDIT_W			0x1FFFFF
+#define	SCH_GET_Q_DWRR_CREDIT(sch,queue)	(*SCH_Q_DWRR_CREDIT(sch,queue) >> SCH_Q_DWRR_CREDIT_BIT) & SCH_Q_DWRR_CREDIT_W
+#define	SCH_SET_Q_DWRR_CREDIT(sch,queue,value)	SET_REG_BITSHIFT(SCH_Q_DWRR_CREDIT(sch,queue),value, SCH_Q_DWRR_CREDIT_W , SCH_Q_DWRR_CREDIT_BIT)
+
+#define SCH_Q_FRAC_RATE_BIT			0
+#define SCH_Q_FRAC_RATE_W			0xFF
+#define	SCH_GET_Q_FRAC_RATE(sch,queue)		(*SCH_Q_FRAC_RATE(sch,queue) >> SCH_Q_FRAC_RATE_BIT) & SCH_Q_FRAC_RATE_W
+#define	SCH_SET_Q_FRAC_RATE(sch,queue,value)	SET_REG_BITSHIFT(SCH_Q_FRAC_RATE(sch,queue),value, SCH_Q_FRAC_RATE_W , SCH_Q_FRAC_RATE_BIT)
+
+#define SCH_Q_MAX_CREDIT_BIT			0
+#define SCH_Q_MAX_CREDIT_W			0x0FFFFF
+#define	SCH_GET_Q_MAX_CREDIT(sch,queue)		(*SCH_Q_MAX_CREDIT(sch,queue) >> SCH_Q_MAX_CREDIT_BIT) & SCH_Q_MAX_CREDIT_W
+#define	SCH_SET_Q_MAX_CREDIT(sch,queue,value)	SET_REG_BITSHIFT(SCH_Q_MAX_CREDIT(sch,queue),value, SCH_Q_MAX_CREDIT_W , SCH_Q_MAX_CREDIT_BIT)
+
+#define SCH_Q_CREDIT_BIT			0
+#define SCH_Q_CREDIT_W				0x1FFFFF
+#define	SCH_GET_Q_INIT_BURST(sch,queue)		(*SCH_Q_CREDIT(sch,queue) >> SCH_Q_CREDIT_BIT) & SCH_Q_CREDIT_W
+#define	SCH_SET_Q_INIT_BURST(sch,queue,value)	SET_REG_BITSHIFT(SCH_Q_CREDIT(sch,queue) ,value, SCH_Q_CREDIT_W , SCH_Q_CREDIT_BIT)
+
+#define SCH_Q_CONTROL_CLOCK_BIT				0
+#define SCH_Q_CONTROL_CLOCK_W				0x0F
+#define SCH_GET_Q_CONTROL_CLOCK(sch,queue)		(*SCH_Q_CONTROL(sch,queue) >> SCH_Q_CONTROL_CLOCK_BIT) & SCH_Q_CONTROL_CLOCK_W
+#define SCH_SET_Q_CONTROL_CLOCK(sch,queue,value)	SET_REG_BITSHIFT(SCH_Q_CONTROL(sch,queue) ,value, SCH_Q_CONTROL_CLOCK_W , SCH_Q_CONTROL_CLOCK_BIT)
+
+#define SCH_Q_CONTROL_ENABLE_BIT			4
+#define SCH_Q_CONTROL_ENABLE_W				0x01
+#define SCH_GET_Q_CONTROL_ENABLE(sch,queue)		(*SCH_Q_CONTROL(sch,queue) >> SCH_Q_CONTROL_ENABLE_BIT) & SCH_Q_CONTROL_ENABLE_W
+#define SCH_SET_Q_CONTROL_ENABLE(sch,queue,value)	SET_REG_BITSHIFT(SCH_Q_CONTROL(sch,queue) ,value, SCH_Q_CONTROL_ENABLE_W , SCH_Q_CONTROL_ENABLE_BIT)
+
+#endif /* _SCH_H */
diff --git a/drivers/net/transcede/qos_fops.h b/drivers/net/transcede/qos_fops.h
new file mode 100644
index 0000000..d59040f
--- /dev/null
+++ b/drivers/net/transcede/qos_fops.h
@@ -0,0 +1,390 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef _QOS_FOPS_H
+#define _QOS_FOPS_H
+
+/*
+* Definitions
+*
+*/
+
+enum QOS_TRAFFIC_TYPES {
+	RESERVED,
+	MANAGED,
+	POLICED,
+	DROPPED,
+	UNMANAGED
+};
+
+
+enum QOS_ERRORS
+{
+	QOS_OK = 0,
+	QOS_TCAM_NO_MORE_ROOM = 1,
+	QOS_TCAM_INDEX_ERR = 2
+};
+
+
+/*
+* ADMITTANCE
+*
+*/
+
+#define ADM0	0
+#define ADM1	1
+#define ADM_MAX_QFULL	1024
+
+//#warning TCAM entries set to 4 (debug purpose only, default is 32)
+#define ADM_MAX_TCAM_ENTRIES	32
+
+#define ADM_MAX_POLICER_ENTRIES	2
+
+#define ADM_RESET_ALL 0
+
+enum ADM_DROP_ZONE
+{
+	ADM_DROPZONE0,
+	ADM_DROPZONE1,
+	ADM_DROPZONE2,
+	ADM_DROPZONE3,
+	ADM_NUM_DROPZONE
+};
+
+enum ADM_MODE
+{
+	DISCARD_MODE,
+	FLOW_CONTROL_MODE,
+	DISABLED_MODE,
+	ADM_MAX_MODE
+};
+
+enum QOS_TCAM_ADD_TYPE
+{
+	TCAM_ADD_TOP,
+	TCAM_ADD_BOTTOM,
+	TCAM_ADD_BEFORE
+};
+
+
+struct _ADM_LRU_STATS
+{
+	unsigned int entry;
+	unsigned int count;
+	unsigned char hit_count[ADM_MAX_TCAM_ENTRIES]; //max hit count reported by HW is 255
+};
+
+struct _ADM_LRU_CONF
+{
+	unsigned int period;
+	unsigned int mask;
+};
+#define LRU_PERIOD_MIN 0
+#define LRU_PERIOD_MAX 0
+#define LRU_PERIOD_DEFAULT 0
+
+#define LRU_MASK_MIN 0
+#define LRU_MASK_MAX 0
+#define LRU_MASK_DEFAULT 0
+
+struct _ADM_TCAM_ENTRY
+{
+	unsigned char	policer; /* 0, 1=policer0, 2=policer1 */
+	unsigned char	drop;
+	unsigned char	reserved;
+	unsigned short	etype;
+	unsigned short	vlan;
+	unsigned char	pppoe_ipv4;
+	unsigned char	pppoe_ipv6;
+	unsigned short	iptos;
+	unsigned short	ipproto;
+	unsigned char	ipfamily; /* not used by hw */
+	unsigned long	saddr[4];
+	unsigned long	daddr[4];
+	unsigned short	sport_max;
+	unsigned short	sport_min;
+	unsigned short	dport_max;
+	unsigned short	dport_min;
+	unsigned char	multicast;
+    unsigned char   broadcast;
+	unsigned short	etype_mask;
+	unsigned short	vlan_mask;
+	unsigned long	saddr_mask; /* select # of low order bits are ignored */
+	unsigned long	daddr_mask;
+	unsigned char	sport_enable; /* selects if sport_max/min are used */
+	unsigned char	dport_enable; /* selects if dport_max/min are used */
+	unsigned long	iptos_mask;
+	unsigned long	pppoe_mask;
+	unsigned long	ipproto_mask;
+	unsigned long	broadcast_mask;
+	unsigned long	multicast_mask;
+};
+
+#define	ADM_TCAM_FREE 		0xFF	/* the tcam slot is free and can be used to add new entry */
+#define TCAM_UNSET_FAMILY	0
+#define TCAM_IPV4_FAMILY	1
+#define TCAM_IPV6_FAMILY	2
+#define TCAM_OTHER_FAMILY	3
+
+struct _ADM_TCAM_CONF
+{
+	unsigned char 	index;
+	unsigned char	add_type; /* top=0, bottom=1, insert before=2*/
+	unsigned char	position; /* insert entry before posistion */
+	unsigned char	state;
+	struct _ADM_TCAM_ENTRY entry;
+};
+
+struct _ADM_TCAM_STATE
+{
+	unsigned char 	index;
+	unsigned char	state;
+};
+
+
+struct _ADM_POLICER_CONF
+{
+	unsigned char index;
+	unsigned char state;
+	unsigned long rate;
+	unsigned long max_credit;
+	unsigned long init_burst;
+};
+#define POLICER_STATE_DEFAULT 0
+
+#define POLICER_RATE_MIN 0
+#define POLICER_RATE_MAX 0
+#define POLICER_RATE_DEFAULT 0
+
+#define POLICER_CREDIT_MIN 0
+#define POLICER_CREDIT_MAX 0
+#define POLICER_CREDIT_DEFAULT 0
+
+#define POLICER_BURST_MIN 0
+#define POLICER_BURST_MAX 0
+#define POLICER_BURST_DEFAULT 0
+
+struct _ADM_COUNTER
+{
+	unsigned long port_byte;
+	unsigned long port_packet;
+	unsigned long reserved_byte;
+	unsigned long reserved_packet;
+	unsigned long managed_byte;
+	unsigned long managed_packet;
+	unsigned long packet_dropped;
+	unsigned long packet_dropped_error;
+	unsigned long packet_dropped_denied;
+	unsigned long packet_dropped_policer0;
+	unsigned long packet_dropped_policer1;
+	unsigned long packet_dropped_qfull;
+	unsigned long packet_dropped_shaper;
+	unsigned long packet_dropped_managed;
+	unsigned long packet_dropped_unmanaged;
+};
+
+struct _ADM_DISCARD_CONF
+{
+	unsigned char 	enable;		/* allows admittace block to drop packets if needed */
+	unsigned long	zone_prob[ADM_NUM_DROPZONE];
+	unsigned long	queue_full_threshold; /* not used anymore as queue length is set by upper layer (linux of fpp) */
+	unsigned long	queue_drop_min; /* below this, accept all traffic except ones in excess of policed rate (expressed in % of qfull) */
+	unsigned long	queue_drop_max; /* only reserved traffic accepted if reached by current q depth (expressed in % of qfull)*/
+	unsigned long	qavg_depth;
+	unsigned long	decay_timer_value;
+};
+#define DISCARD_ENABLE_DEFAULT 0
+
+#define DISACRD_ZONE_PROB3_DEFAULT	8
+#define DISCARD_ZONE_PROB2_DEFAULT	4
+#define DISCARD_ZONE_PROB1_DEFAULT	1
+#define DISCARD_ZONE_PROB0_DEFAULT	0
+#define DISCARD_ZONE_PROBX_MIN
+#define DISCARD_ZONE_PROBX_MAX
+
+#define DISCARD_QFULL_TRESH_MIN		0
+#define DISCARD_QFULL_TRESH_MAX		128
+#define DISCARD_QFULL_TRESH_DEFAULT	DISCARD_QFULL_TRESH_MAX
+
+#define DISCARD_QDROPMIN_DEFAULT 0
+#define DISCARD_QDROPMIN_MIN 0
+#define DISCARD_QDROPMIN_MAX 0
+
+#define DISCARD_QDROPMAX_DEFAULT 0
+#define DISCARD_QDROPMAX_MIN 0
+#define DISCARD_QDROPMAX_MAX 0
+
+struct _ADM_FLOWCTRL_CONF
+{
+	unsigned char	enable;
+	unsigned long	off_tresh;
+	unsigned long	on_tresh;
+	unsigned long	pause_timer;
+};
+#define FLOWCTRL_ENABLE_DEFAULT 0
+
+#define FLOWCTRL_OFFTRESH_DEFAULT 0
+#define FLOWCTRL_OFFTRESH_MIN 0
+#define FLOWCTRL_OFFTRESH_MAX 0
+
+#define FLOWCTRL_ONTRESH_DEFAULT 0
+#define FLOWCTRL_ONTRESH_MIN 0
+#define FLOWCTRL_ONTRESH_MAX 0
+
+
+struct _ADM_SHAPER_CONF
+{
+	unsigned char	enable;
+	unsigned long	rate;
+	unsigned long	max_credit;
+	unsigned long	init_burst;
+	unsigned long	overhead;
+};
+#define SHAPER_ENABLE_DEFAULT 0
+#define SHAPER_RATE_MIN 0
+#define SHAPER_RATE_MAX 0
+#define SHAPER_RATE_DEFAULT SHAPER_RATE_MIN
+
+#define SHAPER_CREDIT_MIN 0
+#define SHAPER_CREDIT_MAX 0
+#define SHAPER_CREDIT_DEFAULT SHAPER_CREDIT_MIN
+
+#define SHAPER_BURST_MIN 0
+#define SHAPER_BURST_MAX 0
+#define SHAPER_BURST_DEFAULT SHAPER_BURST_MIN
+
+#define SHAPER_OVERHEAD_MIN 0
+#define SHAPER_OVERHEAD_MAX 0
+#define SHAPER_OVERHEAD_DEFAULT SHAPER_OVERHEAD_MIN
+
+/*
+* SCHEDULER
+*
+*/
+
+#define SCH0	0
+#define SCH1	1
+
+#define SCH_MAX_SHAPERS			4
+#define SCH_MAX_WEIGHTED_QUEUES		5
+
+#define SCH_RESET_ALL 0
+
+enum SCH_SHAPER
+{
+	SHAPER_PORT,
+	SHAPER_GROUP,
+	SHAPER_Q6,
+	SHAPER_Q7
+};
+
+enum SCH_QUEUE
+{
+	QUEUE_0,
+	QUEUE_1,
+	QUEUE_2,
+	QUEUE_3,
+	QUEUE_4,
+	QUEUE_5,
+	QUEUE_6,
+	QUEUE_7,
+	SCH_MAX_QUEUES
+};
+
+struct _SCH_COUNTER
+{
+	unsigned long port_byte;
+	unsigned long port_packet;
+	unsigned long q_byte[SCH_MAX_QUEUES];
+	unsigned long q_packet[SCH_MAX_QUEUES];
+	unsigned long q_idle[SCH_MAX_QUEUES];
+};
+
+
+struct _SCH_SHAPER_STATE
+{
+	unsigned char shaper;
+	unsigned long state;
+};
+
+struct _SCH_SHAPER_CONF
+{
+	unsigned char shaper;
+	unsigned char state;
+	unsigned long rate;
+	unsigned long max_credit;
+	unsigned long init_burst;
+};
+
+struct _SCH_QUEUE_WEIGHT
+{
+	unsigned char queue;
+	unsigned long weight;
+};
+
+
+/*
+* IOCTLs
+*
+*/
+
+#define	QOS_DEVICE_MAJOR_NUM		126
+#define QOS_DRIVER_NAME			"qoscom"
+
+#define	ADM0_DEVICE_MINOR_NUM		0
+#define	ADM1_DEVICE_MINOR_NUM		1
+#define	SCH0_DEVICE_MINOR_NUM		2
+#define	SCH1_DEVICE_MINOR_NUM		3
+
+#define ADM_IOC_TYPE           'a'
+#define SCH_IOC_TYPE           's'
+
+#define ADM_IOC_SET_MODE		_IOW(ADM_IOC_TYPE, 1, unsigned char *)
+#define ADM_IOC_GET_MODE		_IOR(ADM_IOC_TYPE, 2, unsigned char *)
+
+#define ADM_IOC_SET_DISCARD_CONF	_IOW(ADM_IOC_TYPE, 3, struct _ADM_DISCARD_CONF *)
+#define ADM_IOC_SET_FLOWCTRL_CONF	_IOW(ADM_IOC_TYPE, 4, struct _ADM_FLOWCTRL_CONF *)
+#define ADM_IOC_SET_SHAPER_CONF		_IOW(ADM_IOC_TYPE, 5, struct _ADM_SHAPER_CONF *)
+#define ADM_IOC_ADD_CAMENTRY		_IOWR(ADM_IOC_TYPE, 6, struct _ADM_TCAM_CONF *)
+#define ADM_IOC_CHANGE_CAMENTRY		_IOW(ADM_IOC_TYPE, 7, struct _ADM_TCAM_CONF *)
+#define ADM_IOC_SET_CAMENTRY_STATE	_IOW(ADM_IOC_TYPE, 8, struct _ADM_TCAM_STATE *)
+#define ADM_IOC_REMOVE_CAMENTRY		_IOW(ADM_IOC_TYPE, 9, unsigned char *)
+#define ADM_IOC_GET_CAMENTRY		_IOWR(ADM_IOC_TYPE, 10, struct _ADM_TCAM_CONF *)
+#define ADM_IOC_SET_POLICER_CONF	_IOW(ADM_IOC_TYPE, 11, struct _ADM_POLICER_CONF *)
+#define ADM_IOC_GET_POLICER_CONF	_IOWR(ADM_IOC_TYPE, 12, struct _ADM_POLICER_CONF *)
+#define ADM_IOC_SET_LRU_CONF		_IOW(ADM_IOC_TYPE, 13, struct _ADM_LRU_CONF *)
+#define ADM_IOC_GET_LRU_STATS		_IOR(ADM_IOC_TYPE, 14, struct _ADM_LRU_STATS *)
+#define ADM_IOC_GET_ADM_STATS		_IOR(ADM_IOC_TYPE, 15, struct _ADM_COUNTER *)
+#define SCH_IOC_GET_STATS		_IOR(SCH_IOC_TYPE, 16, struct _SCH_COUNTER *)
+#define SCH_IOC_SET_SHAPER_STATE	_IOW(SCH_IOC_TYPE, 17, struct _SCH_SHAPER_STATE *)
+#define SCH_IOC_GET_SHAPER_STATE	_IOR(SCH_IOC_TYPE, 18, struct _SCH_SHAPER_STATE *)
+#define SCH_IOC_SET_SHAPER_CONF		_IOW(SCH_IOC_TYPE, 19, struct _SCH_SHAPER_CONF *)
+#define SCH_IOC_GET_SHAPER_CONF		_IOW(SCH_IOC_TYPE, 20, struct _SCH_SHAPER_CONF *)
+#define SCH_IOC_SET_QUEUE_WEIGHT	_IOW(SCH_IOC_TYPE, 21, struct _SCH_QUEUE_WEIGHT *)
+#define SCH_IOC_GET_QUEUE_WEIGHT	_IOW(SCH_IOC_TYPE, 22, struct _SCH_QUEUE_WEIGHT *)
+#define ADM_IOC_RESET			_IOW(ADM_IOC_TYPE, 23, unsigned char *)
+#define SCH_IOC_RESET			_IOW(SCH_IOC_TYPE, 24, unsigned char *)
+#define ADM_IOC_GET_REGS		_IOW(ADM_IOC_TYPE, 25, unsigned char *)
+#define ADM_IOC_GET_SHAPER_CONF		_IOW(ADM_IOC_TYPE, 26, struct _ADM_SHAPER_CONF *)
+
+
+#endif /* _QOS_FOPS_H */
diff --git a/drivers/net/transcede/qoscom.c b/drivers/net/transcede/qoscom.c
new file mode 100644
index 0000000..f1bde2a
--- /dev/null
+++ b/drivers/net/transcede/qoscom.c
@@ -0,0 +1,2498 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#include <linux/version.h>
+#include <linux/socket.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/skbuff.h>
+#include <linux/proc_fs.h>
+#include <net/sock.h>
+#include <linux/timer.h>
+#include <linux/time.h>
+
+#include "qoscom.h"
+
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Intel Corporation");
+MODULE_DESCRIPTION("Transcede 4000 QoS Driver");
+
+#define QOSCOM_VERSION	"$Name: qoscom_1_02 $"
+
+struct mutex qoscom_lock;
+
+static unsigned long swapbit(unsigned long x);
+static unsigned char reduce_ip_mask(unsigned long ip_mask);
+static unsigned long expand_ip_mask(unsigned long ip_mask);
+
+
+/* ioctls functions  */
+static int qos_open (struct inode *inode, struct file *file);
+static int qos_release (struct inode *inode, struct file *file);
+static long qos_ioctl (struct file *file, unsigned int cmd, unsigned long arg);
+
+/* commons functions */
+static int qos_init(void);
+static void qos_dump_registers(unsigned char block);
+
+/* admittance functions */
+static int qos_adm_get_counters(struct _QOS_ADM *adm, struct _ADM_COUNTER *stats);
+void qos_adm_dump_counters(void);
+static int qos_adm_set_mode(struct _QOS_ADM *adm, unsigned char mode);
+static unsigned char qos_adm_get_mode(struct _QOS_ADM *adm);
+static int qos_adm_set_discard_conf(struct _QOS_ADM *adm, struct _ADM_DISCARD_CONF *conf);
+static int qos_adm_set_flowctrl_conf(struct _QOS_ADM *adm, struct _ADM_FLOWCTRL_CONF *conf);
+static int qos_adm_set_shaper_conf(struct _QOS_ADM *adm, struct _ADM_SHAPER_CONF *conf);
+static int qos_adm_get_shaper_conf(struct _QOS_ADM *adm, struct _ADM_SHAPER_CONF *conf);
+static int qos_adm_add_camentry(struct _QOS_ADM *this_adm, struct _ADM_TCAM_CONF *conf);
+static int qos_adm_remove_camentry(struct _QOS_ADM *this_adm, unsigned char tcam_entry_id);
+static int qos_adm_change_camentry(struct _QOS_ADM *this_adm, struct _ADM_TCAM_CONF *conf);
+static int qos_adm_set_camentry_state(struct _QOS_ADM *this_adm, struct _ADM_TCAM_STATE *conf);
+static int qos_adm_get_camentry(struct _QOS_ADM *this_adm, struct _ADM_TCAM_CONF *conf);
+static int qos_adm_add_camentry_to_slot(struct _QOS_ADM *this_adm, struct _ADM_TCAM_CONF *conf, unsigned char tcam_slot);
+static int qos_adm_set_policer_conf(struct _QOS_ADM *this_adm, struct _ADM_POLICER_CONF *conf);
+static int qos_adm_get_policer_conf(struct _QOS_ADM *this_adm, struct _ADM_POLICER_CONF *conf);
+static int qos_adm_set_lru_conf(struct _QOS_ADM *this_adm, struct _ADM_LRU_CONF *conf);
+static int qos_adm_get_lru_stats(struct _QOS_ADM *this_adm, struct _ADM_LRU_STATS *p);
+
+/* scheduler functions */
+static int qos_sch_get_counters(struct _QOS_SCH *this_sch, struct _SCH_COUNTER *p);
+void qos_sch_dump_counters(void);
+static int qos_sch_set_shaper_state(struct _QOS_SCH *this_sch, struct _SCH_SHAPER_STATE *conf);
+static int qos_sch_get_shaper_state(struct _QOS_SCH *this_sch, struct _SCH_SHAPER_STATE *conf);
+static int qos_sch_set_shaper_conf(struct _QOS_SCH *this_sch, struct _SCH_SHAPER_CONF *conf);
+static int qos_sch_get_shaper_conf(struct _QOS_SCH *this_sch, struct _SCH_SHAPER_CONF *conf);
+static int qos_sch_set_queue_weight(struct _QOS_SCH *this_sch, struct _SCH_QUEUE_WEIGHT *conf);
+static int qos_sch_get_queue_weight(struct _QOS_SCH *this_sch, struct _SCH_QUEUE_WEIGHT *conf);
+
+
+static void get_shaper_rate_and_clock(unsigned long bps, unsigned long *p_clk_sel, unsigned long *p_fraction);
+static void get_policer_rate_and_clock(unsigned long bps, unsigned long *p_clk_sel, unsigned long *p_fraction);
+static unsigned long shaper_fraction_clk_to_rate(unsigned long fraction, unsigned long clk_sel);
+
+static int qos_sch_global_reset(struct _QOS_SCH *this_sch);
+static int qos_adm_global_reset(struct _QOS_ADM *this_adm);
+
+/* admittance blocks main context */
+static struct _QOS_ADM _adm[2];
+
+/* scheduler blocks main context */
+static struct _QOS_SCH _sch[2];
+
+static char * adm_mode_string[] = {"DISCARD", "FLOW CONTROL", "DISABLED"};
+
+
+#ifdef C1K_REG_SIM
+unsigned long *FAKE_EMAC0_BASE;
+unsigned long *FAKE_EMAC1_BASE;
+unsigned long get_gemac_base(int emac)
+{
+	if(emac)
+		return (unsigned long)FAKE_EMAC1_BASE;
+	else
+		return (unsigned long)FAKE_EMAC0_BASE;
+}
+#endif
+
+/***************************  IOCTLs ******************************************/
+
+/**
+ * qos_ioctl -
+ *
+ *
+ */
+static long qos_ioctl (struct file *file, unsigned int cmd, unsigned long arg)
+{
+        struct _QOS_ADM *this_adm = NULL;
+	struct _ADM_COUNTER adm_stats;
+	struct _ADM_LRU_STATS adm_lru_stats; struct _ADM_LRU_CONF adm_lru_conf;
+	struct _ADM_DISCARD_CONF adm_discard_conf;
+	struct _ADM_FLOWCTRL_CONF adm_flowctrl_conf;
+	struct _ADM_SHAPER_CONF	adm_shaper_conf;
+	struct _ADM_TCAM_CONF adm_tcam_conf; struct _ADM_TCAM_STATE adm_tcam_state;
+	struct _ADM_POLICER_CONF adm_policer_conf;
+
+	struct _QOS_SCH *this_sch = NULL;
+	struct _SCH_COUNTER sch_stats;
+	struct _SCH_SHAPER_CONF	sch_shaper_conf;
+	struct _SCH_SHAPER_STATE sch_shaper_state;
+	struct _SCH_QUEUE_WEIGHT sch_queue_weight;
+
+	unsigned char mode, tcam_entry_id;
+	int minor = MINOR (file->f_dentry->d_inode->i_rdev);
+	int rc = 0;
+
+	if ((_IOC_TYPE (cmd) != ADM_IOC_TYPE) && (_IOC_TYPE (cmd) != SCH_IOC_TYPE))
+        {
+		QOS_PRINTK(QOS_ERR, "qos_ioctl() wrong IOC_TYPE");
+                rc = -EINVAL;
+                goto out;
+        }
+
+	if(file->private_data == NULL)
+	{
+		QOS_PRINTK(QOS_ERR, "qos_ioctl() private data is NULL\n");
+		rc = -EINVAL;
+		goto out;
+	}
+
+	mutex_lock(&qoscom_lock);
+
+	if ((ADM0_DEVICE_MINOR_NUM == minor) || (ADM1_DEVICE_MINOR_NUM == minor))
+	{
+		this_adm = (struct _QOS_ADM *) file->private_data;
+	}
+	else if ((SCH0_DEVICE_MINOR_NUM == minor) || (SCH1_DEVICE_MINOR_NUM == minor))
+	{
+		this_sch = (struct _QOS_SCH *) file->private_data;
+	}
+
+	if (_IOC_TYPE (cmd) == ADM_IOC_TYPE)
+	{
+		switch (cmd)
+		{
+			case ADM_IOC_RESET:
+				QOS_PRINTK (QOS_IOCTL, "ADM_IOC_RESET\n");
+				if(qos_adm_global_reset(this_adm))
+				{
+					QOS_PRINTK(QOS_ERR, "error resetting admittance block");
+					rc = -EFAULT;
+					goto out;
+				}
+				break;
+
+			case ADM_IOC_SET_MODE:
+				QOS_PRINTK (QOS_IOCTL, "ADM_IOC_SET_MODE\n");
+				if (copy_from_user(&mode, (unsigned char *)arg, sizeof(unsigned char)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying from user");
+					rc = -EFAULT;
+					goto out;
+				}
+				if(qos_adm_set_mode(this_adm, mode))
+				{
+					QOS_PRINTK(QOS_ERR, "error setting admittance mode");
+					rc = -EFAULT;
+					goto out;
+				}
+				break;
+
+			case ADM_IOC_GET_MODE:
+				QOS_PRINTK (QOS_IOCTL, "ADM_IOC_GET_MODE\n");
+				mode = qos_adm_get_mode(this_adm);
+				if (copy_to_user((unsigned char *)arg, &mode, sizeof(unsigned char)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying to user");
+					rc = -EFAULT;
+					goto out;
+				}
+				break;
+
+			case ADM_IOC_SET_DISCARD_CONF:
+				QOS_PRINTK (QOS_IOCTL, "ADM_IOC_DISCARD_CONF\n");
+				if (copy_from_user(&adm_discard_conf, (struct _ADM_DISCARD_CONF *)arg, sizeof(struct _ADM_DISCARD_CONF)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying from user");
+					rc = -EFAULT;
+					goto out;
+				}
+
+				if(qos_adm_set_discard_conf(this_adm, &adm_discard_conf))
+				{
+					rc = -EINVAL;
+					goto out;
+				}
+				break;
+
+			case ADM_IOC_SET_FLOWCTRL_CONF:
+				QOS_PRINTK (QOS_IOCTL, "ADM_IOC_FLOWCTRL_CONF\n");
+				if (copy_from_user(&adm_flowctrl_conf, (struct _ADM_FLOWCTRL_CONF *)arg, sizeof(struct _ADM_FLOWCTRL_CONF)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying from user");
+					rc = -EFAULT;
+					goto out;
+				}
+
+				if(qos_adm_set_flowctrl_conf(this_adm, &adm_flowctrl_conf))
+				{
+					rc = -EINVAL;
+					goto out;
+				}
+				break;
+
+			case ADM_IOC_SET_SHAPER_CONF:
+				QOS_PRINTK (QOS_IOCTL, "ADM_IOC_SHAPER_CONF\n");
+				if (copy_from_user(&adm_shaper_conf, (struct _ADM_SHAPER_CONF *)arg, sizeof(struct _ADM_SHAPER_CONF)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying from user");
+					rc = -EFAULT;
+					goto out;
+				}
+
+				if(qos_adm_set_shaper_conf(this_adm, &adm_shaper_conf))
+				{
+					rc = -EINVAL;
+					goto out;
+				}
+				break;
+
+			case ADM_IOC_GET_SHAPER_CONF:
+				QOS_PRINTK (QOS_IOCTL, "ADM_IOC_SET_SHAPER_CONF\n");
+				if (copy_from_user(&adm_shaper_conf, (struct _ADM_SHAPER_CONF *)arg, sizeof(struct _ADM_SHAPER_CONF)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying from user");
+					rc = -EFAULT;
+					goto out;
+				}
+				qos_adm_get_shaper_conf(this_adm, &adm_shaper_conf);
+				if (copy_to_user((struct _ADM_SHAPER_CONF *)arg, &adm_shaper_conf, sizeof(struct _ADM_SHAPER_CONF)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying to user");
+					rc = -EFAULT;
+					goto out;
+				}
+				break;
+
+			case ADM_IOC_ADD_CAMENTRY:
+				QOS_PRINTK (QOS_IOCTL, "ADM_IOC_ADD_CAMENTRY\n");
+				if (copy_from_user(&adm_tcam_conf, (struct _ADM_TCAM_CONF *)arg, sizeof(struct _ADM_TCAM_CONF)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying from user");
+					rc = -EFAULT;
+					goto out;
+				}
+
+				switch(adm_tcam_conf.add_type)
+				{
+					case TCAM_ADD_TOP:
+						break;
+
+					case TCAM_ADD_BEFORE:
+						break;
+
+					case TCAM_ADD_BOTTOM:
+					default:
+						if(qos_adm_add_camentry(this_adm, &adm_tcam_conf))
+						{
+							rc = -EINVAL;
+							goto out;
+						}
+						break;
+				}
+
+				if (copy_to_user((struct _ADM_TCAM_CONF *)arg, &adm_tcam_conf, sizeof(struct _ADM_TCAM_CONF)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying to user");
+					rc = -EFAULT;
+					goto out;
+				}
+				break;
+
+			case ADM_IOC_CHANGE_CAMENTRY:
+				QOS_PRINTK (QOS_IOCTL, "ADM_IOC_CHANGE_CAMENTRY\n");
+				if (copy_from_user(&adm_tcam_conf, (struct _ADM_TCAM_CONF *)arg, sizeof(struct _ADM_TCAM_CONF)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying from user");
+					rc = -EFAULT;
+					goto out;
+				}
+
+				if(qos_adm_change_camentry(this_adm, &adm_tcam_conf))
+				{
+					rc = -EINVAL;
+					goto out;
+				}
+				break;
+
+			case ADM_IOC_SET_CAMENTRY_STATE:
+				QOS_PRINTK (QOS_IOCTL, "ADM_IOC_SET_CAMENTRY_STATE\n");
+				if (copy_from_user(&adm_tcam_state, (struct _ADM_TCAM_STATE *)arg, sizeof(struct _ADM_TCAM_STATE)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying from user");
+					rc = -EFAULT;
+					goto out;
+				}
+
+				if(qos_adm_set_camentry_state(this_adm, &adm_tcam_state))
+				{
+					rc = -EINVAL;
+					goto out;
+				}
+				break;
+
+			case ADM_IOC_REMOVE_CAMENTRY:
+				QOS_PRINTK (QOS_IOCTL, "ADM_IOC_REMOVE_CAMENTRY\n");
+				if (copy_from_user(&tcam_entry_id, (unsigned char *)arg, sizeof(unsigned char)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying from user");
+					rc = -EFAULT;
+					goto out;
+				}
+
+				if(qos_adm_remove_camentry(this_adm, tcam_entry_id))
+				{
+					rc = -EINVAL;
+					goto out;
+				}
+				break;
+
+			case ADM_IOC_GET_CAMENTRY:
+				QOS_PRINTK (QOS_IOCTL, "ADM_IOC_GET_CAMENTRY\n");
+				if (copy_from_user(&adm_tcam_conf, (struct _ADM_TCAM_CONF *)arg, sizeof(struct _ADM_TCAM_CONF)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying from user");
+					rc = -EFAULT;
+					goto out;
+				}
+				QOS_PRINTK (QOS_IOCTL, "ADM_IOC_GET_CAMENTRY for entry %d on adm block %d\n", adm_tcam_conf.index,this_adm->index);
+				qos_adm_get_camentry(this_adm, &adm_tcam_conf);
+
+				if (copy_to_user((struct _ADM_TCAM_CONF *)arg, &adm_tcam_conf, sizeof(struct _ADM_TCAM_CONF)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying to user");
+					rc = -EFAULT;
+					goto out;
+				}
+				break;
+
+			case ADM_IOC_SET_POLICER_CONF:
+				QOS_PRINTK (QOS_IOCTL, "ADM_IOC_SET_POLICER_CONF\n");
+				if (copy_from_user(&adm_policer_conf, (struct _ADM_POLICER_CONF *)arg, sizeof(struct _ADM_POLICER_CONF)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying from user");
+					rc = -EFAULT;
+					goto out;
+				}
+
+				if(qos_adm_set_policer_conf(this_adm, &adm_policer_conf))
+				{
+					rc = -EINVAL;
+					goto out;
+				}
+				break;
+
+			case ADM_IOC_GET_POLICER_CONF:
+				QOS_PRINTK (QOS_IOCTL, "ADM_IOC_GET_POLICER_CONF\n");
+				if (copy_from_user(&adm_policer_conf, (struct _ADM_POLICER_CONF *)arg, sizeof(struct _ADM_POLICER_CONF)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying from user");
+					rc = -EFAULT;
+					goto out;
+				}
+				QOS_PRINTK (QOS_IOCTL, "ADM_IOC_GET_POLICER_CONF for policer %d on adm block %d\n", adm_policer_conf.index,this_adm->index);
+				qos_adm_get_policer_conf(this_adm, &adm_policer_conf);
+
+				if (copy_to_user((struct _ADM_POLICER_CONF *)arg, &adm_policer_conf, sizeof(struct _ADM_POLICER_CONF)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying to user");
+					rc = -EFAULT;
+					goto out;
+				}
+				break;
+
+			case ADM_IOC_SET_LRU_CONF:
+				QOS_PRINTK (QOS_IOCTL, "ADM_IOC_SET_LRU_CONF\n");
+				if (copy_from_user(&adm_lru_conf, (struct _ADM_LRU_CONF *)arg, sizeof(struct _ADM_LRU_CONF)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying from user");
+					rc = -EFAULT;
+					goto out;
+				}
+
+				if(qos_adm_set_lru_conf(this_adm, &adm_lru_conf))
+				{
+					rc = -EINVAL;
+					goto out;
+				}
+				break;
+
+			case ADM_IOC_GET_LRU_STATS:
+				QOS_PRINTK (QOS_IOCTL, "ADM_IOC_GET_LRU_STATS\n");
+				qos_adm_get_lru_stats(this_adm, &adm_lru_stats);
+				if (copy_to_user((struct _ADM_LRU_STATS *)arg, &adm_lru_stats, sizeof(struct _ADM_LRU_STATS)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying to user");
+					rc = -EFAULT;
+					goto out;
+				}
+				break;
+
+			case ADM_IOC_GET_ADM_STATS:
+				QOS_PRINTK (QOS_IOCTL, "ADM_IOC_GET_ADM_STATS\n");
+				qos_adm_get_counters(this_adm, &adm_stats);
+				if (copy_to_user((struct _ADM_COUNTER *)arg, &adm_stats, sizeof(struct _ADM_COUNTER)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying to user");
+					rc = -EFAULT;
+					goto out;
+				}
+				break;
+
+			case ADM_IOC_GET_REGS:
+				QOS_PRINTK (QOS_IOCTL, "COMMON_IOC_GET_REGS\n");
+				qos_dump_registers(QOS_ADM_BLOCK | QOS_SCH_BLOCK);
+				break;
+
+			default:
+				QOS_PRINTK (QOS_ERR, "Unknown IOCTL\n");
+				rc = -EINVAL;
+				break;
+		}
+	}
+	else
+	{
+		switch (cmd)
+		{
+			case SCH_IOC_RESET:
+				QOS_PRINTK (QOS_IOCTL, "SCH_IOC_RESET\n");
+				if(qos_sch_global_reset(this_sch))
+				{
+					QOS_PRINTK(QOS_ERR, "error resetting scheduler block");
+					rc = -EFAULT;
+					goto out;
+				}
+				break;
+
+			case SCH_IOC_GET_STATS:
+				QOS_PRINTK (QOS_IOCTL, "SCH_IOC_GET_STATS\n");
+				qos_sch_get_counters(this_sch, &sch_stats);
+				if (copy_to_user((struct _SCH_COUNTER *)arg, &sch_stats, sizeof(struct _SCH_COUNTER)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying to user");
+					rc = -EFAULT;
+					goto out;
+				}
+				break;
+
+			case SCH_IOC_SET_SHAPER_STATE:
+				QOS_PRINTK (QOS_IOCTL, "SCH_IOC_SET_SHAPER_STATE\n");
+				if (copy_from_user(&sch_shaper_state, (struct _SCH_SHAPER_STATE *)arg, sizeof(struct _SCH_SHAPER_STATE)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying from user");
+					rc = -EFAULT;
+					goto out;
+				}
+
+				if(qos_sch_set_shaper_state(this_sch, &sch_shaper_state))
+				{
+					rc = -EINVAL;
+					goto out;
+				}
+				break;
+
+			case SCH_IOC_GET_SHAPER_STATE:
+				QOS_PRINTK (QOS_IOCTL, "SCH_IOC_GET_SHAPER_STATE\n");
+				if (copy_from_user(&sch_shaper_state, (struct _SCH_SHAPER_STATE *)arg, sizeof(struct _SCH_SHAPER_STATE)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying from user");
+					rc = -EFAULT;
+					goto out;
+				}
+				qos_sch_get_shaper_state(this_sch, &sch_shaper_state);
+				if (copy_to_user((struct _SCH_SHAPER_STATE *)arg, &sch_shaper_state, sizeof(struct _SCH_SHAPER_STATE)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying to user");
+					rc = -EFAULT;
+					goto out;
+				}
+				break;
+
+			case SCH_IOC_SET_SHAPER_CONF:
+				QOS_PRINTK (QOS_IOCTL, "SCH_IOC_SET_SHAPER_CONF\n");
+				if (copy_from_user(&sch_shaper_conf, (struct _SCH_SHAPER_CONF *)arg, sizeof(struct _SCH_SHAPER_CONF)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying from user");
+					rc = -EFAULT;
+					goto out;
+				}
+
+				if(qos_sch_set_shaper_conf(this_sch, &sch_shaper_conf))
+				{
+					rc = -EINVAL;
+					goto out;
+				}
+				break;
+
+			case SCH_IOC_GET_SHAPER_CONF:
+				QOS_PRINTK (QOS_IOCTL, "SCH_IOC_GET_SHAPER_CONF\n");
+				if (copy_from_user(&sch_shaper_conf, (struct _SCH_SHAPER_CONF *)arg, sizeof(struct _SCH_SHAPER_CONF)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying from user");
+					rc = -EFAULT;
+					goto out;
+				}
+				qos_sch_get_shaper_conf(this_sch, &sch_shaper_conf);
+				if (copy_to_user((struct _SCH_SHAPER_CONF *)arg, &sch_shaper_conf, sizeof(struct _SCH_SHAPER_CONF)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying to user");
+					rc = -EFAULT;
+					goto out;
+				}
+				break;
+
+			case SCH_IOC_SET_QUEUE_WEIGHT:
+				QOS_PRINTK (QOS_IOCTL, "SCH_IOC_SET_QUEUE_WEIGHT\n");
+				if (copy_from_user(&sch_queue_weight, (struct _SCH_QUEUE_WEIGHT *)arg, sizeof(struct _SCH_QUEUE_WEIGHT)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying from user");
+					rc = -EFAULT;
+					goto out;
+				}
+
+				if(qos_sch_set_queue_weight(this_sch, &sch_queue_weight))
+				{
+					rc = -EINVAL;
+					goto out;
+				}
+				break;
+
+			case SCH_IOC_GET_QUEUE_WEIGHT:
+				QOS_PRINTK (QOS_IOCTL, "SCH_IOC_GET_QUEUE_WEIGHT\n");
+				if (copy_from_user(&sch_queue_weight, (struct _SCH_QUEUE_WEIGHT *)arg, sizeof(struct _SCH_QUEUE_WEIGHT)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying from user");
+					rc = -EFAULT;
+					goto out;
+				}
+				qos_sch_get_queue_weight(this_sch, &sch_queue_weight);
+				if (copy_to_user((struct _SCH_QUEUE_WEIGHT *)arg, &sch_queue_weight, sizeof(struct _SCH_QUEUE_WEIGHT)))
+				{
+					QOS_PRINTK(QOS_ERR, "error copying to user");
+					rc = -EFAULT;
+					goto out;
+				}
+				break;
+
+			default:
+				QOS_PRINTK (QOS_ERR, "Unknown IOCTL\n");
+				rc = -EINVAL;
+				break;
+		}
+	}
+out:
+	mutex_unlock(&qoscom_lock);
+	return rc;
+
+}
+
+
+/**
+ * qos_adm_global_reset -
+ *
+ *
+ */
+static int qos_adm_global_reset(struct _QOS_ADM *this_adm)
+{
+	unsigned char tcam_entry_id;
+
+	QOS_PRINTK (QOS_IOCTL, "qos_adm_global_reset");
+
+	/* policer 0 */
+	ADM_SET_POLICER_FRAC_RATE(this_adm->index, 0, 0);
+	ADM_SET_POLICER_MAX_CREDIT(this_adm->index, 0, 0);
+	ADM_SET_POLICER_CLOCK(this_adm->index, 0, 0);
+	ADM_SET_POLICER_ENABLE(this_adm->index, 0, 0);
+
+	/* policer 1 */
+	ADM_SET_POLICER_FRAC_RATE(this_adm->index, 1, 0);
+	ADM_SET_POLICER_MAX_CREDIT(this_adm->index, 1, 0);
+	ADM_SET_POLICER_CLOCK(this_adm->index, 1, 0);
+	ADM_SET_POLICER_ENABLE(this_adm->index, 1, 0);
+
+	/* shaper */
+	ADM_SET_PORT_SHA_PKT_OVER(this_adm->index, 0);//sw wa - registers 0x4030 must be written prior to 0x4020
+	ADM_SET_PORT_SHA_FRAC_RATE(this_adm->index, 0);
+	ADM_SET_PORT_SHA_MAX_CREDIT(this_adm->index, 0);
+	ADM_SET_PORT_SHA_CLOCK(this_adm->index, 0);
+	ADM_SET_PORT_SHA_ENABLE(this_adm->index, 0);
+
+	/* TCAM */
+	for(tcam_entry_id = 0; tcam_entry_id <ADM_MAX_TCAM_ENTRIES ;tcam_entry_id++)
+	{
+		if(this_adm->cam_sw_table[tcam_entry_id].slot != ADM_TCAM_FREE)
+		{
+			memset(&this_adm->cam_sw_table[tcam_entry_id].entry, sizeof(struct _ADM_TCAM_ENTRY), 0);
+			qos_adm_remove_camentry(this_adm, tcam_entry_id);
+		}
+	}
+
+	/* LRU */
+	ADM_SET_LRU_MASTER_DISABLE(this_adm->index);
+	ADM_SET_LRU_STATE(this_adm->index, 0);
+	ADM_SET_LRU_MASK(this_adm->index, 0xFFFFFFFF);
+
+	return 0;
+}
+
+/**
+ * qos_sch_global_reset -
+ *
+ *
+ */
+static int qos_sch_global_reset(struct _QOS_SCH *this_sch)
+{
+	QOS_PRINTK (QOS_IOCTL, "qos_sch_global_reset");
+
+	SCH_SET_Q_CONTROL_CLOCK(this_sch->index, 7, 0);
+	SCH_SET_Q_FRAC_RATE(this_sch->index, 7, 0);
+	SCH_SET_Q_MAX_CREDIT(this_sch->index, 7, 0);
+	SCH_SET_Q_CONTROL_ENABLE(this_sch->index, 7, 0);
+
+	SCH_SET_Q_CONTROL_CLOCK(this_sch->index, 6, 0);
+	SCH_SET_Q_FRAC_RATE(this_sch->index, 6, 0);
+	SCH_SET_Q_MAX_CREDIT(this_sch->index, 6, 0);
+	SCH_SET_Q_CONTROL_ENABLE(this_sch->index, 6, 0);
+
+	SCH_SET_GROUP_CONTROL_CLOCK(this_sch->index, 0);
+	SCH_SET_GROUP_FRAC_RATE(this_sch->index, 0);
+	SCH_SET_GROUP_MAX_CREDIT(this_sch->index, 0);
+	SCH_SET_GROUP_CONTROL_ENABLE(this_sch->index, 0);
+
+	SCH_SET_PORT_CONTROL_CLOCK(this_sch->index, 0);
+	SCH_SET_PORT_FRAC_RATE(this_sch->index, 0);
+	SCH_SET_PORT_MAX_CREDIT(this_sch->index, 0);
+	SCH_SET_PORT_CONTROL_ENABLE(this_sch->index, 0);
+
+	return 0;
+}
+
+/**
+ * qos_open -
+ *
+ *
+ */
+static int qos_open (struct inode *inode, struct file *file)
+{
+	int minor = MINOR (inode->i_rdev);
+
+	QOS_PRINTK (QOS_IOCTL, "qos_open(%#lx, %#lx) minor = %d", (unsigned long)inode, (unsigned long)file, minor);
+
+	if (ADM0_DEVICE_MINOR_NUM == minor)
+	{
+		QOS_PRINTK (QOS_IOCTL, "ADM0_DEVICE_MINOR_NUM opened");
+		if(&_adm[ADM0] != NULL)
+			file->private_data = (void *) &_adm[ADM0];
+		else
+			QOS_PRINTK (QOS_ERR, "qos_open ADM0 failed\n");
+	}
+	else if (ADM1_DEVICE_MINOR_NUM == minor)
+	{
+		QOS_PRINTK (QOS_IOCTL, "ADM1_DEVICE_MINOR_NUM opened");
+		file->private_data = (void *) &_adm[ADM1];
+	}
+	else if (SCH0_DEVICE_MINOR_NUM == minor)
+	{
+		QOS_PRINTK (QOS_IOCTL, "SCH0_DEVICE_MINOR_NUM opened");
+		file->private_data = (void *) &_sch[SCH0];
+	}
+	else if (SCH1_DEVICE_MINOR_NUM == minor)
+	{
+		QOS_PRINTK (QOS_IOCTL, "SCH1_DEVICE_MINOR_NUM opened");
+		file->private_data = (void *) &_sch[SCH1];
+	}
+	else
+	{
+		QOS_PRINTK (QOS_ERR, "QoS device %d out of range", minor);
+		return -ENODEV;
+	}
+
+	return 0;
+}
+
+
+/**
+ * qos_release -
+ *
+ *
+ */
+static int qos_release (struct inode *inode, struct file *file)
+{
+	QOS_PRINTK (QOS_IOCTL, "qos_release(%#lx, %#lx)", (unsigned long)inode, (unsigned long)file);
+
+	file->private_data = NULL;
+
+	return 0;
+}
+
+
+/********************************************************************************
+*		ADMITTANCE FUNCTIONS						*
+*********************************************************************************/
+
+
+/**
+ * qos_adm_set_policer_conf -
+ *
+ *
+ */
+static int qos_adm_set_policer_conf(struct _QOS_ADM *this_adm, struct _ADM_POLICER_CONF *conf)
+{
+	int rc = 0;
+	unsigned long bps;
+	unsigned long clk_sel, fraction;
+
+	QOS_PRINTK (QOS_POLICER, "policer %d rate %ld max_credit %ld init_burst %ld state %d", conf->index, conf->rate, conf->max_credit, conf->init_burst, conf->state);
+
+	if(conf->index > ADM_MAX_POLICER_ENTRIES)
+	{
+		QOS_PRINTK (QOS_ERR, "qos_adm_set_policer_conf(): policer %d is out of range", conf->index);
+		rc = 1;
+	}
+	else
+	{
+		if(conf->rate)
+		{
+			bps = conf->rate;
+			get_policer_rate_and_clock(bps, &clk_sel, &fraction);
+			printk("bps=%ld, clk_sel=%ld, fraction=0x%lx\n", bps, clk_sel, fraction);
+		}
+		else
+		{
+			fraction = 0;
+			clk_sel	= 15;
+		}
+
+		ADM_SET_POLICER_FRAC_RATE(this_adm->index, conf->index, fraction);
+		ADM_SET_POLICER_MAX_CREDIT(this_adm->index, conf->index, conf->max_credit);
+		ADM_SET_POLICER_INIT_BURST(this_adm->index, conf->index, conf->max_credit);
+		/* Write control register in one R/W */
+		ADM_SET_POLICER_CONTROL(this_adm->index, conf->index, clk_sel, conf->state);
+		QOS_PRINTK (QOS_POLICER, "POLICER (%p) index %d state is %d", ADM_POL_FRAC_RATE(this_adm->index, conf->index),conf->index, conf->state);
+
+	}
+
+	return rc;
+}
+
+/**
+ * qos_adm_get_policer_conf -
+ *
+ *
+ */
+static int qos_adm_get_policer_conf(struct _QOS_ADM *this_adm, struct _ADM_POLICER_CONF *conf)
+{
+	int rc = 0;
+
+	if(conf->index > ADM_MAX_POLICER_ENTRIES)
+	{
+		QOS_PRINTK (QOS_ERR, "qos_adm_get_policer_conf(): policer %d is out of range", conf->index);
+		rc = 1;
+	}
+	else
+	{
+		conf->rate = ADM_GET_POLICER_FRAC_RATE(this_adm->index, conf->index);
+		conf->max_credit = ADM_GET_POLICER_MAX_CREDIT(this_adm->index, conf->index);
+		conf->init_burst = ADM_GET_POLICER_INIT_BURST(this_adm->index, conf->index);
+		conf->state = ADM_GET_POLICER_ENABLE(this_adm->index, conf->index);
+	}
+	return rc;
+}
+
+
+/**
+ * qos_adm_set_camentry_state -
+ *
+ *
+ */
+static int qos_adm_set_camentry_state(struct _QOS_ADM *this_adm, struct _ADM_TCAM_STATE *conf)
+{
+
+	if((conf->index > ADM_MAX_TCAM_ENTRIES) || (this_adm->cam_sw_table[conf->index].slot == ADM_TCAM_FREE))
+	{
+		QOS_PRINTK (QOS_ERR, "camentry %d not found", conf->index);
+		return QOS_TCAM_INDEX_ERR;
+	}
+	else
+	{
+
+		QOS_PRINTK (QOS_TCAM_DEBUG, "camentry %d state set to %d", conf->index, conf->state);
+
+		ADM_SET_TCAM_ENABLE(this_adm->index, this_adm->cam_sw_table[conf->index].slot, conf->state);
+
+		return QOS_OK;
+	}
+}
+
+
+/**
+ * qos_adm_get_camentry -
+ *
+ *
+ */
+static int qos_adm_get_camentry(struct _QOS_ADM *this_adm, struct _ADM_TCAM_CONF *conf)
+{
+	int rc = 0;
+	unsigned char entry;
+
+	if(conf->index > ADM_MAX_TCAM_ENTRIES)
+	{
+		QOS_PRINTK (QOS_ERR, "camentry %d is out of range", conf->index);
+		rc = 1;
+	}
+	else
+	{
+		entry = this_adm->cam_sw_table[conf->index].slot;
+		if(entry < ADM_MAX_TCAM_ENTRIES)
+		{
+			conf->state = ADM_GET_TCAM_ENABLE(this_adm->index, entry);
+			conf->entry.policer = ADM_GET_TCAM_POLICER(this_adm->index, entry);
+			conf->entry.drop = ADM_GET_TCAM_DROP(this_adm->index, entry);
+			conf->entry.reserved = ADM_GET_TCAM_RSVD(this_adm->index, entry);
+			conf->entry.etype = ADM_GET_TCAM_ETHERTYPE(this_adm->index, entry);
+			conf->entry.vlan = ADM_GET_TCAM_VLANID(this_adm->index, entry);
+			conf->entry.pppoe_ipv4 = ADM_GET_TCAM_PPPOEPROTO_IP4(this_adm->index, entry);
+			conf->entry.pppoe_ipv6 = ADM_GET_TCAM_PPPOEPROTO_IP6(this_adm->index, entry);
+			conf->entry.iptos = ADM_GET_TCAM_IPTOS(this_adm->index, entry);
+			conf->entry.ipproto = ADM_GET_TCAM_IPPROTO(this_adm->index, entry);
+			conf->entry.ipfamily = this_adm->cam_sw_table[conf->index].entry.ipfamily;
+			if(conf->entry.ipfamily == TCAM_IPV4_FAMILY)
+			{
+				conf->entry.saddr[0] = ADM_GET_TCAM_IPSRC(this_adm->index, entry);
+				conf->entry.saddr[0] = swapbit(conf->entry.saddr[0]);
+				conf->entry.daddr[0] = ADM_GET_TCAM_IPDST(this_adm->index, entry);
+				conf->entry.daddr[0] = swapbit(conf->entry.daddr[0]);
+			}
+			else
+			{
+				memcpy((unsigned char*)conf->entry.saddr, (unsigned char*)this_adm->cam_sw_table[conf->index].entry.saddr, 16);
+				memcpy((unsigned char*)conf->entry.daddr, (unsigned char*)this_adm->cam_sw_table[conf->index].entry.daddr, 16);
+			}
+			conf->entry.sport_max = ADM_GET_TCAM_L4_SPORT_MAX(this_adm->index, entry);
+			conf->entry.sport_min = ADM_GET_TCAM_L4_SPORT_MIN(this_adm->index, entry);
+			conf->entry.dport_max = ADM_GET_TCAM_L4_DPORT_MAX(this_adm->index, entry);
+			conf->entry.dport_min = ADM_GET_TCAM_L4_DPORT_MIN(this_adm->index, entry);
+			conf->entry.multicast = ADM_GET_TCAM_MCAST(this_adm->index, entry);
+			conf->entry.broadcast = ADM_GET_TCAM_BCAST(this_adm->index, entry);
+			conf->entry.etype_mask = ADM_GET_TCAM_ETHERTYPE_MASK(this_adm->index, entry);
+			conf->entry.vlan_mask = ADM_GET_TCAM_VLAN_MASK(this_adm->index, entry);
+
+			conf->entry.saddr_mask = ADM_GET_TCAM_SADDR_MASK(this_adm->index, entry);
+			conf->entry.saddr_mask = expand_ip_mask(conf->entry.saddr_mask);
+			conf->entry.daddr_mask = ADM_GET_TCAM_DADDR_MASK(this_adm->index, entry);
+			conf->entry.daddr_mask = expand_ip_mask(conf->entry.daddr_mask);
+
+			conf->entry.sport_enable = ADM_GET_TCAM_SPORT_MASK(this_adm->index, entry);
+			conf->entry.dport_enable = ADM_GET_TCAM_DPORT_MASK(this_adm->index, entry);
+			conf->entry.pppoe_mask =ADM_GET_TCAM_PPPOE_MASK(this_adm->index, entry);
+			conf->entry.iptos_mask = ADM_GET_TCAM_IPTOS_MASK(this_adm->index, entry);
+			conf->entry.ipproto_mask = ADM_GET_TCAM_IPPROTO_MASK(this_adm->index, entry);
+			conf->entry.broadcast_mask = ADM_GET_TCAM_BCAST_MASK(this_adm->index, entry);
+			conf->entry.multicast_mask = ADM_GET_TCAM_MCAST_MASK(this_adm->index, entry);
+
+			QOS_PRINTK(QOS_TCAM_DEBUG, \
+			"qos_adm_get_camentry() %d state %d @ %p\n\
+			policer         %d\n \
+			drop            %d\n \
+			etype           %x\n \
+			vlan            %x\n \
+			pppoe_ipv4      %x\n \
+			pppoe_ipv6      %x\n \
+			iptos           %x\n \
+			ipproto         %x\n \
+			ipfamily	%x\n \
+			saddr           %lx\n \
+			daddr           %lx\n \
+			sport_max       %d\n \
+			sport_min       %d\n \
+			dport_max       %d\n \
+			dport_min       %d\n \
+			multicast       %x\n \
+			broadcast       %x\n \
+			etype_mask      %x\n \
+			vlan_mask       %x\n \
+			saddr_mask      %lx\n \
+			daddr_mask      %lx\n \
+			sport_enable    %d\n \
+			dport_enable    %d\n \
+			iptos_mask      %lx\n \
+			pppoe_mask      %lx\n \
+			ipproto_mask    %lx\n \
+			broadcast_mask  %lx\n \
+			multicast_mask  %lx\n\n",
+			entry, conf->state,
+			ADM_GET_TCAM_ENTRY(this_adm->index, entry),
+			conf->entry.policer,
+			conf->entry.drop,
+			conf->entry.etype,
+			conf->entry.vlan,
+			conf->entry.pppoe_ipv4,
+			conf->entry.pppoe_ipv6,
+			conf->entry.iptos,
+			conf->entry.ipproto,
+			conf->entry.ipfamily,
+			conf->entry.saddr[0],
+			conf->entry.daddr[0],
+			conf->entry.sport_max,
+			conf->entry.sport_min,
+			conf->entry.dport_max,
+			conf->entry.dport_min,
+			conf->entry.multicast,
+                        conf->entry.broadcast,
+			conf->entry.etype_mask,
+			conf->entry.vlan_mask,
+			conf->entry.saddr_mask,
+			conf->entry.daddr_mask,
+			conf->entry.sport_enable,
+			conf->entry.dport_enable,
+			conf->entry.iptos_mask,
+			conf->entry.pppoe_mask,
+			conf->entry.ipproto_mask,
+			conf->entry.broadcast_mask,
+			conf->entry.multicast_mask
+			);
+		}
+	}
+
+	return rc;
+}
+
+/**
+ * qos_adm_remove_camentry -
+ *
+ *
+ */
+static int qos_adm_remove_camentry(struct _QOS_ADM *this_adm, unsigned char tcam_entry_id)
+{
+	struct _ADM_TCAM_CONF tcam_conf;
+	unsigned int tcam_slot;
+
+	if(this_adm->cam_sw_table[tcam_entry_id].slot == ADM_TCAM_FREE)
+	{
+		QOS_PRINTK (QOS_ERR, "camentry %d already free\n", tcam_entry_id);
+		return QOS_TCAM_INDEX_ERR;
+	}
+	else
+	{
+		/* get hw slot associated to the entry to be removed */
+		tcam_slot = this_adm->cam_sw_table[tcam_entry_id].slot;
+
+		/* this id can be used again */
+		this_adm->cam_sw_table[tcam_entry_id].slot = ADM_TCAM_FREE;
+
+		/*clear entry in sw table */
+		memset(&this_adm->cam_sw_table[tcam_entry_id].entry, sizeof(struct _ADM_TCAM_ENTRY), 0);
+
+		/* do not move up once lastest entry or end of array are reached */
+		while((tcam_slot < (ADM_MAX_TCAM_ENTRIES - 1)) &&
+			(this_adm->cam_hw_table[tcam_slot + 1].slot != ADM_TCAM_FREE)  &&
+			(tcam_entry_id  < (ADM_MAX_TCAM_ENTRIES - 1)))
+		{
+			/* disable entry n + 1 before moving it */
+			ADM_SET_TCAM_ENABLE(this_adm->index, tcam_slot + 1, 0);
+
+			/* copy entry n + 1 to entry n */
+			tcam_conf.index = tcam_entry_id + 1;
+			qos_adm_get_camentry(this_adm, &tcam_conf);
+			qos_adm_add_camentry_to_slot(this_adm, &tcam_conf, tcam_slot);
+
+			/* enable entry n */
+			ADM_SET_TCAM_ENABLE(this_adm->index, tcam_slot, 1);
+
+			this_adm->cam_hw_table[tcam_slot].slot = this_adm->cam_hw_table[tcam_slot + 1].slot;
+			this_adm->cam_sw_table[tcam_entry_id + 1].slot = tcam_slot;
+
+			tcam_slot++; tcam_entry_id++;
+		}
+	}
+
+	/* one entry less in the list thus the last one must be marked as free */
+	this_adm->cam_hw_table[tcam_slot].slot = ADM_TCAM_FREE;
+
+	ADM_SET_TCAM_ENABLE(this_adm->index, tcam_slot, 0);
+
+	ADM_SET_TCAM_POLICER(this_adm->index, tcam_slot, 0);
+	ADM_SET_TCAM_DROP(this_adm->index, tcam_slot, 0);
+	ADM_SET_TCAM_RSVD(this_adm->index, tcam_slot, 0);
+	ADM_SET_TCAM_ETHERTYPE(this_adm->index, tcam_slot, 0);
+	ADM_SET_TCAM_VLANID(this_adm->index, tcam_slot, 0);
+	ADM_SET_TCAM_PPPOEPROTO(this_adm->index, tcam_slot, 0, 0);
+	ADM_SET_TCAM_IPTOS(this_adm->index, tcam_slot, 0);
+	ADM_SET_TCAM_IPPROTO(this_adm->index, tcam_slot, 0);
+	ADM_SET_TCAM_IPSRC(this_adm->index, tcam_slot, 0);
+	ADM_SET_TCAM_IPDST(this_adm->index, tcam_slot, 0);
+	ADM_SET_TCAM_L4_SPORT(this_adm->index, tcam_slot, 0, 0);
+	ADM_SET_TCAM_L4_DPORT(this_adm->index, tcam_slot, 0, 0);
+	ADM_SET_TCAM_MCAST_BCAST(this_adm->index, tcam_slot, 0, 0);
+
+        /* TCAM register 0 */
+        ADM_SET_TCAM_MASK_REG0(this_adm->index, tcam_slot, 0, 0);
+
+        /* TCAM register 1 */
+        ADM_SET_TCAM_MASK_REG1(this_adm->index, tcam_slot, 0, 0, 0, 0, 0, 0, 0);
+
+        /* TCAM register 2 */
+        ADM_SET_TCAM_MASK_REG2(this_adm->index, tcam_slot, 0, 0);
+
+	QOS_PRINTK (QOS_TCAM_DEBUG, "camentry id %d @ slot %d cleared\n",tcam_entry_id, tcam_slot);
+
+	return QOS_OK;
+}
+
+/**
+ * qos_adm_add_camentry -
+ * This function allocate a new entry in the TCAM array and set the conf->index to a unique
+ * value corresponding to the identifier associated to the TCAM slot used in the hardware. The identifer and the hw slot
+ * number may be different. This software identifier is to be used by the user space application for any requests concerning
+ * the CAM entry being configured here.
+ */
+static int qos_adm_add_camentry(struct _QOS_ADM *this_adm, struct _ADM_TCAM_CONF *conf)
+{
+	unsigned char i;
+	unsigned char free_handle = ADM_TCAM_FREE;
+	unsigned char free_slot = ADM_TCAM_FREE;
+	unsigned long *tmpU32 = NULL;
+
+	/* go through both sw and hw cam entry number and find a free room */
+	for(i = 0; i < ADM_TCAM_MAX_ENTRIES; i++)
+	{
+		if((this_adm->cam_sw_table[i].slot == ADM_TCAM_FREE) && (free_handle == ADM_TCAM_FREE))
+			free_handle = i; /* this is the indentifer associated to the the tcam entry */
+		if((this_adm->cam_hw_table[i].slot == ADM_TCAM_FREE) && (free_slot == ADM_TCAM_FREE))
+			free_slot = i; /* this is where the tcam entry is located in the hardware */
+	}
+
+	if((free_handle == ADM_TCAM_FREE) || (free_slot == ADM_TCAM_FREE))
+	{
+		QOS_PRINTK (QOS_ERR, "no more room for new cam entry free_handle = %d free_slot = %d\n", free_handle, free_slot);
+		return QOS_TCAM_NO_MORE_ROOM;
+	}
+	else
+	{
+		this_adm->cam_sw_table[free_handle].slot = free_slot; /* sw to hw identifer mapping */
+		this_adm->cam_hw_table[free_slot].slot = free_handle; /* hw to sw identifier mapping */
+
+		conf->index = free_handle; /* this handle is will be used by user space application */
+
+		tmpU32 = (unsigned long *)ADM_GET_TCAM_ENTRY(this_adm->index, free_slot);
+		QOS_PRINTK(QOS_TCAM_DEBUG, "qos_adm_add_camentry() adm %d entry %d is at %p\n", this_adm->index, free_slot, (unsigned long*)tmpU32);
+		QOS_PRINTK(QOS_TCAM_DEBUG, \
+		"\n\
+		policer         %d\n \
+		drop            %d\n \
+		etype           %x\n \
+		vlan            %x\n \
+		pppoe_ipv4      %x\n \
+		pppoe_ipv6      %x\n \
+		iptos           %x\n \
+		ipproto         %x\n \
+		ipfamily	%x\n \
+		saddr           %lx\n \
+		daddr           %lx\n \
+		sport_max       %d\n \
+		sport_min       %d\n \
+		dport_max       %d\n \
+		dport_min       %d\n \
+		multicast       %x\n \
+		broadcast       %x\n \
+		etype_mask      %x\n \
+		vlan_mask       %x\n \
+		saddr_mask      %lx\n \
+		daddr_mask      %lx\n \
+		sport_enable    %d\n \
+		dport_enable    %d\n \
+		iptos_mask      %lx\n \
+		pppoe_mask      %lx\n \
+		ipproto_mask    %lx\n \
+		broadcast_mask  %lx\n \
+		multicast_mask  %lx\n\n",
+		conf->entry.policer,
+		conf->entry.drop,
+		conf->entry.etype,
+		conf->entry.vlan,
+		conf->entry.pppoe_ipv4,
+		conf->entry.pppoe_ipv6,
+		conf->entry.iptos,
+		conf->entry.ipproto,
+		conf->entry.ipfamily,
+		conf->entry.saddr[0],
+		conf->entry.daddr[0],
+		conf->entry.sport_max,
+		conf->entry.sport_min,
+		conf->entry.dport_max,
+		conf->entry.dport_min,
+		conf->entry.multicast,
+                conf->entry.broadcast,
+		conf->entry.etype_mask,
+		conf->entry.vlan_mask,
+		conf->entry.saddr_mask,
+		conf->entry.daddr_mask,
+		conf->entry.sport_enable,
+		conf->entry.dport_enable,
+		conf->entry.iptos_mask,
+		conf->entry.pppoe_mask,
+		conf->entry.ipproto_mask,
+		conf->entry.broadcast_mask,
+		conf->entry.multicast_mask
+		);
+
+		/* only complete mask are supported, so overwrite any mask if not 0 (1bit length masks and
+		ip src/dst masks are not overwritten */
+		if(conf->entry.etype_mask)
+				conf->entry.etype_mask = 0xFFFF;
+		if(conf->entry.ipproto_mask)
+				conf->entry.ipproto_mask = 0xFF;
+		if(conf->entry.iptos_mask)
+				conf->entry.iptos_mask = 0xFF;
+		if(conf->entry.vlan_mask)
+				conf->entry.vlan_mask = 0xFFFF;
+
+		/* now let's configure tcam block using the hw slot number */
+		ADM_SET_TCAM_ENABLE(this_adm->index, free_slot, 0);
+
+		ADM_SET_TCAM_POLICER(this_adm->index, free_slot, conf->entry.policer);
+		ADM_SET_TCAM_DROP(this_adm->index, free_slot, conf->entry.drop);
+		ADM_SET_TCAM_RSVD(this_adm->index, free_slot,conf->entry.reserved);
+		ADM_SET_TCAM_ETHERTYPE(this_adm->index, free_slot, conf->entry.etype);
+		ADM_SET_TCAM_VLANID(this_adm->index, free_slot, conf->entry.vlan);
+		ADM_SET_TCAM_PPPOEPROTO(this_adm->index, free_slot, conf->entry.pppoe_ipv4, conf->entry.pppoe_ipv6);
+		ADM_SET_TCAM_IPTOS(this_adm->index, free_slot, conf->entry.iptos);
+		ADM_SET_TCAM_IPPROTO(this_adm->index, free_slot, conf->entry.ipproto);
+		ADM_SET_TCAM_IPSRC(this_adm->index, free_slot, swapbit(conf->entry.saddr[0]));
+		ADM_SET_TCAM_IPDST(this_adm->index, free_slot, swapbit(conf->entry.daddr[0]));
+		ADM_SET_TCAM_L4_SPORT(this_adm->index, free_slot, conf->entry.sport_min, conf->entry.sport_max);
+		ADM_SET_TCAM_L4_DPORT(this_adm->index, free_slot, conf->entry.dport_min, conf->entry.dport_max);
+		ADM_SET_TCAM_MCAST_BCAST(this_adm->index, free_slot, conf->entry.multicast, conf->entry.broadcast);
+
+		/* TCAM register 0 */
+		ADM_SET_TCAM_MASK_REG0(this_adm->index, free_slot, conf->entry.etype_mask, conf->entry.vlan_mask);
+
+		/* TCAM register 1 */
+		ADM_SET_TCAM_MASK_REG1(this_adm->index, free_slot, conf->entry.pppoe_mask,conf->entry.sport_enable,conf->entry.dport_enable,conf->entry.iptos_mask,conf->entry.ipproto_mask,conf->entry.multicast_mask, conf->entry.broadcast_mask);
+
+		/* TCAM register 2 */
+		ADM_SET_TCAM_MASK_REG2(this_adm->index, free_slot, reduce_ip_mask(conf->entry.saddr_mask), reduce_ip_mask(conf->entry.daddr_mask));
+
+		ADM_SET_TCAM_ENABLE(this_adm->index, free_slot, 1);
+
+		/* save tcam entry (inlcuding extra parameters) in the sw table */
+		memcpy(&this_adm->cam_sw_table[free_handle].entry, &conf->entry, sizeof(struct _ADM_TCAM_ENTRY));
+
+		QOS_PRINTK(QOS_TCAM_DEBUG, "after write to regs\n");
+		for(i = 0; i < ADM_TCAM_ENTRY_SIZE / 4; i++)
+			QOS_PRINTK(QOS_TCAM_DEBUG, "%p = %8lx\n",&tmpU32[i], tmpU32[i]);
+	}
+#if 0
+	// for debug purpose only list all entries
+	QOS_PRINTK(QOS_TCAM_DEBUG,"qos_adm_add_camentry:\n");
+	for(i = 0; i < ADM_TCAM_MAX_ENTRIES; i++)
+	{
+		QOS_PRINTK(QOS_TCAM_DEBUG,"this_adm->cam_sw_table[%d] = %d ", i, this_adm->cam_sw_table[i].slot);
+		QOS_PRINTK(QOS_TCAM_DEBUG,"this_adm->cam_hw_table[%d] = %d \n", i, this_adm->cam_hw_table[i].slot);
+	}
+#endif
+	return QOS_OK;
+}
+
+
+/**
+ * qos_adm_add_camentry_to_slot -
+ *
+ *
+ */
+static int qos_adm_add_camentry_to_slot(struct _QOS_ADM *this_adm, struct _ADM_TCAM_CONF *conf, unsigned char tcam_slot)
+{
+	if(tcam_slot >= ADM_MAX_TCAM_ENTRIES)
+	{
+		QOS_PRINTK (QOS_ERR, "slot %d out of range\n", tcam_slot);
+		return QOS_TCAM_INDEX_ERR;
+	}
+	else
+	{
+		/* only complete mask are supported, so overwrite any mask if not 0 (1bit length masks and
+		ip src/dst masks are not overwritten */
+		if(conf->entry.etype_mask)
+			conf->entry.etype_mask = 0xFFFF;
+		if(conf->entry.ipproto_mask)
+			conf->entry.ipproto_mask = 0xFF;
+		if(conf->entry.iptos_mask)
+			conf->entry.iptos_mask = 0xFF;
+		if(conf->entry.vlan_mask)
+			conf->entry.vlan_mask = 0xFFFF;
+
+		ADM_SET_TCAM_POLICER(this_adm->index, tcam_slot, conf->entry.policer);
+		ADM_SET_TCAM_DROP(this_adm->index, tcam_slot, conf->entry.drop);
+		ADM_SET_TCAM_RSVD(this_adm->index, tcam_slot,conf->entry.reserved);
+		ADM_SET_TCAM_ETHERTYPE(this_adm->index, tcam_slot, conf->entry.etype);
+		ADM_SET_TCAM_VLANID(this_adm->index, tcam_slot, conf->entry.vlan);
+		ADM_SET_TCAM_PPPOEPROTO(this_adm->index, tcam_slot, conf->entry.pppoe_ipv4, conf->entry.pppoe_ipv6);
+		ADM_SET_TCAM_IPTOS(this_adm->index, tcam_slot, conf->entry.iptos);
+		ADM_SET_TCAM_IPPROTO(this_adm->index, tcam_slot, conf->entry.ipproto);
+		ADM_SET_TCAM_IPSRC(this_adm->index, tcam_slot, swapbit(conf->entry.saddr[0]));
+		ADM_SET_TCAM_IPDST(this_adm->index, tcam_slot, swapbit(conf->entry.daddr[0]));
+		ADM_SET_TCAM_L4_SPORT(this_adm->index, tcam_slot, conf->entry.sport_min, conf->entry.sport_max);
+		ADM_SET_TCAM_L4_DPORT(this_adm->index, tcam_slot, conf->entry.dport_min, conf->entry.dport_max);
+		ADM_SET_TCAM_MCAST_BCAST(this_adm->index, tcam_slot, conf->entry.multicast, conf->entry.broadcast);
+
+		/* TCAM register 0 */
+		ADM_SET_TCAM_MASK_REG0(this_adm->index, tcam_slot, conf->entry.etype_mask, conf->entry.vlan_mask);
+
+		/* TCAM register 1 */
+		ADM_SET_TCAM_MASK_REG1(this_adm->index, tcam_slot, conf->entry.pppoe_mask,conf->entry.sport_enable,conf->entry.dport_enable,conf->entry.iptos_mask,conf->entry.ipproto_mask,conf->entry.multicast_mask, conf->entry.broadcast_mask);
+
+		/* TCAM register 2 */
+		ADM_SET_TCAM_MASK_REG2(this_adm->index, tcam_slot, reduce_ip_mask(conf->entry.saddr_mask), reduce_ip_mask(conf->entry.daddr_mask));
+	}
+
+	return QOS_OK;
+}
+
+/**
+ * qos_adm_change_camentry -
+ *
+ *
+ */
+static int qos_adm_change_camentry(struct _QOS_ADM *this_adm, struct _ADM_TCAM_CONF *conf)
+{
+	unsigned int tcam_slot;
+
+	if(this_adm->cam_sw_table[conf->index].slot == ADM_TCAM_FREE)
+	{
+		QOS_PRINTK (QOS_ERR, "camentry %d not found\n", conf->index);
+		return QOS_TCAM_INDEX_ERR;
+	}
+	else
+	{
+		/* only complete mask are supported, so overwrite any mask if not 0 (1bit length masks and
+		ip src/dst masks are not overwritten */
+		if(conf->entry.etype_mask)
+			conf->entry.etype_mask = 0xFFFF;
+		if(conf->entry.ipproto_mask)
+			conf->entry.ipproto_mask = 0xFF;
+		if(conf->entry.iptos_mask)
+			conf->entry.iptos_mask = 0xFF;
+		if(conf->entry.vlan_mask)
+			conf->entry.vlan_mask = 0xFFFF;
+
+		tcam_slot = this_adm->cam_sw_table[conf->index].slot;
+		ADM_SET_TCAM_POLICER(this_adm->index, tcam_slot, conf->entry.policer);
+		ADM_SET_TCAM_DROP(this_adm->index, tcam_slot, conf->entry.drop);
+		ADM_SET_TCAM_RSVD(this_adm->index, tcam_slot,conf->entry.reserved);
+		ADM_SET_TCAM_ETHERTYPE(this_adm->index, tcam_slot, conf->entry.etype);
+		ADM_SET_TCAM_VLANID(this_adm->index, tcam_slot, conf->entry.vlan);
+		ADM_SET_TCAM_PPPOEPROTO(this_adm->index, tcam_slot, conf->entry.pppoe_ipv4, conf->entry.pppoe_ipv6);
+		ADM_SET_TCAM_IPTOS(this_adm->index, tcam_slot, conf->entry.iptos);
+		ADM_SET_TCAM_IPPROTO(this_adm->index, tcam_slot, conf->entry.ipproto);
+		ADM_SET_TCAM_IPSRC(this_adm->index, tcam_slot, swapbit(conf->entry.saddr[0]));
+		ADM_SET_TCAM_IPDST(this_adm->index, tcam_slot, swapbit(conf->entry.daddr[0]));
+		ADM_SET_TCAM_L4_SPORT(this_adm->index, tcam_slot, conf->entry.sport_min, conf->entry.sport_max);
+		ADM_SET_TCAM_L4_DPORT(this_adm->index, tcam_slot, conf->entry.dport_min, conf->entry.dport_max);
+ADM_SET_TCAM_MCAST_BCAST(this_adm->index, tcam_slot, conf->entry.multicast, conf->entry.broadcast);
+
+/* TCAM register 0 */
+ADM_SET_TCAM_MASK_REG0(this_adm->index, tcam_slot, conf->entry.etype_mask, conf->entry.vlan_mask);
+
+/* TCAM register 1 */
+ADM_SET_TCAM_MASK_REG1(this_adm->index, tcam_slot, conf->entry.pppoe_mask,conf->entry.sport_enable,conf->entry.dport_enable,conf->entry.iptos_mask,conf->entry.ipproto_mask,conf->entry.multicast_mask, conf->entry.broadcast_mask);
+
+/* TCAM register 2 */
+ADM_SET_TCAM_MASK_REG2(this_adm->index, tcam_slot, reduce_ip_mask(conf->entry.saddr_mask), reduce_ip_mask(conf->entry.daddr_mask));
+
+		return QOS_OK;
+	}
+}
+
+
+/**
+ * qos_adm_set_discard_conf -
+ *
+ *
+ */
+static int qos_adm_set_discard_conf(struct _QOS_ADM *this_adm, struct _ADM_DISCARD_CONF *conf)
+{
+	int rc = 0;
+	unsigned long qfull, qmin, qmax = 0;
+
+	if(conf->enable == 0)
+	{
+		QOS_PRINTK (QOS_INFO, "ADM_SET_CTRL_DISCARD_DISABLE\n");
+	}
+	else
+	{
+		ADM_SET_CONF_PROB0(this_adm->index, conf->zone_prob[0]);
+		ADM_SET_CONF_PROB1(this_adm->index, conf->zone_prob[1]);
+		ADM_SET_CONF_PROB2(this_adm->index, conf->zone_prob[2]);
+		ADM_SET_CONF_PROB3(this_adm->index, conf->zone_prob[3]);
+
+		qfull = ADM_GET_Q_FULL_THRESHOLD(this_adm->index);
+		qmax = ((conf->queue_drop_max * qfull) / 100);
+		// need at least 16 buffers for reserved traffic (packets in flight in GEM fifo)
+		if ( qmax > (qfull - 16))
+			qmax = qfull - 16;
+		qmin = (conf->queue_drop_min * qfull) / 100;
+		// need at least 4 buffers for the 4 zones (1 per zone)
+		if (qmin > (qmax - 4))
+			qmin = qmax - 4;
+
+		ADM_SET_Q_DROP_MIN(this_adm->index, qmin);
+		ADM_SET_Q_DROP_MAX(this_adm->index, qmax);
+
+		QOS_PRINTK (QOS_INFO, "ADM_SET_CTRL_DISCARD_ENABLE (qfull %lu qmax %lu qmin %lu)\n", qfull, qmax, qmin);
+	}
+
+	return rc;
+}
+
+
+/**
+ * qos_adm_set_flowctrl_conf -
+ *
+ *
+ */
+static int qos_adm_set_flowctrl_conf(struct _QOS_ADM *this_adm, struct _ADM_FLOWCTRL_CONF *conf)
+{
+	int rc = 0;
+	unsigned long qfull, threshon, threshoff = 0;
+
+	if(conf->enable == 0)
+	{
+		QOS_PRINTK (QOS_INFO, "ADM_SET_CTRL_FLOWCTRL_DISABLE\n");
+	}
+	else
+	{
+		qfull = ADM_GET_Q_FULL_THRESHOLD(this_adm->index);
+		threshon = (conf->on_tresh * qfull) / 100;
+		// need at least 20 buffers for reserved traffic (16 packets in flight in GEM fifo + 4 packets on the line)
+		if ( threshon > (qfull - 20))
+			threshon = qfull - 20;
+
+		threshoff = (conf->off_tresh * qfull) / 100;
+		if (threshoff >= threshon)
+			threshoff = threshon - 1;
+
+		ADM_SET_FLOWCTRL_ON_TRESHOLD(this_adm->index, threshon);
+		ADM_SET_FLOWCTRL_OFF_TRESHOLD(this_adm->index, threshoff);
+		ADM_SET_DEPTH_PAUSE_INT_TIMER(this_adm->index, conf->pause_timer);
+
+		QOS_PRINTK (QOS_INFO, "ADM_SET_CTRL_FLOWCTRL_ENABLE\n");
+	}
+
+	return rc;
+}
+
+
+/**
+ * qos_adm_set_shaper_conf -
+ *
+ *
+ */
+static int qos_adm_set_shaper_conf(struct _QOS_ADM *this_adm, struct _ADM_SHAPER_CONF *conf)
+{
+	int rc = 0;
+	unsigned long bps;
+	unsigned long clk_sel, fraction;
+
+	if(conf->enable == 0)
+	{
+		ADM_SET_PORT_SHA_ENABLE(this_adm->index, 0);
+		QOS_PRINTK (QOS_SHAPER, "qos_adm_set_shaper_conf: ADM_SET_PORT_SHA_DISABLE\n");
+	}
+	else
+	{
+		if(conf->rate)
+		{
+			bps = conf->rate;
+			get_shaper_rate_and_clock(bps, &clk_sel, &fraction);
+			QOS_PRINTK (QOS_SHAPER, "qos_adm_set_shaper_conf: bps=%ld, clk_sel=%ld, fraction=%ld\n", bps, clk_sel, fraction);
+		}
+		else
+		{
+			fraction = 0;
+			clk_sel	= 15;
+		}
+
+		ADM_SET_PORT_SHA_PKT_OVER(this_adm->index, conf->overhead); //sw wa - registers 0x4030 must be written prior to 0x4020
+		ADM_SET_PORT_SHA_FRAC_RATE(this_adm->index, fraction);
+		ADM_SET_PORT_SHA_MAX_CREDIT(this_adm->index, conf->max_credit);
+		ADM_SET_PORT_SHA_CLOCK(this_adm->index, clk_sel);
+		ADM_SET_PORT_SHA_ENABLE(this_adm->index, 1);
+		QOS_PRINTK (QOS_SHAPER, "qos_adm_set_shaper_conf: ADM_SET_PORT_SHA_ENABLE\n");
+
+	}
+
+	return rc;
+}
+
+/**
+ * qos_adm_get_shaper_conf -
+ *
+ *
+ */
+static int qos_adm_get_shaper_conf(struct _QOS_ADM *this_adm, struct _ADM_SHAPER_CONF *conf)
+{
+	int rc = QOS_OK;
+	unsigned long clk, fraction;
+
+	fraction = ADM_GET_PORT_SHA_FRAC_RATE(this_adm->index);
+	clk = ADM_GET_PORT_SHA_CLOCK(this_adm->index);
+	conf->rate = shaper_fraction_clk_to_rate(fraction, clk);
+	conf->max_credit = ADM_GET_PORT_SHA_MAX_CREDIT(this_adm->index);
+	conf->overhead = ADM_GET_PORT_SHA_PKT_OVER(this_adm->index);
+	conf->enable = ADM_GET_PORT_SHA_ENABLE(this_adm->index);
+
+	QOS_PRINTK (QOS_SHAPER,"qos_adm_get_shaper_conf: ADM%d fraction=%ld, clk=%ld, bps=%ld\n", this_adm->index, fraction, clk, conf->rate);
+
+	return rc;
+}
+
+
+
+
+/**
+ * qos_adm_set_mode -
+ *
+ *
+ */
+static int qos_adm_set_mode(struct _QOS_ADM *this_adm, unsigned char mode)
+{
+	int rc = 0;
+
+	if(mode >= ADM_MAX_MODE)
+	{
+		rc = 1;
+	}
+	else
+	{
+		if(mode == DISABLED_MODE) {
+			QOS_PRINTK (QOS_INFO, "ADM%d BLOCK DISABLED\n", this_adm->index);
+			ADM_SET_CONF_MODE_DISCARD(this_adm->index);
+			ADM_SET_CTRL_ADM_DISABLE(this_adm->index);
+		}
+		else {
+			if(mode == DISCARD_MODE) {
+				QOS_PRINTK (QOS_INFO, "ADM%d DISCARD_MODE or SHAPED_MODE\n", this_adm->index);
+				ADM_SET_CONF_MODE_DISCARD(this_adm->index);
+			}
+			else {
+				QOS_PRINTK (QOS_INFO, "ADM%d FLOWCTRL_MODE\n", this_adm->index);
+				ADM_SET_CONF_MODE_FLOWCTRL(this_adm->index);
+			}
+			ADM_SET_CTRL_ADM_ENABLE(this_adm->index);
+			QOS_PRINTK (QOS_INFO, "ADM%d BLOCK ENABLED\n", this_adm->index);
+		}
+	}
+
+	return rc;
+}
+
+
+/**
+ * qos_adm_get_mode -
+ *
+ *
+ */
+static unsigned char qos_adm_get_mode(struct _QOS_ADM *this_adm)
+{
+	return ADM_GET_CONF_MODE(this_adm->index);
+}
+
+
+
+/**
+ * qos_adm_set_lru_conf -
+ *
+ *
+ */
+static int qos_adm_set_lru_conf(struct _QOS_ADM *this_adm, struct _ADM_LRU_CONF *conf)
+{
+	int rc = 0;
+
+	if(conf->period)
+	{
+		ADM_SET_LRU_MASTER_ENABLE(this_adm->index);
+		ADM_SET_LRU_PERIOD(this_adm->index, conf->period);
+		ADM_SET_LRU_MASK(this_adm->index, conf->mask);
+		ADM_SET_LRU_STATE(this_adm->index, 1);
+	}
+	else
+	{
+		ADM_SET_LRU_MASTER_DISABLE(this_adm->index);
+		ADM_SET_LRU_STATE(this_adm->index, 0);
+	}
+	return rc;
+}
+
+
+/**
+ * qos_adm_get_lru_stats -
+ *
+ *
+ */
+static int qos_adm_get_lru_stats(struct _QOS_ADM *this_adm, struct _ADM_LRU_STATS *p)
+{
+	int size = 0;
+	unsigned char tcam_index;
+
+	if (p)
+	{
+		size = sizeof(struct _ADM_LRU_STATS);
+		p->entry = ADM_GET_LRU_ENTRY(this_adm->index);
+		p->count = ADM_GET_LRU_ENTRY_HIT(this_adm->index);
+
+		for(tcam_index = 0; tcam_index < ADM_MAX_TCAM_ENTRIES; tcam_index++)
+			p->hit_count[tcam_index] = (unsigned char)(ADM_GET_LRU_HIT_GROUP(this_adm->index, tcam_index) >> ((tcam_index & 3) * 8));
+	}
+
+	return size;
+}
+
+
+
+/**
+ * qos_adm_get_counters -
+ *
+ *
+ */
+static int qos_adm_get_counters(struct _QOS_ADM *this_adm, struct _ADM_COUNTER *p)
+{
+	int size = 0;
+
+	memset(p, 0, sizeof(struct _ADM_COUNTER));
+
+	if (p)
+	{
+		size = sizeof(struct _ADM_COUNTER);
+		memcpy(p, this_adm->counter, size);
+	}
+
+	return size;
+}
+
+/**
+ * qos_adm_dump_counters -
+ *
+ *
+ */
+void qos_adm_dump_counters(void)
+{
+	int i;
+
+	QOS_PRINTK (QOS_STAT, "qos_adm_dump_counters() adm0 stat @%p adm1 stats @%p\n", _adm[0].counter, _adm[1].counter);
+
+	for(i = ADM0; i <= ADM1; i++)
+	{
+		ADM_SET_CTRL_SNAPSHOT_ENABLE(i);
+		QOS_PRINTK(QOS_STAT, \
+		" \
+		Admittance block %d counters:\n \
+		port_byte                %ld\n \
+		port_packet              %ld\n \
+		reserved_byte            %ld\n \
+		reserved_packet          %ld\n \
+		managed_byte             %ld\n \
+		managed_packet           %ld\n \
+		packet_dropped           %ld\n \
+		packet_dropped_error     %ld\n \
+		packet_dropped_denied    %ld\n \
+		packet_dropped_policer0  %ld\n \
+		packet_dropped_policer1  %ld\n \
+		packet_dropped_shaper    %ld\n \
+		packet_dropped_managed   %ld\n \
+		packet_dropped_unmanaged %ld\n \
+		", \
+		i, \
+		_adm[i].counter->port_byte, \
+		_adm[i].counter->port_packet, \
+		_adm[i].counter->reserved_byte, \
+		_adm[i].counter->reserved_packet, \
+		_adm[i].counter->managed_byte, \
+		_adm[i].counter->managed_packet, \
+		_adm[i].counter->packet_dropped, \
+		_adm[i].counter->packet_dropped_error, \
+		_adm[i].counter->packet_dropped_denied, \
+		_adm[i].counter->packet_dropped_policer0, \
+		_adm[i].counter->packet_dropped_policer1, \
+		_adm[i].counter->packet_dropped_shaper, \
+		_adm[i].counter->packet_dropped_managed, \
+		_adm[i].counter->packet_dropped_unmanaged \
+		);
+	}
+
+}
+
+
+/********************************************************************************
+*		SCHEDULER FUNCTIONS						*
+*********************************************************************************/
+
+/**
+ * qos_sch_get_counters -
+ *
+ *
+ */
+static int qos_sch_get_counters(struct _QOS_SCH *this_sch, struct _SCH_COUNTER *p)
+{
+	int i;
+
+	if (p)
+	{
+		p->port_byte = SCH_GET_PORT_BYTE_CNT(this_sch->index);
+		p->port_packet = SCH_GET_PORT_PKT_CNT(this_sch->index);
+		for(i = 0; i < SCH_MAX_QUEUES; i++)
+		{
+			QOS_PRINTK(QOS_STAT, "SCH_GET_Q_BYTE_IN_Q%d @%p = %ld", i ,SCH_Q_BYTE_IN_Q(this_sch->index,i), SCH_GET_Q_BYTE_IN_Q(this_sch->index, i));
+			p->q_byte[i] = SCH_GET_Q_BYTE_IN_Q(this_sch->index, i);
+			p->q_packet[i] = SCH_GET_Q_PKT_IN_Q(this_sch->index, i);
+			p->q_idle[i] = SCH_GET_Q_IDLE(this_sch->index, i);
+		}
+
+		return sizeof(struct _SCH_COUNTER);
+	}
+	else
+	{
+		return 0;
+	}
+}
+
+/**
+ * qos_sch_dump_counters -
+ *
+ *
+ */
+void qos_sch_dump_counters(void)
+{
+	int s, q;
+	struct _SCH_COUNTER sch_stats;
+
+	for(s = SCH0; s <= SCH1; s++)
+	{
+		memset(&sch_stats, 0, sizeof(struct _SCH_COUNTER));
+		qos_sch_get_counters(&_sch[s], &sch_stats);
+
+		QOS_PRINTK(QOS_STAT, \
+		" \
+		Scheduler block %d counters:\n \
+		tx byte                %ld\n \
+		tx packet              %ld\n \
+		", \
+		s,
+		sch_stats.port_byte, \
+		sch_stats.port_packet \
+		);
+
+		for(q = 0; q < SCH_MAX_QUEUES; q++)
+		{
+			QOS_PRINTK(QOS_STAT, \
+			" \
+			Scheduler block %d counters for queue %d:\n \
+			byte in queue		%ld\n \
+			packet in queue		%ld\n \
+			queue idle time		%ld\n \
+			", \
+			s,
+			q,
+			sch_stats.q_byte[q], \
+			sch_stats.q_packet[q], \
+			sch_stats.q_idle[q] \
+			);
+		}
+	}
+}
+
+/**
+ * qos_sch_set_shaper_state -
+ *
+ *
+ */
+static int qos_sch_set_shaper_state(struct _QOS_SCH *this_sch, struct _SCH_SHAPER_STATE *conf)
+{
+	int rc = QOS_OK;
+
+	switch(conf->shaper)
+	{
+		case SHAPER_Q7:
+			SCH_SET_Q_CONTROL_ENABLE(this_sch->index,7,conf->state);
+			break;
+		case SHAPER_Q6:
+			SCH_SET_Q_CONTROL_ENABLE(this_sch->index,6,conf->state);
+			break;
+		case SHAPER_GROUP:
+			SCH_SET_GROUP_CONTROL_ENABLE(this_sch->index, conf->state);
+			break;
+		case SHAPER_PORT:
+			SCH_SET_PORT_CONTROL_ENABLE(this_sch->index, conf->state);
+			break;
+		default:
+			QOS_PRINTK (QOS_IOCTL, "unknown shaper index");
+			rc = -EINVAL;
+			break;
+	}
+	return rc;
+}
+
+/**
+ * qos_sch_get_shaper_state -
+ *
+ *
+ */
+static int qos_sch_get_shaper_state(struct _QOS_SCH *this_sch, struct _SCH_SHAPER_STATE *conf)
+{
+	int rc = QOS_OK;
+
+	switch(conf->shaper)
+	{
+		case SHAPER_Q7:
+			conf->state = SCH_GET_Q_CONTROL_ENABLE(this_sch->index,7);
+			break;
+		case SHAPER_Q6:
+			conf->state = SCH_GET_Q_CONTROL_ENABLE(this_sch->index,6);
+			break;
+		case SHAPER_GROUP:
+			conf->state = SCH_GET_GROUP_CONTROL_ENABLE(this_sch->index);
+			break;
+		case SHAPER_PORT:
+			conf->state = SCH_GET_PORT_CONTROL_ENABLE(this_sch->index);
+			break;
+		default:
+			QOS_PRINTK (QOS_IOCTL, "unknown shaper index");
+			rc = -EINVAL;
+			break;
+	}
+
+	return rc;
+}
+
+/**
+ * get_policer_rate_and_clock -
+ *
+ * cf. c1000 HW Admittance Specs v14 for rate calculation details
+ */
+static void get_policer_rate_and_clock(unsigned long pps, unsigned long *p_clk_sel, unsigned long *p_fraction)
+{
+	unsigned long cycles_per_packet;
+	unsigned long frac_packet_rate;
+	unsigned long clk_sel;
+	unsigned long fraction;
+	unsigned long clock = (REFCLK/256);
+
+	cycles_per_packet = clock / pps;
+	QOS_PRINTK (QOS_POLICER,"cycles_per_packet=%ld, pps=%ld\n", cycles_per_packet, pps);
+
+	/* From Specs: Clock select paramter 0 and 1 are not suported in the Policer block. Starting clk_sel at 0x0010 */
+	for(clk_sel = 2; clk_sel < 16; clk_sel++)
+	{
+		if((1 << clk_sel) >= cycles_per_packet)
+			break;
+	}
+
+	if(clk_sel == 16)
+	{
+		clk_sel = 15;
+		fraction = 0;
+	}
+	else
+	{
+		QOS_PRINTK (QOS_POLICER,"(1 << clk_sel)=%d\n", (1 << clk_sel));
+		if(cycles_per_packet == 0) cycles_per_packet = 1;
+		frac_packet_rate = ((1 << clk_sel) * 100) / cycles_per_packet;
+		QOS_PRINTK (QOS_POLICER,"rate=%ld  -> %ld\n", frac_packet_rate, (frac_packet_rate - 100));
+		fraction = (((frac_packet_rate - 100) * 256) / 100);
+		QOS_PRINTK (QOS_POLICER,"fraction=%ld clk_sel=%ld\n", fraction, clk_sel);
+		if(fraction > 255)
+			fraction = 255;
+	}
+
+	*p_clk_sel = clk_sel;
+	*p_fraction = fraction;
+}
+
+/**
+ * get_shaper_rate_and_clock -
+ *
+ * cf. c1000 HW Scheduler Specs v14 for rate calculation details (section Shaper Block)
+ */
+static void get_shaper_rate_and_clock(unsigned long bps, unsigned long *p_clk_sel, unsigned long *p_fraction)
+{
+	unsigned long byte_rate;
+	unsigned long cycles_per_byte;
+	unsigned long clk_sel;
+	unsigned long frac_byte_rate;
+	unsigned long fraction;
+
+	byte_rate = bps >> 3;
+	cycles_per_byte = ((REFCLK * 10) / byte_rate);
+	QOS_PRINTK (QOS_SHAPER,"cycles_per_byte_x10=%ld byte_rate =%ld\n", cycles_per_byte,byte_rate);
+	for(clk_sel = 0; clk_sel < 16; clk_sel++)
+	{
+		if(((1 << clk_sel) * 10) >= cycles_per_byte)
+			break;
+	}
+	if(clk_sel == 16)
+	{
+		clk_sel = 15;
+		fraction = 0;
+	}
+	else
+	{
+		QOS_PRINTK (QOS_SHAPER,"(1 << clk_sel)=%d\n", (1 << clk_sel));
+		if(cycles_per_byte == 0) cycles_per_byte = 1;
+		frac_byte_rate = (((1 << clk_sel) * 1000) * 10) / cycles_per_byte;
+		QOS_PRINTK (QOS_SHAPER,"rate=%ld  -> %ld\n", frac_byte_rate, (frac_byte_rate - 1000));
+		if(frac_byte_rate > 1000)
+			fraction = (((frac_byte_rate - 1000) * 256) / 1000);
+		else
+			fraction = 0;
+		QOS_PRINTK (QOS_SHAPER,"fraction=%ld clk_sel=%ld\n", fraction, clk_sel);
+		if(fraction > 255)
+			fraction = 255;
+	}
+
+	*p_clk_sel = clk_sel;
+	*p_fraction = fraction;
+}
+
+/**
+ * shaper_fraction_clk_to_rate -
+ *
+ * cf. c1000 HW Scheduler Specs v14 for rate calculation details (section Shaper Block)
+ */
+static unsigned long shaper_fraction_clk_to_rate(unsigned long fraction, unsigned long clk_sel)
+{
+	unsigned long long rate = 0;
+
+	if(clk_sel != 0) {
+		/* without floating point value must be majored (e.g x1000) in order to keep accuracy */
+		if(!fraction)
+			fraction=1;
+		rate = ((((fraction+256) * 1000)/256) * ((((REFCLK / 100)/(1 << clk_sel)) * 8) / 10));
+		QOS_PRINTK (QOS_SHAPER,"shaper_fraction_clk_to_rate fraction=%lu clk_sel=%lu\n", fraction, (unsigned long)1 << clk_sel);
+		QOS_PRINTK (QOS_SHAPER,"(((fraction+256)*1000)/256)=%ld, (((REFCLK/100)/(1<<clk_sel))*8)/10)=%ld\n",(((fraction+256) * 1000)/256),(((REFCLK / 100) / (1 << clk_sel)) * 8)/10);
+	}
+	return (unsigned long)rate;
+}
+
+/**
+ * qos_sch_set_shaper_conf -
+ *
+ *
+ */
+static int qos_sch_set_shaper_conf(struct _QOS_SCH *this_sch, struct _SCH_SHAPER_CONF *conf)
+{
+	int rc = 0;
+	unsigned long bps;
+	unsigned long clk_sel, fraction;
+
+	if((conf->rate) && (conf->state))
+	{
+		bps = conf->rate;
+		get_shaper_rate_and_clock(bps, &clk_sel, &fraction);
+		QOS_PRINTK (QOS_SHAPER,"qos_sch_set_shaper_conf: bps=%ld, clk_sel=%ld, fraction=0x%lx\n", bps, clk_sel, fraction);
+	}
+	else
+	{
+		fraction = 0;
+		clk_sel	= 0;
+	}
+
+	QOS_PRINTK (QOS_SHAPER,"qos_sch_set_shaper_conf SCH %d shaper %d state %d\n", this_sch->index, conf->shaper,conf->state);
+
+	switch(conf->shaper)
+	{
+		case SHAPER_Q7:
+			SCH_SET_Q_CONTROL_CLOCK(this_sch->index, 7, clk_sel);
+			SCH_SET_Q_FRAC_RATE(this_sch->index, 7, fraction);
+			SCH_SET_Q_MAX_CREDIT(this_sch->index, 7, conf->max_credit);
+			SCH_SET_Q_CONTROL_ENABLE(this_sch->index, 7, conf->state);
+			break;
+		case SHAPER_Q6:
+			SCH_SET_Q_CONTROL_CLOCK(this_sch->index, 6, clk_sel);
+			SCH_SET_Q_FRAC_RATE(this_sch->index, 6, fraction);
+			SCH_SET_Q_MAX_CREDIT(this_sch->index, 6, conf->max_credit);
+			SCH_SET_Q_CONTROL_ENABLE(this_sch->index, 6, conf->state);
+			break;
+		case SHAPER_GROUP:
+			SCH_SET_GROUP_CONTROL_CLOCK(this_sch->index, clk_sel);
+			SCH_SET_GROUP_FRAC_RATE(this_sch->index, fraction);
+			SCH_SET_GROUP_MAX_CREDIT(this_sch->index, conf->max_credit);
+			SCH_SET_GROUP_CONTROL_ENABLE(this_sch->index, conf->state);
+			break;
+		case SHAPER_PORT:
+			SCH_SET_PORT_CONTROL_CLOCK(this_sch->index, clk_sel);
+			SCH_SET_PORT_FRAC_RATE(this_sch->index, fraction);
+			SCH_SET_PORT_MAX_CREDIT(this_sch->index, conf->max_credit);
+			SCH_SET_PORT_CONTROL_ENABLE(this_sch->index, conf->state);
+			break;
+		default:
+			QOS_PRINTK (QOS_ERR, "unknown shaper index");
+			rc = -EINVAL;
+			break;
+	}
+	return rc;
+}
+
+/**
+ * qos_sch_get_shaper_conf -
+ *
+ *
+ */
+static int qos_sch_get_shaper_conf(struct _QOS_SCH *this_sch, struct _SCH_SHAPER_CONF *conf)
+{
+	int rc = QOS_OK;
+	unsigned long clk, fraction;
+
+	//printk("qos_sch_get_shaper_conf SCH %d shaper %d\n", this_sch->index, conf->shaper);
+
+	switch(conf->shaper)
+	{
+		case SHAPER_Q7:
+			fraction = SCH_GET_Q_FRAC_RATE(this_sch->index, 7);
+			clk = SCH_GET_Q_CONTROL_CLOCK(this_sch->index, 7);
+			conf->rate = shaper_fraction_clk_to_rate(fraction, clk);
+			conf->max_credit = SCH_GET_Q_MAX_CREDIT(this_sch->index, 7);
+			conf->init_burst = SCH_GET_Q_INIT_BURST(this_sch->index, 7);
+			conf->state = SCH_GET_Q_CONTROL_ENABLE(this_sch->index, 7);
+			break;
+		case SHAPER_Q6:
+			fraction = SCH_GET_Q_FRAC_RATE(this_sch->index, 6);
+			clk = SCH_GET_Q_CONTROL_CLOCK(this_sch->index, 6);
+			conf->rate = shaper_fraction_clk_to_rate(fraction, clk);
+			conf->max_credit = SCH_GET_Q_MAX_CREDIT(this_sch->index, 6);
+			conf->init_burst = SCH_GET_Q_INIT_BURST(this_sch->index, 6);
+			conf->state = SCH_GET_Q_CONTROL_ENABLE(this_sch->index, 6);
+			break;
+		case SHAPER_GROUP:
+			fraction = SCH_GET_GROUP_FRAC_RATE(this_sch->index);
+			clk = SCH_GET_GROUP_CONTROL_CLOCK(this_sch->index);
+			conf->rate = shaper_fraction_clk_to_rate(fraction, clk);
+			conf->max_credit = SCH_GET_GROUP_MAX_CREDIT(this_sch->index);
+			conf->init_burst = SCH_GET_GROUP_INIT_BURST(this_sch->index);
+			conf->state = SCH_GET_GROUP_CONTROL_ENABLE(this_sch->index);
+
+			break;
+		case SHAPER_PORT:
+			fraction = SCH_GET_PORT_FRAC_RATE(this_sch->index);
+			clk = SCH_GET_PORT_CONTROL_CLOCK(this_sch->index);
+			conf->rate = shaper_fraction_clk_to_rate(fraction, clk);
+			conf->max_credit = SCH_GET_PORT_MAX_CREDIT(this_sch->index);
+			conf->init_burst = SCH_GET_PORT_INIT_BURST(this_sch->index);
+			conf->state = SCH_GET_PORT_CONTROL_ENABLE(this_sch->index);
+			break;
+		default:
+			QOS_PRINTK (QOS_ERR, "unknown shaper index");
+			rc = -EINVAL;
+			break;
+	}
+
+	return rc;
+}
+
+
+/**
+ * qos_sch_set_queue_weight -
+ *
+ *
+ */
+static int qos_sch_set_queue_weight(struct _QOS_SCH *this_sch, struct _SCH_QUEUE_WEIGHT *conf)
+{
+	int rc = QOS_OK;
+
+	QOS_PRINTK (QOS_SHAPER,"qos_sch_set_queue_weight SCH %d queue %d\n", this_sch->index, conf->queue);
+
+	if((conf->queue > QUEUE_0) && (conf->queue < QUEUE_6))
+	{
+		SCH_SET_Q_DWRR_WEIGHT(this_sch->index, conf->queue, conf->weight);
+	}
+	else
+	{
+		QOS_PRINTK (QOS_ERR, "can't assigned weight to queue %d", conf->queue);
+		rc = -EINVAL;
+	}
+	return rc;
+}
+
+/**
+ * qos_sch_get_queue_weight -
+ *
+ *
+ */
+static int qos_sch_get_queue_weight(struct _QOS_SCH *this_sch, struct _SCH_QUEUE_WEIGHT *conf)
+{
+	int rc = QOS_OK;
+
+	QOS_PRINTK (QOS_SHAPER,"qos_sch_get_queue_weight SCH %d queue %d\n", this_sch->index, conf->queue);
+
+	if((conf->queue > QUEUE_0) && (conf->queue < QUEUE_6))
+	{
+		conf->weight = SCH_GET_Q_DWRR_WEIGHT(this_sch->index, conf->queue);
+	}
+	else
+	{
+		QOS_PRINTK (QOS_ERR, "can't get weight for queue %d", conf->queue);
+		rc = -EINVAL;
+	}
+
+	return rc;
+}
+/********************************************************************************
+*		COMMON FUNCTIONS						*
+*********************************************************************************/
+
+
+/**
+ * swapbit -
+ *
+ *
+ */
+static unsigned long swapbit(unsigned long x)
+{
+	unsigned long r = x;
+	int s = (sizeof(x) * 8) -1;
+
+	for(x >>= 1; x; x >>= 1)
+	{
+		r <<= 1;
+		r |= x & 1;
+		s--;
+	}
+	r <<= s;
+	return r;
+}
+
+
+/**
+ * reduce_ip_mask -
+ *
+ *
+ */
+static unsigned char reduce_ip_mask(unsigned long ip_mask)
+{
+	unsigned char reduced_ip_mask = 0;
+	unsigned char num_bit = (sizeof(ip_mask) * 8);
+
+	//printk("reduce_ip_mask: from %08lx", ip_mask);
+
+	while(num_bit)
+	{
+		if(!(ip_mask & 1))
+			reduced_ip_mask++;
+		num_bit--; ip_mask >>= 1;
+	}
+
+	if(reduced_ip_mask >= 0x20)
+		reduced_ip_mask = 0x3F;
+
+	//printk(" to %d\n", reduced_ip_mask);
+
+	return reduced_ip_mask;
+}
+
+
+/**
+ * expand_ip_mask -
+ *
+ *
+ */
+static unsigned long expand_ip_mask(unsigned long ip_mask)
+{
+        unsigned long expanded_ip_mask = 0xFFFFFFFF;
+
+	if(ip_mask == 0x3F)
+                ip_mask = 0x20;
+
+	expanded_ip_mask <<= ip_mask;
+
+	//printk("expand_ip_mask: from %d to %08lx\n", ip_mask, expanded_ip_mask);
+
+	return expanded_ip_mask;
+}
+
+
+/**
+ * qos_dump_registers -
+ *
+ *
+ */
+static void qos_dump_registers(unsigned char block)
+{
+	int adm, sch, i;
+
+	if(block & QOS_ADM_BLOCK)
+	{
+		QOS_PRINTK(QOS_REGS_DUMP, "**********C1K HW QoS ADM Registers**********\n");
+
+		for(adm = ADM0; adm <= ADM1; adm++)
+		{
+			QOS_PRINTK(QOS_REGS_DUMP, \
+			"\nAdmittance block %d registers:\n \
+\tADM_STATUS_REG		(%p) = %08lx\n \
+\tADM_PKT_DEQUEUED		(%p) = %08lx\n \
+\tADM_CONF_REG			(%p) = %08lx\n \
+\tADM_CTRL_REG			(%p) = %08lx\n\n \
+\tADM_DEPTH_Q_DEPTH		(%p) = %08lx\n \
+\tADM_DEPTH_AVG_Q_DEPTH	   	(%p) = %08lx\n \
+\tADM_DEPTH_Q_FULL_THRESHOLD 	(%p) = %08lx\n \
+\tADM_DEPTH_Q_DROP_MAX		(%p) = %08lx\n \
+\tADM_DEPTH_Q_DROP_MIN	   	(%p) = %08lx\n \
+\tADM_DEPTH_PAUSE_INT_TIMER	(%p) = %08lx\n \
+\tADM_DEPTH_DECAY_TIMER		(%p) = %08lx\n \
+\tADM_DEPTH_DROP_RANDOM		(%p) = %08lx\n\n \
+\tADM_PORT_SHA_FRAC_RATE	(%p) = %08lx\n \
+\tADM_PORT_SHA_MAX_CREDIT	(%p) = %08lx\n \
+\tADM_PORT_SHA_CREDIT		(%p) = %08lx\n \
+\tADM_PORT_SHA_CTRL		(%p) = %08lx\n \
+\tADM_PORT_SHA_PKT_OVER		(%p) = %08lx\n \
+			",
+			adm,
+			ADM_STATUS_REG(adm), *(ADM_STATUS_REG(adm)),
+			ADM_PKT_DEQUEUED_REG(adm), *ADM_PKT_DEQUEUED_REG(adm),
+			ADM_CONF_REG(adm), *ADM_CONF_REG(adm),
+			ADM_CTRL_REG(adm), *ADM_CTRL_REG(adm),
+			ADM_DEPTH_Q_DEPTH(adm), *ADM_DEPTH_Q_DEPTH(adm),
+			ADM_DEPTH_AVG_Q_DEPTH(adm), *ADM_DEPTH_AVG_Q_DEPTH(adm),
+			ADM_DEPTH_Q_FULL_THRESHOLD(adm), *ADM_DEPTH_Q_FULL_THRESHOLD(adm),
+			ADM_DEPTH_Q_DROP_MAX(adm), *ADM_DEPTH_Q_DROP_MAX(adm),
+			ADM_DEPTH_Q_DROP_MIN(adm), *ADM_DEPTH_Q_DROP_MIN(adm),
+			ADM_DEPTH_PAUSE_INT_TIMER(adm), *ADM_DEPTH_PAUSE_INT_TIMER(adm),
+			ADM_DEPTH_DECAY_TIMER(adm), *ADM_DEPTH_DECAY_TIMER(adm),
+			ADM_DEPTH_DROP_RANDOM(adm), *ADM_DEPTH_DROP_RANDOM(adm),
+			ADM_PORT_SHA_FRAC_RATE(adm), *ADM_PORT_SHA_FRAC_RATE(adm),
+			ADM_PORT_SHA_MAX_CREDIT(adm), *ADM_PORT_SHA_MAX_CREDIT(adm),
+			ADM_PORT_SHA_CREDIT(adm), *ADM_PORT_SHA_CREDIT(adm),
+			ADM_PORT_SHA_CTRL(adm), *ADM_PORT_SHA_CTRL(adm),
+			ADM_PORT_SHA_PKT_OVER(adm), *ADM_PORT_SHA_PKT_OVER(adm)
+			);
+
+			QOS_PRINTK(QOS_REGS_DUMP, "\nAdmittance block %d TCAM:", adm);
+
+			for(i=0; i< ADM_TCAM_MAX_ENTRIES; i++)
+			{
+				if(*ADM_TCAM_VALID(adm, i)) {
+					QOS_PRINTK(QOS_REGS_DUMP, \
+					"\tTCAM %d [%p]\n \
+\tADM_TCAM_VALID        = %08lx\tADM_TCAM_STATE        = %08lx\tADM_TCAM_ETHERTYPE    = %08lx\n \
+\tADM_TCAM_VLANID       = %08lx\tADM_TCAM_PPPOEPROTO   = %08lx\tADM_TCAM_IPTOS        = %08lx\n \
+\tADM_TCAM_IPPROTO      = %08lx\tADM_TCAM_IPSRC        = %08lx\tADM_TCAM_IPDST        = %08lx\n \
+\tADM_TCAM_L4_SPORT     = %08lx\tADM_TCAM_L4_DPORT     = %08lx\tADM_TCAM_MCAST        = %08lx\n \
+\tADM_TCAM_MASK_REG0    = %08lx\tADM_TCAM_MASK_REG1    = %08lx\tADM_TCAM_MASK_REG2    = %08lx\n \
+					",
+					i,
+					ADM_GET_TCAM_ENTRY(adm, i),
+					*ADM_TCAM_VALID(adm, i),
+					*ADM_TCAM_STATE(adm, i),
+					*ADM_TCAM_ETHERTYPE(adm,i),
+					*ADM_TCAM_VLANID(adm,i),
+					*ADM_TCAM_PPPOEPROTO(adm,i),
+					*ADM_TCAM_IPTOS(adm,i),
+					*ADM_TCAM_IPPROTO(adm,i),
+					*ADM_TCAM_IPSRC(adm,i),
+					*ADM_TCAM_IPDST(adm,i),
+					*ADM_TCAM_L4_SPORT(adm,i),
+					*ADM_TCAM_L4_DPORT(adm,i),
+					*ADM_TCAM_MCAST_BCAST(adm,i),
+					*ADM_TCAM_MASK_REG0(adm,i),
+					*ADM_TCAM_MASK_REG1(adm,i),
+					*ADM_TCAM_MASK_REG2(adm,i)
+					);
+				} else {
+					QOS_PRINTK(QOS_REGS_DUMP, "\tTCAM %d [%p]\n\tnot used\n ", i, ADM_GET_TCAM_ENTRY(adm, i));
+				}
+			}
+
+			QOS_PRINTK(QOS_REGS_DUMP, \
+			"\nAdmittance block %d LRU Registers:\n \
+\tADM_LRU_ENTRY       (%p) = %08lx\n \
+\tADM_LRU_ENTRY_MASK  (%p) = %08lx\n \
+\tADM_LRU_HIT_CNT_0_3 (%p) = %08lx\n \
+\tADM_LRU_HIT_CNT_4_7 (%p) = %08lx\n \
+\tADM_LRU_TIMER       (%p) = %08lx\n \
+\tADM_LRU_PERIOD      (%p) = %08lx\n \
+\tADM_LRU_CTRL        (%p) = %08lx\n \
+			",
+			adm,
+			ADM_LRU_ENTRY(adm), *ADM_LRU_ENTRY(adm),
+			ADM_LRU_ENTRY_MASK(adm), *ADM_LRU_ENTRY_MASK(adm),
+			ADM_LRU_HIT_CNT_0_3(adm),*ADM_LRU_HIT_CNT_0_3(adm),
+			ADM_LRU_HIT_CNT_4_7(adm),*ADM_LRU_HIT_CNT_4_7(adm),
+			ADM_LRU_TIMER(adm), *ADM_LRU_TIMER(adm),
+			ADM_LRU_PERIOD(adm), *ADM_LRU_PERIOD(adm),
+			ADM_LRU_CTRL(adm), *ADM_LRU_CTRL(adm)
+			);
+		}
+	}
+
+	if(block & QOS_SCH_BLOCK)
+	{
+		QOS_PRINTK(QOS_REGS_DUMP, "**********C1K HW QoS SCH Registers**********");
+
+		for(sch = SCH0; sch <= SCH1; sch++)
+		{
+			QOS_PRINTK(QOS_REGS_DUMP, \
+			"\nScheduler block %d registers:\n \
+\tSCH_STATUS_REG    (%p) = %08lx\n \
+\tSCH_CTRL_REG      (%p) = %08lx\n \
+\tSCH_PKT_QUEUED    (%p) = %08lx\n \
+\tSCH_PORT_BYTE_CNT (%p) = %08lx\n \
+\tSCH_PORT_PKT_CNT  (%p) = %08lx\n \
+\tSCH_PKT_OVERHEAD  (%p) = %08lx\n \
+			",
+			sch,
+			SCH_STATUS_REG(sch), *SCH_STATUS_REG(sch),
+			SCH_CTRL_REG(sch), *SCH_CTRL_REG(sch),
+			SCH_PKT_QUEUED(sch), *SCH_PKT_QUEUED(sch),
+			SCH_PORT_BYTE_CNT(sch), *SCH_PORT_BYTE_CNT(sch),
+			SCH_PORT_PKT_CNT(sch), *SCH_PORT_PKT_CNT(sch),
+			SCH_PKT_OVERHEAD(sch), *SCH_PKT_OVERHEAD(sch)
+			);
+		}
+	}
+}
+
+
+/**
+ * qos_init -
+ *
+ *
+ */
+static int qos_init(void)
+{
+	int i;
+
+	QOS_PRINTK(QOS_INFO, "qos_init()\n");
+
+	memset(_adm, 0, sizeof(struct _QOS_ADM) * 2);
+	memset(_sch, 0, sizeof(struct _QOS_SCH) * 2);
+
+	/* needed to retrieve block index through IOCTLs calls */
+	_adm[0].index = ADM0;
+	_adm[1].index = ADM1;
+	_sch[0].index = SCH0;
+	_sch[1].index = SCH1;
+
+	/* initialize TCAM entries table */
+	for(i = 0; i < ADM_TCAM_MAX_ENTRIES; i++)
+	{
+		_adm[0].cam_sw_table[i].slot = ADM_TCAM_FREE;
+		_adm[1].cam_sw_table[i].slot = ADM_TCAM_FREE;
+		_adm[0].cam_hw_table[i].slot = ADM_TCAM_FREE;
+		_adm[1].cam_hw_table[i].slot = ADM_TCAM_FREE;
+	}
+
+#ifdef C1K_REG_SIM
+#define FAKE_EMAC_SZ	0x8180
+	if ((FAKE_EMAC0_BASE = kmalloc (FAKE_EMAC_SZ, GFP_KERNEL)) == NULL)
+	{
+		QOS_PRINTK(QOS_ERR, "cannot allocate memory for fake registers()\n");
+		return -1;
+	}
+	if ((FAKE_EMAC1_BASE = kmalloc (FAKE_EMAC_SZ, GFP_KERNEL)) == NULL)
+	{
+		QOS_PRINTK(QOS_ERR, "cannot allocate memory for fake registers()\n");
+		return -1;
+	}
+	QOS_PRINTK(QOS_INIT, "FAKE_GEMAC0_BASE @ %p FAKE_GEMAC1_BASE @ %p\n", FAKE_EMAC0_BASE, FAKE_EMAC1_BASE);
+	memset(FAKE_EMAC0_BASE, 0, FAKE_EMAC_SZ);
+	memset(FAKE_EMAC1_BASE, 0, FAKE_EMAC_SZ);
+#endif
+
+	/* Map HW counters */
+	_adm[ADM0].counter = (struct _ADM_COUNTER *)(adm_statis(ADM0));
+	_adm[ADM1].counter = (struct _ADM_COUNTER *)(adm_statis(ADM1));
+
+	/* The default configuration of the Ethernet admittance block is to run in discard mode, with no
+	proportional dropping, dropping only when the receive queue is full. The shaper is disabled, and the
+	classifier is disabled, resulting in all traffic being treated as Unmanaged.*/
+#if 0
+	/* For debufg purpose only, let's admittance accepted errored packets */
+	ADM_SET_CONF_ADMITERR(0);
+#endif
+
+	return 0;
+}
+
+/**
+ *  qos_proc_info -
+ *
+ *
+ */
+int qos_proc_info_adm(char* page, char** start, off_t off, int count, int *eof, void* data)
+{
+	struct _QOS_ADM *this_adm = (struct _QOS_ADM *)data;
+	struct _ADM_LRU_STATS lru_stats;
+	int adm = this_adm->index;
+	int tcam, adm_state, len = 0;
+
+	len += sprintf(page+len, "\n");
+	len += sprintf(page+len, "Comcerto 1000 HW QoS Stats for ADM %d:\n\n", adm);
+
+	ADM_SET_CTRL_SNAPSHOT_ENABLE(adm);
+
+	adm_state = ADM_GET_CTRL_ADM_ENABLE(adm);
+	if(adm_state)
+		len += sprintf(page+len, "mode %s\n", adm_mode_string[ADM_GET_CONF_MODE(adm)]); /* DISCARD_MODE or FLOWCTRL_MODE */
+	else
+		len += sprintf(page+len, "mode %s\n", adm_mode_string[2]);/* DISABLED_MODE */
+	len += sprintf(page+len, "port bytes %lu\n", _adm[adm].counter->port_byte);
+	len += sprintf(page+len, "port packets %lu\n", _adm[adm].counter->port_packet);
+	len += sprintf(page+len, "reserved bytes %lu\n", _adm[adm].counter->reserved_byte);
+	len += sprintf(page+len, "reserved packets %lu\n", _adm[adm].counter->reserved_packet);
+	len += sprintf(page+len, "managed bytes %lu\n", _adm[adm].counter->managed_byte);
+	len += sprintf(page+len, "managed packets %lu\n", _adm[adm].counter->managed_packet);
+	len += sprintf(page+len, "dropped %lu\n", _adm[adm].counter->packet_dropped);
+	len += sprintf(page+len, "dropped upon errors %lu\n", _adm[adm].counter->packet_dropped_error);
+	len += sprintf(page+len, "dropped denied %lu\n", _adm[adm].counter->packet_dropped_denied);
+	len += sprintf(page+len, "dropped policer0 %lu\n", _adm[adm].counter->packet_dropped_policer0);
+	len += sprintf(page+len, "dropped policer1 %lu\n", _adm[adm].counter->packet_dropped_policer1);
+	len += sprintf(page+len, "dropped qfull %lu\n", _adm[adm].counter->packet_dropped_qfull);
+	len += sprintf(page+len, "dropped shaped %lu\n", _adm[adm].counter->packet_dropped_shaper);
+	len += sprintf(page+len, "dropped managed %lu\n", _adm[adm].counter->packet_dropped_managed);
+	len += sprintf(page+len, "dropped unmanaged %lu\n", _adm[adm].counter->packet_dropped_unmanaged);
+
+	qos_adm_get_lru_stats(&_adm[adm], &lru_stats);
+	len += sprintf(page+len, "LRU TCAM entry: %u (%u)\n", lru_stats.entry, lru_stats.count);
+	len += sprintf(page+len, "TCAM hit count:\n");
+	for(tcam = 0; tcam < ADM_MAX_TCAM_ENTRIES; tcam++)
+		len += sprintf(page+len, "[%u]=%u ", tcam, lru_stats.hit_count[tcam]);
+
+	len += sprintf(page+len, "\n");
+
+	return len;
+}
+
+/**
+ * qos_proc_info_sch -
+ *
+ */
+int qos_proc_info_sch(char* page, char** start, off_t off, int count, int *eof, void* data)
+{
+	struct _QOS_SCH *this_sch = (struct _QOS_SCH *)data;
+	int sch = this_sch->index;
+	int q;
+	struct _SCH_COUNTER sch_stats;
+	int len = 0;
+
+	len += sprintf(page+len, "\n");
+	len += sprintf(page+len, "Comcerto 1000 HW QoS Stats for SCH %d:\n\n", sch);
+
+	memset(&sch_stats, 0, sizeof(struct _SCH_COUNTER));
+	SCH_SET_SNAPSHOT(sch);
+	qos_sch_get_counters(&_sch[sch], &sch_stats);
+
+	len += sprintf(page+len, "port bytes %lu\n", sch_stats.port_byte);
+	len += sprintf(page+len, "port packets %lu\n", sch_stats.port_packet);
+
+	for(q = 0; q < SCH_MAX_QUEUES; q++)
+	{
+		len += sprintf(page+len, "\nqueue %u: ", q);
+		len += sprintf(page+len, "bytes %lu ", sch_stats.q_byte[q]);
+		len += sprintf(page+len, "packets %lu ", sch_stats.q_packet[q]);
+		len += sprintf(page+len, "idle time %lu ", sch_stats.q_idle[q]);
+	}
+
+	len += sprintf(page+len, "\n");
+
+	return len;
+}
+
+struct file_operations qos_fops = {
+	.owner =	THIS_MODULE,
+	.open =		qos_open,
+	.release =	qos_release,
+	.unlocked_ioctl =	qos_ioctl,
+};
+
+
+/**
+ * qos_module_init -
+ *
+ */
+static int __init qos_module_init(void)
+{
+	int rc = 0;
+
+	QOS_PRINTK(QOS_INIT, "Initializing Comcerto 1000 QoS Driver version %s\n", QOSCOM_VERSION);
+
+	if(qos_init() < 0)
+	{
+		QOS_PRINTK(QOS_ERR, "QOS: qos init failed\n");
+
+		return -1;
+	}
+
+	rc = register_chrdev (QOS_DEVICE_MAJOR_NUM, QOS_DRIVER_NAME, &qos_fops);
+	if (rc)
+	{
+		QOS_PRINTK (QOS_ERR, "unable to register QoS char device (%d %s)", QOS_DEVICE_MAJOR_NUM, QOS_DRIVER_NAME);
+		return -1;
+	}
+
+	/* Create /proc/qos entry */
+	if(proc_mkdir ("qos", 0))
+	{
+		if(proc_mkdir ("qos/wan", 0))
+		{
+			create_proc_read_entry("qos/wan/adm", 0, 0, qos_proc_info_adm, (void*)&_adm[ADM0]); //ADM0
+			create_proc_read_entry("qos/wan/sch", 0, 0, qos_proc_info_sch, (void*)&_sch[SCH0]);//SCH0
+		}
+		if(proc_mkdir ("qos/lan", 0))
+		{
+			create_proc_read_entry("qos/lan/adm", 0, 0, qos_proc_info_adm, (void*)&_adm[ADM1]); //ADM1
+			create_proc_read_entry("qos/lan/sch", 0, 0, qos_proc_info_sch, (void*)&_sch[SCH1]);//SCH1
+		}
+	}
+
+	/*Init qoscom mutex*/
+	mutex_init(&qoscom_lock);
+
+	return 0;
+}
+
+
+/**
+ * qos_module_exit -
+ *
+ */
+static void __exit qos_module_exit(void)
+{
+	char s[30];
+	QOS_PRINTK(QOS_INIT, "Unloading Comcerto 1000 QoS Driver\n");
+
+#ifdef C1K_REG_SIM
+	kfree(FAKE_EMAC0_BASE);
+	kfree(FAKE_EMAC1_BASE);
+#endif
+
+	/* Remove /proc/qos entry */
+	snprintf(s, 30, "qos/wan/adm");
+	remove_proc_entry(s, NULL);
+
+	snprintf(s, 30, "qos/wan/sch");
+	remove_proc_entry(s, NULL);
+
+	snprintf(s, 30, "qos/wan");
+	remove_proc_entry(s, NULL);
+
+	snprintf(s, 30, "qos/lan/adm");
+	remove_proc_entry(s, NULL);
+
+	snprintf(s, 30, "qos/lan/sch");
+	remove_proc_entry(s, NULL);
+
+	snprintf(s, 30, "qos/lan");
+	remove_proc_entry(s, NULL);
+
+	remove_proc_entry("qos", NULL);
+
+	unregister_chrdev (QOS_DEVICE_MAJOR_NUM, QOS_DRIVER_NAME);
+}
+
+module_init(qos_module_init);
+module_exit(qos_module_exit);
diff --git a/drivers/net/transcede/qoscom.h b/drivers/net/transcede/qoscom.h
new file mode 100644
index 0000000..aae2e2e
--- /dev/null
+++ b/drivers/net/transcede/qoscom.h
@@ -0,0 +1,95 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef _QOSDRIVER_H
+#define _QOSDRIVER_H
+
+#include "m84xxx_adm.h"
+#include "m84xxx_sch.h"
+#include "qos_fops.h"
+
+
+/*
+* Debug macros
+*/
+
+#define QOS_PRINT	1
+#define QOS_ERR		1
+#define QOS_INFO	0
+#define QOS_INIT	0
+#define QOS_STAT	0
+#define QOS_REGS_DUMP	1
+#define QOS_IOCTL	0
+#define QOS_TCAM_DEBUG	0
+#define QOS_POLICER	0
+#define QOS_SHAPER	0
+
+#ifdef QOS_PRINT
+#define QOS_PRINTK(type, info, args...) do {if(type) printk("\n" info, ## args);} while(0);
+#else
+#define QOS_PRINTK(type, info, args...) do {} while(0);
+#endif
+
+
+#define QOS_ADM_BLOCK	1
+#define QOS_SCH_BLOCK	2
+
+struct _ADM_TCAM_DESC
+{
+	unsigned char slot;
+	struct _ADM_TCAM_ENTRY entry;
+};
+
+
+
+struct _QOS_ADM
+{
+	/* To differentiate between the two admittance blocks */
+	unsigned char	index;
+
+	/* Ingress counters */
+	struct _ADM_COUNTER	*counter;
+
+	/* CAM entries table*/
+	//unsigned char	cam_slot_table[ADM_TCAM_MAX_ENTRIES];
+	//unsigned char	cam_handle_table[ADM_TCAM_MAX_ENTRIES];
+	struct _ADM_TCAM_DESC cam_hw_table[ADM_TCAM_MAX_ENTRIES];
+	struct _ADM_TCAM_DESC cam_sw_table[ADM_TCAM_MAX_ENTRIES];
+
+	/* LRU counters */
+	struct _ADM_LRU_STATS	*lru_stats;
+};
+
+
+
+
+struct _QOS_SCH
+{
+	/* To differentiate between the two scheduler blocks */
+	unsigned int index;
+};
+
+int qos_proc_info_adm(char* page, char** start, off_t off, int count, int *eof, void* data);
+int qos_proc_info_sch(char* page, char** start, off_t off, int count, int *eof, void* data);
+
+
+#endif /* _QOSDRIVER_H */
diff --git a/drivers/net/transcede/t2200_eth.c b/drivers/net/transcede/t2200_eth.c
new file mode 100644
index 0000000..0d51869
--- /dev/null
+++ b/drivers/net/transcede/t2200_eth.c
@@ -0,0 +1,4780 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+  *
+  * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+  *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+  *
+  * You should have received a copy of the GNU General Public License
+  * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+  */
+
+#include <linux/kernel.h>
+#include <linux/kthread.h>
+#include <linux/interrupt.h>
+#include <linux/dma-mapping.h>
+#include <linux/dmapool.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/mii.h>
+#include <linux/phy.h>
+#include <linux/platform_device.h>
+
+#include <net/ip.h>
+#include <net/sock.h>
+
+#include <asm/io.h>
+#include <asm/irq.h>
+#include <asm/delay.h>
+#include <asm/mach/irq.h>
+
+#include <mach/hardware.h>
+#include <mach/memory.h>
+
+#include <linux/timer.h>
+#include <linux/hrtimer.h>
+
+#if defined(CONFIG_INET_IPSEC_OFFLOAD) || defined(CONFIG_INET6_IPSEC_OFFLOAD)
+#include <net/xfrm.h>
+#endif
+
+#include <linux/errqueue.h>
+
+#ifdef CONFIG_AMP_STACK
+#include <mach/mmu_protect.h>
+#endif  /* CONFIG_AMP_STACK */
+
+#include <net/xfrm.h>
+
+#include "t2200_eth.h"
+#include "transcede_mii.h"
+#include "t3300_reth.h"
+
+extern struct transcede_eth_platform_data transcede_gem0_pdata;
+extern struct transcede_eth_platform_data transcede_gem1_pdata;
+extern struct ethtool_ops              c4k_ethtool_ops;
+
+/* Local defines */
+
+#define GEM_TX_CLEAN_THRESHOLD		5
+#define GEM_DONOT_UNMAP_TX_BUF
+#define GEMAC_MAX_TX_PROC_TIME		(1000*250) // time in timer ticks to process one TX DMA descriptor, for SYSBUS-140Mhz it's also OK
+#define GEM_HANG_PRN_NUM		50
+//#define GEM_FORCED_DROP_TX		10000
+
+#define RX_PKT_BUF_SZ		1600						/**< Fixed RX HW frame size, should have enough space for max Ethernet frame + some extra */
+#define JUMBO_FRAME_SIZE	1600
+#define RX_SKB_ALLOC_SZ		(RX_PKT_BUF_SZ + RX_SKB_HEAD_EXTRA_SZ)		/**< Fixed skb allocation size to have all desired room spaces */
+
+#define RX_BUF_MAP_SZ		RX_PKT_BUF_SZ					/**< A size used for DMA mapping of RX frames */
+//FUTURE TEST: #define RX_BUFFER_MAP_SIZE 1518 /* Max RX frame size instead of buffer size */
+#define RX_BUF_UNMAP_SZ		RX_PKT_BUF_SZ					/**< A size used for DMA unmapping of RX frames */
+
+#define RX_POLL_BUDGET		64
+
+static PSKB_RECYCLER pSkbRecycler = NULL;
+
+#if 0
+/** Define to force use of sending single TX descriptor at a time instead of using
+ *  multiple descriptors and TX descriptor ring
+ *  NOTE: This define temporarily added for performance improvements to allow for
+ *  compiling for test purposes for single shot send for comparison purposes.
+ */
+#define GEM_TX_ONE_DESCRIPTOR_AT_A_TIME
+#endif
+
+#if 0
+#define GEM_ETHDRV_TEMP_DEBUG
+#endif
+
+#if 1
+#define GEM_RX_CHECKSUM_OFFLOAD_ENABLED
+#endif
+
+#if 1
+#define GEM_TX_CHECKSUM_OFFLOAD_ENABLED
+#endif
+
+#if 0
+#define BUG_69620_WORKAROUND
+#endif
+
+#if 0
+/** Define for fixing Atheros PHY delay for T2200/T3300 boards that have this PHY and
+ * for some reason, do not set this up properly in u-boot (which is where it should
+ * be done).
+ */
+#define ATHEROS_PHY_CHECK_AND_UPDATE_RGMII_DELAY
+#endif
+
+#if 1
+/** Define for assuming if the Ethernet driver cannot attach/connect/enable/etc. the
+ * the Ethrenet MDIO bus and PHY device, then the GEM etherent port is instead
+ * connected to a switch, so no MDIO operations are possible (or practical/standard)
+ * for PHY status, link status, speed, duplex checking, etc.
+ */
+#define ASSUME_SWITCH_IF_PHY_CHECK_FAILED
+#endif
+
+#define inc_rx_idx(idxname) do { idxname = (idxname+1) & (priv->RxRingSize -1); } while (0)
+
+/** @brief dummy TX descriptor for to disable additional TX queues */
+static const struct tTXdesc tx_ring_fake  __attribute__ ((aligned (8)))
+    = {
+         0x00000000,
+         { 0xC0000000 }, // USED and WRAP bits set
+      } ;
+
+/** @brief dummy TX descriptor for to disable additional TX queues */
+static const struct tRXdesc rx_ring_fake  __attribute__ ((aligned (8)))
+    = { 0x00000003, // USED and WRAP bits set
+        0x00000000,
+      } ;
+
+/* Local Function prototypes and external function references */
+
+/* Local function prototypes */
+static void t2200_eth_release_buffers(struct net_device *dev);
+static int  t2200_eth_init_buffers   (struct net_device *dev);
+irqreturn_t t2200_eth_rx_interrupt   (int irq, void *dev_id);
+static void t2200_adjust_link        (struct net_device *dev);
+int         t2200_set_tx_csum        (struct net_device *dev, uint32_t data);
+static u32  t2200_ring_to_phys       (struct net_device *dev, volatile u32* va);
+int t2200_eth_free_tx_packets(struct net_device *dev, uint lock);
+
+#define RECYCLER_RX_ON		(1 << 0)
+#define RECYCLER_TX_ON		(1 << 1)
+static u32 recycler_ctrl = RECYCLER_RX_ON | RECYCLER_TX_ON;  // by default <recycler is ON>
+
+struct net_device * t2200_dev_ptr[2] = {NULL,NULL};
+static struct hrtimer t2200_hrtimer[2];
+static uint t2200_eth_rx_latency = 0;
+static uint GEM_IRQ_RX_DONE_FLAG = GEM_IRQ_RX_DONE;
+static phy_interface_t t2200_get_interface(struct net_device *dev);
+static uint tx_sched_time[2][MAX_TX_DESC_NT];
+
+#ifdef CONFIG_IPSEC_DMA_MAP_HACK_RX
+uint8_t ipsec_dma_map_hack_rx_enabled = 1;
+EXPORT_SYMBOL(ipsec_dma_map_hack_rx_enabled);
+#endif
+
+static int t2200_eth_thread_on = 0; // the threads are disabled for ethernet interface by default
+static atomic_t t2200_thread_wakeup_flag[2];
+static wait_queue_head_t t2200_thread_wait_q[2];
+struct task_struct* t2200_thread[2] = {NULL,NULL};
+static int t2200_gem_create_thread(struct net_device * pdev, int amp_cpu_id);
+static int t2200_gem_thread (void*p);
+int t2200_eth_poll(struct napi_struct *napi, int budget);
+
+int (*gemac_fp_rx_cb) (uint gid, struct sk_buff * pSkb) = NULL;
+EXPORT_SYMBOL(gemac_fp_rx_cb);
+
+int (*gemac_fp_xfrm_rx_cb) (uint cpuid, struct sk_buff * pSkb) = NULL;
+EXPORT_SYMBOL(gemac_fp_xfrm_rx_cb);
+
+struct xfrm_state* elp_ipsec_xfrm_state = NULL;
+EXPORT_SYMBOL(elp_ipsec_xfrm_state);
+
+int (*elpxfrm_fp_input_cb)(struct xfrm_state* x, struct sk_buff *skb) = NULL;
+EXPORT_SYMBOL(elpxfrm_fp_input_cb);
+
+// KSD API, to route encrypted by IPSec packet directly to GEMAC driver
+// without usage of native Linux network stack and in this way to save some MIPS
+int (*elpxfrm_fp_output_cb)(struct xfrm_state* x, struct sk_buff *skb);
+EXPORT_SYMBOL(elpxfrm_fp_output_cb);
+
+int t2200_reinit_bufs_for_dev(struct net_device * pdev, int amp_cpu_id)
+{
+    int                   rtf;        /**< RX to fill  current index  */
+    int                   rtc;        /**< RX to clean current index */
+    struct eth_c4k_priv * priv = netdev_priv(pdev);
+    struct sk_buff *      skb, *skb_free;        /**< Current socket buffer pointer */
+    int refill_num = 0;
+    u32 		  addr_u32;
+    u32                   rx_data;    /* Current receive data       */
+    struct tRXdesc *      ThisRXdesc; /**< Current Receive descriptor pointer */
+    int                   rc = 0;
+
+    printk ("\n- t2200/t3300 skb reinitialization (dev=%p, dev-id:%d)..\n",pdev, priv->id);
+
+    gem_disable_rx(&priv->gemdev);
+
+    rtc = priv->RxtocleanIndex;
+    rtf = priv->RxtofillIndex;
+
+    do
+    {
+	/* Get pointer to current descriptor:  */
+	ThisRXdesc = priv->RxBase + rtc;
+
+	// the descriptor should not be used by the GEM
+	// mainly the packet should not be received
+
+	if ((skb_free = priv->RxSkbRing[rtc]) != NULL && (ThisRXdesc->rx_data & GEM_RX_DESCR_OWNERSHIP)==0)
+	{
+		skb = sd_dev_alloc_skb(RX_SKB_ALLOC_SZ);
+		if (unlikely(skb == NULL))
+		{
+			printk ("\n\nThe system was not able to allocate new SKB\n\n");
+			return -1;
+		}
+        if (recycler_ctrl & RECYCLER_RX_ON)
+            skb->opt_flags |= SKB_FLAGS_RX_PACKET;
+
+	    rx_data = ThisRXdesc->rx_data;
+
+	    dma_unmap_single(&pdev->dev,
+		             GEM_RX_DESCR_BUF_ADDR(rx_data),
+			     RX_BUF_UNMAP_SZ,
+		             DMA_FROM_DEVICE
+		            );
+
+	    dev_kfree_skb(skb_free);       // no one will touch this anymore, just free it.
+
+	    priv->RxSkbRing[rtc] = skb;
+	    /*
+	     * Reserve packet headroom in buffer to:
+	     * - align IP part to 32 bit boundary
+	     * - have some extra header space to avoid reallocations
+	     */
+	    skb_reserve(skb, RX_SKB_RESERVE_SZ);
+
+	    /* Setup the DMA memory for this socket buffer */
+	    /* Ensure that any data held in the cache is appropriately discarded or written back. */
+	    addr_u32 = dma_map_single(&pdev->dev,
+				      skb->data - NET_IP_ALIGN,
+				      RX_BUF_MAP_SZ,
+				      DMA_FROM_DEVICE);
+	    addr_u32 += NET_IP_ALIGN;
+	    addr_u32 &= 0xFFFFFFFC;
+
+	    if(unlikely(ThisRXdesc == (priv->RxBase + priv->RxRingSize - 1)))
+	    {
+		ThisRXdesc->rx_data = (GEM_RX_DESCR_WRAP | addr_u32);
+	    }
+	    else
+	    {
+		ThisRXdesc->rx_data = addr_u32;
+	    }
+
+	    refill_num ++;
+	}
+
+	inc_rx_idx(rtc);
+    }while (rtc != rtf);
+
+    if (t2200_eth_thread_on != 0 && t2200_eth_rx_latency != 0)
+    {
+       rc = t2200_gem_create_thread(pdev, amp_cpu_id);
+    }
+
+    gem_enable_rx(&priv->gemdev);
+
+    if (t2200_eth_thread_on != 0 && t2200_eth_rx_latency != 0)
+       printk ("The number of reinitialized SKB is %d, t2200-thread-rc=%d\n", refill_num, rc);
+    else
+       printk ("The number of reinitialized SKB is %d\n", refill_num);
+    return rc;
+}
+
+
+#ifdef CONFIG_GLOBAL_POLLING
+static void t2200_hrtimer_callback(unsigned long id)
+#else  /* !CONFIG_GLOBAL_POLLING */
+static enum hrtimer_restart t2200_hrtimer_callback( struct hrtimer *timer )
+#endif /* CONFIG_GLOBAL_POLLING */
+{
+    struct eth_c4k_priv * priv;
+    struct tRXdesc * ThisRXdesc; /**< Current Receive descriptor pointer */
+    struct napi_struct *  napi;
+
+#ifndef CONFIG_GLOBAL_POLLING
+    uint id = 0;
+    if (timer == &t2200_hrtimer[1])
+    {
+	id = 1;
+    }
+#endif /* !CONFIG_GLOBAL_POLLING */
+
+    priv = netdev_priv(t2200_dev_ptr[id]);
+    napi = &priv->napi;
+
+    if (!t3300_reth_is_enabled())
+    {
+    // here we need to check do we need to schedule pooling or to
+    // just restart the time to check RX packets later
+    // (if at this moment we do not have any packets)
+
+    ThisRXdesc = priv->RxBase + priv->RxtocleanIndex;
+
+    // if no any packets
+    if ((ThisRXdesc->rx_data & GEM_RX_DESCR_OWNERSHIP)==0)
+    {
+#ifdef CONFIG_GLOBAL_POLLING
+        return;  // return, do not schedule eth poll
+        /*
+            note: on highly loaded system with high-priority tasks delay
+            between shceduling poll and actual poll execution may exceed 200 us.
+            To reduce RX packet latency poll may be scheduled even if there are
+            no packets in RX queue at this moment
+        */
+#else  /* !CONFIG_GLOBAL_POLLING */
+	ktime_t ktime;
+	ktime = ktime_set( 0, t2200_eth_rx_latency * 1000);
+	hrtimer_forward(timer, ktime_get(), ktime);
+	return HRTIMER_RESTART;
+#endif /* CONFIG_GLOBAL_POLLING */
+    }
+    }
+    // at this moment we detected that we have something to receive
+    // so we need to schedule the polling process
+    // the timer will be rescheduled later after polling
+
+    if (t2200_thread[priv->id] != NULL)
+    {
+        if (likely(atomic_read(&t2200_thread_wakeup_flag[priv->id]) == 0))
+	{
+	    atomic_inc(&t2200_thread_wakeup_flag[priv->id]);
+	    wake_up_interruptible(&t2200_thread_wait_q[priv->id]);
+	}
+    }
+    else if (likely(napi_schedule_prep(napi))) {
+        __napi_schedule(napi);
+    }
+
+#ifndef CONFIG_GLOBAL_POLLING
+    return HRTIMER_NORESTART;
+#endif /* !CONFIG_GLOBAL_POLLING */
+}
+
+int t2200_init_timer(struct net_device * pdev)
+{
+    struct eth_c4k_priv * priv = netdev_priv(pdev);
+
+    if (t2200_eth_rx_latency == 0)
+	return 0;
+
+    hrtimer_init( &t2200_hrtimer[priv->id], CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+
+#ifndef CONFIG_GLOBAL_POLLING
+    t2200_hrtimer[priv->id].function = &t2200_hrtimer_callback;
+#endif
+    return 0;
+}
+
+int t2200_start_timer(struct net_device * pdev)
+{
+    struct eth_c4k_priv * priv = netdev_priv(pdev);
+#ifndef CONFIG_GLOBAL_POLLING
+    ktime_t ktime;
+#endif /* !CONFIG_GLOBAL_POLLING */
+
+    if (t2200_eth_rx_latency == 0)
+	return 0;
+
+#ifdef CONFIG_GLOBAL_POLLING
+    priv->periodic_task = periodic_task_add(0, t2200_eth_rx_latency, t2200_hrtimer_callback, priv->id, "ethRx");
+#else  /* !CONFIG_GLOBAL_POLLING */
+    ktime = ktime_set( 0, t2200_eth_rx_latency * 1000);
+    hrtimer_start( &t2200_hrtimer[priv->id], ktime, HRTIMER_MODE_REL);
+#endif /* CONFIG_GLOBAL_POLLING */
+
+    return 0;
+}
+
+int t2200_stop_timer(struct net_device * pdev)
+{
+    struct eth_c4k_priv * priv = netdev_priv(pdev);
+    int err = 0;
+
+    if (t2200_eth_rx_latency == 0)
+	return 0;
+
+#ifdef CONFIG_GLOBAL_POLLING
+    if (priv->periodic_task) {
+	    err = periodic_task_remove(priv->periodic_task);
+	    priv->periodic_task = NULL;
+    }
+#else  /* !CONFIG_GLOBAL_POLLING */
+    err = hrtimer_cancel( &t2200_hrtimer[priv->id]);
+#endif /* CONFIG_GLOBAL_POLLING */
+
+    return err;
+}
+
+static int t2200_gem_create_thread(struct net_device * pdev, int amp_cpu_id)
+{
+	char name [16];
+	struct eth_c4k_priv * priv = netdev_priv(pdev);
+
+	init_waitqueue_head(&t2200_thread_wait_q[priv->id]);
+	atomic_set(&t2200_thread_wakeup_flag[priv->id], 0);
+
+	sprintf (name, "T2200_eth%d_thr", priv->id);
+
+	#ifdef CONFIG_AMP_STACK
+	if (amp_cpu_id != -1)
+	{
+		printk(KERN_INFO "T2200-THREAD: AMP stack is used (CPU-%d)\n", amp_cpu_id);
+		t2200_thread[priv->id] = kthread_amp_create(amp_cpu_id, t2200_gem_thread, pdev, name);
+	}
+	else
+	#endif
+	{
+		printk(KERN_INFO "T2200-THREAD: SMP stack is used\n");
+		t2200_thread[priv->id] = kthread_create(t2200_gem_thread, pdev, name);
+	}
+	wake_up_process(t2200_thread[priv->id]);
+	return 0;
+}
+
+static int t2200_gem_thread (void* p)
+{
+	int rc = 0;
+        struct net_device * pdev = (struct net_device *)p;
+	struct eth_c4k_priv * priv = netdev_priv(pdev);
+	printk (KERN_INFO "T2200 GEM thread is started for Eth-%d\n", priv->id);
+	while (1)
+	{
+		atomic_set(&t2200_thread_wakeup_flag[priv->id], 0);
+		rc = wait_event_interruptible(t2200_thread_wait_q[priv->id], atomic_read(&t2200_thread_wakeup_flag[priv->id]));
+		rc = t2200_eth_poll(&priv->napi, RX_POLL_BUDGET);
+		//printk (KERN_INFO "Eth-Thread tick (done:%d)\n", rc);
+	}
+
+	do_exit(rc);
+}
+
+#ifdef GEM_ETHDRV_TEMP_DEBUG
+#define MAX_SKB_DATA_DUMP 32
+static void print_skb_info(struct sk_buff * skb)
+{
+	unsigned char * pTemp;
+	int             i;
+
+	if (skb == NULL)
+		return;
+
+	printk(KERN_INFO "sk_buff:%p head:%p data:%p tail:%p end:%p len:%u truesize:%u\n",
+		(void *)virt_to_phys(skb),
+		(void *)virt_to_phys(skb->head),
+		(void *)virt_to_phys(skb->data),
+		(void *)virt_to_phys(skb->tail),
+		(void *)virt_to_phys(skb->end),
+		skb->len,
+		skb->truesize
+		);
+	if ((skb->data == NULL) || (skb->len == 0))
+		return;
+
+	pTemp = (unsigned char *)skb->data;
+
+	for (i=0; ((i< skb->len) && (i < MAX_SKB_DATA_DUMP)) ; i++,pTemp++)
+	{
+		printk("%02x ",*pTemp);
+		if ((i & 31) == 31)
+		{
+			printk("\n");
+		}
+	}
+	if ((i & 31) != 0)
+	{
+		printk("\n");
+	}
+}
+#endif
+
+
+//External Function removed (not currently used):
+//extern irqreturn_t t2200_gemac_interrupt(int irq, void *dev_id);
+
+static void sock_rmem_free2(struct sk_buff *skb)
+{
+    struct sock *sk = skb->sk;
+
+    atomic_sub(skb->truesize, &sk->sk_rmem_alloc);
+}
+
+
+int sock_queue_err_skbtri(struct sock *sk, struct sk_buff *skb)
+{
+    int len = skb->len;
+    unsigned int aaa = 0;
+    aaa = atomic_read(&sk->sk_rmem_alloc);
+
+    if ( (atomic_read(&sk->sk_rmem_alloc) + skb->truesize) >= (unsigned int)sk->sk_rcvbuf)
+    {
+        return -ENOMEM;
+    }
+
+    skb_orphan(skb);
+    skb->sk = sk;
+    skb->destructor = sock_rmem_free2;
+    atomic_add(skb->truesize, &sk->sk_rmem_alloc);
+    /* before exiting rcu section, make sure dst is refcounted */
+    skb_dst_force(skb);
+    skb_queue_tail(&sk->sk_error_queue, skb);
+
+    if (!sock_flag(sk, SOCK_DEAD))
+            sk->sk_data_ready(sk, len);
+
+    return 0;
+}
+EXPORT_SYMBOL(sock_queue_err_skb);
+
+
+int skb_tstamp_txtri(struct sk_buff *orig_skb, struct skb_shared_hwtstamps *hwtstamps)
+{
+    struct sock *sk = orig_skb->sk;
+    struct sock_exterr_skb *serr;
+    struct sk_buff *skb;
+    int err = 1;
+
+    if (!sk)
+        return -1;
+
+    skb = skb_clone(orig_skb, GFP_ATOMIC);
+    if (!skb)
+        return -2;
+
+    if (hwtstamps)
+    {
+        *skb_hwtstamps(skb) = *hwtstamps;
+    } else
+    {
+        /*
+         * no hardware time stamps available,
+         * so keep the shared tx_flags and only
+         * store software time stamp
+         */
+        skb->tstamp = ktime_get_real();
+    }
+
+    serr = SKB_EXT_ERR(skb);
+    memset(serr, 0, sizeof(*serr));
+    serr->ee.ee_errno = ENOMSG;
+    serr->ee.ee_origin = SO_EE_ORIGIN_TIMESTAMPING;
+
+    err = sock_queue_err_skbtri((unsigned int)sk, (unsigned int)skb);
+
+    if (err)
+    {
+        kfree_skb(skb);
+        return -3;
+    }
+
+    return 0;
+}
+
+
+void ntg_config (u32 enable,  u32 pllnum, u32 inputfreq, u32 outputfreq)
+{
+    u32  DivisorInteger;
+    u32  DivisorFraction;
+    u32  DivisorRemainder;
+    unsigned long long  temp64;
+    void *  ntg_base_vaddr;
+
+    /*64k bytes i/o register map*/
+    ntg_base_vaddr = (void *)ioremap_nocache(DEVICE_CLK_RST_CTRL_BASE, 1024*64);
+
+    if (ntg_base_vaddr == 0)
+        return;
+
+    //
+    // Make sure PLL is setup for input clocking to TSU NTG
+    //
+    // Select PLL1(ARM CLK, 1GHz as rouce,  and enable clock branch
+    REG32(ntg_base_vaddr + TSUNTG_REF_CLK_CNTRL_OFFSET) = (pllnum << 1) | 1;
+
+    // Set bypass mode (write only bit) and divide by 1 (information as set to bypass mode)
+    // (0xF4CF0144), in this mode, clock out == clock in
+    REG32(ntg_base_vaddr + TSUNTG_REF_CLK_DIV_CNTRL_OFFSET) = 0x81;
+
+    //
+    // Get rate of PLL in Hz
+    //
+    //
+    // Release NTG from reset (note: NTG may have alread been released from previous call to GemNtgConfig,
+    // if so, this will do no harm)
+    //
+    // TSU NTG release from reset
+    REG32(ntg_base_vaddr + TSUNTG_RESET_OFFSET_OFFSET) = 0x0;
+
+     DivisorInteger   = inputfreq / outputfreq;
+     DivisorRemainder = inputfreq % outputfreq;
+
+     if (DivisorRemainder == 0)
+     {
+          DivisorFraction = 0;
+     }
+     else
+     {
+        temp64 =   DivisorRemainder;
+        temp64 =  temp64 << 32;
+
+        div_u64(temp64, outputfreq);
+        // Modulus is not zero, so we must also calculate the fraction to set
+        DivisorFraction = (u32)  (temp64);
+     }
+
+    if (enable)
+    {
+        //
+        // Setup GEM NTG for TSU and/or dejitter circuit clocking
+        // Suspend register updates
+        REG32(ntg_base_vaddr + GEMNTG_CONTROL_OFFSET)  = 0
+#if 1
+            // | NTG_CONTROL_HALF_ENABLE                // (1<<0)
+               | NTG_CONTROL_PROGRAMMABLE_FRAME_WIDTH   // (0<<1)
+            // | NTG_CONTROL_50_PERCENT_PULSE_FRAME_OUT // (1<<1)
+            // | NTG_CONTROL_HALF_CLOCK_BIT_OUT_PULSE   // (2<<1)
+               | NTG_CONTROL_FRAME_COUNT_RESET          // (1<<3)
+            // | NTG_CONTROL_CLOCK_BIT_INVERTED         // (1<<4)
+               | NTG_CONTROL_CLOCK_BIT_OUT              // (0<<5)
+            // | NTG_CONTROL_CLOCK_BIT_IN               // (1<<5)
+               | NTG_CONTROL_FRAME_PULSE_OUT            // (0<<6)
+            // | NTG_CONTROL_FRAME_PULSE_IN             // (1<<6)
+            // | NTG_CONTROL_REGISTER_UPDATE            // (1<<7)
+               ;
+#endif
+        // Enable GEM NTG clock:
+        // suspend register updates
+        //REG32(GEMNTG_CONTROL     &= ~NTG_CONTROL_REGISTER_UPDATE; // 0x80
+
+        //change clk source need reset first
+        //*(VUINT16 *)TSUNTG_RESET |= 1;
+
+        //enable and use GEM NTG default (see .h files) as source clock
+        //*(VUINT16 *)TSUNTG_REF_CLK_CNTRL) =  DEFAULT_GEM_NTG_PLL | NTG_GLB_CLOCK_EN;
+
+        //Set Divisor and bypass (default 0x82)
+        //*(VUINT16 *)TSUNTG_REF_CLK_DIV_CNTRL) = NTG_CLK_DIV_BYPASS | // Bypass enabled
+        //                                     NTG_MIN_DIVISOR;     // Minimum divisor (ignored due to bypass)
+        //Clock out of reset
+        //*(VUINT16 *)TSUNTG_RESET &= ~(1);
+
+        REG32(ntg_base_vaddr +  GEMNTG_FREQ_SET_INT_OFFSET) = DivisorInteger; // Calculated integer    divisor value
+        REG32(ntg_base_vaddr +GEMNTG_FREQ_SET_FRA_OFFSET) = DivisorFraction;// Calculated fractional divisor value
+
+        REG32(ntg_base_vaddr + GEMNTG_PHASE_ADJ_FREQ_INT_OFFSET) = 0; // Disable
+        REG32(ntg_base_vaddr + GEMNTG_PHASE_ADJ_FREQ_FRA_OFFSET) = 0; // Disable
+        REG32(ntg_base_vaddr + GEMNTG_PHASE_ADJ_DUR_OFFSET) = 0; // Disable
+        REG32(ntg_base_vaddr + GEMNTG_PHASE_ADJ_START_OFFSET) = 0; // Disable
+        REG16(ntg_base_vaddr +  GEMNTG_PULSE_WIDTH_OFFSET       ) = 0; // One clock width
+        REG16(ntg_base_vaddr +  GEMNTG_FRAME_LENGTH_OFFSET) = 0; // No further division by "frame"
+
+        // Set mode (0x80; SIM: 0x80)
+        //REG32(GEMNTG_CONTROL) = NTG_CONTROL_REGISTER_UPDATE; // |
+                                 // NTG_CONTROL_HALF_ENABLE;
+        REG32(ntg_base_vaddr + GEMNTG_CONTROL_OFFSET)  = 0
+            // | NTG_CONTROL_HALF_ENABLE                // (1<<0)
+               | NTG_CONTROL_PROGRAMMABLE_FRAME_WIDTH   // (0<<1)
+            // | NTG_CONTROL_50_PERCENT_PULSE_FRAME_OUT // (1<<1)
+            // | NTG_CONTROL_HALF_CLOCK_BIT_OUT_PULSE   // (2<<1)
+               | NTG_CONTROL_FRAME_COUNT_RESET          // (1<<3)
+            // | NTG_CONTROL_CLOCK_BIT_INVERTED         // (1<<4)
+               | NTG_CONTROL_CLOCK_BIT_OUT              // (0<<5)
+            // | NTG_CONTROL_CLOCK_BIT_IN               // (1<<5)
+               | NTG_CONTROL_FRAME_PULSE_OUT            // (0<<6)
+            // | NTG_CONTROL_FRAME_PULSE_IN             // (1<<6)
+               | NTG_CONTROL_REGISTER_UPDATE            // (1<<7)  Resume register updates
+               ;
+
+       // NOW WE ARE DONE WITH NTG setup
+
+    }
+    else
+    {
+        // suspend register updates
+        REG32(ntg_base_vaddr + GEMNTG_CONTROL_OFFSET)  &= ~NTG_CONTROL_REGISTER_UPDATE;
+
+        // turn off output clock and frame pulse
+         REG32(ntg_base_vaddr + GEMNTG_CONTROL_OFFSET)   =  NTG_CONTROL_REGISTER_UPDATE  |  NTG_CONTROL_CLOCK_BIT_IN     | NTG_CONTROL_FRAME_PULSE_IN;
+
+    // put NTG into  reset mode;
+         REG32(ntg_base_vaddr + TSUNTG_RESET_OFFSET_OFFSET)  = 1;
+    }
+
+
+     iounmap (ntg_base_vaddr);
+
+    return;
+}
+
+/* This is the tx work task to put the real hardware time stamp into kernel, and then user application */
+
+
+static void t2200_tx_hwtstamp_work(struct work_struct *work)
+{
+    struct eth_c4k_priv *adapter = container_of(work, struct eth_c4k_priv,
+                                                tx_hwtstamp_work);
+    struct skb_shared_hwtstamps shhwtstamps;
+    static int cc =0;
+    int ccq;
+    int icv;
+    int flags;
+
+    if (unlikely(!spin_trylock_irqsave(&adapter->txlock,flags)))
+    {
+        /* Collision - tell upper layer to requeue */
+        if (netif_msg_tx_err(adapter))
+            printk(KERN_DEBUG "t2200_tx_hwtstamp_work: %s: TX collision\n", __func__);
+        return NETDEV_TX_LOCKED;
+    }
+
+    if (!adapter->tx_hwtstamp_skb)
+    {
+        /*    schedule_work(&adapter->tx_hwtstamp_work); */	 /* reschedule to check later */
+        spin_unlock_irqrestore(&adapter->txlock, flags);
+        return;
+    }
+
+    /* we wait for another scheduled work till we can be sure no ptp sent out,
+    so we can take the register value and give back to socket call via skb,
+    this is not 100% accurate, but should be close enough*/
+
+    adapter->hwts_work_counter ++;
+
+    /* we are scheduled two times and see no PTP going out, using driver captured timestamp*/
+    if (adapter->hwts_work_counter > 1)
+    {
+         shhwtstamps.hwtstamp  = ktime_set(adapter->tx_reg_time.tv_sec,adapter->tx_reg_time.tv_nsec);
+
+        for (icv =0; icv <10; icv ++)
+        {
+            ccq = skb_tstamp_txtri(adapter->tx_hwtstamp_skb, &shhwtstamps);
+
+            if (ccq != 0)
+                continue;
+            else
+                break;
+        }
+
+        dev_kfree_skb_any(adapter->tx_hwtstamp_skb);
+        adapter->tx_hwtstamp_skb = NULL;
+        adapter->hwts_work_counter = 0;
+        cc ++;
+     }
+     else
+     {
+        /* reschedule to check later */
+        schedule_work(&adapter->tx_hwtstamp_work);
+     }
+
+     spin_unlock_irqrestore(&adapter->txlock, flags);
+
+}
+
+
+/**
+ * t2k_hwtstamp_ioctl - control hardware time stamping
+ * @netdev: network interface device structure
+ * @ifreq: interface request
+ *
+ * Outgoing time stamping is still questionable if we really need that, will be added later
+ *
+ *
+ * Incoming time stamping has to be configured via the hardware filters.
+ * The T2K Cadence GEMAC only has one enable bit to enable all PTP related hardware timestamp capability
+ * So in reality only HWTSTAMP_FILTER_SOME can be supported.
+
+ * specified. Matching the kind of event packet is not supported, with the
+ * exception of "all V2 events regardless of level 2 or 4".
+ **/
+static int t2k_hwtstamp_ioctl(struct net_device *netdev, struct ifreq *ifr)
+{
+
+    struct eth_c4k_priv *priv_adapter = netdev_priv(netdev);
+    struct hwtstamp_config config;
+    int flags;
+
+    if (copy_from_user(&config, ifr->ifr_data, sizeof(config)))
+        return -EFAULT;
+
+    if (config.flags)          /* must be zero for future purpose*/
+        return -EINVAL;
+
+    priv_adapter->hwts_config.flags = 0;
+
+    switch (config.tx_type)
+    {
+        case HWTSTAMP_TX_OFF:
+            priv_adapter->hwts_config.tx_type =  HWTSTAMP_TX_OFF;
+
+            if (unlikely(!spin_trylock_irqsave(&priv_adapter->txlock,flags)))
+            {
+                /* Collision - tell upper layer to requeue */
+                if (netif_msg_tx_err(priv_adapter))
+                    printk(KERN_DEBUG "t2k_hwtstamp_ioctl: %s: TX collision\n", __func__);
+                return NETDEV_TX_LOCKED;
+            }
+
+            /*  cancel_work_sync(&priv_adapter->tx_hwtstamp_work); */
+            if (priv_adapter->tx_hwtstamp_skb)
+            {
+                dev_kfree_skb_any(priv_adapter->tx_hwtstamp_skb);
+                priv_adapter->tx_hwtstamp_skb = NULL;
+            }
+
+            spin_unlock_irqrestore(&priv_adapter->txlock, flags);
+
+            gem_mask_irq(&(priv_adapter->gemdev),
+                (GEM_INT_TSU_TX_PTP_DLY_REQ|GEM_INT_TSU_TX_PTP_SYNC_FRM|GEM_INT_TSU_TX_PTP_PDLY_REQ|GEM_INT_TSU_TX_PTP_PDLY_RESP));
+        break;
+
+
+        case HWTSTAMP_TX_ON:
+            priv_adapter->hwts_config.tx_type =  HWTSTAMP_TX_ON;
+
+            /* enable  sending ptp packets interrupt here*/
+            gem_enable_irq(&(priv_adapter->gemdev),
+            (GEM_INT_TSU_TX_PTP_DLY_REQ |GEM_INT_TSU_TX_PTP_SYNC_FRM|GEM_INT_TSU_TX_PTP_PDLY_REQ|GEM_INT_TSU_TX_PTP_PDLY_RESP));
+
+            /* enable task for tx sampling */
+        break;
+
+
+        default:
+            return -ERANGE;
+    }
+
+    switch (config.rx_filter)
+    {
+        case HWTSTAMP_FILTER_NONE:
+            priv_adapter->hwts_config.rx_filter = HWTSTAMP_FILTER_NONE;
+
+            /* disble GEM timstamp */
+            REG32( priv_adapter->gemdev.registers + GEM_STATS_BASE) &= ~GEM_STORE_RX_TIMESTAMP_TO_MEMORY;
+            /*   REG32( priv_adapter->gemdev.registers + GEM_STATS_BASE) |=  ~GEM_NETCFG_FCS_REMOVE;  */   /* THIS IS NOT NEEDED SINCE it was set as ZERO*/
+        break;
+
+/* all below filters become a single value HWTSAMP_FILTER_SOME,
+   since T2200 GMAC has no capability to filter out last dedicated one type of packets */
+        case HWTSTAMP_FILTER_PTP_V1_L4_SYNC:
+        case HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ:
+        case HWTSTAMP_FILTER_PTP_V2_L2_SYNC:
+        case HWTSTAMP_FILTER_PTP_V2_L2_DELAY_REQ:
+        case HWTSTAMP_FILTER_PTP_V2_L4_SYNC:
+        case HWTSTAMP_FILTER_PTP_V2_SYNC:
+        case HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ:
+        case HWTSTAMP_FILTER_PTP_V2_DELAY_REQ:
+        case HWTSTAMP_FILTER_PTP_V2_L4_EVENT:
+        case HWTSTAMP_FILTER_PTP_V2_L2_EVENT:
+        case HWTSTAMP_FILTER_PTP_V2_EVENT:
+        case HWTSTAMP_FILTER_PTP_V1_L4_EVENT:
+        case HWTSTAMP_FILTER_SOME:
+            config.rx_filter = HWTSTAMP_FILTER_SOME;
+            priv_adapter->hwts_config.rx_filter = HWTSTAMP_FILTER_SOME;
+            REG32( priv_adapter->gemdev.registers + GEM_STATS_BASE) |=  GEM_STORE_RX_TIMESTAMP_TO_MEMORY;
+            REG32( priv_adapter->gemdev.registers + GEM_STATS_BASE) &=  ~GEM_NETCFG_FCS_REMOVE;
+            break;
+/*below is to stamp all packets, which is not supported by the hardware */
+
+        case HWTSTAMP_FILTER_ALL:
+            priv_adapter->hwts_config.rx_filter = HWTSTAMP_FILTER_ALL;
+            REG32( priv_adapter->gemdev.registers + GEM_STATS_BASE) |=  GEM_STORE_RX_TIMESTAMP_TO_MEMORY;
+            REG32( priv_adapter->gemdev.registers + GEM_STATS_BASE) &=  ~GEM_NETCFG_FCS_REMOVE;
+        break;
+
+        default:
+            return -ERANGE;
+    }
+
+    return copy_to_user(ifr->ifr_data,
+                        &config,
+                        sizeof(config)) ? -EFAULT : 0;
+}
+
+static int t2k_ptp_ioctl  (struct net_device *netdev, struct ifreq *ifr, int cmd)
+{
+
+    struct eth_c4k_priv *priv_adapter = netdev_priv(netdev);
+    struct tsusetupprofile config;
+    struct ntgsetupprofile ntgconfig;
+    u64    setreg;
+    u32 config_t1, config_t2;
+    struct timespec   ptptime;
+    phy_interface_t interface;
+
+
+    switch (cmd)
+    {
+        case PTP_SETUP_GEMNTG:
+            if (copy_from_user(&ntgconfig, ifr->ifr_data, sizeof(ntgconfig)))
+                return -EFAULT;
+            ntg_config (ntgconfig.enable, ntgconfig.pllnum, ntgconfig.inputfreq, ntgconfig.outputfreq);
+        break;
+
+        case PTP_SETUP_GEMTSU:
+            if (copy_from_user(&config, ifr->ifr_data, sizeof(config)))
+            {
+                return -EFAULT;
+            }
+
+            //now we begin to setup the registers for tsu and ntg
+            //disable timer update.
+            REG32( priv_adapter->gemdev.registers + GEM_1588_TIMER_INCREMENT) = 0;
+
+            config_t1 =  config.tsuclocksource &  0x00000003; //last two bits are for clock selection.
+            config_t1  = config_t1<<  13;
+            config_t2 = config.outputpps  &  0x1;
+            config_t2  = config_t1 << 12; //enable pps output bit in register of gpio control.
+            config_t1  |=  config_t2;
+            // clear timer value
+            REG32( priv_adapter->gemdev.registers + GEM_1588_TIMER_NANOSECONDS) = 0x0;
+            //clear timer value
+            REG32( priv_adapter->gemdev.registers + GEM_1588_TIMER_SECONDS) = 0x13336f;
+           if (config_t2 == 0)   //enable pps output
+           {
+                //set nano seconds of the comparison register to be  0.5 seconds, the middle of register value;
+                //fhis is half a second 1953125 * 256 = 0.5s;
+                REG32( priv_adapter->gemdev.registers + GEM_1588_TIMER_COMPARE_NANOSECONDS) = 0x1dcd65;
+                //setit to be one second
+                REG32( priv_adapter->gemdev.registers + GEM_1588_TIMER_COMPARE_SECONDS) = 0x1000001;
+
+                /*enable the second pps interrupt */
+                gem_enable_irq(&(priv_adapter->gemdev), GEM_INT_TSU_SECONDS_INCREMENT);
+           }
+           else
+           {
+                //this is half a second 1953125 * 256 = 0.5s;
+                REG32( priv_adapter->gemdev.registers + GEM_1588_TIMER_COMPARE_NANOSECONDS) = 0x1dcd65;
+                //setit to a big value so no chance to get triggered one second
+                REG32( priv_adapter->gemdev.registers + GEM_1588_TIMER_COMPARE_SECONDS) = 0x77777777;
+                /*enable the  pps interrupt */
+                gem_mask_irq(&(priv_adapter->gemdev), GEM_INT_TSU_SECONDS_INCREMENT);
+            }
+
+            REG32(TRANSCEDE_MISC_PIN_SELECT) |=  config_t1;
+
+
+            interface = t2200_get_interface(netdev);
+
+            // this is SGMII interface, so it is GEM0, and we choose the GEM0 device.
+            if (interface ==  PHY_INTERFACE_MODE_SGMII)
+            {
+                config_t1 = config.masterslave & 0x1;
+                config_t1 |= (0x3 << 1);     //this is to gem0 TSU clock increment mode control, always put to normal mode instead of extra or less clock cycles.
+                config_t1 &= ~(0x1 << 8);    //bit8 to set pp1s use gem0 bit8 ==0;
+
+                REG32(TRANSCEDE_TSU_CONFIG_REG) |=  config_t1;
+            }
+            else  if ( interface == PHY_INTERFACE_MODE_RGMII)   //this RGMII interface, so it is GEM1
+            {
+                config_t1 = config.masterslave &  0x1;
+                //left move 4 bits to get the GEM1 bits
+                config_t1  = config_t1 << 4;
+                //this is to gem0 TSU clock increment mode control, always put to normal mode instead of extra or less clock cycles.
+                config_t1 |= (0x3 << 5);
+                //bit8 to set gps use gem1 bit8 ==1;
+                config_t1 |= (0x1 << 8);
+
+                REG32(TRANSCEDE_TSU_CONFIG_REG) |=  config_t1;
+            }
+
+             /* now it is time to setup the real TSU timers registers to the correct mode */
+            /*enable interrupt for every second, so we can update the timer comparison register second value for every second increment */
+            //enable the  timer update now, so it will begin ticking
+            REG32( priv_adapter->gemdev.registers + GEM_1588_TIMER_INCREMENT) = config.clockcycletime;
+
+            /* set timer comparison value as in the middle 	*/
+        break;
+
+        case PTP_GET_TIME:
+            // Get time.
+            ptptime.tv_sec  =  gem_get_tsu_sec(&priv_adapter->gemdev);
+            ptptime.tv_nsec = gem_get_tsu_nsec(&priv_adapter->gemdev);
+            config_t1 =  gem_get_tsu_sec(&priv_adapter->gemdev);
+
+            /* assume first read is good, then decide if we need second read */
+
+            if (ptptime.tv_sec != config_t1)
+            {
+                // Second counter wrapped during read
+                // pick correct one based on nanoseconds read
+                //
+                if (ptptime.tv_nsec < 500000000UL)
+                {
+                    // Get second read
+                    ptptime.tv_sec = config_t1;
+                }
+            }
+
+            if (copy_to_user(ifr->ifr_data, &ptptime, sizeof(struct timespec)) )
+            {
+                return -EFAULT;
+            }
+        break;
+
+       case PTP_SET_TIME:
+            if (copy_from_user(&ptptime, ifr->ifr_data, sizeof(ptptime)))
+
+            // Set nanoseconds to zero first so we don't get a counter wrap
+               gem_set_tsu_nsec(&priv_adapter->gemdev , 0UL);
+            // Now set seconds first, then nanoseconds so we don't get a counter wrap
+            // while setting time value
+
+             gem_set_tsu_sec(&priv_adapter->gemdev, ptptime.tv_sec);
+             gem_set_tsu_nsec(&priv_adapter->gemdev , ptptime.tv_nsec);
+        break;
+
+        case PTP_SHOW_REGISTERS:
+            {
+            printk("GEM_1588_TIMER_SECONDS %08x\n",    REG32( priv_adapter->gemdev.registers + GEM_1588_TIMER_SECONDS) );
+            printk("GEM_1588_TIMER_NANOSECONDS %08x\n", REG32( priv_adapter->gemdev.registers + GEM_1588_TIMER_NANOSECONDS ));
+            printk("GEM_1588_TIMER_ADJUST %08x\n",REG32( priv_adapter->gemdev.registers +GEM_1588_TIMER_ADJUST ));
+            printk("GEM_1588_TIMER_INCREMENT %08x\n",REG32( priv_adapter->gemdev.registers + GEM_1588_TIMER_INCREMENT ));
+            printk("GEM_1588_PTP_EVENT_TX_SECONDS %08x\n",REG32( priv_adapter->gemdev.registers + GEM_1588_PTP_EVENT_TX_SECONDS ));
+            printk("GEM_1588_PTP_EVENT_TX_NANOSECONDS %08x\n",REG32( priv_adapter->gemdev.registers + GEM_1588_PTP_EVENT_TX_NANOSECONDS ));
+            printk("GEM_1588_PTP_EVENT_RX_SECONDS %08x\n",REG32( priv_adapter->gemdev.registers + GEM_1588_PTP_EVENT_RX_SECONDS ));
+            printk("GEM_1588_PTP_EVENT_RX_NANOSECONDS %08x\n",REG32( priv_adapter->gemdev.registers + GEM_1588_PTP_EVENT_RX_NANOSECONDS ));
+            printk("GEM_1588_PTP_PEER_EVENT_TX_SECONDS %08x\n",REG32( priv_adapter->gemdev.registers + GEM_1588_PTP_PEER_EVENT_TX_SECONDS ));
+            printk("GEM_1588_PTP_PEER_EVENT_TX_NANOSECONDS %08x\n",REG32( priv_adapter->gemdev.registers + GEM_1588_PTP_PEER_EVENT_TX_NANOSECONDS ));
+            printk("GEM_1588_PTP_PEER_EVENT_RX_SECONDS %08x\n",REG32( priv_adapter->gemdev.registers + GEM_1588_PTP_PEER_EVENT_RX_SECONDS ));
+            printk("GEM_1588_PTP_PEER_EVENT_RX_NANOSECONDS %08x\n",REG32( priv_adapter->gemdev.registers + GEM_1588_PTP_PEER_EVENT_RX_NANOSECONDS ));
+            printk("GEM_1588_TSYNC_STROBE_NANOSECONDS %08x\n",REG32( priv_adapter->gemdev.registers + GEM_1588_TSYNC_STROBE_NANOSECONDS ));
+            printk("GEM_1588_INT_ENABLE %08x\n",REG32( priv_adapter->gemdev.registers + GEM_1588_INT_ENABLE ));
+            printk("GEM_1588_INT_DISABLE %08x\n",REG32( priv_adapter->gemdev.registers + GEM_1588_INT_DISABLE ));
+            printk("GEM_1588_INT_MASK %08x\n",REG32( priv_adapter->gemdev.registers + GEM_1588_INT_MASK ));
+            printk("GEM_1588_INT_status %08x\n",REG32( priv_adapter->gemdev.registers + GEM_IRQ_STATUS ));
+            printk("GEM_1588_INT_MASK %08x\n",REG32( priv_adapter->gemdev.registers + GEM_IRQ_MASK ));
+            }
+        break;
+
+        case  PTP_SET_REGISTER:
+            //this is for debugging purpose, so we will take the input physical address from user, and write that registers. ;
+            //We need an address and a value, address is an offset from the GEM top.   value is 32 bits.
+
+            if (copy_from_user(&setreg, ifr->ifr_data, sizeof(setreg)))
+            {
+                return -EFAULT;
+            }
+
+            config_t1 =  (u32) (setreg >> 32);
+            config_t2 =  (u32) (setreg &&  0xffffffffUL);
+
+            gem_set_register(&priv_adapter->gemdev, config_t1, config_t2);
+        break;
+
+        case PTP_GET_REGISTER:
+
+            //this is for debugging purpose, so we will take the input from customer, and read that register.
+            if (copy_from_user(&config_t1, ifr->ifr_data, sizeof(config_t1)))
+            {
+                return -EFAULT;
+            }
+
+            config_t2 =   gem_get_register(&priv_adapter->gemdev, config_t1);
+
+            if (copy_to_user(ifr->ifr_data, &config_t2, sizeof(config_t2)) )
+            {
+               return -EFAULT;
+            }
+        break;
+
+        default:
+            return -ERANGE;
+    }
+
+    return 0 ;
+}
+
+
+/**
+ * c4k_eth_ioctl - c4k ioctl interface (only PHY functions supported)
+ */
+static int t2200_eth_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)
+{
+    struct eth_c4k_priv *priv = netdev_priv(dev);
+
+
+    if (!priv->dev)
+        return -ENODEV;
+
+    switch (cmd)
+    {
+        case SIOCSHWTSTAMP:
+            return t2k_hwtstamp_ioctl(priv->dev, ifr);
+
+        case SIOCGMIIPHY:
+        case SIOCGMIIREG:
+        case SIOCSMIIREG:
+            return phy_mii_ioctl(priv->phydev, ifr, cmd);
+
+        case PTP_GET_COUNT:
+        case PTP_SETUP_GEMNTG:
+        case PTP_SETUP_GEMTSU:
+        case PTP_SHOW_REGISTERS:
+        case PTP_SET_REGISTER:
+        case PTP_SET_TIME:
+        case PTP_GET_TIME:
+        case PTP_GET_REGISTER:
+            return t2k_ptp_ioctl(priv->dev, ifr, cmd);
+
+        default:
+            return -EOPNOTSUPP;
+    }
+
+}
+
+/**
+ * Function to return the current value of interface
+ * mode the GEM device is running based on current
+ * MII mode read from priv->einfo->mii_config
+ *
+ * @return
+ * Returns PHY_INTERFACE_MODE_SGMII if GEM 0 or PHY_INTERFACE_MODE_RGMII if GEM 1
+ */
+static phy_interface_t t2200_get_interface(struct net_device *dev)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+
+	/* T2200 device supports only SGMII on GEM 0 and RGMII on GEM 1 */
+//	return PHY_INTERFACE_MODE_RGMII;
+	printk (KERN_INFO "%s: %s\n", __func__, dev->name);
+	if (priv->id == 0)
+		return PHY_INTERFACE_MODE_SGMII;
+	else
+		return PHY_INTERFACE_MODE_RGMII;
+}
+
+/**
+ * Function to initialize driver's PHY state, and attach to the PHY.
+ *
+ * @return
+ * Returns 0 on success, returns PTR_ERR(phydev) if
+ * unable to connect to the PHY device, returns -ENODEV if
+ * able to connect, but PHY register read sanity test fails.
+ */
+static int t2200_phy_start(struct net_device *dev)
+{
+	struct eth_c4k_priv *  priv            = netdev_priv(dev);
+	uint                   gigabit_support = SUPPORTED_1000baseT_Full;
+		// DD priv->einfo->phy_flags & GEMAC_PHY_1000 ?
+		// DD (SUPPORTED_1000baseT_Full | SUPPORTED_1000baseT_Half): 0;
+	struct phy_device *    phydev;
+	char                   phy_id[MII_BUS_ID_SIZE + 3];
+	phy_interface_t        interface;
+	int                    i;
+
+	/* Initialize Link up/down, speed and duplex data to starting
+	 * values
+	 */
+	priv->oldlink   = 0;
+	priv->oldspeed  = 0;
+	priv->oldduplex = -1;
+
+	/* Setup BUS ID and PHY address into PHY_ID_FMT field for
+	 * connecting to the PHY device
+	 */
+	snprintf(phy_id,
+	         sizeof(phy_id),
+	         PHY_ID_FMT,
+	         priv->einfo->bus_id,
+	         priv->einfo->phy_id
+	        );
+	printk(KERN_INFO "PHY start: %s\n", phy_id);
+
+	/* Get interface type from kernel configuration (SGMII, RGMII, etc.) */
+	interface = t2200_get_interface(dev);
+
+#if 0
+	printk (KERN_INFO "%s %s: MDIO BUS:PHY %s Interface type:%u\n",
+	        dev->name,
+	        __func__,
+		phy_id,
+	        interface
+	       );
+#endif
+
+	/* Attempt to connect and setup PHY driver for selected MDIO bus ID
+	 * and PHY address
+	 */
+	phydev = phy_connect(dev,                // Net device structure pointer
+	                     phy_id,             // PHY name (combination of BUS ID and PHY address on the MDIO bus)
+	                     &t2200_adjust_link, // Call back function called when PHY status changes
+	                     0,                  // PHY specific flags (if any)
+	                     interface           // Interface type (T2200 GEM0 is SGMII, GEM1 is RGMII)
+	                    );
+
+	/* Check if connect/init successful or not */
+	if (IS_ERR(phydev))
+	{
+#ifdef ASSUME_SWITCH_IF_PHY_CHECK_FAILED
+		/* PHY check failed, but we could be on a board where this GEM controller
+		 * is connected to a switch instead of PHY.  If so, ignore error and
+		 * continue processing (i.e. allow GEM device driver to come up anyway)
+		 */
+		printk(KERN_INFO "%s %s: Error %d Could not attach to MDIO BUS/PHY %s, assuming connected to Switch\n",
+		       dev->name,
+		       __func__,
+		       (int)PTR_ERR(phydev),
+		       phy_id
+		      );
+		priv->phydev = NULL;
+#else
+		printk(KERN_ERR  "%s %s: Could not attach to MDIO BUS/PHY %s\n", dev->name, __func__, phy_id);
+#endif
+		/* Return error for case where unable to connect to the PHY */
+		return PTR_ERR(phydev);
+	}
+	else
+	{
+		/* Found in testing MSPD PC73300 board that phy_connect may still
+		 * return non-error (OK) status for a PHY that is not present (unsure why).
+		 * So before using/trusting that the PHY is really there,
+		 * make sure by doing a PHY read of register 0x03 (OUI register).
+		 * If register value is 0x0000 or 0xFFFF, then a valid Ethernet
+		 * PHY is not really present
+		 */
+		i = phy_read(phydev,3);
+		printk(KERN_INFO "%s %s: phy_connect MDIO BUS/PHY %s OK, Register 3 value read: 0x%04X\n",
+		       dev->name,
+		       __func__,
+		      phy_id,
+		      (unsigned int)i
+		     );
+		if ((i == 0x0000) || (i == 0xFFFF))
+		{
+			/* Read of OUI register gave invalid data, assume PHY not present
+			 * and setup for fixed 1 GBPS mode.
+			 * Disconnect PHY device (won't use it anymore).
+			 * This will mean that we will also stay in fixed 1 GBPS speed
+			 * mode as all PHY commands (if any) will be ignored by this driver
+			 */
+			phy_disconnect(phydev);
+			printk(KERN_ERR  "%s %s: MDIO BUS/PHY %s invalid register 3 data, assuming no PHY present\n",
+			       dev->name,
+			       __func__,
+			       phy_id
+			      );
+			priv->phydev = NULL;
+			return -ENODEV;
+		}
+	}
+	/* phy_connect OK and OUI register 0x03 read not 0 or all F's,
+	 * assume PHY is present and OK to use
+	 */
+	/* Remove any features not supported by the GEM controller */
+	phydev->supported  |= (TRANSCEDE_SUPPORTED | gigabit_support);
+	phydev->advertising = phydev->supported;
+
+	/* Setup pointer to PHY device for usage later (NULL if PHY connect failed) */
+	priv->phydev = phydev;
+
+	return 0;
+}
+
+/** Function to stop the PHY device for this Ethernet device */
+static void t2200_phy_stop(struct net_device *dev)
+{
+
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+
+	printk (KERN_INFO "%s: %s\n", __func__, dev->name);
+
+	if(priv->phydev != NULL) {
+		phy_disconnect(priv->phydev);
+		priv->phydev = NULL;
+	}
+}
+
+//HL. 08/31. check carefully the following sequence.
+/**
+ * Function to bring the controller up and running
+ *
+ * @see t2200_eth_init_buffers
+ * @see phy_start
+ * @see gem_enable_rx
+ * @see gem_enable_tx
+ * @see t2200_ring_to_phys
+ */
+int t2200_eth_start(struct net_device *dev)
+{
+	struct eth_c4k_priv * priv = netdev_priv(dev);
+	unsigned int i;
+
+//AKB	if (netif_msg_drv(priv))
+		printk (KERN_INFO "%s: %s\n", __func__, dev->name);
+
+	if (t2200_eth_init_buffers(dev))
+	{
+		printk(KERN_ERR "%s: Could not allocate buffer descriptors!\n", dev->name);
+		return -ENOMEM;
+	}
+
+	/* Startup PHY driver interface for this GEM if priv->phydev is not NULL */
+
+	if (priv->phydev)
+	{
+		phy_start(priv->phydev);
+	}
+
+	/* Setup the base pointer for the scheduler's queue 0
+	 * (best effort queue) at the start of the TX descriptor ring queue
+	 */
+	/* Write single buffer descriptor pointer to GEM transmit queue for base queue 0 */
+	writel(t2200_ring_to_phys(dev, (u32*) priv->TxBase), priv->baseaddr + GEM_IP + GEM_TX_QUEUE0);
+
+	/* Setup descriptor rings for TX and RX priority queues to point
+	 * to dummy desciptor ring entries so if queue is processed (TX always
+	 * processed, RX extended queues if enabled (currently not enabled)
+	 * the GEMAC will get to queue 0 where there is actual data
+	 */
+	for (i = 1; i <= 7; i++)
+	{
+		writel(virt_to_phys((u32*)(&tx_ring_fake)), priv->baseaddr + GEM_IP + GEM_TX_PQUEUE(i));
+		writel(virt_to_phys((u32*)(&rx_ring_fake)), priv->baseaddr + GEM_IP + GEM_RX_PQUEUE(i));
+	}
+
+	/* Start the controller */
+	gem_enable_tx(&priv->gemdev);
+	gem_enable_rx(&priv->gemdev);
+	return 0;
+}
+
+/**
+ * Function to stop the ethernet device
+ * @see gem_abort_tx
+ * @see gem_disable_rx
+ * @see t2200_eth_release_buffers
+ */
+void t2200_eth_stop(struct net_device *dev)
+{
+	struct eth_c4k_priv * priv = netdev_priv(dev);
+	unsigned long         flags;
+
+//AKB	if (netif_msg_drv(priv))
+		printk (KERN_INFO "%s: %s\n", __func__, dev->name);
+
+	/* Lock it down */
+	spin_lock_irqsave(&priv->txlock, flags);
+
+        /* Shutdown transmit and receive for this GEM */
+	gem_abort_tx  (&priv->gemdev);
+	gem_disable_rx(&priv->gemdev);
+
+        /* Release the lock */
+	spin_unlock_irqrestore(&priv->txlock, flags);
+
+	/* Stop the Ethernet PHY device if PHY device driver pointer is not NULL */
+	if (priv->phydev)
+	{
+		phy_stop(priv->phydev);
+	}
+
+	/* Release all transmit and receive buffers */
+	t2200_eth_release_buffers(dev);
+}
+
+/** Function to open the ethernet device */
+int t2200_eth_open(struct net_device *dev)
+{
+	struct eth_c4k_priv * priv = netdev_priv(dev);
+	int                   rc;
+
+#ifdef ATHEROS_PHY_CHECK_AND_UPDATE_RGMII_DELAY
+	u16 tmp;
+#endif
+
+	t2200_dev_ptr[priv->id] = dev;
+	t2200_init_timer(dev);
+	if (t2200_eth_rx_latency != 0) {
+	    printk ("Eth%d dev(idx=%d) is opened by the system in polling mode, resolution is %d"
+#ifdef CONFIG_GLOBAL_POLLING
+	            "(ms)\n",
+#else  /* !CONFIG_GLOBAL_POLLING */
+	            "(us)\n",
+#endif	/* CONFIG_GLOBAL_POLLING */
+	    priv->id+1, priv->id, t2200_eth_rx_latency);
+	} else {
+	    printk ("Eth%d dev(idx=%d) is opened by the system in IRQ mode\n",
+	    priv->id+1, priv->id);
+	}
+
+//AKB	if (netif_msg_ifup(priv))
+		printk (KERN_INFO "%s: %s IRQ %u\n",
+		        __func__,
+		        dev->name,
+		        priv->phys_rx_int
+		       );
+
+	dev->irq = priv->phys_rx_int;
+
+	/* Clear out TX and RX descriptor and socket buffer pointer rings */
+	priv->RxRingSize    = DEFAULT_RX_DESC_NT;
+	priv->RxMaxRingSize = MAX_RX_DESC_NT;
+	priv->TxRingSize    = DEFAULT_TX_DESC_NT;
+	priv->TxMaxRingSize = MAX_TX_DESC_NT;
+
+	/*
+	 * Clear TX and RX socket buffer rings.  These rings are parallel and have
+	 * the same number of entries as the associated GEM TX and RX descriptor
+	 * rings
+	 */
+	memset(priv->RxSkbRing, 0, MAX_RX_DESC_NT* sizeof(struct sk_buff *));
+	memset(priv->TxSkbRing, 0, MAX_TX_DESC_NT* sizeof(struct sk_buff *));
+
+	if (!is_valid_ether_addr(dev->dev_addr))
+	{
+		printk(KERN_ERR "%s %s: invalid MAC address\n", dev->name, __func__);
+		rc = -EADDRNOTAVAIL;
+		goto err0;
+	}
+
+	//
+	// Setup GEM RX offset for T2200/T3300 to have it so receive frames
+	// are offset by 2 bytes so that IP headers are 32 bit aligned
+	// for maximum effeciency in subsequent processing
+	//
+	gem_set_rx_offset(&priv->gemdev, 2);
+
+	//
+	// Enable receive checksum offload
+	//
+	if (priv->flags & RX_CSUM_OFFLOAD_ENABLED)
+		gem_enable_rx_checksum_offload(&priv->gemdev);
+
+	//
+	// Enable receive checksum offload
+	//
+	if (priv->flags & TX_CSUM_OFFLOAD_ENABLED)
+		gem_enable_tx_checksum_offload(&priv->gemdev);
+
+	/* Setup GEM MAC address */
+	gem_add_arc_entry(&priv->gemdev, dev->dev_addr);
+
+	/* Setup PHY as long as the "NO PHY" flag is not set */
+	if (!(priv->einfo->phy_flags & GEMAC_NO_PHY))
+	{
+		// GEMAC_NO_PHY is false, try to startup T2200 PHY device
+		// driver for PHY device and bus as specified in kernel config
+		rc = t2200_phy_start(dev);
+		if (rc)
+		{
+#ifdef ASSUME_SWITCH_IF_PHY_CHECK_FAILED
+			printk(KERN_INFO "%s %s: Failed to start PHY, assuming connected to switch\n", dev->name, __func__);
+#else
+			printk(KERN_ERR  "%s %s: failed to get phy\n", dev->name, __func__);
+			goto err0;
+#endif
+		}
+		else
+		{
+			/* PHY driver started up OK and PHY device found */
+
+#ifdef ATHEROS_PHY_CHECK_AND_UPDATE_RGMII_DELAY
+			// PHY is started, check if necessary to add RGMII delay
+			// This is setup for Atheros PHY
+			if (priv->einfo->phy_flags & GEMAC_PHY_RGMII_ADD_DELAY)
+			{
+				// Need to add delay to Atheros PHY, read register
+				// 20 and set bits 8 and 2
+				tmp = phy_read(priv->phydev, 20);   // Read Atheros PHY register 20
+				tmp |= 0x0082;                      // enable additional delay on Rx_clk and TX_clk
+				phy_write(priv->phydev, 20, tmp);   // Write back register 20
+				// sw reset
+				phy_write(priv->phydev, MII_BMCR, BMCR_RESET);
+				while(phy_read(priv->phydev, MII_BMCR) & BMCR_RESET);
+			}
+#endif
+		}
+	}
+	else
+	{
+		printk(KERN_INFO "%s: %s: GEMAC_NO_PHY option set\n", dev->name, __func__);
+		priv->phydev = NULL;
+	}
+
+	/* Enable receive interrupts */
+	gem_enable_irq(&priv->gemdev, GEM_IRQ_RX_DONE_FLAG);
+
+	/* Start up and enable the Ethernet function */
+	rc = t2200_eth_start(dev);
+	if (unlikely(rc))
+	{
+		printk(KERN_ERR "%s %s: failed to start the device", dev->name, __func__);
+		goto err1;
+	}
+
+	/* Enable napi to inform Linux we are ready to go */
+	napi_enable(&priv->napi);
+
+	/* Wakeup the interface to inform Linux we are ready to send and receive
+	 * Ethernet frames
+	 */
+	netif_wake_queue(dev);
+
+	/* Request an interrupt service from Linux for the RX interrupt function */
+	rc = request_irq(dev->irq,
+		         t2200_eth_rx_interrupt,
+			 IRQF_DISABLED,
+			 "t2200_gemRx",
+			 dev
+			);
+	if (unlikely(rc))
+	{
+		printk(KERN_ERR "%s %s: failed to get the Rx IRQ = %d\n", dev->name,
+		       __func__,
+		       dev->irq
+		      );
+		goto err2;
+	}
+
+	t2200_start_timer(dev);
+
+	/* If we are here, then the Ethernet device is now open and functioning */
+	return rc;
+
+err2:
+
+	napi_disable(&priv->napi);
+	netif_stop_queue(dev);
+
+	t2200_eth_stop(dev);
+
+err1:
+//	netif_poll_disable(dev);
+
+	/* Disconnect from the PHY if NO PHY flag is not set
+	 * and PHY device structure pointer is OK
+	 */
+	if ((!(priv->einfo->phy_flags & GEMAC_NO_PHY)) && (priv->phydev != NULL))
+		t2200_phy_stop(dev);
+
+err0:
+	return rc;
+}
+
+/**
+ * Function to close the ethernet device
+ */
+int t2200_eth_close(struct net_device *dev)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+
+//AKB	if (netif_msg_ifdown(priv))
+		printk(KERN_INFO "%s: %s\n", dev->name, __func__);
+
+	t2200_stop_timer(dev);
+	t2200_dev_ptr[priv->id] = NULL;
+
+	// Disable Network API
+	napi_disable(&priv->napi);
+
+	// Free device interrupt request
+	free_irq(dev->irq, dev);
+
+	// Stop Linux Network interface queue
+	netif_stop_queue(dev);
+
+	// Stop GEM device
+	t2200_eth_stop(dev);
+
+	// Stop PHY device if present
+	if (!(priv->einfo->phy_flags & GEMAC_NO_PHY))
+		t2200_phy_stop(dev);
+
+	return 0;
+}
+
+/**
+ * Function to setup local MAC address for specified ethernet device
+ */
+static int t2200_set_mac_address(struct net_device *dev, void *addr)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+	struct sockaddr *sa = addr;
+
+	if (!is_valid_ether_addr(sa->sa_data))
+		return -EADDRNOTAVAIL;
+
+	memcpy(dev->dev_addr, sa->sa_data, ETH_ALEN);
+
+	gem_add_arc_entry(&priv->gemdev, dev->dev_addr);
+
+	return 0;
+}
+
+/**
+ * Function to setup promiscuous, broadcast, multicast and loopback
+ * options for Ethernet device
+ */
+static void t2200_eth_set_multi(struct net_device *dev)
+{
+	struct eth_c4k_priv * priv   = netdev_priv(dev);
+	GEM_DEVICE *          gemdev = &priv->gemdev;
+	MAC_ADDR              hash_addr;          /**< hash register structure */
+
+#if 1
+	printk(KERN_INFO "%s: Set Multicast options:%08X\n", dev->name, dev->flags);
+#endif
+
+#if 1
+	// NOTE: WHY IS THIS RETURN HERE?  THIS WILL CAUSE MULTICAST OPTIONS
+	// TO NOT BE PROGRAMMED
+	return;
+#endif
+
+	/* Enable promiscuous mode (receive all ethernet frames) if requested
+	 * (normally it is not)
+	 */
+
+
+	if (unlikely(dev->flags & IFF_PROMISC))
+	{
+
+		if (netif_msg_drv(priv))
+			printk(KERN_INFO "%s : Entering promiscuous mode.\n", dev->name);
+
+		gem_enable_copy_all(gemdev);
+	}
+	else
+	{
+		if (t3300_reth_is_allowed ())
+  		    gem_enable_copy_all(gemdev);
+		else
+		    gem_disable_copy_all(gemdev);
+	}
+
+	/* Enable broadcast frame reception if requested (normally it is) */
+	if (likely(dev->flags & IFF_BROADCAST))
+	{
+
+		gem_allow_broadcast(gemdev);
+	}
+	else
+	{
+
+		if (netif_msg_drv(priv))
+			printk(KERN_INFO "%s: disabling broadcast frame reception.\n", dev->name);
+
+		gem_no_broadcast(gemdev);
+	}
+
+	/* Enable all multicast frames to be received if requested */
+	if (dev->flags & IFF_ALLMULTI)
+	{
+
+
+		/* Set the hash to rx all multicast frames if IFF_ALLMULTI requested */
+		hash_addr.bottom = 0xFFFFFFFF;
+		hash_addr.top    = 0xFFFFFFFF;
+		gem_set_hash(gemdev, &hash_addr);
+		gem_enable_multicast(gemdev);
+
+	}
+#if 0
+	/* Setup multicast hash table if multicast address count > 0 */
+	else if (dev->mc_count > 0)
+	{
+
+
+		/* Multicast address count > 0, setup and enable multicast addresses */
+		/* Clear the hash table: */
+		hash_addr.bottom = 0;
+		hash_addr.top    = 0;
+		gem_set_hash(gemdev, &hash_addr);
+
+		/* Get the multicast list pointer */
+		mcptr = dev->mc_list;
+
+		/* Loop through the list to generate hash table entries: */
+		for (i = 0; i < dev->mc_count; i++) {
+			addr_start = (unsigned char *)&mcptr->dmi_addr;
+
+			if (netif_msg_drv(priv))
+				printk(KERN_DEBUG "%s: adding multicast address %X:%X:%X:%X:%X:%X to gem filter\n",
+					dev->name,
+					addr_start[0], addr_start[1], addr_start[2],
+					addr_start[3], addr_start[4], addr_start[5]);
+
+			temp1 =   addr_start[0] & 0x3F;
+			temp2 = ((addr_start[0] & 0xC0) >> 6) | ((addr_start[1] & 0x0F) << 2);
+			temp3 = ((addr_start[1] & 0xF0) >> 4) | ((addr_start[2] & 0x03) << 4);
+			temp4 =  (addr_start[2] & 0xFC) >> 2;
+			temp5 =   addr_start[3] & 0x3F;
+			temp6 = ((addr_start[3] & 0xC0) >> 6) | ((addr_start[4] & 0x0F) << 2);
+			temp7 = ((addr_start[4] & 0xF0) >> 4) | ((addr_start[5] & 0x03) << 4);
+			temp8 = ((addr_start[5] & 0xFC) >> 2);
+
+			result = temp1 ^ temp2 ^ temp3 ^ temp4 ^ temp5 ^ temp6 ^ temp7 ^ temp8;
+
+			if (result >= GEM_HASH_REG_BITS) {
+				break;
+			} else {
+				if (result < 32) {
+					hash_addr.bottom |= (1 << result);
+				} else {
+					hash_addr.top |= (1 << (result - 32));
+				}
+			}
+
+			mcptr = mcptr->next;
+		}
+		/* Setup the hash table and enable multicast: */
+		gem_set_hash(gemdev, &hash_addr);
+		gem_enable_multicast(gemdev);
+	}
+#endif
+	/* Setup Ethernet GEM internal loopback if requested */
+	if (dev->flags & IFF_LOOPBACK)
+	{
+
+		gem_set_loop(gemdev, LB_LOCAL);
+	}
+
+	return;
+}
+
+// Static variable with network statistics data as net_device_stats structure type
+//struct net_device_stats net_stats;
+
+/** Function to return the pointer to the ethernet statistics for this device */
+static struct net_device_stats *t2200_eth_get_stats(struct net_device *dev)
+{
+	struct eth_c4k_priv * priv     = netdev_priv(dev);
+	GEM_DEVICE *          gemdev   = &priv->gemdev;
+	struct gem_reg *      regs     = gemdev->registers;
+	struct gem_stats *    reg_stat = &regs->stats;
+	u32                   u;
+
+	// Get selected statistics from GEM statistics registers
+	// NOTE: When GEM statistics are read, this will also
+	// clear them back to zero inside the GEM device
+	// for those statistics (and only those statistics) read
+
+	priv->stats.rx_packets += reg_stat->frames_rx;     /* total packets received	*/
+	priv->stats.tx_packets += reg_stat->frames_tx;     /* total packets transmitted	*/
+	priv->stats.rx_bytes   += reg_stat->octets_rx_bot; /* total bytes received 	*/
+	priv->stats.tx_bytes   += reg_stat->octets_tx_bot; /* total bytes transmitted	*/
+
+	priv->stats.multicast  += reg_stat->multicast_rx;
+
+	u = reg_stat->length_check_errors;
+	priv->stats.rx_length_errors  += u;
+	priv->stats.rx_errors         += u; /* bad packets received		*/
+
+	u = reg_stat->fcs_errors;
+	priv->stats.rx_crc_errors     += u; /* recved pkt with crc error	*/
+	priv->stats.rx_errors         += u;
+
+	u = reg_stat->rx_orun;
+	priv->stats.rx_fifo_errors    += u; /* recv'r fifo overrun		*/
+	priv->stats.rx_errors         += u;
+
+	u = reg_stat->align_errors;
+	priv->stats.rx_frame_errors   += u; /* recv'd frame alignment error */
+	priv->stats.rx_errors         += u;
+
+	u = reg_stat->tx_urun;
+	priv->stats.tx_fifo_errors    += u;
+	priv->stats.tx_errors         += u; /* packet transmit problems	*/
+
+	u = reg_stat->crs_errors;
+	priv->stats.tx_carrier_errors += u;
+	priv->stats.tx_errors         += u;
+	return &priv->stats;
+}
+
+/** Function to change the maximum transmit unit (MTU) size */
+static int t2200_eth_change_mtu(struct net_device *dev, int new_mtu)
+{
+#ifndef CONFIG_TRANSCEDE_MTU_CTRL_ENABLED
+	//MTU size change currently not supported, always return code
+	printk (KERN_INFO "%s: %s MTU:%u\n", __func__, dev->name,new_mtu);
+	return 0;
+#else
+	//Previous code to support MTU size changes, including
+	//Jumbo frame support
+	struct eth_c4k_priv * priv     = netdev_priv(dev);
+	int			frame_size = new_mtu;
+	int 			restart = 0;
+
+	if (dev->mtu == new_mtu)
+	{
+		printk ("GEMAC: the same MTU size (%d) is detected\n", new_mtu);
+		return 0;
+	}
+
+	if (unlikely((frame_size < 64) || (frame_size > JUMBO_FRAME_SIZE)))
+	{
+		if (netif_msg_drv(priv))
+			printk(KERN_ERR "%s: Invalid MTU setting\n",
+					dev->name);
+		return -EINVAL;
+	}
+
+	/* Only stop and start the controller if it isn't already
+	 * stopped, and we changed something
+	 */
+	if ((dev->mtu != new_mtu) && (dev->flags & IFF_UP))
+	{
+	    restart = 1;
+	    gem_disable_rx(&priv->gemdev);
+	}
+
+	dev->mtu = new_mtu;
+
+	/* If the mtu is larger than the max size for standard
+	 * ethernet frames (ie, a jumbo frame), setup Jumbo frames
+	 */
+
+	if (new_mtu > 1500)
+	{
+	    gem_enable_rx_jmb_ex(&priv->gemdev, new_mtu);
+	}
+	else
+	{
+	    gem_disable_rx_jmb_ex(&priv->gemdev, new_mtu);
+	}
+
+	if (restart != 0)
+	{
+	    gem_enable_rx(&priv->gemdev);
+	}
+
+	printk (KERN_INFO "%s: %s MTU:%u\n", __func__, dev->name,new_mtu);
+	return 0;
+#endif
+}
+
+/**
+ * Function to release all transmit and receive socket buffers and
+ * cleanup/initialize transmit and receive desciptors/queues
+ * for a given device
+ *
+ * AKB: Recoded transmit release to release the data based
+ * on what is in the socket buffer rather than the descriptor
+ *
+ */
+static void t2200_eth_release_buffers(struct net_device *dev)
+{
+	struct eth_c4k_priv * priv = netdev_priv(dev);
+	struct tRXdesc *      ThisRXdesc;
+	struct tTXdesc *      ThisTXdesc;
+	int                   i;
+	struct sk_buff *      skb;
+	u32                   length;
+	u32                   temp_u32;
+
+	if (netif_msg_drv(priv))
+		printk (KERN_INFO "%s: %s\n", __func__, dev->name);
+
+
+	/* Go through all RX descriptors, umap DMA, release all all receive
+	 * socket buffers, set RX descriptor data pointers to NULL
+	 * and set all socket ring buffer entries to NULL
+	 */
+	spin_lock(&priv->rxlock);
+	for (i = 0; i < priv->RxRingSize; i++)
+	{
+		/* Test if socket buffer pointer NULL, if so, skip release */
+		if(priv->RxSkbRing[i])
+		{
+			ThisRXdesc                = &(priv->RxBase[i]); // AKB fixed 2010-09-28, was priv->RxBase + i
+			/* Unmap a single streaming mode DMA translation.
+			 * The handle and size must match what was provided in the previous dma_map_single call.
+			 * All other usages are undefined.
+			 * After this call, reads by the CPU to the buffer are guaranteed to see
+			 * whatever the device wrote there.
+			 */
+			dma_unmap_single(&dev->dev,
+			                 GEM_RX_DESCR_BUF_ADDR(ThisRXdesc->rx_data),
+			                 RX_BUF_UNMAP_SZ,
+			                 DMA_FROM_DEVICE
+					); //just be safe
+			ThisRXdesc->rx_data       = (u32)NULL;
+			//
+			// TODO: SHOULDN'T RX DESCRIPTOR CONTROL WORD
+			// ALSO BE CLEANED UP?
+			//
+
+			// DD ThisRXdesc->rx_extstatus |= GEMRX_OWN;
+
+			// Free the socket buffer from this ring entry and set
+			// socket buffer entry pointer to NULL
+			dev_kfree_skb(priv->RxSkbRing[i]);
+			priv->RxSkbRing[i]        = NULL;
+
+		}
+	}
+	spin_unlock(&priv->rxlock);
+
+	/* Go through all TX descriptors, unmap DMA, release all transmit
+	 * socket buffers and set all socket buffer ring entries to NULL
+	 * all descriptors
+	 */
+	spin_lock(&priv->txlock);
+	ThisTXdesc = priv->TxBase;
+	for (i = 0; i < priv->TxRingSize; i++)
+	{
+		/* Get current descriptor data length and socket buffer pointer (if any) */
+		length = ThisTXdesc->txctl & GEM_TX_DESCR_LEN_MASK;
+		skb    = priv->TxSkbRing[i];
+		if ((length != 0) && (ThisTXdesc->txdata != 0))
+		{
+			/* Non-zero length in descriptor, give it back to the Linux CPU */
+			/* Unmap a single streaming mode DMA translation.
+			 * The handle and size must match what was provided in the previous dma_map_single call.
+			 * All other usages are undefined.
+			 * After this call, reads by the CPU to the buffer are guaranteed to see
+			 * whatever the device wrote there.
+			 */
+			/* Possible TODO add, may need to add check for errors and possibly
+			 * update local statistcis here...
+			 */
+			dma_unmap_single(&dev->dev,
+			                 ThisTXdesc->txdata,
+			                 length,
+			                 DMA_TO_DEVICE
+			                );
+			/* Cleanup the TX descriptor pointer*/
+			ThisTXdesc->txdata = 0;
+
+			/* Clear all desriptor control bits except used and
+			 * wrap bits, make sure used bit is set
+			 */
+			temp_u32          = ThisTXdesc->txctl;
+			temp_u32         &= GEM_TX_DESCR_WRAP;
+			temp_u32         |= GEM_TX_DESCR_USED;
+			ThisTXdesc->txctl = temp_u32;
+		}
+		/* Get current socket buffer pointer (if any) */
+		skb    = priv->TxSkbRing[i];
+		if(skb != NULL)
+		{
+			// Free socket buffer if socket buffer ring is not NULL
+			dev_kfree_skb(skb);
+			priv->TxSkbRing[i] = NULL;
+		}
+		/* Bump TX descriptor pointer */
+		ThisTXdesc++;
+	}
+	spin_unlock(&priv->txlock);
+}
+
+/**
+ * Function to map a given Descriptor virtual address to the physical address
+ *
+ * Calculate address to use by:
+ *  - Taking passed address
+ *  - Get relative address by Subtracting Base virtual address of Descriptors base used by the driver
+ *  - Adding Descriptors base physical address to relative address to get physical address
+ */
+static u32 t2200_ring_to_phys(struct net_device *dev, volatile u32* va)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+
+	return (u32)(priv->Descriptors_baseaddr_pa + ((u32)va - (u32)priv->Descriptors_baseaddr_v));
+}
+
+/**
+ * Function to initialize TX and RX buffer for C4K ethernet device
+ *
+ * @return Returns 0 if success, returns 1 if unable to allocate memory
+ */
+static int t2200_eth_init_buffers(struct net_device *dev)
+{
+	struct eth_c4k_priv * priv = netdev_priv(dev);
+	struct tRXdesc *      ThisRXdesc;
+	struct tTXdesc *      ThisTXdesc;
+	struct sk_buff *      skb;
+	int                   i,j;
+
+//AKB	if (unlikely(netif_msg_drv(priv)))
+	printk (KERN_INFO "%s: %s\n",
+	        dev->name,
+	        __func__
+	       );
+
+	//
+	// Setup pointers and clear memory for TX and RX descriptors
+	// based on previously setup memory space for them
+	//
+	i = priv->RxMaxRingSize * sizeof(struct tRXdesc);
+	j = priv->TxMaxRingSize * sizeof(struct tTXdesc);
+
+	priv->RxBase = (struct tRXdesc *) priv->Descriptors_baseaddr_v;
+	priv->TxBase = (struct tTXdesc *) (priv->RxBase + priv->RxMaxRingSize);
+
+	//
+	// Clear descriptor ring memory
+	//
+	memset(priv->RxBase, 0, (i+j));
+
+#if 1
+	printk(KERN_INFO "%s: Descriptor RX Base 0x%08X, RX Size:0x%08X, TX Base:0x%08X, TX Size:0x%08X\n",
+	       dev->name,
+	       (u32) priv->RxBase,
+               i,
+	       (u32) priv->TxBase,
+               j
+	      );
+#endif
+
+	//
+	// Loop through receive descriptor ring (GEM ring) and receive socket ring
+	// (ring of socket buffer pointers used by software), allocate a socket buffer for every ring
+	// entry, initialize key socket buffer fields, setup socket buffer pointers
+	// and call dma_map_single to setup for proper memory/cache control
+	//
+	ThisRXdesc = priv->RxBase;
+	for (i = 0; i < priv->RxRingSize; i++)
+	{
+		//
+		// Request socket buffer for this socket ring entry, exit if allocation
+		// failed
+		//
+		if(unlikely(!(skb = sd_dev_alloc_skb(RX_SKB_ALLOC_SZ))))
+		{
+			if (unlikely(netif_msg_drv(priv)))
+				printk (KERN_ERR "%s: %s RX skb allocation\n", dev->name,  __func__);
+			goto err;
+		}
+#if 0
+//#ifdef GEM_ETHDRV_TEMP_DEBUG
+		//
+		// Dump socket info for first seven buffers for debug
+		// check for alignment, where allocated, etc.
+		//
+		if (i < 7)
+		{
+			printk (KERN_INFO "%s: RX Socket buffer %u\n",
+			        dev->name,
+			        i
+			       );
+			print_skb_info(skb);
+		}
+#endif
+
+		// Got a socket buffer OK for this entry, setup pointer to it in receive
+		// socket buffer ring
+		priv->RxSkbRing[i] = skb;
+		/*
+		 * Reserve packet headroom in buffer to:
+		 * - align IP part to 32 bit boundary
+		 * - have some extra header space to avoid reallocations
+		 */
+		skb_reserve(skb, RX_SKB_RESERVE_SZ);
+
+#if 0
+//#ifdef GEM_ETHDRV_TEMP_DEBUG
+		//
+		// Dump socket info for first seven buffers
+		//
+		if (i < 7)
+		{
+			printk (KERN_INFO "%s: RX Socket buffer %u\n",
+			        dev->name,
+			        i
+			       );
+			print_skb_info(skb);
+
+		}
+#endif
+
+//		ThisRXdesc->rx_data= virt_to_phys(skb->data);
+//		printk("RX%d: %x %x\n", i, skb, skb->data);
+
+		/* Setup GEM descriptor pointer to receive socket buffer data pointer */
+		/* Ensure that any data held in the cache is appropriately discarded or written back. */
+		ThisRXdesc->rx_data    = dma_map_single(&dev->dev,
+		                                        skb->data - NET_IP_ALIGN,
+		                                        RX_BUF_MAP_SZ,
+		                                        DMA_FROM_DEVICE
+						       );
+		ThisRXdesc->rx_data += NET_IP_ALIGN;
+		/* Clear offset address (handled in GEM by offset value, not by pointer */
+		ThisRXdesc->rx_data &= 0xFFFFFFFC;
+
+		// Point to next GEM descriptor.
+		ThisRXdesc++;
+	}
+
+	/*
+	 * Move pointer back by 1 and set the WRAP bit on the
+	 * last GEM receive descriptor entry
+	 */
+	ThisRXdesc--;
+	ThisRXdesc->rx_data |= GEM_RX_DESCR_WRAP;
+
+	/*
+	 * Write descriptor ring pointer to GEM RX queue 0 (non-classified data) pointer
+	 */
+	writel(t2200_ring_to_phys(dev, (u32*) priv->RxBase), priv->baseaddr + GEM_IP + GEM_RX_QUEUE0);
+
+#ifdef LAST_DESC_IS_FAKE_FOR_PRIORITY_QUEUES
+	/* For receive priority queues, initialize the GEM pointer to a single
+	 * dummy descriptor with USED bit set
+	 */
+	j = 0xF403FFF0;
+	printk(KERN_INFO "%s: Setting Fake Descriptors for RX priority queues @ 0x%08X\n",
+		dev->name,
+		j
+		);
+	*(volatile u32*) j = 0x3;
+	for (i = 1; i <= 7; i++) {
+		writel(j, priv->baseaddr + GEM_IP + GEM_RX_PQUEUE(i));
+	}
+#endif
+
+	/*
+	 * Initialize the TX descriptor ring
+	 */
+	ThisTXdesc = priv->TxBase;
+	for (i = 0; i < priv->TxRingSize; i++)
+	{
+		ThisTXdesc->txdata =  0;                    /* Initialize data pointer to NULL */
+		ThisTXdesc->txctl  = (0|GEM_TX_DESCR_USED); /* Initialize TX control to GEMTX_USED_MASK */
+		ThisTXdesc++;                               /* Increment pointer to next descriptor until done */
+	}
+	/*
+	 * Move pointer back by 1 and set the WRAP bit on the last entry
+	 */
+	ThisTXdesc--;
+	ThisTXdesc->txctl |= GEM_TX_DESCR_WRAP;
+
+	/*
+	 * Setup the base pointer to the TX descriptor ring in the GEM device's
+	 * best effort (no priority) queue
+	 */
+	writel(t2200_ring_to_phys(dev,
+	                          (u32*) priv->TxBase),
+	                          priv->baseaddr + GEM_IP + GEM_TX_QUEUE0
+	                         );
+
+#ifdef LAST_DESC_IS_FAKE_FOR_PRIORITY_QUEUES
+#if 1
+	/* For transmit priority queues, initialize the GEM pointer to a single
+	 * dummy descriptor with USED bit set
+	 */
+	printk(KERN_INFO "%s: Setting Fake Descriptors for TX priority queues\n",
+		dev->name
+		);
+#endif
+	/* Setup priority queues to dummy descriptor entry with used bit and wrap bit set */
+	j = 0xF403FFF4;
+	printk(KERN_INFO "%s: Setting Fake Descriptors for TX priority queues @ 0x%08X\n",
+		dev->name,
+		j
+		);
+	*(volatile u32*) j = 0xC0000000;
+
+	for (i = 1; i <= 7; i++) {
+		writel((j-4), priv->baseaddr + GEM_IP + GEM_TX_PQUEUE(i));
+	}
+#endif
+
+	/* Init RX and TX ring control variables */
+	priv->RxtocleanIndex = 0;
+	priv->RxtofillIndex  = 0;
+	priv->Txtosend       = 0;
+	priv->Txdone         = 0;
+	priv->Txavail        = priv->TxRingSize - 1; // always keep one open to avoid GEM TX ring overruns
+	priv->Txqueued       = 0;
+
+	/* Returns 0 if OK*/
+	return 0;
+
+err:
+	t2200_eth_release_buffers(dev);
+
+	return -ENOMEM;
+}
+
+#ifdef GEM_FORCED_DROP_TX
+static int _num_drop = 0;
+#endif
+
+/**
+ * Function to handle the hardware specifics of transmitting a single packet
+ * based on pointer to device and pointer to socket buffer passed as parameters
+ */
+//static
+int t2200_hardware_send_packet(struct net_device *dev, struct sk_buff *skb)
+{
+	struct eth_c4k_priv * priv = netdev_priv(dev); /**< Pointer to GEM network driver private data */
+	GEM_DEVICE *          gemdev = &priv->gemdev;  /**< Pointer to GEM structure to R/W registers */
+	int                   err = 0;
+	int                   Txavail = priv->Txavail; /**< Number of available (empty/unused) slots in the TX descriptor ring */
+	u32                   i = 0, txctrl;
+	int                   length  = skb->len, total_len=skb->len;  /**< Amount to send taken from socket buffer length */
+	struct tTXdesc *      ThisTXdesc, *BlockTXdesc;          /**< Current Transmit descriptor used to send actual frame/packet */
+
+	struct skb_shared_info * pinfo;
+	int dma_map_len;
+
+#ifdef BUG_69620_WORKAROUND
+	struct tTXdesc *      NextTXdesc;          /**< Current Transmit descriptor used to send actual frame/packet */
+#endif
+
+#ifdef GEM_TX_ONE_DESCRIPTOR_AT_A_TIME
+	// Variables for handling single actual GEM transmit descriptor
+	// followed by dummy descriptor for sending one frame at a time
+	// and waiting for completion.
+	struct tTXdesc *      TailTXdesc;
+#endif
+
+#if 0
+	// Option to print message on every transmit frame if interface messages.
+	// Be careful if you use this option
+	// as this is called on every transmit ethernet frame
+	printk("t2200_hardware_send_packet skb=%X\n", skb);
+	//if (netif_msg_intr(priv))
+	//	printk(KERN_DEBUG "\n%s %s: txavail %d txtosend %d txdone %d\n",
+	//	       dev->name,
+	//	       __func__,
+	//	       priv->Txavail,
+	//	       priv->Txtosend,
+	//	       priv->Txdone
+	//	      );
+#endif
+
+
+#ifdef GEM_ETHDRV_TEMP_DEBUG
+	//AKB TEMP DEBUG:
+	if ((priv->Txtosend > 1020) || (priv->Txdone > 1020))
+	printk(KERN_NOTICE "%s: TX a:%04u s:%04u d:%04u p:%p l:%08X\n",
+		dev->name,
+		Txavail,
+		priv->Txtosend,
+		priv->Txdone,
+		(void *)virt_to_phys(skb->data),
+		skb->len
+		);
+
+	//print_skb_info(skb);
+#endif
+
+#ifndef GEM_TX_ONE_DESCRIPTOR_AT_A_TIME
+	/* Before proceeding, first make sure that there is at least two
+	 * transmit queue entries available
+	 */
+	if (unlikely(Txavail <= 1))
+	{
+		//AKB TEMP DEBUG:
+		printk(KERN_ERR "\nTA:%u p:%p l:%u\n",
+			Txavail,
+			(void *)virt_to_phys(skb->data),
+			skb->len
+			);
+
+		err = -ENOMEM;
+		goto err;
+	}
+#endif
+	/* There is at least two transmit queue entries available: */
+	/* Make sure we have exclusive use of this socket buffer, if not,
+	 * exit
+	 *
+	if (unlikely((skb = skb_unshare(skb, GFP_ATOMIC)) == NULL))
+	{
+		//AKB TEMP DEBUG:
+		printk(KERN_ERR "\nUS\n");
+
+		priv->stats.tx_dropped++;
+		goto err;
+	}*/
+
+#ifdef GEM_TX_ONE_DESCRIPTOR_AT_A_TIME
+	/* Save pointer to socket buffer structure for later release
+	 * on completion of transmit
+	 */
+	priv->TxSkbRing[priv->Txtosend] = skb;
+
+	/* Force current transmit descriptor pointer to the first entry
+	 * and tail entry to the first entry plus 1
+	 */
+	ThisTXdesc = priv->TxBase; // always uses the first two.
+	TailTXdesc = ThisTXdesc + 1;
+
+	// Use actual socket buffer data's pointer
+	// and setup descriptor pointer to it (no copy, much more efficient
+	/* Ensure that any data held in the cache is appropriately discarded or written back. */
+	ThisTXdesc->txdata = dma_map_single(&dev->dev, skb->data, length, DMA_TO_DEVICE);
+
+	/* Setup Current Transmit descriptor for single buffer (last bit set)
+	 * Set Length bit
+	 */
+	ThisTXdesc->txctl  = GEM_TX_DESCR_LAST_BUF_BIT;
+	ThisTXdesc->txctl |= length & GEM_TX_DESCR_LEN_MASK;
+
+	/* Setup next descriptor as used and wrap bit set */
+	TailTXdesc->txctl  = GEM_TX_DESCR_WRAP | GEM_TX_DESCR_USED;
+
+	/* Update transmit ring queue for next entry and decrement
+	 * number of available slots
+	 */
+	priv->Txtosend = (priv->Txtosend + 1) & (priv->TxRingSize - 1);
+	priv->Txavail--;
+	priv->Txqueued++;
+
+	/* Write single buffer descriptor pointer to GEM transmit queue for base queue 0 */
+	writel(t2200_ring_to_phys(dev, (u32 *)ThisTXdesc), priv->baseaddr +  GEM_IP + GEM_TX_QUEUE0);
+
+	/* Issue TX start and enable command to GEM */
+	gem_start_tx(gemdev);
+
+	/* Wait for frame to be transmitted (again terribly innefficient) */
+	while ((ThisTXdesc->txctl & GEM_TX_DESCR_USED) == 0) {
+               udelay(100);
+               i++;
+               if (i == 5000) {
+                       printk("Emac: tx timed out!\n");
+                       err = 1;
+                       goto err;
+               }
+	}
+	// For the workaround code, we only transmit one Ethernet frame at a
+	// time.  So with that we unmap the DMA and free the socket buffer
+	// here rather than in the poll routine
+	/* Unmap a single streaming mode DMA translation.
+	 * The handle and size must match what was provided in the previous dma_map_single call.
+	 * All other usages are undefined.
+	 * After this call, reads by the CPU to the buffer are guaranteed to see
+	 * whatever the device wrote there.
+	 */
+	dma_unmap_single(&dev->dev,
+	                 ThisTXdesc->txdata,
+	                 length,
+	                 DMA_TO_DEVICE
+	                );
+
+	/* Data transmitted, so OK to free the socket buffer here */
+	dev_kfree_skb(skb);
+	priv->TxSkbRing[(priv->Txtosend - 1) & (priv->TxRingSize - 1)] = NULL;
+	priv->Txavail++;
+	priv->Txqueued--;
+#else
+	//
+	// Transmit multiple frames at a time using circular queuing
+	// processing for the Transmit Descriptor running as a ring.
+	//
+	/* Save pointer to socket buffer structure for later release
+	 * on completion of transmit
+	 */
+	priv->TxSkbRing[priv->Txtosend] = skb;
+	tx_sched_time[priv->id][priv->Txtosend] = REG32(0xFE050004); //jiffies;
+
+	/* Get pointer to current descriptor to send to */
+	ThisTXdesc = &(priv->TxBase[priv->Txtosend]);
+
+	/* Update transmit ring queue for next entry and decrement
+	 * number of available slots
+	 * Note as code is written, this must be a power of 2 number for
+	 * total number of descriptors
+	 */
+	priv->Txtosend = (priv->Txtosend + 1) & (priv->TxRingSize - 1);
+	priv->Txavail--;
+	priv->Txqueued++;
+
+	// Use actual socket buffer data's pointer
+	// and setup descriptor pointer to it (no copy, much more efficient
+	/* Ensure that any data held in the cache is appropriately discarded or written back. */
+	//ThisTXdesc->txdata = dma_map_single(&dev->dev, skb->data, length, DMA_TO_DEVICE);
+
+	pinfo = skb_shinfo(skb);
+	for (i = 1; i < pinfo->map_fr_num; i++)
+	{
+	    // we need to decrease the size of attached block from total size of the frame
+	    // to put correct length in the first gem dma block and to optimize dma mapping
+	    length -= pinfo->frags[i].size;
+	    //printk ("GEM: +1 TX block:{%p,%d}, total=%d", (void*)pinfo->frags[i].page, pinfo->frags[i].size, pinfo->map_fr_num);
+
+	    BlockTXdesc = &(priv->TxBase[priv->Txtosend]);
+
+	    // we are using the last one skb to free mapped skb
+	    // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+	    priv->TxSkbRing[priv->Txtosend] = skb;
+
+	    txctrl  = BlockTXdesc->txctl; // Get current descriptor value (including wrap bit set if last entry)
+	    txctrl &= GEM_TX_DESCR_WRAP; // Clear all but wrap bit, this will also clear the used bit and any error bits
+
+	    if (i+1>=pinfo->map_fr_num)
+	    {
+		//printk ("-end\r\n");
+		BlockTXdesc->txctl = GEM_TX_DESCR_LAST_BUF_BIT | txctrl | pinfo->frags[i].size;
+	    }
+	    else
+	    {
+		//printk ("\n");
+		BlockTXdesc->txctl = txctrl | pinfo->frags[i].size;
+	    }
+
+	    dma_map_len = pinfo->frags[i].size;
+#ifdef CONFIG_IPSEC_DMA_MAP_HACK_TX
+	    if (unlikely(skb->dma_mapped_data)) {
+		const int dma_not_mapped_len = skb->dma_mapped_data - (uint8_t*)pinfo->frags[i].page;
+		if (dma_not_mapped_len > 0 && dma_not_mapped_len < pinfo->frags[i].size)
+		    dma_map_len = dma_not_mapped_len;
+	    }
+#endif
+	    BlockTXdesc->txdata = dma_map_single(&dev->dev, (void*)pinfo->frags[i].page, dma_map_len, DMA_TO_DEVICE);
+
+	    priv->Txavail--;
+	    priv->Txtosend = (priv->Txtosend + 1) & (priv->TxRingSize - 1);
+	}
+
+	// Use actual socket buffer data's pointer
+	// and setup descriptor pointer to it (no copy, much more efficient
+	/* Ensure that any data held in the cache is appropriately discarded or written back. */
+	dma_map_len = length;
+#ifdef CONFIG_IPSEC_DMA_MAP_HACK_TX
+	if (unlikely(skb->dma_mapped_data)) {
+	    const int dma_not_mapped_len = skb->dma_mapped_data - skb->data;
+	    if (dma_not_mapped_len > 0 && dma_not_mapped_len < length)
+		dma_map_len = dma_not_mapped_len;
+	}
+#endif
+	ThisTXdesc->txdata = dma_map_single(&dev->dev, skb->data, dma_map_len, DMA_TO_DEVICE);
+
+#ifdef BUG_69620_WORKAROUND
+
+	if ((pinfo->map_fr_num & 1)!=0 || pinfo->map_fr_num == 0){
+	/* Calculate next buffer to send based on ring size */
+	NextTXdesc = &(priv->TxBase[priv->Txtosend]);
+
+	/* Update transmit ring queue for next entry and decrement
+	 * number of available slots
+	 * Note as code is written, this must be a power of 2 number for
+	 * total number of descriptors
+	 */
+	priv->Txtosend = (priv->Txtosend + 1) & (priv->TxRingSize - 1);
+	priv->Txavail--;
+
+	/* Implement workaround, setup next descriptor as "dummy"
+	 * descriptor with zero length frame descriptor and
+	 * NULL buffer pointer
+	 */
+	NextTXdesc->txctl  &= GEM_TX_DESCR_WRAP; // Clear all control bits except the wrap bit
+	NextTXdesc->txdata = 0;
+	}
+
+#endif
+	/* Setup Current Transmit descriptor  (last bit set)
+	 * Set Length bit
+	 */
+	i  = ThisTXdesc->txctl; // Get current descriptor value (including wrap bit set if last entry)
+	i &= GEM_TX_DESCR_WRAP; // Clear all but wrap bit, this will also clear the used bit and any error bits
+
+	if (pinfo->map_fr_num <= 1)
+	i |= GEM_TX_DESCR_LAST_BUF_BIT | length;
+	else
+	    i |= length;
+
+	#ifdef GEM_FORCED_DROP_TX
+	_num_drop++;
+	if (_num_drop >= GEM_FORCED_DROP_TX)
+	{
+		printk ("Wait to stop ... ");
+		gem_transmitting(gemdev);
+		printk ("OK\n");
+	}
+	#endif
+
+	ThisTXdesc->txctl = i;  // Write new descriptor with used bit clear, transfer owner from CPU to GEM
+
+#ifdef GEM_ETHDRV_TEMP_DEBUG
+	if ((priv->Txtosend > 1020) || (priv->Txdone > 1020))
+	printk(KERN_NOTICE "%s: TXREADY: p:%p l:%u s:%u a:%u d:%u desc:%p ctrl:%08X buf:%08x\n",
+		dev->name,
+		(void *)virt_to_phys(skb->data),
+		skb->len,
+		priv->Txtosend,
+		priv->Txavail,
+		priv->Txdone,
+		(void *)virt_to_phys(ThisTXdesc),
+		ThisTXdesc->txctl,
+		ThisTXdesc->txdata
+		);
+#endif // printk debug print
+
+	/* Issue TX start and enable command to GEM, if GEM already started
+	 * this will do no harm
+	 */
+
+	#ifdef GEM_FORCED_DROP_TX
+	if (_num_drop < GEM_FORCED_DROP_TX)
+	{
+	    gem_start_tx(gemdev);
+	}
+	else
+	{
+	    if(_num_drop == GEM_FORCED_DROP_TX)
+		printk ("Dropped packet, iter=%d\n", _num_drop);
+	}
+	#else
+	gem_start_tx(gemdev);
+	#endif
+
+#endif // #else Transmit multiple frames at a time, circular queuing
+
+	/* Normal exit */
+	/* Bump transmit statistics */
+	priv->stats.tx_packets++;
+	priv->stats.tx_bytes += total_len;
+
+err:
+	return(err);
+}
+
+/**
+ * Entry function to transmit a single packet based on pointer to device
+ * and pointer to socket buffer passed as parameters.
+ * The function first tries to lock the TX function
+ * using spin_trylock(&priv->txlock) and if sucessfull
+ * then calls t2200_hardware_send_packet which takes care
+ * of the hardware specific functions.
+ *
+ * @return
+ * Returns:
+ *   - NETDEV_TX_OK:      Packet queued for transmission OK
+ *   - NETDEV_TX_LOCKED:  Unable to unlock transmit using spin_trylock
+ *   - NETDEV_TX_BUSY:    -ENOMEM error (hard error) returned from t2200_hardware_send_packet
+ */
+//
+// AKB TEMP REMOVED STATIC FOR DEBUG
+//static
+int t2200_eth_send_packet(struct sk_buff *skb, struct net_device *dev)
+{
+	struct eth_c4k_priv * priv = netdev_priv(dev);
+	unsigned long         flags;
+	int                   result;
+    int                   t1_time;
+
+	if (unlikely(!spin_trylock_irqsave(&priv->txlock,flags)))
+	{
+		/* Collision - tell upper layer to requeue */
+		if (netif_msg_tx_err(priv))
+			printk(KERN_DEBUG "%s %s: TX collision\n", dev->name, __func__);
+		return NETDEV_TX_LOCKED;
+	}
+   /*  now we send packet out OK, and we need to log
+    the time stamp for future use, in case it is not ptp packets*/
+
+    if (unlikely((((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) && (!priv->tx_hwtstamp_skb)))) )
+    {
+        priv->tx_reg_time.tv_sec  =  gem_get_tsu_sec(&priv->gemdev);
+        priv->tx_reg_time.tv_nsec = gem_get_tsu_nsec(&priv->gemdev);
+
+        t1_time  =  gem_get_tsu_sec(&priv->gemdev);
+
+        if ( (priv->tx_reg_time.tv_sec != t1_time)   &&  (priv->tx_reg_time.tv_nsec <500000000UL ) )
+        {
+            priv->tx_reg_time.tv_sec  = t1_time;
+        }
+        priv->hwts_work_counter = 1;
+
+        skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+        priv->tx_hwtstamp_skb = skb_get(skb);
+        schedule_work(&priv->tx_hwtstamp_work);
+    }
+
+#ifndef GEM_TX_ONE_DESCRIPTOR_AT_A_TIME
+	t2200_eth_free_tx_packets(dev, 0); //dd
+#endif
+	result = t2200_hardware_send_packet(dev, skb);
+
+	if (unlikely(result))
+	{
+
+		switch (result) {
+		case -ENOSPC:
+			/* We queued the skb, but now we're out of space. */
+			// AKB TEMP: ALWAYS PRINT ERROR MESSAGES if (netif_msg_tx_err(priv))
+			printk(KERN_NOTICE "%s %s: no space for tx\n", dev->name, __func__);
+			/* Stop upper layer from sending us additional packets */
+			netif_stop_queue(dev);
+
+			break;
+
+		case -ENOMEM:
+			/* This is a hard error - log it. */
+			// AKB TEMP: ALWAYS PRINT ERROR MESSAGES if (netif_msg_tx_err(priv))
+			printk(KERN_ERR "%s %s: out of tx resources, returning skb\n", dev->name, __func__);
+
+			netif_stop_queue(dev);
+			spin_unlock_irqrestore(&priv->txlock, flags);
+
+			return NETDEV_TX_BUSY;
+
+		default:
+			break;
+		}
+	}
+
+	dev->trans_start = jiffies;
+	spin_unlock_irqrestore(&priv->txlock, flags);
+
+	return NETDEV_TX_OK;
+}
+
+#if 0
+static void ShowData32(PVOID ptr, U16 count)
+{
+	U32 *d = (U32 *)ptr;
+	int i;
+	for(i = 0; i < count; i++)
+	{
+		if ( !(i & 0x7) )
+			printk("\n");
+		printk("%08x ", d[i]);
+	}
+	printk("\n");
+}
+#endif
+
+/**
+*******************************************************************************
+*
+* @fn    t2200_retry_recycler
+* @brief tries to redoze recycler and re-read the dma list
+*
+* @param[i]  n - number of descriptors missing in GEM RX ring
+* @return    pointer to SkBuffer containing array of DMA descs
+*
+* @description
+*    The routine check if fill level of GEM RX ring is critically low, if
+* so it does redoze the recycler and trying to re-read the list of prepared
+* DMA descriptors and SKB array which is needed to refill the GEM Rx ring.
+* to ICC service communication matrix.
+*
+* @ingroup gem_rx_recycler
+*
+******************************************************************************/
+PSK_BUFF t2200_retry_recycler(U32 rtf, U32 rtc, U32 n)
+{
+	PSK_BUFF pSkBuff = NULL;
+
+	if (n > 1024)
+	{
+		printk(KERN_ERR "RX refill failed @rtf_%d/rtc_%d.. %lu buffers missing\n", rtf, rtc, n);
+		// try to doze
+		skb_rx_recycler_doze(pSkbRecycler);
+		pSkBuff = (PSK_BUFF)SFL_Dequeue(pSkbRecycler->pq);
+		if (pSkBuff)
+			printk("redoze succeed skb_list_%p\n", pSkBuff);
+		else
+			printk("redoze failed\n");
+	}
+	return pSkBuff;
+}
+
+
+
+
+/*
+ During SK releasing the following two arrays are formed inside the
+ payload of SK buffer. Whereas at zero offset DMA descriptors for GEM
+ are created, and at 1KB offset from SK buffer payload beginning - an
+ array of corresponding pointers to SK buffers are located
+
+ 0 bytes offset              1KB offset
+ +-------------------   ------+------------------------------   -----+
+ | DMA descriptors   ...      | array of ptrs to SK Buffers   ...    |
+ +--------------------   -----+-------------------------------   ----+
+*/
+
+
+/**
+*******************************************************************************
+*
+* @fn    t2200_refill_rxdma
+* @brief the routine is used to refill RX DMA ring
+*
+* @param[h]  h - pointer to GEM dev handle
+@ @param[i]  pSkbRecycler - pointer to available RX recycler
+* @return    number of refilled descriptors
+*
+* @description
+*    The routine does read prepared lists of DMA descriptors from RX recycler
+* and tried to refill GEM RX DMA ring with them. Each list may contain 64 or
+* 128 descriptors (it depends on RX_RECYCLER_GRANULE) parameter.
+*
+* @ingroup gem_rx_recycler
+*
+******************************************************************************/
+int t2200_refill_rxdma(void *h, PSKB_RECYCLER pSkbRecycler)
+{
+	extern void (*sd_free_data)(void*phead);
+
+	struct net_device *dev = (struct net_device *)h;
+	extern unsigned long (*__dma_cpu_sync)(const void * kaddr, size_t size, uint dir);
+
+	struct eth_c4k_priv * priv = netdev_priv(dev);
+	int                   rtf = priv->RxtofillIndex;
+	int                   rtc = priv->RxtocleanIndex;
+	U32                   n = (rtc - rtf) & (priv->RxRingSize -1);
+	struct tRXdesc *      ThisRXdesc; /**< Current Receive descriptor pointer */
+	const U32 Count = RX_RECYCLER_GRANULE;
+
+	while (n > Count)
+	{
+		PSK_BUFF pSkBuff = (PSK_BUFF)SFL_Dequeue(pSkbRecycler->pq);
+		U8 *Payload;
+
+		// here we may need to check if pSkBuff is non-NULL
+		if (!pSkBuff)
+		{
+			if ((pSkBuff = t2200_retry_recycler(rtf, rtc, n)) == NULL )
+				return 0;
+		}
+
+		Payload = pSkBuff->head;
+		ThisRXdesc = (struct tRXdesc *)Payload;
+
+		// refill from recycler
+		ThisRXdesc = priv->RxBase + rtf;
+
+		// copy pointers to SK buffers first
+		memcpy(&priv->RxSkbRing[rtf], Payload + 1024, sizeof(PVOID) * Count);
+
+		// copy received descriptors
+		// TODO:  we may want to set "wrap" bit in the list before copying it to the actual DMA desc area
+		memcpy(ThisRXdesc, Payload, sizeof(*ThisRXdesc) * Count);
+
+		__dma_cpu_sync((const void *)Payload, 1536 + 256, DMA_FROM_DEVICE);
+
+		rtf += Count;
+		if (rtf >= priv->RxRingSize)
+		{
+			ThisRXdesc += Count - 1;
+			ThisRXdesc->rx_data |= GEM_RX_DESCR_WRAP;
+			rtf = 0;
+		}
+		priv->RxtofillIndex = rtf;
+		n -= Count;
+	}
+	return 0;
+}
+
+/**
+ * Function to replenish the RX queue (RX ring) of socket buffers
+ * and to setup for additional reception for previously used
+ * slots
+ *
+ * @return
+ * Returns
+ * 0: OK
+ * -EAGAIN: Unable to get a new receive socket buffer
+ */
+//static
+int t2200_eth_rx_refill(struct net_device *dev, U32 Count)
+{
+	struct eth_c4k_priv * priv = netdev_priv(dev);
+	int                   rtf;        /**< RX to fill  current index  */
+	int                   rtc;        /**< RX to clean current index */
+	int                   ret = 0;    /**< Return code, assumes OK */
+	struct tRXdesc *      ThisRXdesc; /**< Current Receive descriptor pointer */
+	struct sk_buff *      skb;        /**< Current socket buffer pointer */
+	u32                   temp_u32;
+
+	/* Check if anything to do (nothing to do if "to fill"
+	 * index is equal to "to clean" index)
+	 */
+	if (priv->RxtofillIndex != priv->RxtocleanIndex)
+	{
+		rtf = priv->RxtofillIndex;
+		rtc = priv->RxtocleanIndex;
+		/* Loop through descriptor table until receive to fill and
+		 * receive to clean
+		 * indeces are equal
+		 */
+		while (Count-- && rtc != rtf)
+		{
+			/* Indeces not equal, still work to do */
+			/* Get pointer to current descriptor:  */
+			ThisRXdesc = priv->RxBase + rtf;
+
+			/* Request a new socket buffer from the system */
+			skb = sd_dev_alloc_skb(RX_SKB_ALLOC_SZ);
+			/* Test if we go a buffer OK */
+			if (likely(skb))
+			{
+				/* We got a new socket buffer OK, set the pointer to the
+				 * socket buffer in the current index of the receive
+				 * socket buffer ring
+				 */
+				priv->RxSkbRing[rtf] = skb;
+
+				/*
+				 * Reserve packet headroom in buffer to:
+				 * - align IP part to 32 bit boundary
+				 * - have some extra header space to avoid reallocations
+				 */
+				skb_reserve(skb, RX_SKB_RESERVE_SZ);
+
+				/* Setup the DMA memory for this socket buffer */
+				/* Ensure that any data held in the cache is appropriately discarded or written back. */
+				temp_u32 = dma_map_single(&dev->dev, skb->data - NET_IP_ALIGN, RX_BUF_MAP_SZ, DMA_FROM_DEVICE);
+				temp_u32 += NET_IP_ALIGN;
+
+				/* Clear the descriptor status field: */
+				*(volatile u32*) &ThisRXdesc->rx_status    = 0;
+
+				/* Clear least significant 2 bits of address to get 32 bit based
+				 * address for GEM (GEM offset put in RX offset value in GEM
+				 * register
+				 * This will also set the used bit to 0 giving the buffer to the GEM for its use.
+				 */
+				temp_u32 &= 0xFFFFFFFC;
+
+				/* Setup the Wrap bit if this is the last descriptor in the ring */
+				/* Also the code below will write the RX data pointer and used and wrap
+				 * bits in one atomic operation to avoid race conditions with the GEM
+				 */
+				if(unlikely(ThisRXdesc == (priv->RxBase + priv->RxRingSize - 1))) {
+					*(volatile u32*) &ThisRXdesc->rx_data = (GEM_RX_DESCR_WRAP | temp_u32);
+				}
+				else {
+					*(volatile u32*) &ThisRXdesc->rx_data = temp_u32;
+				}
+				ret++;
+			}
+			else
+			{
+				/* Oops, we didn't get a socket buffer, will have to try
+				 * again later.  Indicate low on memory by setting
+				 * return code to -EAGAIN and exit the loop
+				 */
+				// AKB TEMP: ALWAYS PRINT ERROR MESSAGES if (unlikely(netif_msg_rx_err(priv)))
+					printk(KERN_ERR "%s %s: low on mem\n", dev->name, __func__);
+
+				ret = -EAGAIN;
+				break;
+			}
+			/* Set next return to fill index and keep looping until done */
+			inc_rx_idx(rtf);
+		}
+		/* All done looping, update the receive "to fill" index */
+		priv->RxtofillIndex = rtf;
+	}
+
+	return (ret);
+}
+
+
+/**
+*******************************************************************************
+*
+* @fn    t2200_refill_rxdma_init
+* @brief used to align RTF of RX DMA ring to RX_RECYCLER_GRANULE
+*
+* @param[h]  h - pointer to GEM dev handle
+@ @param[i]  pSkbRecycler - pointer to available RX recycler
+* @return    number of refilled descriptors
+*
+* @description
+*    The routine refills GEM RX DMA ring by one descriptor in order to get
+* RTF be aligned to RX_RECYCLER_GRANULE border. Once done the callback
+* responsible of refilling DMA ring is switched to batch routine
+* t2200_refill_rxdma()
+*
+* @ingroup gem_rx_recycler
+*
+******************************************************************************/
+static int t2200_refill_rxdma_init(void *h, PSKB_RECYCLER pSkbRecycler)
+{
+	struct net_device *dev = (struct net_device *)h;
+	// purpose of this routine is to make appropriate alignment of "rtf" for DMA lists
+	// such that t2200_refill_rxdma will take over
+	struct eth_c4k_priv * priv = netdev_priv(dev);
+
+	// check if index aligned to RX_RECYCLER_GRANULE (which is always power of 2 - e.g. 64 or 128)
+	while (priv->RxtofillIndex & (RX_RECYCLER_GRANULE - 1))
+	{
+		if (t2200_eth_rx_refill(dev, 1) == 0)
+		{
+			// nothing added
+			return 0;
+		}
+	}
+
+	// we've got out of the loop, switch the callback to normal function
+	printk("T2200_eth: switch to accelerated RX allocator done\n");
+//	skb_rx_recycler_doze(pSkbRecycler);
+//	skb_rx_recycler_doze(pSkbRecycler);
+
+	pSkbRecycler->refill = t2200_refill_rxdma;
+}
+
+int t2200_reinit_bufs_amp(int amp_cpu_id)
+{
+    int i,rc, num=0;
+    for (i = 0; i < sizeof (t2200_dev_ptr)/sizeof (t2200_dev_ptr[0]); i++)
+    {
+	// this code is added here to provide storage driver with some time.
+	// to prepare new portion of free blocks and prevent situation when
+	// new SKB cannot be created due to empty pool, also this delay
+	// jas to be added only after some allocation
+
+	if (num!=0)
+	    msleep_interruptible (500);
+
+	if (t2200_dev_ptr[i]!=NULL)
+	{
+	    rc = t2200_reinit_bufs_for_dev(t2200_dev_ptr[i], amp_cpu_id);
+	    if (rc != 0)
+		return rc;
+	    num++;
+	    t3300_reth_init(t2200_dev_ptr[i]);
+	}
+    }
+
+    return 0;
+}
+
+int t2200_reinit_bufs(U32 *pListOfIccRxBuffers)
+{
+	// to initialize a recycler only if it was not initialized
+	// and this is possible by system configuration
+
+        if (recycler_ctrl)
+        {
+   	    if (!pSkbRecycler && pListOfIccRxBuffers)
+	    {
+		PSKB_RECYCLER pRecycler = skb_recycler_init(pListOfIccRxBuffers);
+		PSK_BUFF skb;
+
+		pRecycler->refill = t2200_refill_rxdma_init;
+		// recycler ready, enable it by initializing global pointer
+		pSkbRecycler = pRecycler;
+
+		// append some number of descriptors to the recycler
+		// skb_rx_recycler_doze(pSkbRecycler);
+  	     }
+        }
+
+	return t2200_reinit_bufs_amp(-1);
+}
+
+EXPORT_SYMBOL(t2200_reinit_bufs);
+EXPORT_SYMBOL(t2200_reinit_bufs_amp);
+
+#ifdef CONFIG_IPSEC_DMA_MAP_HACK_RX
+typedef struct eth_ip4_ip4_esp_frame
+{
+	struct ethhdr		ethh;
+	struct iphdr		iph0;
+	struct iphdr		iph1;
+	struct ip_esp_hdr	esph;
+} __attribute__((packed)) eth_ip4_ip4_esp_frame_t;
+
+typedef struct eth_ip4_udp_esp_frame
+{
+	struct ethhdr		ethh;
+	struct iphdr		iph;
+	struct udphdr		udph;
+	struct ip_esp_hdr	esph;
+} __attribute__((packed)) eth_ip4_udp_esp_frame_t;
+
+typedef struct eth_ip4_esp_frame
+{
+	struct ethhdr		ethh;
+	struct iphdr		iph;
+	struct ip_esp_hdr	esph;
+} __attribute__((packed)) eth_ip4_esp_frame_t;
+
+static inline int is_eth_ip4_ip4_esp_frame(void *data)
+{
+	eth_ip4_ip4_esp_frame_t *frame = (eth_ip4_ip4_esp_frame_t*)data;
+
+	return frame->ethh.h_proto == htons(ETH_P_IP) &&
+	       frame->iph0.protocol == IPPROTO_IPIP &&
+	       !(frame->iph0.frag_off & htons(IP_MF | IP_OFFSET)) && /* No fragmentation */
+	       frame->iph1.protocol == IPPROTO_ESP &&
+	       !(frame->iph1.frag_off & htons(IP_MF | IP_OFFSET)); /* No fragmentation */
+}
+
+#define IPSEC_NATT_UDP_PORT 4500
+
+static inline int is_eth_ip4_udp_esp_frame(void *data)
+{
+	eth_ip4_udp_esp_frame_t *frame = (eth_ip4_udp_esp_frame_t*)data;
+
+	return frame->ethh.h_proto == htons(ETH_P_IP) &&
+	       frame->iph.protocol == IPPROTO_UDP &&
+	       !(frame->iph.frag_off & htons(IP_MF | IP_OFFSET)) && /* No fragmentation */
+	       frame->udph.dest == htons(IPSEC_NATT_UDP_PORT);
+}
+
+static inline int is_eth_ip4_esp_frame(void *data)
+{
+	eth_ip4_esp_frame_t *frame = (eth_ip4_esp_frame_t*)data;
+
+	return frame->ethh.h_proto == htons(ETH_P_IP) &&
+	       frame->iph.protocol == IPPROTO_ESP &&
+	       !(frame->iph.frag_off & htons(IP_MF | IP_OFFSET)); /* No fragmentation */
+}
+#endif
+
+//static
+int t2200_eth_rx_packet(struct net_device *dev,   /**< Pointer to device structure */
+                        unsigned int *work_done,  /**< Pointer to variable of amount of frames processed as received for by this function */
+                        unsigned int work_to_do   /**< Budget for max frame to receive per call */
+                       )
+{
+	struct eth_c4k_priv * priv = netdev_priv(dev);
+	struct tRXdesc *      ThisRXdesc;               /* Current Receive descriptor */
+	int                   rtc;                      /* Receive to clean index     */
+	u32                   rx_data;                  /* Current receive data       */
+	u32                   rx_status;                /* Current receive status     */
+	int                   length;                   /* Current receive length     */
+	struct sk_buff *      skb;                      /* Pointer to socket buffer   */
+	int                   ret = 0;                  /* Current return code        */
+    struct timespec   hwtstamp;
+    struct skb_shared_hwtstamps * hwtstamp_t;
+    s32                  secfromreg;
+    u32                  nsfromreg;
+    unsigned char*                  nsfromcrc;
+
+#if 0
+	// TEMP DEBUG CODE TO CHECK FOR INCONSISTENCIES BETWEEN RX DESCRIPTOR
+	// AND SOCKET POINTER DATA
+	int fatal_error = 0;
+#endif
+
+	//if (netif_msg_intr(priv))
+	//	printk(KERN_DEBUG "%s: %s\n", dev->name, __func__);
+
+	rtc = priv->RxtocleanIndex;
+
+	// Loop through the receive descriptors
+	while (1)
+	{
+		// Check if there is work to do, if not, return
+		if (unlikely(*work_done >= work_to_do))
+		{
+			ret = -EAGAIN;
+
+		//	if (netif_msg_intr(priv))
+		//	{
+		//		printk (KERN_DEBUG "%s %s: FDesc %#lx, RxtocleanIndex %d\n", dev->name, __func__, (unsigned long)ThisRXdesc, rtc);
+		//		printk (KERN_DEBUG "%s %s: work_done %d, work_to_do %d\n", dev->name, __func__, *work_done, work_to_do);
+		//	}
+
+			goto done_exit;
+		}
+		//
+		// loop through the receive descriptors
+		// Get pointer to current receive descriptor and get descriptor info
+		//
+		ThisRXdesc   = priv->RxBase + rtc;
+		rx_data      = ThisRXdesc->rx_data;
+		//
+		// Check if current descriptor's used bit is set, if so, then it is still owned
+		// by the GEM and there is nothing left to process for this call to processing
+		// the receive ring
+		//
+		if (!(rx_data & GEM_RX_DESCR_OWNERSHIP))
+		{
+//			if (unlikely(netif_msg_intr(priv)))
+	//			printk(KERN_DEBUG "%s %s: done, FDesc %#lx, RxtocleanIndex %d\n", dev->name, __func__, (unsigned long)ThisRXdesc, rtc);
+			break;
+		}
+		//
+		// Get receive status bits for this descriptor
+		//
+		rx_status = ThisRXdesc->rx_status;
+		//
+		// Get length
+		//
+		length = (rx_status & GEM_RX_DESCR_LEN_MASK);
+		//
+		// Get associated socket buffer pointer
+		//
+		skb = priv->RxSkbRing[rtc];
+#if 0
+		/* dma_sync_single_for_cpu is for cases where you have a long-lived
+                 * (mapping with multiple accesses from both CPU and device)
+		 */
+		dma_sync_single_for_cpu(&dev->dev,
+		                        GEM_RX_DESCR_BUF_ADDR(rx_data),
+		                        length + NET_IP_ALIGN,  // 2 bytes for frame offset
+		                        DMA_FROM_DEVICE
+		                       );
+
+		{
+			// DEBUG PRINT DEBUG DATA, NOTE USED ON ALL FRAMES SO USE CAUTIOUSLY
+			int ii = length;
+			printk("\r\n");
+			printk("%08X %08X %08X %08X\n",
+			       (unsigned int)ThisRXdesc,
+			       ThisRXdesc->rx_data,
+			       ThisRXdesc->rx_status,
+			       length
+			      );
+			for (ii=0; ii<length; ii++)
+				printk("%02X ", *((u8*)rx_data + ii) );
+			printk("\r\n");
+
+		}
+#endif
+		/* Unmap a single streaming mode DMA translation.
+		 * The handle and size must match what was provided in the previous dma_map_single call.
+		 * All other usages are undefined.
+		 * After this call, reads by the CPU to the buffer are guaranteed to see
+		 * whatever the device wrote there.
+		 */
+#ifdef CONFIG_IPSEC_DMA_MAP_HACK_RX
+		if (likely(ipsec_dma_map_hack_rx_enabled)) {
+			uint32_t dma_unmap_len = 0;
+
+			if (length > sizeof(eth_ip4_ip4_esp_frame_t))
+				dma_unmap_len = sizeof(eth_ip4_ip4_esp_frame_t);
+			else if (length > sizeof(eth_ip4_udp_esp_frame_t))
+				dma_unmap_len = sizeof(eth_ip4_udp_esp_frame_t);
+			else if (length > sizeof(eth_ip4_esp_frame_t))
+				dma_unmap_len = sizeof(eth_ip4_esp_frame_t);
+
+			if (dma_unmap_len) {
+				dma_unmap_single(&dev->dev,
+						 GEM_RX_DESCR_BUF_ADDR(rx_data),
+						 dma_unmap_len + NET_IP_ALIGN,
+						 DMA_FROM_DEVICE
+						);
+
+				if ((dma_unmap_len >= sizeof(eth_ip4_ip4_esp_frame_t) && is_eth_ip4_ip4_esp_frame(skb->data)) ||
+				    (dma_unmap_len >= sizeof(eth_ip4_udp_esp_frame_t) && is_eth_ip4_udp_esp_frame(skb->data)) ||
+				    (dma_unmap_len >= sizeof(eth_ip4_esp_frame_t) && is_eth_ip4_esp_frame(skb->data)))
+					skb->dma_mapped_data = skb->data + dma_unmap_len;
+				else {
+					dma_unmap_single(&dev->dev,
+							 GEM_RX_DESCR_BUF_ADDR(rx_data) + dma_unmap_len + NET_IP_ALIGN,
+							 length - dma_unmap_len,
+							 DMA_FROM_DEVICE
+							 );
+				}
+			}
+		} else
+#endif
+			dma_unmap_single(&dev->dev,
+					 GEM_RX_DESCR_BUF_ADDR(rx_data),
+					 length + NET_IP_ALIGN,  // Adjust length for RX offset
+					 DMA_FROM_DEVICE
+					);
+
+		if (likely(!(rx_status & GEM_RX_DESCR_FCS_STAT)))
+		{
+			/* RX Ethernet frame is OK: */
+#if defined(CONFIG_INET_IPSEC_OFFLOAD) || defined(CONFIG_INET6_IPSEC_OFFLOAD)
+			// CODE EXECUTED CONDITIONALLY IF IPSEC OFFLOAD IS DEFINED AT COMPILE TIME
+			if (rx_extstatus & RX_IPSEC_IN) {
+				struct sec_path *sp;
+				struct xfrm_state *x;
+				u16 *sah = (u16*)&ThisRXdesc->pad;
+				int i = 0;
+
+				sp = secpath_dup(skb->sp);
+
+				if (!sp) {
+					kfree_skb(skb);
+					goto pkt_drop;
+				}
+
+				skb->sp = sp;
+				while ((*sah)) {
+					if ((i > 3) || ((x = xfrm_state_lookup_byhandle(*sah++)) == NULL)) {
+						kfree_skb(skb);
+						goto pkt_drop;
+					}
+
+					sp->xvec[i++] = x;
+					if (!x->curlft.use_time)
+						x->curlft.use_time = (unsigned long)xtime.tv_sec;
+				}
+
+				sp->len = i;
+			}
+#endif
+                        if (unlikely(skb == NULL))
+                        {
+                                ThisRXdesc->rx_data &= (GEM_RX_DESCR_OWNERSHIP | GEM_RX_DESCR_WRAP);
+                                /*
+                                * OK to clear the status bits of the descriptor now
+                                */
+                                ThisRXdesc->rx_status = 0;
+                                /*
+                                * Increment RX to clean index
+                                */
+                                inc_rx_idx(rtc);
+
+                                goto done_exit;
+                        }
+
+			/* Update socket buffer fields */
+			skb->dev       = dev;
+			skb_put(skb, length);
+			skb->protocol  = eth_type_trans(skb, dev);
+
+			//
+			// Checksum offload is enabled, check fields
+			// Check descriptor status for IP checksum fields
+			//
+			if (likely(priv->flags & RX_CSUM_OFFLOAD_ENABLED)) {
+				if (likely(rx_status & GEM_RX_DESCR_TYPE_ID_OR_CHKSUM_INFO_MASK))
+				{
+					// Non-zero checksum field, TCP, UDP and/or
+					// IP checksum was already validated as OK
+					// by the GEM.  Mark as Checksum UNNECESSARY
+					skb->ip_summed = CHECKSUM_UNNECESSARY;
+#if 0
+					// DEBUG PRINT, HAVE VALIDATED THAT THIS CODE IS EXECUTING :-)
+					printk(KERN_INFO "U%u\n",length);
+#endif
+				}
+				else
+				{
+					//
+					// This frame's CRC/FCS was OK, but packet
+					// was not in TCP, UDP or IP format.
+					// Mark as checksum NONE
+					//
+					skb->ip_summed = CHECKSUM_NONE;
+				}
+			} else {
+				//
+				// Checksum offload is disabled, always set
+				// socket buffer ip_summed to NONE
+				// Up to upper level SW to take care of all checksums
+				// if needed
+				//
+				skb->ip_summed = CHECKSUM_NONE;
+			}
+
+//we add our timing stamp here if 1588 time stamp is  enabled
+    if  (priv ->hwts_config.rx_filter !=  HWTSTAMP_FILTER_NONE)
+    {
+        /* We have been asked to include the hardware timestamp of the
+           received packet in the socket buffer */
+
+        /* Read seconds value from hardware */
+        secfromreg =   gem_get_tsu_sec(&priv->gemdev);
+
+        /* Read nS value from the hardware */
+        nsfromreg =  gem_get_tsu_nsec(&priv->gemdev);
+
+        /* Re read seconds value from hardware */
+        hwtstamp.tv_sec =       gem_get_tsu_sec(&priv->gemdev);
+
+        /* Read the nS portion of the timestamp from
+           the received packet */
+
+        /* The T2200 GEMAC, once enable the 1588 padding, it will pad the ns portion to CRC field, which is 4 bytes at end of the packet */
+        nsfromcrc = (unsigned char *) (skb->data + (skb->len - 4));
+        hwtstamp.tv_nsec   =    (u32) *(nsfromcrc)
+                                 | (((u32) *(nsfromcrc+1)) <<  8)
+                                 | (((u32) *(nsfromcrc+2)) << 16)
+                                 | (((u32) *(nsfromcrc+3)) << 24);
+
+        /* may not be 4 bytes aligned, neeed a solution here*/
+        /*    hwtstamp.tv_nsec =  hwtstamp.tv_nsec << 2; */
+        /* Check for  second roll over */
+        if (secfromreg != hwtstamp.tv_sec)
+        {
+            /* We have a seconds roll over, therefore
+               use the initial value read. */
+            hwtstamp.tv_sec = secfromreg;
+        }
+        else
+            /* if no second rollover, still we might have rollover from capture of the packets, suppose this is less than 0.5s delay */
+            if (nsfromreg < hwtstamp.tv_nsec)
+            {
+                hwtstamp.tv_sec--;
+            }
+
+        /* Set hardware timestamp value in the socket buffer */
+        hwtstamp_t = skb_hwtstamps(skb);
+        hwtstamp_t ->hwtstamp = ktime_set(hwtstamp.tv_sec,hwtstamp.tv_nsec );
+
+    }
+			/* Pass received socket buffer to upper layer */
+			if (likely (t3300_reth_is_routing_enabled () == 0))
+			{
+				if (gemac_fp_rx_cb != NULL)
+				{
+					if (gemac_fp_rx_cb(priv->id, skb) < 0)
+					{
+						netif_receive_skb(skb);
+					}
+				}
+				else
+				{
+					netif_receive_skb(skb);
+				}
+			}
+			else
+			{
+				u8* p = ((u8*)skb->mac_header);
+				if (memcmp (p, dev->dev_addr, 6) == 0)
+				{
+					netif_receive_skb(skb);
+				}
+				else if (t3300_reth_is_mac (p))
+				{
+					t3300_reth_forward_rx (skb, 0);
+					dev_kfree_skb(skb);
+				}
+				else
+				{
+					t3300_reth_forward_rx (skb, 1);
+					netif_receive_skb(skb);
+				}
+			}
+
+			/* Update receive queue and statistics */
+			priv->RxSkbRing[rtc]  = NULL;
+			priv->stats.rx_packets++;
+			priv->stats.rx_bytes += length;
+			dev->last_rx          = jiffies;
+			/*
+			 * Increment work done variable
+			 */
+			(*work_done)++;
+		}
+		else
+		{
+			/* Receive Ethernet frame has an error */
+			dev_kfree_skb(skb);       // no one will touch this anymore, just free it.
+			priv->stats.rx_errors++;  // increment error count
+		}
+		/* Socket buffer given to upper layer if CRC OK,
+		 * or released if CRC error, but descriptor
+		 * has not been refilled yet with a replacement
+		 * socket buffer
+		 *
+		 * Clear all but the ownership bit and the wrap bit
+		 * so we don't accidentally re-use this buffer.
+		 *
+		 * rx_data field is updated to a real buffer when re-filled
+		 * and at that point, the GEM is given ownership of the descriptor
+		 */
+		ThisRXdesc->rx_data &= (GEM_RX_DESCR_OWNERSHIP | GEM_RX_DESCR_WRAP);
+		/*
+		 * OK to clear the status bits of the descriptor now
+		 */
+		ThisRXdesc->rx_status = 0;
+		/*
+		 * Increment RX to clean index
+		 */
+		inc_rx_idx(rtc);
+	}
+
+done_exit:
+	priv->RxtocleanIndex = rtc;
+	return ret;
+}
+
+#ifdef GEMAC_MAX_TX_PROC_TIME
+static uint proc_hang_prn_num = 0;
+int t2200_proc_gemac_tx_hang(struct net_device *dev, struct eth_c4k_priv *priv)
+{
+    struct tTXdesc* ThisTXdesc;
+    uint dev_id = priv->id;
+    uint length, txctl;
+    struct sk_buff *skb;
+    uint ticks_to_wait_tx_done = 250*250; // 250 us for 250MHz bus
+    int drop_num = 0;
+    GEM_DEVICE *mac = &priv->gemdev;
+    struct timeval t;
+    struct tm broken;
+    uint start_tick;
+
+    // if GEMAC has nothing to do, let's stop executing this code
+    if (priv->Txdone == priv->Txtosend)
+	return 0;
+
+    ThisTXdesc  =  priv->TxBase + priv->Txdone;
+
+    // if TX USED bit is set by GEMAC, this
+    // descriptor is processed by GEMAC and we may skipp
+    // 'reset' operation for this case
+    if ((ThisTXdesc->txctl & GEM_TX_DESCR_USED) != 0)
+	return 0;
+
+    // if lifetime of TX DMA descriptor is not out of range,
+    // let's stop 'reset' procedure
+    if ((REG32(0xFE050004) - tx_sched_time[dev_id][priv->Txdone]) < GEMAC_MAX_TX_PROC_TIME)
+	return 0;
+
+    if (proc_hang_prn_num < GEM_HANG_PRN_NUM)
+    {
+	//printk ("Processing hang %d, num:%d, txd:%p\n", REG32(0xFE050004) - tx_sched_time[dev_id][priv->Txdone], proc_hang_prn_num, ThisTXdesc);
+	printk ("Processing hang %d, num:%d, txd:%p, ticks:0x%x\n", REG32(0xFE050004) - tx_sched_time[dev_id][priv->Txdone], proc_hang_prn_num, ThisTXdesc, REG32(0xFE050004));
+	printk (" TX-CTL: 0x%x\n", ThisTXdesc->txctl);
+	printk (" TX-PTR: 0x%x\n", ThisTXdesc->txdata);
+	printk ("DMA-CFG: 0x%x\n", REG32(mac->registers + GEM_DMA_CONFIG));
+	printk (" TX-STS: 0x%x\n", REG32(mac->registers + GEM_TX_STATUS));
+	printk ("INT-STS: 0x%x\n", REG32(mac->registers + 0x24));
+	do_gettimeofday(&t);
+	time_to_tm(t.tv_sec, 0, &broken);
+	printk ("TIME   : %2d:%02d:%02d\n", broken.tm_hour, broken.tm_min, broken.tm_sec);
+    }
+
+    // here we detected GEMAC TX hang issue, we are going to do:
+    //   - wait possible TX_GO (just to be in sync, theoretically this cannot be)
+    //   - disable TX
+    //   - free scheduled TX SKBs
+    //   - set GEMAC TX pointer to the head of TX queue
+    //   - reinitalize TX queue
+    //   - enable TX operation
+
+    gem_stop_tx(&priv->gemdev);
+
+    start_tick = REG32(0xFE050004);
+    while ( gem_transmitting(&priv->gemdev) &&
+            (REG32(0xFE050004) - start_tick) < ticks_to_wait_tx_done);
+    printk ("WAITING: %d\n", REG32(0xFE050004) - start_tick);
+
+    gem_abort_tx(&priv->gemdev);
+
+    while (priv->Txdone != priv->Txtosend)
+    {
+	ThisTXdesc = priv->TxBase + priv->Txdone;
+	txctl = ThisTXdesc->txctl;
+	length = txctl & GEM_TX_DESCR_LEN_MASK;
+
+	if ((length != 0) && (ThisTXdesc->txdata != 0))
+	{
+	    #ifndef GEM_DONOT_UNMAP_TX_BUF
+	    dma_unmap_single(&dev->dev,
+			    ThisTXdesc->txdata,
+			    length,
+			    DMA_TO_DEVICE
+			    );
+	    #endif
+	}
+
+	// Only for the last one packet DMA descriptor we need
+	// to de-allocate SKB, because chained DMAs points to the same SKB
+	if (txctl & GEM_TX_DESCR_LAST_BUF_BIT)
+	{
+	    priv->Txqueued--;
+	    drop_num ++;
+	    if((skb = priv->TxSkbRing[priv->Txdone]) != NULL)
+	    {
+		dev_kfree_skb(skb);
+	    }
+	    else
+	    {
+		printk(KERN_ERR "%s: %s encountered errors while freeing tx descriptor, socket buffer NULL\n",
+				dev->name,
+				__func__);
+	    }
+        }
+
+        ThisTXdesc->txdata = 0;  /* Clear current descriptor TX data pointer */
+        ThisTXdesc->txctl  = (txctl & GEM_TX_DESCR_WRAP) | (GEM_TX_DESCR_USED | GEM_TX_DESCR_LAST_BUF_BIT);
+
+        priv->TxSkbRing[priv->Txdone] = NULL;
+        priv->Txavail++;
+
+        priv->Txdone = (priv->Txdone + 1) & (priv->TxRingSize - 1);
+    }
+
+    // to clear all the statuses provided by GEMAC core
+    REG32(mac->registers + GEM_TX_STATUS) = REG32(mac->registers + GEM_TX_STATUS);
+
+    priv->Txdone   = 0;
+    priv->Txtosend = 0;
+    priv->Txavail  = priv->TxRingSize - 1;
+
+    /* Write single buffer descriptor pointer to GEM transmit queue for base queue 0 */
+    writel(t2200_ring_to_phys(dev, (u32*) priv->TxBase), priv->baseaddr + GEM_IP + GEM_TX_QUEUE0);
+
+    #ifdef GEM_FORCED_DROP_TX
+    _num_drop = 0;
+    #endif
+
+    if (proc_hang_prn_num < GEM_HANG_PRN_NUM)
+    {
+	printk("GEMAC tx-reset is done,dropped %d packets\n", drop_num);
+	proc_hang_prn_num++;
+    }
+
+    return drop_num;
+}
+
+#endif //GEMAC_MAX_TX_PROC_TIME
+
+/**
+ * Function to scan the transmit queue and determine which
+ * descriptors have completed transmission.
+ * For those descriptors that have completed transmission
+ * the transmit socket buffer is freed, the desciptors
+ * are setup to allow for a future ethernet frame to be
+ * sent and the transmit queue (TX ring) control variables
+ * are updated.
+ *
+ * @param dev  Pointer to struct net_device with device driver information
+ * @return     Returns number of Transmit descriptors cleaned (buffers released)
+ *
+ * @todo
+ * This code currently does not check any of the error bits after parsing
+ * through the descriptors to free the descriptors up and return
+ * any socket buffers back to the system.  It would be better
+ * if this code also checked the error status and incremented
+ * error statistics.  NOTE: There are hardware statistics stored
+ * in the chip as well, so if this is needed, it should only be
+ * done for those errors not counted by the chip itself.
+ *
+ */
+//static
+int t2200_eth_free_tx_packets(struct net_device *dev, uint lock)
+{
+	struct eth_c4k_priv * priv =  netdev_priv(dev); /**< Private data pointer */
+	struct tTXdesc *      ThisTXdesc;               /**< Current TX descriptor */
+	int                   tx_cleaned = 0;           /**< Transmit cleaned counter */
+	struct sk_buff *      skb;                      /**< Pointer to current socket buffer */
+	u32                   length;                   /**< length extracted from TX descriptor */
+	int                   LastBitPending;           /**< Variable to keep processing multiple descriptors until LAST bit seen */
+
+#if 0
+	/* Print debug message on function entry if enabled: */
+	//if (unlikely(netif_msg_intr(priv)))
+	if ((priv->Txtosend > 1020) || (priv->Txdone > 1020))
+		printk(KERN_INFO "%s: %s a:%u d:%u s:%u\n",
+		       dev->name,
+		       __func__,
+		       priv->Txavail,
+		       priv->Txdone,
+		       priv->Txtosend
+		      );
+#endif
+
+	/* Spin lock on transmit lock */
+
+	if (priv->Txqueued < GEM_TX_CLEAN_THRESHOLD)
+	    return 0;
+
+	if (lock)
+	    spin_lock(&priv->txlock);
+
+
+	#ifdef GEMAC_MAX_TX_PROC_TIME
+	// if GEMAC stopped to process TX traffic
+	// let's drop the scheduled packets
+	// and restart TX DMA
+	if (t2200_proc_gemac_tx_hang(dev, priv))
+	{
+	    if (lock)
+		spin_unlock(&priv->txlock);
+
+	    return 0;
+	}
+	#endif
+
+	/*
+	 * Loop through transmit queue (TX ring) from last done position
+	 * until bits indicate there is a transmit in process
+	 */
+	while (1)
+	{
+		/* Check if head and tail indeces are equal, if so, then
+		 * no more work to do...
+		 */
+		if (priv->Txdone == priv->Txtosend)
+		{
+			// Head and tail pointers are equal, TX queue is empty
+			break;
+		}
+		/*
+		 * head and tail are not equal, so there is TX data that
+		 * was sent but not yet checked to see if OK to free or not...
+		 */
+		/* Get current starting point (base plus "done" index) */
+		ThisTXdesc  =  priv->TxBase + priv->Txdone;
+
+#ifdef GEM_ETHDRV_TEMP_DEBUG
+		if ((priv->Txtosend > 1020) || (priv->Txdone > 1020))
+			printk(KERN_INFO "%s: %s a:%u d:%u s:%u dc:%08X dp:%08X bc:%08X bp:%08X\n",
+			       dev->name,
+			       __func__,
+			       priv->Txavail,
+			       priv->Txdone,
+			       priv->Txtosend,
+			       ThisTXdesc->txctl,
+			       ThisTXdesc->txdata,
+			       priv->TxBase->txctl,
+			       priv->TxBase->txdata
+			      );
+#endif
+
+		/* Quick simple test (also reduces execution time)
+		 * Check if current descriptor's used bit is set.
+		 * If not, under normal circumstances there should be nothing
+		 * to do (NOTE: may have to add code here to check for inactivity
+		 * to recover from transmit timeout here if queue has entries that are not
+		 * "moving")
+		 */
+		if ((ThisTXdesc->txctl & GEM_TX_DESCR_USED) == 0)
+		{
+
+			// Current entry is still owned by GEM, done processing exit loop
+#if 0
+			printk(KERN_INFO "TX descriptor used bit zero index:%u\n",priv->Txdone);
+#endif
+			break;
+		}
+		//
+		// Current entry is used, so this packet has been sent or there was some error
+		// in sending.  Check if error.  This is important as if there was a
+		// fatal error in sending, the GEM stops transmitting on all queues
+		// and we need to restart it
+		//
+		if (ThisTXdesc->txctl & (  GEM_TX_DESCR_RETRY_EXCEEDED     /* Bit 29 */
+		                         | GEM_TX_DESCR_UNDERRUN_BIT       /* Bit 28 */
+		                         | GEM_TX_DESCR_FRM_CORRUPT_BIT    /* Bit 27 */
+		                         | GEM_TX_DESCR_LATE_COLLISION_BIT /* Bit 26 */
+		                        )
+		   )
+		{
+#if 0
+			printk(KERN_INFO "TX descriptor Error bits detected index:%u\n",priv->Txdone);
+#endif
+			//
+			// If we are here, then the TX queue is non-empty (has pending entries)
+			// Current descriptor used bit is set, and also there are error bits
+			// in the current descriptor.
+			// This means the GEM is now STOPPED and all transmissions are done
+			// and this frame should not have been transmitted
+			//
+			// Try and restart the GEM, then come back later and try again...
+			//
+			// TODO: This logic only works with single queue.
+			// To support multiple queues, we will need to check the
+			// the other queues for active data and only restart
+			// the GEM if there is pending data in the other
+			// queues.  For now, we just restart the queue
+			// if there is an error and there is still data pending
+			// in the queue
+			//
+			// TODO: May have to add SW detected error counters for any
+			// errors not covered by GEM statistics
+			//
+			// Clear used and error bits, leave all other bits alone
+			//
+			ThisTXdesc->txctl &= ~(  GEM_TX_DESCR_RETRY_EXCEEDED     /* Bit 29 */
+		                               | GEM_TX_DESCR_UNDERRUN_BIT       /* Bit 28 */
+		                               | GEM_TX_DESCR_FRM_CORRUPT_BIT    /* Bit 27 */
+		                               | GEM_TX_DESCR_LATE_COLLISION_BIT /* Bit 26 */
+		                               | GEM_TX_DESCR_USED               /* Bit 31 */
+		                              );
+			//
+			// If data still in the queue, it needs a restart
+			//
+			if (   (priv->Txqueued > 1)
+			    || (ThisTXdesc->txctl & GEM_TX_DESCR_LEN_MASK) // Check if current length non-zero
+			   )
+			{
+				//
+				// Error hit and there is still data in the queue
+				//
+				/* Issue TX start and enable command to GEM */
+#if 0
+				printk(KERN_INFO "TX restart, non-empty queue index:%u\n",priv->Txdone);
+#endif
+				gem_start_tx(&priv->gemdev);
+			}
+			//
+			// Restart or not, with this error, we need to exit the free loop
+			// as this is now the start of the queue.
+			//
+			break;
+		}
+		//
+		// Used bit is set, and there are no errors,
+		// OK to proceed with freeing up this descriptor
+		// (or set of descriptors)
+		// Last descriptor is indicated by last bit set in the descriptor
+		//
+		do
+		{
+			//
+			// For each descriptor, see if there
+			// is an associated socket buffer and
+			// if so release it.  Also re-initialize
+			// fields of descriptors so they can be used
+			// for future transmissions
+			//
+			length      = ThisTXdesc->txctl & GEM_TX_DESCR_LEN_MASK;
+//#if 0
+#ifdef GEM_ETHDRV_TEMP_DEBUG
+			if ((priv->Txtosend > 1020) || (priv->Txdone > 1020))
+			printk(KERN_INFO "%s: FR a:%04u s:%04u d:%04u p:%p f:%08X tp:%p\n",
+				dev->name,
+				priv->Txavail,                   //a:
+				priv->Txtosend,                  //s:
+				priv->Txdone,                    //d:
+				(void *)ThisTXdesc->txdata,      //p:
+				ThisTXdesc->txctl,               //f: (flags)
+				priv->TxSkbRing[priv->Txdone]    //tp:
+				);
+
+#endif
+			/*
+			 * Process descriptor for transmission completion
+			 * A transmission is completed when the first descriptor's USED
+			 * bit is set.  Note that for transmissions where there
+			 * is more than one descriptor per ethernet frame sent,
+			 * only the first descriptor's USED bit is set.
+			 * To process all the buffers for a set, we must release
+			 * all descriptors in the set until the last bit is seen
+			 * length of this descriptor is non-zero
+			 */
+
+			if (
+			        (length != 0)         /* Test Length non-zero */
+			     && (ThisTXdesc->txdata)  /* Make sure descriptor pointer is not NULL */
+			   )
+			{
+				/* This descriptor has valid data that needs processing
+				 */
+				/* First return memory for this descriptor back to
+				 * Linux CPU by performing an unmap operation
+				 */
+				/* Unmap a single streaming mode DMA translation.
+				 * The handle and size must match what was provided in
+				 * the previous dma_map_single call.
+				 * All other usages are undefined.
+				 * After this call, reads by the CPU to the buffer are
+				 * guaranteed to see
+				 * whatever the device wrote there.
+				 */
+				#ifndef GEM_DONOT_UNMAP_TX_BUF
+				dma_unmap_single(&dev->dev,
+				                 ThisTXdesc->txdata,
+				                 length,
+				                 DMA_TO_DEVICE
+				                );
+				#endif
+			}
+			/*
+			 * Test if LAST bit is set, if not, then this was a frame
+			 * transmitted using more than descriptor.  If not,
+			 * then this we are either at the end of a
+			 * multi-descriptor sequence for the
+			 * tranmitted frame or there was only one descriptor
+			 * for the frame to begin with.
+			 */
+			if (ThisTXdesc->txctl & GEM_TX_DESCR_LAST_BUF_BIT)
+			{
+				/* Last bit is set, set Last bit pending to
+				 * false to terminate this inner loop
+				 */
+				LastBitPending = 0;
+
+				/* The index of the last or only descriptor
+				 * of a sequence is also used
+				 * to store the socket buffer pointer in
+				 * the socket buffer ring.
+				 * Get the pointer to the current socket buffer pointer
+				 * and test if it is non-zero
+				 */
+				//if(1)
+				{
+					/*
+					 * Socket buffer pointer is non-zero,
+					 * stop the DMA and release the
+					 * socket buffer back to the kernel
+					 */
+					if ((skb = priv->TxSkbRing[priv->Txdone]) != NULL)
+					{
+						dev_kfree_skb(skb);
+					}
+					else
+					{
+						// Socket buffer may be NULL if non-sbk packet was sent out
+						// by using r-eth device
+						extern unsigned long (*__dma_ext_unmap_mem)(const void * paddr, size_t size, unsigned dir);
+						if (__dma_ext_unmap_mem != NULL)
+							t3300_reth_free_fwd_tx (__dma_ext_unmap_mem(ThisTXdesc->txdata, 0, 0));
+					}
+					tx_cleaned++;
+					priv->Txqueued--;
+					priv->TxSkbRing[priv->Txdone] = NULL;
+				}
+				#if 0
+				else
+				{
+					/*
+					 * Socket buffer pointer is NULL for a
+					 * transmitted packet, this
+					 * should not happen.
+					 * Print error message and exit
+					 */
+					printk(KERN_ERR "%s: %s encountered errors while freeing tx descriptor, socket buffer NULL\n",
+						dev->name,
+						__func__
+						);
+
+					if (lock)
+					    spin_unlock(&priv->txlock);
+					return (0);
+				}
+				#endif
+			}
+			else
+			{
+				/* Last bit is not set for a sequence where the USED bit
+				 * was set at least once in the first descriptor of the
+				 * list.
+				 * This means this is a multi-descriptor
+				 * frame sequence and we need to keep freeing descriptors'
+				 * data until the last bit is seen.
+				 */
+				LastBitPending = 1;  /* Set LAST bit is still pending */
+				skb            = 0;  /* Clear temporary socket buffer pointer */
+				
+				if (priv->TxSkbRing[priv->Txdone] == NULL)
+				{
+				    // Socket buffer may be NULL if non-sbk packet was sent out
+				    // by using r-eth device
+				    extern unsigned long (*__dma_ext_unmap_mem)(const void * paddr, size_t size, unsigned dir);
+				    if (__dma_ext_unmap_mem != NULL)
+					t3300_reth_free_fwd_tx (__dma_ext_unmap_mem(ThisTXdesc->txdata, 0, 0));
+				}
+			}
+			//
+			// Update Pointers and control information
+			//
+			/*
+			 * Now that socket buffer is returned (if present), setup TX descriptor
+			 * to allow for another packet to be transmitted.
+			 */
+			ThisTXdesc->txdata = 0;                     /* Clear current descriptor TX data pointer */
+			/*
+			 * AKB TODO: Need to also check for errors here
+			 * For now, clear all bits except WRAP bit
+			 * then make sure USED and LAST bits is set
+			 */
+			ThisTXdesc->txctl  = (ThisTXdesc->txctl & GEM_TX_DESCR_WRAP)
+			                   | (GEM_TX_DESCR_USED | GEM_TX_DESCR_LAST_BUF_BIT)
+			                   ;
+
+			/* Set current socket buffer ring entry to NULL,
+			 * bump the TX avaiable count and cleaned count
+			 * and set "done" entry to next entry based on the Ring size
+			 * NOTE: TxRingSize must be set to a power of 2 in the #define section
+			 * for the code below to work.
+			 */
+			priv->TxSkbRing[priv->Txdone] = NULL;
+			/*
+			 * Increment the number of available descriptors in the
+			 * Tx descriptor ring
+			 */
+			priv->Txavail++;
+			//
+			// Update Transmit done index
+			//
+			priv->Txdone = (priv->Txdone + 1) & (priv->TxRingSize - 1);
+
+			if (LastBitPending)
+			{
+				//
+				// Still more to do in this descriptor list, setup "This descriptor"
+				// as next.
+				//
+				ThisTXdesc  =  priv->TxBase + priv->Txdone;
+			}
+			//
+			// Interior loop, loop until last bit is set in current descriptor
+			// If still pending, then keep looping until last bit set
+			//
+		} while (LastBitPending);
+		//
+		// Exterior loop, loop until no more work to
+		// to (either used bit clear, meaning GEM is
+		// still working on it, or ring has been emptied
+		//
+	} // while (1)
+
+	if (unlikely(priv->Txavail && netif_queue_stopped(dev)))
+	{
+		//AKB TEMP: PRINT ALL DEBUG MESSAGES if (unlikely(netif_msg_intr(priv)))
+		printk(KERN_DEBUG "%s  t2200_eth_free_tx_packets :  netif_wake_queue \n", dev->name);
+		netif_wake_queue(dev);
+	}
+
+	if (lock)
+	    spin_unlock(&priv->txlock);
+
+	return(tx_cleaned);
+}
+
+int proc_reth_tx_pack (struct net_device * dev, void * pVPtr, u32 size)
+{
+	struct eth_c4k_priv * priv = netdev_priv(dev); /**< Pointer to GEM network driver private data */
+	GEM_DEVICE *          gemdev = &priv->gemdev;  /**< Pointer to GEM structure to R/W registers */
+	int                   err = 0;
+	int                   Txavail = priv->Txavail; /**< Number of available (empty/unused) slots in the TX descriptor ring */
+	u32                   i = 0, txctrl;
+	int                   length  = size;  /**< Amount to send taken from socket buffer length */
+	struct tTXdesc *      ThisTXdesc, *BlockTXdesc;          /**< Current Transmit descriptor used to send actual frame/packet */
+	int dma_map_len;
+
+#ifdef BUG_69620_WORKAROUND
+	struct tTXdesc *      NextTXdesc;          /**< Current Transmit descriptor used to send actual frame/packet */
+#endif
+	//
+	// Transmit multiple frames at a time using circular queuing
+	// processing for the Transmit Descriptor running as a ring.
+	//
+	/* Save pointer to socket buffer structure for later release
+	 * on completion of transmit
+	 */
+	priv->TxSkbRing[priv->Txtosend] = NULL;
+	tx_sched_time[priv->id][priv->Txtosend] = REG32(0xFE050004); //jiffies;
+
+	/* Get pointer to current descriptor to send to */
+	ThisTXdesc = &(priv->TxBase[priv->Txtosend]);
+
+	/* Update transmit ring queue for next entry and decrement
+	 * number of available slots
+	 * Note as code is written, this must be a power of 2 number for
+	 * total number of descriptors
+	 */
+	priv->Txtosend = (priv->Txtosend + 1) & (priv->TxRingSize - 1);
+	priv->Txavail--;
+	priv->Txqueued++;
+
+	// Use actual socket buffer data's pointer
+	// and setup descriptor pointer to it (no copy, much more efficient
+	/* Ensure that any data held in the cache is appropriately discarded or written back. */
+	dma_map_len = length;
+	ThisTXdesc->txdata = dma_map_single(&dev->dev, pVPtr, dma_map_len, DMA_TO_DEVICE);
+
+#ifdef BUG_69620_WORKAROUND
+
+	if ((pinfo->map_fr_num & 1)!=0 || pinfo->map_fr_num == 0){
+	/* Calculate next buffer to send based on ring size */
+	NextTXdesc = &(priv->TxBase[priv->Txtosend]);
+
+	/* Update transmit ring queue for next entry and decrement
+	 * number of available slots
+	 * Note as code is written, this must be a power of 2 number for
+	 * total number of descriptors
+	 */
+	priv->Txtosend = (priv->Txtosend + 1) & (priv->TxRingSize - 1);
+	priv->Txavail--;
+
+	/* Implement workaround, setup next descriptor as "dummy"
+	 * descriptor with zero length frame descriptor and
+	 * NULL buffer pointer
+	 */
+	NextTXdesc->txctl  &= GEM_TX_DESCR_WRAP; // Clear all control bits except the wrap bit
+	NextTXdesc->txdata = 0;
+	}
+
+#endif
+	/* Setup Current Transmit descriptor  (last bit set)
+	 * Set Length bit
+	 */
+	i  = ThisTXdesc->txctl; // Get current descriptor value (including wrap bit set if last entry)
+	i &= GEM_TX_DESCR_WRAP; // Clear all but wrap bit, this will also clear the used bit and any error bits
+	i |= GEM_TX_DESCR_LAST_BUF_BIT | length;
+
+	ThisTXdesc->txctl = i;  // Write new descriptor with used bit clear, transfer owner from CPU to GEM
+
+	/* Issue TX start and enable command to GEM, if GEM already started
+	 * this will do no harm
+	 */
+	gem_start_tx(gemdev);
+
+	/* Normal exit */
+	/* Bump transmit statistics */
+	priv->stats.tx_packets++;
+	priv->stats.tx_bytes += length;
+
+	return(err);
+}
+
+int proc_reth_fastpath_pack (struct net_device * dev, fastpath_pack * ppack)
+{
+	struct eth_c4k_priv * priv = netdev_priv(dev); /**< Pointer to GEM network driver private data */
+	GEM_DEVICE *          gemdev = &priv->gemdev;  /**< Pointer to GEM structure to R/W registers */
+	u32                   i = 0, txctrl;
+	int                   len, total_len;		/**< Amount to send taken from socket buffer length */
+	struct tTXdesc *      ThisTXdesc;	/**< Current Transmit descriptor used to send actual frame/packet */
+
+	if (priv->Txavail < ppack->fragnum)
+	{
+	    printk(KERN_DEBUG "%s proc_reth_fastpath_pack, drop tx packet, no space in TX GEMAC-DMA\n", dev->name);
+	    return -1;
+	}
+
+	// Transmit multiple frames at a time using circular queuing
+	// processing for the Transmit Descriptor running as a ring
+	tx_sched_time[priv->id][priv->Txtosend] = REG32(0xFE050004);
+
+	for (i = 0; i < ppack->fragnum; i++)
+	{
+	    len = ppack->frags[i].size;
+	    total_len += len;
+	
+	    priv->TxSkbRing[priv->Txtosend] = NULL;
+
+	    ThisTXdesc = &(priv->TxBase[priv->Txtosend]);
+	    ThisTXdesc->txdata = dma_map_single(&dev->dev, ppack->frags[i].data, len, DMA_TO_DEVICE);
+
+	    /* Setup Current Transmit descriptor
+	     * Set Length bit*/
+
+	    txctrl = ThisTXdesc->txctl;		// Get current descriptor value (including wrap bit set if last entry)
+	    txctrl &= GEM_TX_DESCR_WRAP;	// Clear all but wrap bit, this will also clear the used bit and any error bits
+	
+	    if (i+i >= ppack->fragnum)
+	    {
+		txctrl |= GEM_TX_DESCR_LAST_BUF_BIT | len;
+	    }
+	    else
+	    {
+		txctrl |= len;
+	    }
+
+	    ThisTXdesc->txctl = txctrl;  // Write new descriptor with used bit clear, transfer owner from CPU to GEM
+
+	    /* Update transmit ring queue for next entry and decrement
+	     * number of available slots
+	     * Note as code is written, this must be a power of 2 number for
+	     * total number of descriptors
+	     */
+	    priv->Txtosend = (priv->Txtosend + 1) & (priv->TxRingSize - 1);
+	    priv->Txavail--;
+	    priv->Txqueued++;
+	}
+
+	gem_start_tx(gemdev);
+
+	priv->stats.tx_packets++;
+	priv->stats.tx_bytes += total_len;
+
+	return 0;
+}
+
+
+int proc_reth_tx (struct net_device * dev, struct eth_c4k_priv * priv)
+{
+	int err = 0;
+	u32 size;
+	void * pVPtr;
+	unsigned long flags;
+	fastpath_pack * ppack;
+
+	if (unlikely(!spin_trylock_irqsave(&priv->txlock,flags)))
+	{
+		/* Collision - tell upper layer to requeue */
+		//printk(KERN_DEBUG "%s %s: TX collision\n", dev->name, __func__);
+		return NETDEV_TX_LOCKED;
+	}
+
+	t2200_eth_free_tx_packets(dev, 0);
+
+	while ( (pVPtr = t3300_reth_get_forwarded_tx (&size)) != NULL)
+	{
+		#if 0
+		printk ("r-eth: forwarded-tx (vmem:%x, size:%u), mymac: %02x %02x %02x %02x %02x %02x\n", (u32)pVPtr, size,
+				dev->dev_addr[0],
+				dev->dev_addr[1],
+				dev->dev_addr[2],
+				dev->dev_addr[3],
+				dev->dev_addr[4],
+				dev->dev_addr[5]
+			);
+		#endif
+		err = proc_reth_tx_pack (dev, pVPtr, size);
+	}
+
+	while ( (ppack = t3300_reth_get_fastpath_pack()) != NULL)
+	{
+		err = proc_reth_fastpath_pack (dev, ppack);
+		if (err != 0)
+		{
+		    // Drop the packet,
+		    // mainly this is abnormal situation, because
+		    // GEMAC TX queue is very long and also we have
+		    // hang-detector mechanism which guarantees we do not
+		    // overflow this queue
+
+		    t3300_reth_get_fastpath_drop_pack(ppack);
+		    break;
+		}
+	}
+
+	spin_unlock_irqrestore(&priv->txlock, flags);
+	return err;
+}
+
+/**
+ * Function that is setup polled on a periodic and frequent basis.
+ *
+ * @par
+ * This function first calls t2200_eth_free_tx_packets to
+ * inspect the transmit queue for completed packets
+ * and if any are completed, the socket buffer is released
+ * and the queue is updated.
+ *
+ * @par
+ * It next goes to check if any receive packets are available
+ * and process the receive queue accordingly
+ *
+ * @return
+ * Returns number of packets received or zero
+ * if no packets are received on this call to the poll function
+ */
+int t2200_eth_poll(struct napi_struct *napi, int budget)
+{
+	struct eth_c4k_priv * priv       = container_of(napi, struct eth_c4k_priv, napi);
+	struct net_device *   dev        = priv->dev;
+	GEM_DEVICE *          gemdev     = &priv->gemdev;
+	unsigned int          work_to_do = budget;
+	unsigned int          work_done  = 0;
+	//int                   rc;
+	int                   tx_cleaned;
+
+	//printk("t2200_eth_poll, budget:%u\n",budget);
+
+#ifdef GEM_TX_ONE_DESCRIPTOR_AT_A_TIME
+	tx_cleaned = 0;
+#else
+	/* Free up any completed transmitted socket buffers and descriptors
+	 * in the transmit queue (TX ring)
+	 */
+	//tx_cleaned = t2200_eth_free_tx_packets(dev, 1);
+	tx_cleaned = 0;
+#endif
+
+	if (t3300_reth_is_enabled ())
+		proc_reth_tx (dev, priv);
+
+	//if (unlikely(netif_msg_intr(priv)))
+	//	printk(KERN_DEBUG "%s: %s %d tx descriptors freed\n", dev->name, __func__, tx_cleaned);
+
+	/* Process the receive buffer queue (RX ring) for any current packets */
+	t2200_eth_rx_packet(dev, &work_done, work_to_do);
+
+	/* Refill the receive queue's socket buffers as necessary */
+	if (pSkbRecycler)
+	{
+		pSkbRecycler->refill(dev, pSkbRecycler);
+	}
+	else
+		t2200_eth_rx_refill(dev, 2048);
+
+	//if (unlikely(netif_msg_intr(priv)))
+	//	printk(KERN_DEBUG "%s %s: work_done %d\n", dev->name, __func__, work_done);
+
+#if(1)
+    if(work_done < budget)
+    {
+        // Complete current napi to avoid extra call of t2200_eth_poll
+        if (t2200_thread[priv->id] == NULL)
+            napi_complete(napi);
+
+        // reenable IRQ: will be reenabled only if polling is disabled
+        // in the bootargs
+        gem_enable_irq(gemdev,         GEM_IRQ_RX_DONE_FLAG);
+
+#ifndef CONFIG_GLOBAL_POLLING
+        // restart timer: will be restarted if polling is enabled
+        t2200_start_timer(dev);
+#endif /* !CONFIG_GLOBAL_POLLING */
+    }
+
+#else
+
+    //
+    // This part is commented as obsolete, substituted by the code above
+    //
+
+	/* Disable polling and enable receive interrupts when:
+	 *  - There are no TX buffers to be returned/cleaned
+	 *  - The work done flag is 0
+	 *  - The return code from refilling the receive buffer rings is 0
+	 */
+	if ((!tx_cleaned && !work_done && !rc))
+	{
+		//
+		// Time to turn off polling and enable receive interrupts
+		//
+		if (t2200_thread[priv->id] == NULL)
+			napi_complete(napi);
+		// if (unlikely(netif_msg_intr(priv)))
+		//	printk(KERN_DEBUG "%s %s: re-enable irqs\n", dev->name, __func__);
+
+
+		// basically re-enable interrupt
+		//re-enable irq for this port(exit polling)
+		gem_enable_irq(gemdev, 	GEM_IRQ_RX_DONE_FLAG);
+
+#ifndef CONFIG_GLOBAL_POLLING
+		t2200_start_timer(dev);
+#endif /* !CONFIG_GLOBAL_POLLING */
+	}
+	else
+		work_done = budget;
+#endif
+
+	return work_done;		//return received packet count
+}
+
+/** Function to process a GEM receive interrupt (when enabled) */
+irqreturn_t t2200_eth_rx_interrupt(int irq, void *dev_id)
+{
+	struct net_device *   dev = (struct net_device *)dev_id;
+	struct eth_c4k_priv * priv = netdev_priv(dev);
+	GEM_DEVICE *          gemdev = &priv->gemdev;
+	struct napi_struct *  napi = &priv->napi;
+	unsigned              irq_stat;
+	u32                     utemp;
+    struct skb_shared_hwtstamps shhwtstamps;
+
+	//printk("t2200_eth_rx_interrupt");
+	//if (unlikely(netif_msg_intr(priv)))
+	//	printk(KERN_DEBUG "%s %s: irq %d\n", dev->name, __func__, irq);
+
+    // since all GEM interrupts are using one single interrupt line to ARM A9, we have to share it, and query status to check what to do:
+    irq_stat = gem_get_irq_stat(gemdev);
+
+    //most likely this is a receiving interrupt.  ONly a small chance  <1% of  PTP transmit interrupt. )
+    if (likely(irq_stat &   GEM_IRQ_RX_DONE))
+    {
+	if (likely(napi_schedule_prep(napi))) {
+	    gem_mask_irq(gemdev, GEM_IRQ_RX_DONE_FLAG);
+	    __napi_schedule(napi);
+	}
+	else if (unlikely(netif_running(dev))) {
+	    gem_mask_irq(gemdev, GEM_IRQ_RX_DONE_FLAG);
+	//   printk(KERN_ERR "%s %s: bug! interrupt while in poll\n", dev->name, __func__);
+	}
+    }
+#if 0
+    else
+    {
+                printk("irq_stat is %x\n", 	irq_stat);
+    }
+#endif
+    //below 4 interrupts for for PTP transmit packets out.
+    /* we normally only come here once or twice every seconds or so, so less likely to impact performance much */
+    if ( unlikely (  irq_stat & (GEM_INT_TSU_TX_PTP_DLY_REQ
+                          |GEM_INT_TSU_TX_PTP_SYNC_FRM
+                          |GEM_INT_TSU_TX_PTP_PDLY_REQ
+                          |GEM_INT_TSU_TX_PTP_PDLY_RESP
+                          |GEM_INT_TSU_SECONDS_INCREMENT)  )  )
+    {
+
+        //we assume only one interrupt is generated since only one PTP packet can be transmitted a time due to hardware limitation.
+        //if  the interrupt here, then we store the value of timer event registers and wait for ioctl to query
+        //below 4 interrupts for for PTP transmit packets out.
+        //timer second register value increased, this will generate interrupt every one second
+        switch ( irq_stat & (GEM_INT_TSU_TX_PTP_DLY_REQ
+                              |GEM_INT_TSU_TX_PTP_SYNC_FRM
+                              |GEM_INT_TSU_TX_PTP_PDLY_REQ
+                              |GEM_INT_TSU_TX_PTP_PDLY_RESP
+                              |GEM_INT_TSU_SECONDS_INCREMENT) )
+        {
+            case  GEM_INT_TSU_TX_PTP_DLY_REQ:
+            case  GEM_INT_TSU_TX_PTP_SYNC_FRM:
+                priv->tx_ptp_time.tv_sec = gem_get_tsu_ptp_tx_sec(&priv->gemdev);
+                priv->tx_ptp_time.tv_nsec   = gem_get_tsu_ptp_tx_nsec(&priv->gemdev);
+                priv->valid_tx_ptp = 1;
+            break;
+
+            case  GEM_INT_TSU_TX_PTP_PDLY_REQ:
+            case  GEM_INT_TSU_TX_PTP_PDLY_RESP:
+                priv->tx_ptp_time.tv_sec = gem_get_tsu_ptp_peer_tx_sec(&priv->gemdev);
+                priv->tx_ptp_time.tv_nsec   = gem_get_tsu_ptp_peer_tx_nsec(&priv->gemdev);
+                priv->valid_tx_ptp = 1;
+            break;
+
+            case  GEM_INT_TSU_SECONDS_INCREMENT:
+                utemp =  REG32(priv->gemdev.registers + GEM_1588_TIMER_SECONDS);
+                REG32(priv->gemdev.registers +  GEM_1588_TIMER_COMPARE_SECONDS) = utemp + 1;
+            break;
+
+            default:
+                printk("we have error\n");
+                printk("irq_stat is %x\n", 	irq_stat);
+            break;
+        }
+    }
+
+    gem_set_irq_stat(gemdev, irq_stat);
+
+	/* Get receive interrupt status from GEM and write it back
+	 * this will clear the interrupt in the GEM
+	 */
+
+	irq_stat = gem_get_irq_stat(gemdev);
+	gem_set_irq_stat(gemdev, irq_stat);
+
+	return IRQ_HANDLED;
+}
+
+/** Mapping of Network device operations to GEM C code functions */
+static const struct net_device_ops t4k_netdev_ops = {
+	.ndo_open               = t2200_eth_open,
+	.ndo_start_xmit         = t2200_eth_send_packet,
+	.ndo_stop               = t2200_eth_close,
+	.ndo_do_ioctl           = t2200_eth_ioctl,
+	.ndo_get_stats          = t2200_eth_get_stats,
+	.ndo_change_mtu         = t2200_eth_change_mtu,
+	.ndo_set_multicast_list = t2200_eth_set_multi,
+	.ndo_set_mac_address    = t2200_set_mac_address,
+};
+
+static int t2200_eth_probe(struct platform_device *pdev)
+{
+	struct net_device *                  dev = NULL;
+	struct eth_c4k_priv *                priv = NULL;
+	GEM_DEVICE *                         gemdev;
+	struct transcede_eth_platform_data * einfo;
+	struct resource *                    r;
+	int                                  idx;
+	int                                  err;
+
+	#ifdef GEMAC_MAX_TX_PROC_TIME
+	    printk ("GEMAC (tx-reset:%d, (clean-treshold:%d, tx-unmap:", GEMAC_MAX_TX_PROC_TIME, GEM_TX_CLEAN_THRESHOLD);
+	    #ifndef GEM_DONOT_UNMAP_TX_BUF
+	    printk ("ON)\n\n");
+	    #else
+	    printk ("OFF)\n\n");
+	    #endif
+	#else
+	    printk ("GEMAC non-tx-reset\n\n");
+	#endif
+
+	printk(KERN_INFO "%s: gemac %d\n", __func__, pdev->id);
+
+	einfo = (struct transcede_eth_platform_data *) pdev->dev.platform_data;
+	if (!einfo) {
+		printk(KERN_ERR "%s: gemac %d missing additional platform data\n", __func__, pdev->id);
+		err = -ENODEV;
+		goto err0;
+	}
+
+	// Create an ethernet device instance
+	dev = alloc_etherdev(sizeof (*priv));
+	if (!dev) {
+		printk(KERN_ERR "%s: gemac %d device allocation failed\n", __func__, pdev->id);
+		err = -ENOMEM;
+		goto err0;
+	}
+
+	priv      = netdev_priv(dev);
+	priv->dev = dev;
+	gemdev    = &priv->gemdev;
+
+	priv->id = pdev->id;
+
+	// Set the info in the priv to the current info
+	priv->einfo = einfo;
+
+	// fill out IRQ fields
+	priv->phys_rx_int = platform_get_irq_byname(pdev, "rx");
+
+	if (priv->phys_rx_int < 0) {
+		printk(KERN_ERR "%s: gemac %d missing resource information\n", __func__, pdev->id);
+		err = -EINVAL;
+		goto err1;
+	}
+
+	/* get a pointer to the resource data for the GEM in order
+	 * to get the pointer for this GEM's base pointer for GEM
+	 * registers (different pointer for GEM0 versus GEM1)
+	 */
+	r = platform_get_resource_byname(pdev, IORESOURCE_MEM, "gemac");
+	if (!r) {
+		printk(KERN_ERR "%s: gemac %d missing resource information\n", __func__, pdev->id);
+		err = -EINVAL;
+		goto err1;
+	}
+
+	priv->baseaddr = (void *)(r->start);
+
+	// Kernel may need to access registers before device is fully opened
+	gemdev->gemac_baseaddr = priv->baseaddr;
+	gemdev->registers      = priv->baseaddr + GEM_IP;
+
+	/* allocate Descriptors from fixed location specified in the board resource definition */
+#if 0
+	printk(KERN_INFO "%s: gemac %u allocate from DDR\n", __func__, pdev->id);
+	priv->Descriptors_baseaddr    = kmalloc(1024 * 1024, GFP_ATOMIC);
+	priv->Descriptors_baseaddr_pa = (void *) virt_to_phys(priv->Descriptors_baseaddr);
+	priv->Descriptors_baseaddr_v  = (void *) ioremap((unsigned long)priv->Descriptors_baseaddr_pa, 1024 * 1024);
+#else
+	if (pdev->id)
+	{
+		// GEM 1, RGMII allocate descriptors in fixed IRAM address
+		printk(KERN_INFO "%s: gemac %u allocate from IRAM @ 0x%08X\n", __func__, pdev->id, (unsigned int)TRANSCEDE_IRAM_ETH_DESC_BASE);
+		priv->Descriptors_baseaddr    = (void *) TRANSCEDE_IRAM_ETH_DESC_BASE;
+		priv->Descriptors_baseaddr_pa = priv->Descriptors_baseaddr;
+		priv->Descriptors_baseaddr_v  = priv->Descriptors_baseaddr;
+	}
+	else
+	{
+		// GEM 0, SGMII allocate descriptors from DDR heap
+		printk(KERN_INFO "%s: gemac %u allocate from DDR\n", __func__, pdev->id);
+		priv->Descriptors_baseaddr    = kmalloc(1024 * 1024, GFP_ATOMIC);
+		priv->Descriptors_baseaddr_pa = (void *) virt_to_phys(priv->Descriptors_baseaddr);
+		priv->Descriptors_baseaddr_v  = (void *) ioremap((unsigned long)priv->Descriptors_baseaddr_pa, 1024 * 1024);
+	}
+#endif
+
+	if ( !priv->Descriptors_baseaddr_v ) {
+		printk(KERN_ERR "%s: gemac %d failed to remap rx memory\n", __func__, pdev->id);
+		err = -EINVAL;
+		goto err1;
+	}
+
+	spin_lock_init(&priv->rxlock);
+	spin_lock_init(&priv->txlock);
+
+    INIT_WORK(&priv->tx_hwtstamp_work, t2200_tx_hwtstamp_work);
+	platform_set_drvdata(pdev, dev);
+
+	/* Copy the station address into the dev structure, */
+	memcpy(dev->dev_addr, einfo->mac_addr, MAC_ADDR_LEN);
+
+	err = dev_alloc_name(dev, einfo->name);
+	if (err < 0) {
+		printk(KERN_ERR "%s: cannot allocate net device name %s, aborting.\n", __func__, einfo->name);
+		err = -EINVAL;
+		goto err2;
+	}
+	SET_NETDEV_DEV(dev, &pdev->dev);
+
+	/* Fill in the dev structure (mostly with our function pointers
+	 * to common device driver functions
+	 */
+	dev->netdev_ops		= &t4k_netdev_ops;
+	dev->mtu                = 1500;
+
+#ifdef GEM_TX_CHECKSUM_OFFLOAD_ENABLED
+	dev->features |= NETIF_F_HW_CSUM; // GEM in T2200/T3300 can support TCP, UDP, IPv4 and IPv6 checksums
+#endif
+
+	/* Setup t2200_eth_poll to be run on a regular basis for device polling operations
+	 * (see function above)
+	 * Weight is set to 64 which is recommended value for Gigabit adapters
+	 */
+	netif_napi_add(dev, &priv->napi, t2200_eth_poll, RX_POLL_BUDGET);
+
+	dev->ethtool_ops = &c4k_ethtool_ops; // NOTE: ethtool is shared with C4000/C5000/T4000/T3000
+	priv->flags = 0;
+
+#ifdef GEM_TX_CHECKSUM_OFFLOAD_ENABLED
+	priv->flags |= TX_CSUM_OFFLOAD_ENABLED;
+#endif
+
+#ifdef GEM_RX_CHECKSUM_OFFLOAD_ENABLED
+	if (t2200_get_interface(dev) != PHY_INTERFACE_MODE_SGMII)
+		priv->flags |= RX_CSUM_OFFLOAD_ENABLED;
+#endif
+
+	/* Enable most messages by default */
+	priv->msg_enable = NETIF_MSG_IFUP
+	                 | NETIF_MSG_IFDOWN
+	                 | NETIF_MSG_LINK
+	                 | NETIF_MSG_PROBE
+	                 | NETIF_MSG_INTR;
+
+	/* Register device driver with the system */
+	err = register_netdev(dev);
+	if (err) {
+		printk(KERN_ERR "%s: cannot register net device, aborting.\n", dev->name);
+		goto err2;
+	}
+
+	/* Print out the device info */
+	printk(KERN_INFO DEVICE_NAME, dev->name);
+	for (idx = 0; idx < 6; idx++)
+		printk("%2.2x%c", dev->dev_addr[idx], idx == 5 ? ' ' : ':');
+	printk("\n");
+
+	return 0;
+#if 0
+/* Case for error after registering device via register_netdev call
+ * currently not used...
+ */
+out_unregister:
+	unregister_netdev(dev);
+#endif
+
+err2:
+	platform_set_drvdata(pdev, NULL);
+
+err1:
+	free_netdev(dev);
+err0:
+	return err;
+}
+
+/** Function to set port to full or half duplex */
+static void t2200_gemac_setduplex(struct net_device *dev, int duplex)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+	GEM_DEVICE *gemdev = &priv->gemdev;
+
+	if (duplex == DUPLEX_HALF)
+	{
+		printk(KERN_INFO "%s %s: Half Duplex\n",
+		       dev->name,
+		       __func__
+		      );
+		gem_half_duplex(gemdev);
+	}
+	else
+	{
+		printk(KERN_INFO "%s %s: Full Duplex\n",
+		       dev->name,
+		       __func__
+		      );
+		gem_full_duplex(gemdev);
+	}
+}
+
+/** Function to set port's speed 10, 100 or 1000 Megabits per second */
+static void t2200_gemac_setspeed(struct net_device *dev, int speed)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+	GEM_DEVICE *gemdev = &priv->gemdev;
+
+	printk(KERN_INFO "%s %s: Speed %u Megabits/second\n",
+	       dev->name,
+	       __func__,
+	       speed
+	      );
+
+	switch (speed) {
+	case 10:
+		writel(readl(priv->baseaddr + GEM_IP + GEM_NET_CONFIG) | GEM_RX_BAD_PREAMBLE, priv->baseaddr + GEM_IP + GEM_NET_CONFIG);
+		gem_set_speed(gemdev, SPEED_10M);
+	        break;
+	case 100:
+		writel(readl(priv->baseaddr + GEM_IP + GEM_NET_CONFIG) & ~GEM_RX_BAD_PREAMBLE, priv->baseaddr + GEM_IP + GEM_NET_CONFIG);
+		gem_set_speed(gemdev, SPEED_100M);
+	        break;
+	case 1000:
+	default:
+		writel(readl(priv->baseaddr + GEM_IP + GEM_NET_CONFIG) & ~GEM_RX_BAD_PREAMBLE, priv->baseaddr + GEM_IP + GEM_NET_CONFIG);
+		gem_set_speed(gemdev, SPEED_1000M);
+	        break;
+	}
+}
+
+static void t2200_adjust_link(struct net_device *dev)
+{
+	struct eth_c4k_priv * priv      = netdev_priv(dev);
+	unsigned long         flags;
+	struct phy_device *   phydev    = priv->phydev;
+	int                   new_state = 0;
+
+	if (unlikely(netif_msg_drv(priv) && net_ratelimit()))
+		printk(KERN_DEBUG "%s t2200_adjust_link\n", dev->name);
+
+	spin_lock_irqsave(&priv->txlock, flags);
+
+	/* Check if PHY link status is currently up or down */
+	if (phydev->link)
+	{
+		/* Link ix up */
+		/* Check if duplex status has changed (half duplex
+		 * to full, or full duplex to half
+		 */
+		if (phydev->duplex != priv->oldduplex)
+		{
+			/* Duplex status has changed, call function
+			 * to change duplex status for this GEM
+			 */
+			new_state = 1;
+
+			t2200_gemac_setduplex(dev, phydev->duplex);
+			priv->oldduplex = phydev->duplex;
+
+		}
+
+		/* Check if speed has changed */
+		if (phydev->speed != priv->oldspeed)
+		{
+			/* Speed status has changed, call function
+			 * to change speed for this GEM
+			 */
+			new_state = 1;
+			t2200_gemac_setspeed(dev, phydev->speed);
+			priv->oldspeed = phydev->speed;
+		}
+
+		/* Check if link status has changed from down to up */
+		if (!priv->oldlink)
+		{
+			/* Status has gone from down to up, startup Network interface
+			 * (inform upper layer)
+			 */
+			new_state = 1;           /* Link state changed */
+			priv->oldlink = 1;       /* Link up            */
+			netif_start_queue(dev);  /* Inform upper layers that link is OK now */
+		}
+
+	}
+	else if (priv->oldlink)
+	{
+		new_state       =  1; /* Link state changed */
+		priv->oldlink   =  0; /* Link down          */
+		priv->oldspeed  =  0; /* Unknown speed      */
+		priv->oldduplex = -1; /* Unknown duplex     */
+	}
+
+	/* Print message if link status has changed */
+	if (new_state && netif_msg_link(priv))
+	{
+		phy_print_status(phydev);
+	}
+
+	spin_unlock_irqrestore(&priv->txlock, flags);
+}
+
+static int t2200_eth_remove(struct platform_device *pdev)
+{
+	struct net_device *dev = platform_get_drvdata(pdev);
+
+	unregister_netdevice(dev);
+
+	platform_set_drvdata(pdev, NULL);
+
+	/* Free the device, then exit */
+	free_netdev(dev);
+
+	return 0;
+}
+
+
+/** Structure for a device driver */
+static struct platform_driver t2200_eth_driver = {
+	.probe  = t2200_eth_probe,
+	.remove = t2200_eth_remove,
+	.driver	= {
+	.name = "t2200-eth",
+	},
+};
+
+/**
+ *	This is a kernel command line parameter. The format is as following:
+ *	hwaddress=<interface name>,<mac address>,<interface name>,<mac address> ....
+ *	This parameter is mainly use when mounting the root filesytem over NFS.
+ *	For all the other cases, the MAC address will be given through the ifconfig application
+ *	i.e: ifconfig <interface name> hw ether <mac address>
+ */
+static int __init hwaddress_setup(char *str)
+{
+	int             index = 0;
+	int             i     = 0;
+	unsigned char * pchar = (unsigned char*) str;
+
+	for (i = 0; i < 2; i++) {
+		if (strncmp(pchar, ETH1, 4) == 0) {
+			pchar = strpbrk(pchar,",");
+			++pchar;
+			index = 0;
+
+			while (pchar && (index < ETH_ALEN)) {
+				if (pchar) {
+					unsigned char tmp = simple_strtol(pchar, NULL, 16);
+					transcede_gem0_pdata.mac_addr [index++] = (unsigned char)tmp;
+					pchar +=3;
+				}
+			}
+		} else if (strncmp(pchar, ETH2, 4) == 0) {
+			pchar = strpbrk(pchar,",");
+			++pchar;
+			index = 0;
+
+			while (pchar && (index < ETH_ALEN)) {
+				if (pchar) {
+					unsigned char tmp = simple_strtol(pchar, NULL, 16);
+					transcede_gem1_pdata.mac_addr [index++] = (unsigned char)tmp;
+					pchar +=3;
+				}
+			}
+		}
+	}
+
+	return 1;
+}
+
+__setup("hwaddress=", hwaddress_setup);
+
+static int __init ethpoll_setup(char *str)
+{
+    t2200_eth_rx_latency = simple_strtol(str, NULL, 10);
+
+#ifdef CONFIG_GLOBAL_POLLING
+    /* us -> ms with round up */
+    t2200_eth_rx_latency = (1000 - 1 + t2200_eth_rx_latency) / 1000;
+#endif	/* CONFIG_GLOBAL_POLLING */
+
+    if (t2200_eth_rx_latency != 0)
+	GEM_IRQ_RX_DONE_FLAG = 0;
+
+    return 1;
+}
+
+__setup("ethpoll=", ethpoll_setup);
+
+static int __init recycler_setup(char *str)
+{
+    recycler_ctrl = simple_strtol(str, NULL, 10);
+
+    if ((recycler_ctrl & RECYCLER_RX_ON) == 0)
+    {
+	printk ("GEM: RX recycler feature is completely turned off\n");
+	recycler_ctrl = 0;
+    }
+
+    return 1;
+}
+
+__setup("recycler=", recycler_setup);
+
+static int __init eth_thread_on_setup(char *str)
+{
+    t2200_eth_thread_on = simple_strtol(str, NULL, 10);
+    if (t2200_eth_thread_on == 0)
+        printk ("T2200 ETH: thread(s) is disabled in Ethernet driver\n");
+    else
+        printk ("T2200 ETH: thread(s) is allowed in Ethernet driver\n");
+    return 1;
+}
+
+__setup("eththread=", eth_thread_on_setup);
+
+/** Standard linux module initialization function for Transcede ethernet driver */
+static int __init t2200_eth_init(void)
+{
+	return platform_driver_register(&t2200_eth_driver);
+}
+
+/** Standard linux module exit function for Transcede ethernet driver */
+static void __exit t2200_eth_exit(void)
+{
+	platform_driver_unregister(&t2200_eth_driver);
+}
+
+module_init(t2200_eth_init);
+module_exit(t2200_eth_exit);
+
+/* eof t2200_eth.c */
diff --git a/drivers/net/transcede/t2200_eth.h b/drivers/net/transcede/t2200_eth.h
new file mode 100644
index 0000000..c3c8333
--- /dev/null
+++ b/drivers/net/transcede/t2200_eth.h
@@ -0,0 +1,1782 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+  *
+  * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+  *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+  *
+  * You should have received a copy of the GNU General Public License
+  * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+  */
+
+#ifndef _T2200_ETH_H
+#define _T2200_ETH_H
+
+//#if !defined (AUTOCONF_INCLUDED)
+//#include <linux/config.h>
+//#endif
+#include <linux/netdevice.h>	/* struct device, and other headers */
+#include <linux/mii.h>
+#include <linux/ethtool.h>
+#include "transcede_gemac.h"
+#include <mach/syslib.h>
+// added here for HWTS and 1588
+#include <linux/clocksource.h>
+#include <linux/net_tstamp.h>
+
+#include <linux/ptp_clock_kernel.h>
+#include <linux/ptp_classify.h>
+
+//Define our own IOCTL here to match symetricomm command; these are raw commands and not matching Linux Standard of system calls.
+
+#define PTP_GET_COUNT       SIOCDEVPRIVATE
+#define PTP_SETUP_GEMNTG    (SIOCDEVPRIVATE + 1)
+#define PTP_SETUP_GEMTSU    (SIOCDEVPRIVATE + 2)
+#define PTP_SHOW_REGISTERS  (SIOCDEVPRIVATE + 3)
+#define PTP_SET_REGISTER    (SIOCDEVPRIVATE + 4)
+#define PTP_GET_TIME        (SIOCDEVPRIVATE + 5)
+#define PTP_SET_TIME        (SIOCDEVPRIVATE + 6)
+#define PTP_GET_REGISTER    (SIOCDEVPRIVATE + 7)
+
+struct  tsusetupprofile {
+u32 masterslave;              //0 is slave, 1 is master
+u32 outputpps;                 // 0 enable this TSU to output pps signal; 1 enable PPS_CLK pin as IRQ_REQ_EXT (T3300 only); t200 will disable PPS_CLK pinout.
+u32 tsuclocksource;         //get tsu clock source:  from gemntg clock(0x0), from external tsu_clk pin(0x1);, or from Framk clk input pin(0x02) ntg framesync, or disable tsu (0x3)
+u32 clockcycletime;         //TSU input each clock cycle duration in ns,  like for 125MHz clock, each cycle is 8ns, then this is 8.
+                               //If clock cycle is not an integer ns, then it needs translate into  0xaabbcc where    cc(integer ns0) *aa(cycle counts) + bb (integer ns1) *1 == integer ns.
+                               //Like for 10.2Mhz clock,  every clock cycle is 98.03ns, it can translate to cc(98)* aa(50)+100 *1 =5000ns., then the value of the register should be set to
+                               // 0x00326462.
+};
+
+struct  ntgsetupprofile {
+u32 enable;              //0 is disable NTG, 1 is enable NTG
+u32 inputfreq;          // input freq in Hz, this can't be any random value, but the real pll output value setup by customer configuration, normally we use pll1 to generate 1GHZ clock.
+u32 outputfreq;         //ntg output freq in Hz, normally set to 125MHz
+u32 pllnum;                      //pll number that NTG will use.    can be 0, 1, 2, 3
+//NTG registers programming value, suppose input clock is frequency x, what value we should program ntg.
+};
+
+
+#define GEMAC_VERSION_STRING	"2.0"
+
+#define DEVICE_NAME "%s: T2200 Ethernet Controller"
+#define DRV_NAME "t2200-geth"
+#define TRANSCEDE_INFOSTR_LEN   32
+
+#ifndef UINT32
+#define UINT32 unsigned int
+#endif
+
+#ifndef UINT16
+#define UINT16 unsigned short
+#endif
+
+#ifndef VUINT32
+#define VUINT32 volatile UINT32
+#endif
+
+#if 0
+
+/** Default number of Receive  descriptors */
+#define DEFAULT_RX_DESC_NT  8192
+
+/** Default number of Transmit descriptors */
+#define DEFAULT_TX_DESC_NT  8192
+
+/** Maximum number of Receive  descriptors */
+#define MAX_RX_DESC_NT      8192
+
+/** Maximum number of Transmit descriptors */
+#define MAX_TX_DESC_NT      8192
+
+#else
+/* Note as code is written, all descriptor ring counts
+ * must be a power of 2 number for
+ * total number of descriptors
+ */
+
+/** Default number of Receive  descriptors */
+#define DEFAULT_RX_DESC_NT  2048
+
+/** Default number of Transmit descriptors */
+#define DEFAULT_TX_DESC_NT  1024
+
+/** Maximum number of Receive  descriptors */
+#define MAX_RX_DESC_NT      2048
+
+/** Maximum number of Transmit descriptors */
+#define MAX_TX_DESC_NT      1024
+
+
+#endif
+
+//#define LAST_DESC_IS_FAKE_FOR_PRIORITY_QUEUES
+//#define SLOW_PATH_DESC_NT   128
+
+#define GEMAC0_ID   0
+#define GEMAC1_ID   1
+
+#define ETH1		"eth1"
+#define ETH2		"eth2"
+
+#define RX_SKB_HEAD_EXTRA_SZ	64						/**< Extra size to avoid reallocations that could be done by Linux when it needs to extend head of skb (for example when adding IP header when forwarding into the IP tunnel) */
+#define RX_SKB_RESERVE_SZ	(RX_SKB_HEAD_EXTRA_SZ + NET_IP_ALIGN)		/**< A size used for skb_reserve() */
+
+/** GEM receive descriptor data structure */
+struct tRXdesc
+{
+        volatile u32 rx_data;          /**< Physical pointer to data to receive into, least significant bit is used bit */
+        volatile u32 rx_status;        /**< Control word */
+};
+
+/* M822XX GEM defines */
+
+/******************************************************************************
+*       Definition of the GEM DMA buffer descriptor and macros                *
+*******************************************************************************/
+
+// RX DESCRIPTOR WORD-0
+#define GEM_RX_DESCR_BUF_ADDR(x)               ((x) & 0xFFFFFFFC)/* 31:2 Address of beginning of buffer */
+#define GEM_RX_DESCR_WRAP                      (1 << 1)          /* Wrap - marks last descriptor in receive buffer descriptor list */
+#define GEM_RX_DESCR_OWNERSHIP                 (1 << 0)          /* needs to be zero for the GEM to write data to the receive buffer */
+
+// RX DESCRIPTOR WORD-1
+#define GEM_RX_DESCR_BROADCAST                 ((U32)1L << 31)   /* Global all ones broadcast address detected */
+#define GEM_RX_DESCR_MULTICAST                 (1 << 30)         /* Multicast Hash match */
+#define GEM_RX_DESCR_UNICAST                   (1 << 29)         /* Unicast Hash match */
+
+#define GEM_RX_DESCR_EXTERNAL_ADDR_MATCH       (1 << 28)         /* External address match */
+#define GEM_RX_DESCR_SPECIFIC_ADDR_MATCH       (1 << 27)         /* Specific address match */
+#define GEM_RX_DESCR_SPEC_ADDR_MASK            (3 << 25)         /* Specific address register match found */
+
+#define GEM_RX_DESCR_EXTRA_SPECIFIC_ADDR_MATCH (1 << 28)         /* Specific address match if EXTRA (>4) registers used*/
+#define GEM_RX_DESCR_EXTRA_SPEC_ADDR_MASK      (7 << 25)         /* Specific address register match found if EXTRA (>4) registers used*/
+
+/** @brief Bit 24 Type ID register match or SNAP/CFI Info (User guide version 25 or later)
+@details
+This bit has a different meaning depending on whether RX checksum
+offloading is enabled.
+
+@par With RX checksum offloading disabled:
+(bit 24 clear in Network Configuration)
+Type ID register match found, bit 22 and bit 23 indicate which type ID register
+causes the match.
+@par With RX checksum offloading enabled:
+(bit 24 set in Network Configuration)
+- 0 - the frame was not SNAP encoded and/or had a VLAN tag with the CFI bit
+set.
+- 1 - the frame was SNAP encoded and had either no VLAN tag or a VLAN tag
+with the CFI bit not set.
+*/
+#define GEM_RX_DESCR_TYPE_MATCH_OR_PKT_INFO   (1 << 24)
+
+/** @brief Bits 23:22
+@details
+These bits have different meaning depending on whether RX checksum
+offloading is enabled.
+
+@par With RX checksum offloading disabled:
+(bit 24 clear in Network Configuration)
+Type ID register match. Encoded as follows:
+- 00 - Type ID register 1 match
+- 01 - Type ID register 2 match
+- 10 - Type ID register 3 match
+- 11 - Type ID register 4 match
+If more than one Type ID is matched only one is indicated with priority 4 down
+to 1.
+@par With RX checksum offloading enabled:
+(bit 24 set in Network Configuration)
+- 00 - Neither the IP header checksum nor the TCP/UDP checksum was
+checked.
+- 01 - The IP header checksum was checked and was correct. Neither the
+TCP nor UDP checksum was checked.
+- 10 - Both the IP header and TCP checksum were checked and were correct.
+- 11 - Both the IP header and UDP checksum were checked and were correct.
+*/
+#define GEM_RX_DESCR_TYPE_ID_OR_CHKSUM_INFO_MASK (3 << 22)
+
+/** @brief  VLAN tag detected type ID of 0x8100
+ *
+ * For packets incorporating the
+ * stacked VLAN processing feature, this bit will be set if the second VLAN tag
+ * has a type ID of 0x8100
+ */
+#define GEM_RX_DESCR_VLAN                     (1 << 21)
+
+/** @brief Priority tag detected type ID of 0x8100 and null VLAN identifier
+ *
+ * For packets incorporating the stacked VLAN processing feature,
+ * this bit will be set if the second VLAN tag has a type ID of 0x8100
+ * and a null VLAN identifier.
+ */
+#define GEM_RX_DESCR_PRIOR                    (1 << 20)
+
+/** @brief VLAN priority - only valid if bit 21 is set */
+#define GEM_RX_DESCR_VLAN_PRIOR_MASK          (7 << 17)
+
+/** @brief Canonical format indicator (CFI) bit - only valid if bit 21 is set */
+#define GEM_RX_DESCR_CFI                      (1 << 16)
+
+/** @brief Receive Descriptor End Of Frame bit
+ *
+ * End of frame - when set the buffer contains the end of a frame. If end of frame
+ * is not set, then the only valid status bit is start of frame (bit 14).
+ */
+#define GEM_RX_DESCR_EOF                      (1 << 15)
+
+/** @brief Receive Descriptor Start of frame
+ *
+ * Start of frame - when set the buffer contains the start of a frame. If both bits 15
+ * and 14 are set, the buffer contains a whole frame.
+ */
+#define GEM_RX_DESCR_SOF                      (1 << 14)
+
+/** @brief FCS status (1 - bad FCS), valid if FCS enabled (FCS discard disable) and jumbo disabled
+ *
+ * This bit has a different meaning depending on whether jumbo frames and
+ * ignore FCS mode are enabled. If neither mode is enabled this bit will be zero.
+ * With jumbo frame mode enabled: (bit 3 set in Network Configuration
+ * Register)
+ *
+ * Additional bit for length of frame (bit[13]), that is concatenated with bits[12:0]
+ *
+ * With ignore FCS mode enabled and jumbo frames disabled: (bit 26 set in
+ * Network Configuration Register and bit 3 clear in Network Configuration
+ * Register)
+ *
+ * This indicates per frame FCS status as follows:
+ *   - 0 : Frame had good FCS
+ *   - 1 : Frame had bad FCS, but was copied to memory as ignore FCS enabled
+ */
+#define GEM_RX_DESCR_FCS_STAT                 (1 << 13)
+
+/** @brief Length mask for descriptor flags field if jumbo frames  enabled (bits 13:0) */
+#define GEM_RX_DESCR_JUMBO_LEN_MASK           (0x03FFF)
+
+/** @brief Length mask for descriptor flags field if jumbo frames disabled (bits 12:0) */
+#define GEM_RX_DESCR_LEN_MASK                 (0x01FFF)
+
+
+/** GEM Transmit descriptor structure */
+struct tTXdesc
+{
+	volatile u32 txdata;            /**< Physical pointer of data to transmit */
+	union
+	{
+		volatile u32 txctl;     /**< Transmit control for prior to transmit */
+		volatile u32 txstatus;  /**< Transmit status  after completion of transmit */
+	} ;
+};
+
+// TX DESCRIPTOR WORD-0 - data buffer address
+
+// TX DESCRIPTOR WORD-1
+
+/** @brief Used bit, must be zero for the GEM to read data to the transmit buffer.
+ *
+ * The GEM sets this to one for the first buffer of a frame once it has been successfully
+ * transmitted. Software must clear this bit before the buffer can be used again.
+ */
+#define GEM_TX_DESCR_USED                     ((UINT32)1L << 31)   /* Must be zero for the GEM to read memory to transmit the transmit buffer */
+
+/** @brief Wrap - marks last descriptor in transmit buffer descriptor list.
+ *
+ * This can be set for any buffer within the frame.
+ */
+#define GEM_TX_DESCR_WRAP                     (1 << 30)   /* Last descriptor */
+
+/** @brief Retry limit exceeded, transmit error detected */
+#define GEM_TX_DESCR_RETRY_EXCEEDED           (1 << 29)   /* Retry limit exceeded, transmit error detected */
+
+/** @brief Transmit under run - occurs when the start of packet data has been written into
+ * the FIFO and either hresp is not OK, or the transmit data could not be fetched
+ * in time, or when buffers are exhausted.
+ *
+ * This is not set when the DMA is configured for packet buffer mode.
+ */
+#define GEM_TX_DESCR_UNDERRUN_BIT             (1 << 28)     /* Transmit under run */
+
+/** @brief 27 Transmit frame corruption due to AHB or AXI error.
+ *
+ * Set if an error occurs whilst midway through reading transmit frame from the AHB/AXI, including
+ * HRESP/RRESP/BRESP errors and buffers exhausted mid frame (if the buffers
+ * run out during transmission of a frame then transmission stops, FCS shall be
+ * bad and tx_er asserted).
+ *
+ * Also set in DMA packet buffer mode if single frame is too large for configured
+ * packet buffer memory size.
+ *
+ * Also set if a multiple descriptors are used for transmitting a single frame
+ * and the GEM reads a descriptor with a USED bit prior to reading the
+ * descriptor with the last bit set.
+ */
+#define GEM_TX_DESCR_FRM_CORRUPT_BIT          (1 << 27)     /* Transmit frame corruption due to AHB or AXI error */
+
+/** @brief Late collision, transmit error detected. Late collisions only force this status bit
+ * to be set in gigabit mode.
+ */
+#define GEM_TX_DESCR_LATE_COLLISION_BIT       (1 << 26)     /* Late collision, transmit error detected */
+
+/** @brief Bits 22:20 Transmit IP/TCP/UDP checksum generation offload errors:
+@details
+Valid bit values for bits 22:20
+  - 000 No Error
+  - 001 The Packet was identified as a VLAN type, but the header was not fully
+         complete, or had an error in it
+  - 010 The Packet was identified as a SNAP type, but the header was not fully
+        complete, or had an error in it
+  - 011 The Packet was not of an IP type, or the IP packet was invalidly short, or
+        the IP was not of type IPv4/IPv6
+  - 100 The Packet was not identified as VLAN, SNAP or IP
+  - 101 Non supported packet fragmentation occurred. For IPv4 packets, the IP
+        checksum was generated and inserted
+  - 110 Packet type detected was not TCP or UDP. TCP/UDP checksum was
+        therefore not generated. For IPv4 packets, the IP checksum was
+        generated and inserted
+  - 111 A premature end of packet was detected and the TCP/UDP checksum
+        could not be generated
+*/
+#define GEM_TX_DESCR_CHECKSUM_RESULT_MASK     (7 << 20)
+#define GEM_TX_DESCR_CHECKSUM_NO_ERROR        (0 << 20)
+#define GEM_TX_DESCR_CHECKSUM_VLAN_ERROR      (1 << 20)
+#define GEM_TX_DESCR_CHECKSUM_SNAP_ERROR      (2 << 20)
+#define GEM_TX_DESCR_CHECKSUM_NOT_IP_TYPE     (3 << 20)
+#define GEM_TX_DESCR_CHECKSUM_NOT_IDENTIFIED  (4 << 20)
+#define GEM_TX_DESCR_CHECKSUM_FRAGMENTED      (5 << 20)
+#define GEM_TX_DESCR_CHECKSUM_NOT_TCP_UDP     (6 << 20)
+#define GEM_TX_DESCR_CHECKSUM_PREMATURE_END   (7 << 20)
+
+/** @brief No CRC to be appended by MAC.
+ *
+ * When set this implies that the data in the
+ * buffers already contains a valid CRC and hence no CRC or padding is to be
+ * appended to the current frame by the MAC.
+ *
+ * This control bit must be set for the first buffer in a frame and will be ignored for
+ * the subsequent buffers of a frame. This operation is different from Cadences
+ * Ethernet MAC 10/100 (Enhanced), which reads the no CRC bit from the final
+ * buffer descriptor in the frame.
+ *
+ * Note that this bit must be clear when using the transmit IP/TCP/UDP checksum
+ * generation offload, otherwise checksum generation and substitution will not
+ * occur.
+ */
+#define GEM_TX_DESCR_NO_CRC_BIT               (1 << 16)   /* No CRC to be appended by MAC */
+
+/** @brief Last buffer, when set this bit will indicate the last buffer in the current frame has
+ * been reached.
+ */
+#define GEM_TX_DESCR_LAST_BUF_BIT             (1 << 15)   /* Last buffer in the frame */
+
+/** @brief Length of buffer mask.  Bits 13:0 contain length for this buffer */
+#define GEM_TX_DESCR_LEN_MASK                 (0x03FFF)   /* Length of buffer [13:0] */
+
+/** @brief Length of buffer mask.  Bits 13:0 contain length for this buffer */
+#define GEM_TX_DESCR_ERROR_MASK             ( 0                                                                                                         \
+                                              | GEM_TX_DESCR_RETRY_EXCEEDED          /* (1 << 29)  Retry limit exceeded, transmit error detected */     \
+                                              | GEM_TX_DESCR_UNDERRUN_BIT            /* (1 << 28)  Transmit under run */                                \
+                                              | GEM_TX_DESCR_FRM_CORRUPT_BIT         /* (1 << 27)  Transmit frame corruption due to AHB or AXI error */ \
+                                              | GEM_TX_DESCR_LATE_COLLISION_BIT      /* (1 << 26)  Late collision, transmit error detected */           \
+                                            )
+
+/** Expt extended descriptor (IPsec) */
+struct tTXextdesc
+{
+	volatile u16 SAhandle[2];
+	u32          RSVD;
+};
+
+/** Private driver flag for indicating GEMAC RX checksum offload is enabled */
+#define RX_CSUM_OFFLOAD_ENABLED		(1 << 0)  /* Receive  L3/L4 checksum offload feature enabled */
+
+/** Private driver flag for indicating GEMAC TX checksum offload is enabled */
+#define TX_CSUM_OFFLOAD_ENABLED		(1 << 1)  /* Transmit L3/L4 checksum offload feature enabled */
+
+/***********************************************************************
+**               Misc. useful Ethrnet contstants                       *
+************************************************************************/
+
+/* AKB: IEEE 1588v2 Precision timing protocol mis. constants */
+
+/* Event messages (TX & RX timestamping required, UDP port 0x13F)*/
+#define SYNC_MESSAGE                  0x0
+#define DELAY_REQ_MESSAGE             0x1
+#define PDELAY_REQ_MESSAGE            0x2
+#define PDELAY_RESP_MESSAGE           0x3
+
+ /* Reserved 0x4-0x7 */
+
+ /* General messages (No timestamping necessary, UDP port 0x140) */
+ #define FOLLOWUP_MESSAGE             0x8
+ #define DELAY_RESP_MESSAGE           0x9
+ #define PDELAY_RESP_FOLLOWUP_MESSAGE 0xA
+ #define ANNOUNCE_MESSAGE             0xB
+ #define SIGNALING_MESSAGE            0xC
+ #define MANAGEMENT_MESSAGE           0xD
+
+/* Common Ethernet Ethertypes */
+#define ETHERTYPE_IPV4                  0x0800  /**< @brief Internet Protocol version 4 */
+#define ETHERTYPE_ARP                   0x0806  /**< @brief IP Address Resolution Protocol */
+#define ETHERTYPE_IPV6                  0x86DD  /**< @brief Internet Protocol version 6 */
+#define ETHERTYPE_VLAN                  0x8100  /**< @brief VLAN 802.1Q */
+#define ETHERTYPE_PTP                   0x88F7  /**< @brief Precision Timing Protocol Ethernet direct encapsulation */
+#define ETHERTYPE_EXPERIMENTAL_1        0x88B5  /**< @brief IEEE 802 local experimental Ethertype #1 */
+#define ETHERTYPE_EXPERIMENTAL_2        0x88B6  /**< @brief IEEE 802 local experimental Ethertype #2 */
+
+#define BYTE_REVERSE_ETHERTYPE(ethertype) ((UINT16) (((ethertype & 0xFF)<<8) | (ethertype>>8)))
+
+#define MAC_ADDRESS_SIZE                6
+#define MAC_ADDRESSES_FIELD_SIZE        12
+#define ETHERTYPE_SIZE                  2
+
+//
+// GEM bit definitions for GEM registers
+//
+
+/**
+@brief Network Control Register, Offset:0x000, Access: R/W Bit:22 Store UDP / TCP offset to memory.
+
+Setting this bit to
+one will cause the upper 16-bits of the CRC of every
+received frame to be replaced with the offset from
+start of frame to the beginning of the UDP or TCP
+header. The lower 16-bits of the CRC are replaced
+with zero and reserved for future use. The offset is
+measured in units of 2 bytes.
+Set to zero for normal operation.
+
+Reset value: 0
+*/
+#define GEM_NETCTRL_STORE_L4_HDR_OFFSET (1 << 22)   /* Store UDP / TCP offset to memory */
+#define GEM_NETCTRL_ALT_SGMII_MODE      (1 << 21)   /* Alternative sgmii mode */
+#define GEM_NETCTRL_PTP_UNICAST_ENABLE  (1 << 20)   /* Enable detection of unicast PTP unicast frames */
+#define GEM_NETCTRL_LPI_TX_ENABLE       (1 << 19)   /* Enable LPI transmission */
+#define GEM_NETCTRL_FLUSH_PKT_RX_DPRAM  (1 << 18)   /* Flush the next packet from the external RX DPRAM */
+#define GEM_NETCTRL_TX_PFC_PAUSE_FRAME  (1 << 17)   /* Transmit PFC Priority Based Pause Frame */
+#define GEM_NETCTRL_PFC_PAUSE_RX_ENABLE (1 << 16)   /* Enable PFC Priority Based Pause Reception */
+/**
+@brief Network Control Register, Offset:0x000, Access: R/W Bit:15 Store receive time stamp to memory.
+
+Setting this bit
+to one will cause the CRC of every received frame to
+be replaced with the value of the nanoseconds field
+of the 1588 timer that was captured as the receive
+frame passed the message time stamp point. Set to
+zero for normal operation.
+
+Reset value: 0
+*/
+#define GEM_NETCTRL_STORE_TIMESTAMP     (1 << 15)   /* Setting this bit stores RX 1588 timestamps to CRC field */
+/**
+@brief Network Control Register, Offset:0x000, Access: Bit:14 Read snapshot
+
+Writing a one means that the
+snapshot value of the statistics register will be read
+back, otherwise the raw statistic register will be read.
+
+Reset Value: 0
+*/
+#define GEM_NETCTRL_READ_SNAPSHOT       (1 << 14)   /* Read Snapshot (1) or current (0) statsistics register */
+#define GEM_NETCTRL_SNAPSHOT            (1 << 13)   /* Record current statistics in snapshot registers, clear stats registers  */
+#define GEM_NETCTRL_TX_ZERO_PAUSE       (1 << 12)   /* Transmit a single pause frame with zero quantum */
+#define GEM_NETCTRL_TX_PAUSE            (1 << 11)   /* Transmit a single pause frame */
+#define GEM_NETCTRL_HALT_TX             (1 << 10)   /* Halt  transmission after current TX frame if any (1 stops  TX) */
+#define GEM_NETCTRL_START_TX            (1 << 9)    /* Start transmission (1 starts TX) */
+#define GEM_NETCTRL_BACK_PRESSURE       (1 << 8)    /* Back pressure in 10M or 100M half duplex */
+#define GEM_NETCTRL_STAT_WR_ENB         (1 << 7)    /* Allow write access for statistics registers (test only) */
+#define GEM_NETCTRL_INC_STAT            (1 << 6)    /* Increment all statistics register by one    (test only) */
+#define GEM_NETCTRL_CLR_STAT            (1 << 5)    /* Clear statistics */
+#define GEM_NETCTRL_MDIO_ENB            (1 << 4)    /* MDIO     enable */
+#define GEM_NETCTRL_TX_ENB              (1 << 3)    /* Transmit enable */
+#define GEM_NETCTRL_RX_ENB              (1 << 2)    /* Receive  enable */
+#define GEM_NETCTRL_RX_LB               (1 << 1)    /* Receive  loopback */
+#define GEM_NETCTRL_TX_LB               (1 << 0)    /* Transmit loopback */
+
+//
+// Network Configuration Register offset 0x004
+//
+/**
+@brief Network Configuration Register, Offset:0x004 Access: R/W Bit:31 Uni-direction-enable.
+
+When low the PCS will
+transmit idle symbols if the link goes down. When
+high the PCS can transmit frame data when the link
+is down.
+
+Reset Value: 0
+*/
+#define GEM_NETCFG_UNIDIR               (1 << 31)   /* Uni-direction-enable */
+/**
+@brief Network Configuration Register, Offset:0x004, Access:R/W Bit:30 Ignore IPG rx_er.
+
+When set rx_er has no effect on
+the GEMs operation when rx_dv is low. Set this
+when using the RGMII wrapper in half-duplex mode.
+
+Reset Value:0
+*/
+#define GEM_NETCFG_IGNORE_RX_ERR        (1 << 30)   /**< @brief Ignore RX_ER when RX_DV is low */
+/**
+@brief Network Configuration Register, Offset:0x004 Access:R/W Bit:29 Receive bad preamble.
+
+When set frames with
+non-standard preamble are not rejected.
+
+Reset Value: 0
+*/
+#define GEM_NETCFG_BAD_PREAMBLE         (1 << 29)   /* Receive bad preamble */
+
+/**
+@brief Network Configuration Register, Offset:0x004 Access:R/W Bit:28 IPG stretch enable
+
+When set the transmit IPG can
+be increased above 96 bit times depending on the
+previous frame length using the IPG stretch register.
+
+Reset Value: 0
+*/
+#define GEM_NETCFG_IPG                  (1 << 28)   /**< @brief IPG stretch enable */
+
+/**
+@brief Network Configuration Register, Offset:0x004 Access:R/W Bit:27 SGMII mode enable
+
+Changes behaviour of the
+auto-negotiation advertisement and link partner
+ability registers to meet the requirements of SGMII
+and reduces the duration of the link timer from 10 ms
+to 1.6 ms.
+
+Reset value: 0
+*/
+#define GEM_NETCFG_SGMII_MODE           (1 << 27)   /* SGMII mode enable */
+
+/**
+@brief Network Configuration Register, Offset:0x004 Access:R/W Bit:26 Ignore RX FCS
+
+When set frames with FCS/CRC
+errors will not be rejected. FCS error statistics will
+still be collected for frames with bad FCS and FCS
+status will be recorded in frames DMA descriptor.
+For normal operation this bit must be set to zero.
+
+Reset Value: 0
+*/
+#define GEM_NETCFG_IGNORE_RX_FCS        (1 << 26)   /* Ignore RX FCS - when set frames with FCS/CRC errors will not be rejected */
+
+/**
+@brief Network Configuration Register, Offset:0x004 Access:R/W Bit:25 Enable frames to be received in half-duplex mode
+while transmitting.
+
+Reset Value: 0
+*/
+#define GEM_NETCFG_RX_HALF_ENB          (1 << 25)   /* Enable frames to be received in half-duplex mode while transmitting */
+
+/**
+@brief Network Configuration Register, Offset:0x004 Access:R/W Bit:24 Receive checksum offload enable
+
+When set, the receive checksum engine is enabled. Frames with
+bad IP, TCP or UDP checksums are discarded.
+
+Reset Value: 0
+*/
+#define GEM_NETCFG_CALC_CHKS_ENB        (1 << 24)   /* Receive checksum offload enable */
+
+/**
+@brief Network Configuration Register, Offset:0x004 Access:R/W Bit:23 Disable copy of pause frames
+
+Set to one to prevent
+valid pause frames being copied to memory. When
+set, pause frames are not copied to memory
+regardless of the state of the copy all frames bit;
+whether a hash match is found or whether a type ID
+match is identified. If a destination address match is
+found the pause frame will be copied to memory.
+Note that valid pause frames received will still
+increment pause statistics and pause the
+transmission of frames as required.
+
+Reset Value: 0
+*/
+#define GEM_NETCFG_DONOT_COPY_PAUSE     (1 << 23)   /* Do not copy Ethernet pause frames */
+
+/**
+@brief Network Configuration Register, Offset:0x004 Access:R/W Bits:20:18 MDC clock division (Set according to pclk speed.)
+
+These three bits determine the number pclk will be
+divided by to generate MDC. For conformance with
+the 802.3 specification, MDC must not exceed
+2.5 MHz (MDC is only active during MDIO read and
+write operations).
+
+  - 000: divide pclk by 8 (pclk up to 20 MHz)
+  - 001: divide pclk by 16 (pclk up to 40 MHz)
+  - 010: divide pclk by 32 (pclk up to 80 MHz) (reset value)
+  - 011: divide pclk by 48 (pclk up to 120MHz)
+  - 100: divide pclk by 64 (pclk up to 160 MHz)
+  - 101: divide pclk by 96 (pclk up to 240 MHz)
+  - 110: divide pclk by 128 (pclk up to 320 MHz)
+  - 111: divide pclk by 224 (pclk up to 540 MHz)
+
+Reset Value: binary 010 (2)
+*/
+#define GEM_NETCFG_MDC_CLOCK_DIV_MASK   (7 << 18)
+#define GEM_NETCFG_MDC_CLOCK_DIV_8      (0 << 18)
+#define GEM_NETCFG_MDC_CLOCK_DIV_16     (1 << 18)
+#define GEM_NETCFG_MDC_CLOCK_DIV_32     (2 << 18)
+#define GEM_NETCFG_MDC_CLOCK_DIV_48     (3 << 18)
+#define GEM_NETCFG_MDC_CLOCK_DIV_64     (4 << 18)
+#define GEM_NETCFG_MDC_CLOCK_DIV_96     (5 << 18)
+#define GEM_NETCFG_MDC_CLOCK_DIV_128    (6 << 18)
+#define GEM_NETCFG_MDC_CLOCK_DIV_224    (7 << 18)
+
+#define GEM_NETCFG_100BM                (1 << 0)    /* Speed - set to logic one to indicate 100Mbps */
+#define GEM_NETCFG_FULL_DUPLEX          (1 << 1)    /* Full duplex */
+#define GEM_NETCFG_HALF_DUPLEX          (0 << 1)    /* Half duplex */
+#define GEM_NETCFG_DISCARD_NON_VLAN     (1 << 2)    /* Discard non-VLAN frames */
+/**
+@brief Network Configuration Register, Offset:0x004 Access:R/W Bit:3 Jumbo frames
+
+Set to one to enable jumbo frames
+up to `gem_jumbo_max_length bytes to be
+accepted. The default length is 10,240 bytes.
+
+Reset Value: 0
+*/
+#define GEM_NETCFG_JUMBO_ENB            (1 << 3)    /* Jumbo frames */
+
+#define GEM_NETCFG_COPY_ALL             (1 << 4)    /* Copy all frames (i.e. promiscuous mode) */
+#define GEM_NETCFG_NO_BROADCAST         (1 << 5)    /* No broadcast */
+#define GEM_NETCFG_MULTICAST_ENB        (1 << 6)    /* Multicast hash enable */
+#define GEM_NETCFG_UNICAST_ENB          (1 << 7)    /* Unicast hash enable */
+#define GEM_NETCFG_1536_ENB             (1 << 8)    /* Receive 1536 byte frames */
+#define GEM_NETCFG_EXT_ADDR_ENB         (1 << 9)    /* External address match enable */
+
+/**
+@brief Network Configuration Register, Offset:0x004 Access:R/W Bit:10 Gigabit mode enable
+
+Setting this bit configures the
+GEM for 1000 Mbps operation.
+  - 0: 10/100 operation using MII or TBI interface
+  - 1: Gigabit operation using GMII or TBI interface
+
+Reset value: 0
+*/
+#define GEM_NETCFG_1GB_MODE             (1 << 10)   /* setting this bit configures the GEM for 1000 Mbps operation */
+
+/**
+@brief Network Configuration Register, Offset:0x004 Access:R/W Bit:11 PCS select, selects between MII/GMII and TBI.
+
+Must be set for SGMII operation.
+  - 0: GMII/MII interface enabled, TBI disabled
+  - 1: TBI enabled, GMII/MII disabled
+
+Reset Value: 0
+*/
+#define GEM_NETCFG_TBI_ENB              (1 << 11)   /* PCS select 1: TBI enabled, 0: GMII/MII enabled */
+#define GEM_NETCFG_RETRY_TEST           (1 << 12)   /* Retry test, must be set to zero for normal operation */
+#define GEM_NETCFG_PAUSE_ENB            (1 << 13)   /* Pause enable */
+
+/**
+@brief Network Configuration Register, Offset:0x004 Access:R/W Bit:15:14 Receive buffer offset
+
+Indicates the number of bytes
+by which the received data is offset from the start of
+the receive buffer.
+
+Reset Value: 0
+*/
+#define GEM_NETCFG_RX_OFFS_BYTES(x)     (x << 14)   /* Bits 15:14 Receiver buffer offset value (0-3 bytes from base address) */
+#define GEM_NETCFG_RX_OFFS_MASK         (3 << 14)   /* Receive buffer offset mask */
+
+
+/**
+@brief Network Configuration Register, Offset:0x004 Access:R/W Bit:16 Length field error frame discard
+
+Setting this bit
+causes frames with a measured length shorter than
+the extracted length field (as indicated by bytes 13
+and 14 in a non-VLAN tagged frame) to be
+discarded. This only applies to frames with a length
+field less than 0x0600.
+
+Reset Value: 0
+*/
+#define GEM_NETCFG_NO_LEN_ERR           (1 << 16)   /* Length field error frame discard */
+
+/**
+@brief Network Configuration Register, Offset:0x004 Access:R/W Bit:17 FCS remove
+
+Setting this bit will cause received
+frames to be written to memory without their frame
+check sequence (last 4 bytes). The frame length
+indicated will be reduced by four bytes in this mode.
+Reset value: 0
+*/
+#define GEM_NETCFG_FCS_REMOVE           (1 << 17)   /* FCS remove */
+
+/**
+@brief Register 0x004 R/W Bits 22:21 Data bus width
+
+Bit values:
+  - 00:  32 bit AMBA AHB/AXI data bus width
+  - 01:  64 bit AMBA AHB/AXI data bus width
+  - 10: 128 bit AMBA AHB/AXI data bus width
+  - 11: 128 bit AMBA AHB/AXI data bus width
+*/
+#define GEM_NETCFG_BUS_WIDTH_MASK       (3 << 21)
+#define GEM_NETCFG_BUS_WIDTH_32_BIT     (0 << 21)
+#define GEM_NETCFG_BUS_WIDTH_64_BIT     (1 << 21)
+#define GEM_NETCFG_BUS_WIDTH_128_BIT    (2 << 21)
+#define GEM_NETCFG_BUS_WIDTH_128_BITS   (3 << 21)
+
+
+//
+// Network Status Register, offset 0x008
+//
+#define GEM_NETSTAT_RESERVED            (0 << 8)    /* Bits 31:8 Reserved, read as zero, ignored on write */
+#define GEM_NETSTAT_LPI_DETECTED        (1 << 7)    /* Low Power Idle (LPI) Indication */
+#define GEM_NETSTAT_PFC_NEGOTIATED      (1 << 6)    /* Set when PFC Priority Based Pause has been negotiated */
+#define GEM_NETSTAT_PCS_AUTONEG_TX      (1 << 5)    /* PCS auto-negotiation pause transmit resolution */
+#define GEM_NETSTAT_PCS_AUTONEG_RX      (1 << 4)    /* PCS auto-negotiation pause receive resolution */
+#define GEM_NETSTAT_PCS_AUTONEG_DUPLEX  (1 << 3)    /* PCS auto-negotiation duplex resolution */
+#define GEM_NETSTAT_PHY_MANGEMENT_IDLE  (1 << 2)    /* PHY management logic is idle (i.e. has completed)*/
+#define GEM_NETSTAT_MDIO_IN_STATUS      (1 << 1)    /* Returns status of the mdio_in pin */
+#define GEM_NETSTAT_PCS_LINK_STATE      (1 << 0)    /* Returns status of PCS link state */
+
+//
+// DMA Configuration Register
+//
+/**
+@brief DMA Configuration Register, Offset 0x010, Access R/W, Bits 4:0 fixed burst length for DMA data operations
+
+Selects the burst length to attempt to use on the AHB or AXI
+when transferring frame data. Not used for DMA
+management operations and only used where space
+and data size allow. Otherwise SINGLE type AHB or
+AXI transfers are used.
+
+Upper bits become non-writeable if the configured
+DMA TX and RX FIFO sizes are smaller than
+required to support the selected burst size.
+
+One-hot priority encoding enforced automatically on
+register writes as follows, where x represents dont
+care:
+  - 1xxxx: Attempt to use bursts of up to 16
+  - 01xxx: Attempt to use bursts of up to 8
+  - 001xx: Attempt to use bursts of up to 4 (reset value)
+  - 0001x: Always use SINGLE bursts
+  - 00001: Always use SINGLE bursts
+  - default: Attempt to use bursts of up to 4
+
+Reset Value: binary 00100 (4)
+*/
+#define GEM_DMA_BURST_MASK              (0x1F << 0)
+#define GEM_DMA_BURST_SINGLE            (1 << 0)    /* 00001: Always     use SINGLE AHB burst  */
+#define GEM_DMA_BURST_INCR4             (1 << 2)    /* 001xx: Attempt to use INCR4  AHB bursts */
+#define GEM_DMA_BURST_INCR8             (1 << 3)    /* 01xxx: Attempt to use INCR8  AHB bursts */
+#define GEM_DMA_BURST_INCR16            (1 << 4)    /* 1xxxx: Attempt to use INCR16 AHB bursts */
+
+#define GEM_DMA_BIG_ENDIAN_DESC_ACCESS  (1 << 6)    /* AHB endian swap mode for management decristor accesses enable */
+#define GEM_DMA_BIG_ENDIAN              (1 << 7)    /* AHB endian swap mode for packet data accesses enable */
+
+/**
+@brief DMA Configuration Register, Offset 0x010, Access R/W, Bits 9:8 Receiver packet buffer memory size select
+
+Having these bits at less than 11 reduces the amount of
+memory used for the receive packet buffer. This
+reduces the amount of memory used by the GEM. It
+is important to set these bits both to one if the full
+configured physical memory is available. The value
+in brackets below represents the size that would
+result for the default maximum configured memory
+size of 8 Kbytes.
+  - 11: Use full configured addressable space (8 Kb)
+  - 10: Do not use top address bit (4 Kb)
+  - 01: Do not use top two address bits (2 Kb)
+  - 00: Do not use top three address bits (1 Kb)
+
+If the GEM is not configured to use the DMA packet
+buffer, these bits are not implemented and will be
+treated as reserved, read as zero, ignored on write.
+
+Reset Value: Binary 11 (3)
+*/
+#define GEM_DMA_RX_PB_SIZE_MASK         (3 << 8)
+#define GEM_DMA_RX_PB_SIZE_1K           (0 << 8)    /* 00: Do not use top three address bits (1 Kb) */
+#define GEM_DMA_RX_PB_SIZE_2K           (1 << 8)    /* 01: Do not use top two address bits (2 Kb) */
+#define GEM_DMA_RX_PB_SIZE_4K           (2 << 8)    /* 10: Do not use top address bit (4 Kb) */
+#define GEM_DMA_RX_PB_SIZE_8K           (3 << 8)    /* 11: Use full configured addressable space (8 Kb) */
+
+/**
+@brief DMA Configuration Register, Offset 0x010, Access R/W, Bit 10 Transmitter packet buffer memory size select 
+@details
+Having this bit at zero halves the amount of memory
+used for the transmit packet buffer. This reduces the
+amount of memory used by the GEM. It is important
+to set this bit to one if the full configured physical
+memory is available. The value in brackets below
+represents the size that would result for the default
+maximum configured memory size of 4 Kbytes.
+  - 1: Use full configured addressable space (4 Kb)
+  - 0: Do not use top address bit (2 Kb)
+
+If the GEM is not configured to use the DMA packet
+buffer, this bit is not implemented and will be treated
+as reserved, read as zero, ignored on write.
+
+Reset Value: 1
+*/
+#define GEM_DMA_TX_PACKET_BUFFER_SIZE_SELECT(x)   (x << 10)
+#define GEM_DMA_TX_PB_SIZE_2K           (0 << 10)   /* 0: Do not use top address bit (2 Kb) */
+#define GEM_DMA_TX_PB_SIZE_4K           (1 << 10)   /* 1: Use full configured addressable space (4 Kb) */
+
+#define GEM_DMA_CHK_SUM_ENB             (1 << 11)   /* Transmitter IP, TCP and UDP checksum generation offload enable */
+
+/**
+@brief Register 0x010 R/W Bits 23:16 DMA receive buffer size in external AHB or AXI system memory.
+@details
+The value defined by these bits determines the size
+of buffer to use in main system memory when writing
+received data.
+The value is defined in multiples of 64 bytes such
+that a value of 0x01 corresponds to buffers of 64
+bytes, 0x02 corresponds to 128 bytes etc.
+
+For example:
+  - 0x02: 128 byte
+  - 0x18: 1536 byte (1*max length frame/buffer)
+  - 0xA0: 10240 byte (1*10K jumbo frame/buffer)
+
+Note that this value should never be written as zero.
+*/
+#define GEM_DMA_DEF_RX_SIZE(size)       ((((size)   >>  6) & 0xFF) << 16)
+#define GEM_DMA_RX_SIZE_MASK            (0xFF << 16)
+#define GEM_DMA_GET_RX_SIZE(dmareg)     ((((dmareg) >> 16) & 0xFF) *  64)
+#define GEM_DMA_RX_64                   (1 << 16)
+#define GEM_DMA_RX_128                  (2 << 16)
+#define GEM_DMA_RX_192                  (3 << 16)
+#define GEM_DMA_RX_256                  (4 << 16)
+#define GEM_DMA_RX_320                  (5 << 16)
+#define GEM_DMA_RX_384                  (6 << 16)
+#define GEM_DMA_RX_448                  (7 << 16)
+#define GEM_DMA_RX_512                  (8 << 16)
+#define GEM_DMA_RX_1024                 (16<< 16)
+#define GEM_DMA_RX_1536                 (24<< 16)
+#define GEM_DMA_RX_2048                 (32<< 16)
+#define GEM_DMA_RX_4096                 (64<< 16)
+#define GEM_DMA_RX_8192                (128<< 16)
+#define GEM_DMA_RX_10240               (160<< 16)
+
+/**
+@brief DMA Configuration Register, Offset 0x010, Access R/W, Bit 24 Receive DMA bus resource discard packet control
+@details
+When set, the GEM DMA will automatically discard
+receive packets from the receiver packet buffer
+memory when no AHB or AXI resource is available.
+When low, then received packets will remain to be
+stored in the SRAM based packet buffer until AHB or
+AXI buffer resource next becomes available.
+A write to this bit is ignored if the DMA is not
+configured in the packet buffer full store and forward
+mode.
+
+Reset value: 0
+*/
+
+#define GEM_DMA_RX_DISCARD_BUS_RESOURCE (1<<24) /* Discard RX packets when no Bus resources */
+
+/**
+@brief DMA Configuration Register, Offset 0x010, Access R/W, Bit 25 RX DMA EOP/EOB burst control
+@details
+Force the RX DMA to always issue max length
+bursts on EOP(end of packet) or EOB(end of
+buffer)transfers, even if there is less than max burst
+real packet data required to write. Any extra bytes of
+pad data is set to 0x00.
+Does not apply on bursts that break 1k boundary
+rule.
+
+Reset Value:0
+*/
+#define GEM_DMA_FORCE_RX_MAX_LEN_BURSTS (1<<25) /* Force the RX DMA to always issue max length */
+
+/**
+@brief DMA Configuration Register, Offset 0x010, Access R/W, Bit 26 TX DMA EOP/EOB burst control
+@details
+Force the TX DMA to always issue max length
+bursts on EOP(end of packet) or EOB(end of buffer)
+transfers as defined by bits 4:0 of this register, even
+when there is less that max burst data bytes to read.
+Residual data read is ignored.
+Does not apply on bursts that break 1k boundary
+rule.
+
+Reset value:0
+*/
+#define GEM_DMA_FORCE_TX_MAX_LEN_BURSTS (1<<26) /* Always Force TX maximum length bursts */
+
+// Transmit Status Register (offset 0x014) bit values
+
+#define GEM_TX_STAT_USED_WAS_READ       (1 << 0)    /**< R/W Used bit read */
+#define GEM_TX_STAT_COLLISION           (1 << 1)    /**< R/W Collision occurred */
+#define GEM_TX_STAT_RETRY_EXCEEDED      (1 << 2)    /**< R/W Retry limit exceeded */
+#define GEM_TX_STAT_GO                  (1 << 3)    /**< RO  Transmit go, if high transmit is active */
+#define GEM_TX_STAT_AHB_ERR             (1 << 4)    /**< R/W Transmit frame corruption due to AHB error */
+#define GEM_TX_STAT_COMPLETE            (1 << 5)    /**< R/W Transmit complete - set when a frame has been transmitted */
+#define GEM_TX_STAT_UNDERRUN            (1 << 6)    /**< R/W Transmit under run */
+#define GEM_TX_STAT_LATE_COLLISION      (1 << 7)    /**< R/W Late collision occurred */
+#define GEM_TX_STAT_HRESP_ERR           (1 << 8)    /**< R/W Hresp not OK - set when the DMA block sees hresp not OK */
+
+#ifdef BUG_69620_WORKAROUND
+//
+// For workaround, the "AHB/AXI" error (bit 5) will be set anytime there is
+// a transmit that hits the intentional buffer descriptor error setup
+// at the current end of the TX queue
+//
+#define GEM_TX_ERROR_MASK               (0                          |\
+                                        GEM_TX_STAT_COLLISION       |\
+                                        GEM_TX_STAT_RETRY_EXCEEDED  |\
+                                        GEM_TX_STAT_UNDERRUN        |\
+                                        GEM_TX_STAT_LATE_COLLISION  |\
+                                        GEM_TX_STAT_HRESP_ERR       |\
+                                        0)
+
+#define GEM_TX_STUCK_MASK               (0                          |\
+                                        GEM_TX_STAT_COLLISION       |\
+                                        GEM_TX_STAT_RETRY_EXCEEDED  |\
+                                        GEM_TX_STAT_AHB_ERR         |\
+                                        GEM_TX_STAT_UNDERRUN        |\
+                                        GEM_TX_STAT_LATE_COLLISION  |\
+                                        GEM_TX_STAT_HRESP_ERR       |\
+                                        0)
+
+
+#else
+#define GEM_TX_ERROR_MASK               (0                          |\
+                                        GEM_TX_STAT_COLLISION       |\
+                                        GEM_TX_STAT_RETRY_EXCEEDED  |\
+                                        GEM_TX_STAT_AHB_ERR         |\
+                                        GEM_TX_STAT_UNDERRUN        |\
+                                        GEM_TX_STAT_LATE_COLLISION  |\
+                                        GEM_TX_STAT_HRESP_ERR       |\
+                                        0)
+
+#define GEM_TX_STUCK_MASK               GEM_TX_ERROR_MASK
+#endif
+
+// Receive Status Register (offset 0x020) bit values
+
+#define GEM_RX_STAT_NO_BUF              (1 << 0)    /**< Buffer not available */
+#define GEM_RX_STAT_FRM_RECV            (1 << 1)    /**< Frame received       */
+#define GEM_RX_STAT_OVERRUN             (1 << 2)    /**< Receive overrun      */
+#define GEM_RX_STAT_HRESP_ERR           (1 << 3)    /**< Hresp not OK         */
+
+#define GEM_RX_ERROR_MASK               (0                          |\
+                                        GEM_RX_STAT_NO_BUF          |\
+                                        GEM_RX_STAT_OVERRUN         |\
+                                        GEM_RX_STAT_HRESP_ERR       |\
+                                        0)
+
+//
+// IRQ ENABLE, IRQ DISABLE, IRQ STATUS
+//
+// GEM base interrupts
+#define GEM_INT_BIT_PHY_COMPLETE            0       /**< @brief Management frame sent - the PHY maintenance register has completed its operation */
+#define GEM_INT_BIT_RX_COMPLETE             1       /**< @brief Receive complete - a frame has been stored in memory */
+#define GEM_INT_BIT_RX_USED_BIT_READ        2       /**< @brief RX used bit read - set when a receive buffer descriptor is read with its used bit set */
+#define GEM_INT_BIT_TX_USED_BIT_READ        3       /**< @brief TX used bit read - set when a transmit buffer descriptor is read with its used bit set */
+#define GEM_INT_BIT_TX_UNDERRUN             4       /**< @brief Transmit under run */
+#define GEM_INT_BIT_RETRY_EXCEEDED          5       /**< @brief Retry limit exceeded or late collision - transmit error */
+#define GEM_INT_BIT_AHB_ERR                 6       /**< @brief Transmit frame corruption due to AHB error */
+#define GEM_INT_BIT_TX_COMPLETE             7       /**< @brief Transmit complete */
+#define GEM_INT_BIT_LINK_CHANGE             9       /**< @brief Link change */
+#define GEM_INT_BIT_RX_OVERRUN              10      /**< @brief Receive overrun */
+#define GEM_INT_BIT_HRESP_ERR               11      /**< @brief Hresp not OK */
+#define GEM_INT_BIT_RX_PAUSE_FRM            12      /**< @brief Pause frame with non-zero pause quantum received */
+#define GEM_INT_BIT_PAUSE_ZERO              13      /**< @brief Pause time zero */
+#define GEM_INT_BIT_TX_PAUSE_FRM            14      /**< @brief Pause frame transmitted */
+#define GEM_INT_BIT_EXTERNAL                15      /**< @brief External interrupt */
+#define GEM_INT_BIT_PCS_COPLETE             16      /**< @brief PCS auto-negotiation complete */
+#define GEM_INT_BIT_PCS_LINK_RECEIVED       17      /**< @brief PCS link partner page received */
+
+// Timestamp interrupts for PTP/IEEE-1588
+#define GEM_INT_BIT_TSU_RX_PTP_DLY_REQ      18      /**< @brief PTP delay_request   frame received */
+#define GEM_INT_BIT_TSU_RX_PTP_SYNC_FRM     19      /**< @brief PTP sync            frame received */
+#define GEM_INT_BIT_TSU_TX_SYNC_FRM         20      /**< @brief PTP sync            frame transmitted */
+#define GEM_INT_BIT_TSU_TX_PDLY_REQ         21      /**< @brief PTP pdelay_request  frame transmitted */
+#define GEM_INT_BIT_TSU_RX_PTP_PDLY_REQ     22      /**< @brief PTP pdelay_request  frame received */
+#define GEM_INT_BIT_TSU_RX_PTP_PDLY_RESP    23      /**< @brief PTP pdelay_response frame received */
+#define GEM_INT_BIT_TSU_TX_PTP_DLY_REQ      24      /**< @brief PTP delay_request   frame transmitted */
+#define GEM_INT_BIT_TSU_TX_PDLY_RESP        25      /**< @brief PTP pdelay_response frame transmitted */
+#define GEM_INT_BIT_TSU_SECONDS_INCREMENT   26      /**< @brief PTP Seconds counter has incremented */
+
+#define GEM_INT_PHY_COMPLETE            (1 << 0)    /**< @brief Management frame sent - the PHY maintenance register has completed its operation */
+#define GEM_INT_RX_COMPLETE             (1 << 1)    /**< @brief Receive complete - a frame has been stored in memory */
+#define GEM_INT_RX_USED_BIT_READ        (1 << 2)    /**< @brief RX used bit read - set when a receive buffer descriptor is read with its used bit set */
+#define GEM_INT_TX_USED_BIT_READ        (1 << 3)    /**< @brief TX used bit read - set when a transmit buffer descriptor is read with its used bit set */
+#define GEM_INT_TX_UNDERRUN             (1 << 4)    /**< @brief Transmit under run */
+#define GEM_INT_RETRY_EXCEEDED          (1 << 5)    /**< @brief Retry limit exceeded or late collision - transmit error */
+#define GEM_INT_BUS_ERR                 (1 << 6)    /**< @brief Transmit frame corruption due to AHB error */
+#define GEM_INT_TX_COMPLETE             (1 << 7)    /**< @brief Transmit complete */
+#define GEM_INT_NOT_USED_8              (1 << 8)    /**< @brief Not used, reserved for possible future use */
+#define GEM_INT_LINK_CHANGE             (1 << 9)    /**< @brief Link change */
+#define GEM_INT_RX_OVERRUN              (1 << 10)   /**< @brief Receive overrun */
+#define GEM_INT_HRESP_ERR               (1 << 11)   /**< @brief Hresp not OK */
+#define GEM_INT_RX_PAUSE_FRM            (1 << 12)   /**< @brief Pause frame with non-zero pause quantum received */
+#define GEM_INT_PAUSE_ZERO              (1 << 13)   /**< @brief Pause time zero */
+#define GEM_INT_TX_PAUSE_FRM            (1 << 14)   /**< @brief Pause frame transmitted */
+#define GEM_INT_EXTERNAL                (1 << 15)   /**< @brief External interrupt */
+#define GEM_INT_PCS_COMPLETE            (1 << 16)   /**< @brief PCS auto-negotiation complete */
+#define GEM_INT_PCS_LINK_RECEIVED       (1 << 17)   /**< @brief PCS link partner page received */
+
+#define GEM_INT_TSU_RX_PTP_DLY_REQ      (1 << 18)   /**< @brief PTP delay_request   frame received */
+#define GEM_INT_TSU_RX_PTP_SYNC_FRM     (1 << 19)   /**< @brief PTP sync frame received */
+#define GEM_INT_TSU_TX_PTP_DLY_REQ      (1 << 20)   /**< @brief PTP delay_request   frame transmitted */
+#define GEM_INT_TSU_TX_PTP_SYNC_FRM     (1 << 21)   /**< @brief PTP sync            frame transmitted */
+#define GEM_INT_TSU_RX_PTP_PDLY_REQ     (1 << 22)   /**< @brief PTP pdelay_request  frame received */
+#define GEM_INT_TSU_RX_PTP_PDLY_RESP    (1 << 23)   /**< @brief PTP pdelay_response frame received */
+#define GEM_INT_TSU_TX_PTP_PDLY_REQ     (1 << 24)   /**< @brief PTP pdelay_request  frame transmitted */
+#define GEM_INT_TSU_TX_PTP_PDLY_RESP    (1 << 25)   /**< @brief PTP pdelay_response frame transmitted */
+#define GEM_INT_TSU_SECONDS_INCREMENT   (1 << 26)   /**< @brief PTP Seconds counter has incremented */
+
+// New bits for version 25:
+#define GEM_INT_WAKE_ON_LAN             (1 << 27)   /**< @brief WOL interrupt. Indicates a WOL event has been received */
+#define GEM_INT_RX_LPI_STATUS_CHANGE    (1 << 28)   /**< @brief Receive LPI indication status bit change */
+#define GEM_INT_TSU_TIMER_COMPARE_EQUAL (1 << 29)   /**< @brief Inidcates when TSU timer count value is equal to programmed value */
+
+
+
+//
+// GEMCORE Configuration Register - Offset 0xF000
+//
+
+//
+// Interface Mode select MODE SEL (read/write)
+//
+#define GEM_CFG_MODE_SEL_PIN                (0 << 0)  /**< (RW) Pin Mode: Interface mode selected based on GEM#_MODE external pins */
+#define GEM_CFG_MODE_SEL_GEM                (1 << 0)  /**< (RW) GEM Mode: Interface mode selected based on GEMCORE configuraiton register bits [3:1] */
+//
+// Interface selection control GEM_MODE[3:1]
+//
+#define GEM_CFG_MODE_GEM_RGMII              (0 << 1)  /**< (RW) GEM_MODE[3:1], RGMII Mode: If GEM_CFG_MODE_SEL_GEM set, else ignored */
+#define GEM_CFG_MODE_GEM_SGMII              (4 << 1)  /**< (RW) GEM_MODE[3:1], SGMII Mode: If GEM_CFG_MODE_SEL_GEM set, else ignored */
+#define GEM_CFG_MODE_GEM_S3MII              (6 << 1)  /**< (RW) GEM_MODE[3:1], S3MII Mode: If GEM_CFG_MODE_SEL_GEM set, else ignored */
+
+//
+// Interface pin strap mode GEM_MODE[6:4]
+//
+#define GEM_CFG_PIN_STRAP_GEM_RGMII         (0 << 4)  /**< (RW) GEM_MODE[3:1], RGMII Mode: If GEM_CFG_MODE_SEL_GEM clear, pin selects GEM mode */
+#define GEM_CFG_PIN_STRAP_GEM_SGMII         (4 << 4)  /**< (RW) GEM_MODE[3:1], SGMII Mode: If GEM_CFG_MODE_SEL_GEM clear, pin selects GEM mode */
+#define GEM_CFG_PIN_STRAP_GEM_S3MII         (6 << 4)  /**< (RW) GEM_MODE[3:1], S3MII Mode: If GEM_CFG_MODE_SEL_GEM clear, pin selects GEM mode */
+
+/**
+ * @brief bit 7 Half Duplex flow control enable, Default: 0x0, Access: RW
+ * Bit values:
+ *  - 0: Flow control disabled
+ *  - 1: Flow control enabled - back pressure
+ */
+#define GEM_CFG_HDX_FLOW_CONTROL_ENABLE     (1 << 7)
+
+//
+// GEM Duplex MUX select (read/write):
+//
+#define GEM_CFG_DUPLEX_SEL_PHY              (0 << 8)  /**< (RW) Duplex Mux Select, controlled by PHY */
+#define GEM_CFG_DUPLEX_SEL_GEM              (1 << 8)  /**< (RW) Duplex Mux Select, controlled by GEM */
+//
+// GEM Duplex Mux select (if GEM_CFG_DUPLEX_SEL_GEM is true) (read/write):
+//
+#define GEM_CFG_GEM_DUPLEX_SHIFT_COUNT      9         /**< Shift count for getting GEM duplex as a number 0 or 1 */
+#define GEM_CFG_DUPLEX_GEM_HALF             (0 << 9)  /**< (RW) Duplex Mux Select: Half duplex (if GEM_CFG_DUPLEX_SEL_GEM is set) */
+#define GEM_CFG_DUPLEX_GEM_FULL             (1 << 9)  /**< (RW) Duplex Mux Select: Full duplex (if GEM_CFG_DUPLEX_SEL_GEM is set) */
+//
+// RGMII: PHY Duplex from PHY's side-band or in-band signal (read only)
+//
+#define GEM_CFG_PHY_DUPLEX_SHIFT_COUNT      10        /**< Shift count for getting PHY duplex as a number 0 or 1 */
+#define GEM_CFG_DUPLEX_PHY_HALF             (0 << 10) /**< (RO) RGMII PHY indicates half duplex status */
+#define GEM_CFG_DUPLEX_PHY_FULL             (1 << 10) /**< (RO) RGMII PHY indicates full duplex status */
+//
+// Speed Mux select
+//
+#define GEM_CFG_SPEED_SEL_PHY               (0 << 11) /**< (RW) RGMII: Speed Mux controlled by external Ethernet PHY */
+#define GEM_CFG_SPEED_SEL_GEM               (1 << 11) /**< (RW) RGMII: Speed Mux controlled by GEMCORE configuration speed (bits 13:12) */
+
+//
+// GEM speed control bits (read/write)
+//
+#define GEM_CFG_GEM_SPEED_SHIFT_COUNT       12        /**< Shift count for getting GEM speed as a number 0, 1 or 2 */
+#define GEM_CFG_SPEED_MASK                  (3 << 12) /**< Mask for getting/setting GEM speed control */
+#define GEM_CFG_SPEED_GEM_10M               (0 << 12) /**< (RW) RGMII: GEM speed  10 Megabits/second */
+#define GEM_CFG_SPEED_GEM_100M              (1 << 12) /**< (RW) RGMII: GEM speed 100 Megabits/second */
+#define GEM_CFG_SPEED_GEM_1G                (2 << 12) /**< (RW) RGMII, SGMII: GEM speed 1 Gigabit/second */
+//
+// PHY Speed status bits (read only)
+//
+#define GEM_CFG_PHY_SPEED_SHIFT_COUNT       14        /**< Shift count for getting PHY speed as a number 0, 1 or 2 */
+#define GEM_CFG_SPEED_PHY_10M               (0 << 14) /**< (RO) RGMII: PHY speed  10 Megabits/second */
+#define GEM_CFG_SPEED_PHY_100M              (1 << 14) /**< (RO) RGMII: PHY speed 100 Megabits/second */
+#define GEM_CFG_SPEED_PHY_1G                (2 << 14) /**< (RO) RGMII: PHY speed   1 Gigaabit/second */
+//
+// PHY Link status (read only)
+//
+#define GEM_CFG_PHY_LINK_DOWN               (0 << 16) /**< (RO) RGMII: PHY link from PHY's side band or in-band signal, 0: link down, 1: Link up */
+#define GEM_CFG_PHY_LINK_UP                 (1 << 16) /**< (RO) RGMII: PHY link from PHY's side band or in-band signal, 0: link down, 1: Link up */
+//
+// Test mode GEM loopback (Ethernet frames sent to GEM are internally looped back to receive side
+//
+#define GEM_CFG_GEM_LOOPBACK                (1 << 17) /**< (RO) GEMCORE internal interface level loopback, 1: enable, 0: disable */
+//
+// Bits 24:18
+// 24:18	Mac-to-Mac Connections	Hardware emulation of 7 in-band controlled bits sent after each TX frame. Can be used in direct MAC-to-
+// MAC connection.
+// Used for chip testing only
+//
+// Bit 18 - 1: Error in connection
+// Bit 19 - 1: 100Mbps
+// Bit 20 - 1: Full Duplex
+// Bit 21 - 1: Link up
+// Bit 22 - 0: No Jabber
+// Bit 23 - 1: Reserved
+// Bit 24 - 1: Reserved
+//
+//
+#define GEM_CFG_MAC_MAC_ERROR               (1 << 18)
+#define GEM_CFG_MAC_MAC_100M                (1 << 19)
+#define GEM_CFG_MAC_MAC_FULL_DUPLEX         (1 << 20)
+#define GEM_CFG_MAC_MAC_LINK_UP             (1 << 21)
+#define GEM_CFG_MAC_MAC_NO_JABBER           (1 << 22)
+#define GEM_CFG_MAC_MAC_RESERVED_23         (1 << 23)
+#define GEM_CFG_MAC_MAC_RESERVED_24         (1 << 24)
+
+#define GEM_CFG_MAC_MAC_S3MII               ( GEM_CFG_MAC_MAC_ERROR         \
+                                            | GEM_CFG_MAC_MAC_100M          \
+                                            | GEM_CFG_MAC_MAC_FULL_DUPLEX   \
+                                            | GEM_CFG_MAC_MAC_LINK_UP       \
+                                            | GEM_CFG_MAC_MAC_RESERVED_23   \
+                                            | GEM_CFG_MAC_MAC_RESERVED_24   \
+                                            )
+
+
+/******************************************************************************
+*       Definition of the GEM 1588 timer and PTP control macros               *
+*******************************************************************************/
+
+#define GEM_1588_NANSECONDS_MASK        0x3FFFFFFF  /**< @brief Mask for all nanosecond based registers */
+#define GEM_1588_GET_NANOSECONDS(reg)   (*((UINT32*)reg) & GEM_1588_NANOSECONDS_MASK)
+#define GEM_1588_GET_SECONDS(reg)       (*((UINT32*)reg))
+
+// 1588 Timer Adjust register: 0x1D8
+#define GEM_1588_TMR_ADJUST_ADD         (0UL << 31)
+#define GEM_1588_TMR_ADJUST_SUBTRACT    (1UL << 31)
+#define GEM_1588_TMR_ADJUST_RSVD        (0UL << 30)
+
+// 1588 Timer Increment register: 0x1DC
+#define GEM_1588_TME_INCR_RSVD_MASK     (0xFF << 24)
+#define GEM_1588_TMR_INCR_ALT_NUM_MASK  (0xFF << 16)
+#define GEM_1588_TMR_INCR_ALT_NSECS     (0xFF <<  8)
+#define GEM_1588_TMR_INCR_NSEC_COUNT    (0xFF)
+
+#define GEM_1588_TMR_INCR_SET(altnum, altnsecs, nsecs) ((altnum   << 16) |\
+                                                        (altnsecs <<  8) |\
+                                                        nsecs             \
+                                                       )
+
+#define NANOSECONDS_PER_SECOND 1000000000UL
+#define SECONDS_AND_NANOSECONDS_TO_NANOSECONDS(secs,nsecs) ((UINT64)((((UINT64)(secs)) * NANOSECONDS_PER_SECOND) + nsecs))
+
+//
+// Defines for T2200/T3300 GPIO block TSU configuration register
+//
+
+/* 3.1.19	[0x0B0 RW] TSU Configuration Register
+(Default: 0x00000122)
+*/
+/** @brief Bit 8: TSU PPS source select: (one pulse every second).
+
+Bit Values:
+  - '0': Use GEM0 TSU timer compare pulse as PPS clock output.
+  - '1': Use GEM1 TSU timer compare pulse as PPS clock output.
+*/
+#define SET_TSU_CONFIG_PPS_SOURCE_SELECT(x) (x <<8)
+#define TSU_CONFIG_PPS_SOURCE_GEM0          SET_TSU_CONFIG_PPS_SOURCE_SELECT(0)
+#define TSU_CONFIG_PPS_SOURCE_GEM1          SET_TSU_CONFIG_PPS_SOURCE_SELECT(1)
+#define TSU_CONFIG_PPS_SOURCE_MASK          SET_TSU_CONFIG_PPS_SOURCE_SELECT(1)
+
+/** @brief Bits 6:5 GEM1 (RGMII) TSU Incremental Control.
+
+Used to control incrementing of the TSU and synchronous to tsu_clk.
+
+Bit values:
+  - '11' -  timer register increments as normal.
+  - '01' - timer register increments by an additional nanosecond.
+  - '10' -  timer increments by a nanosecond less.
+  - '00' -  When gem_tsu_ms = '1', the "nanoseconds" timer register is cleared
+            and the "seconds" timer register is incremented with each clock
+            cycle. \n
+            When gem_tsu_ms = '0', the timer register increments as normal
+            but the timer value is copied to the sync strobe register.
+*/
+#define TSU_CONFIG_GEM1_TSU_INCREMENTAL_CONTROL(x)               (x << 5)
+#define TSU_CONFIG_GEM1_INCREMENTAL_CONTROL_MASK                 TSU_CONFIG_GEM1_TSU_INCREMENTAL_CONTROL(3)
+#define TSU_CONFIG_GEM1_NORMAL_INCREMENTS                        TSU_CONFIG_GEM1_TSU_INCREMENTAL_CONTROL(3)
+#define TSU_CONFIG_GEM1_ADDITIONAL_NANOSECOND_INCREMENTS         TSU_CONFIG_GEM1_TSU_INCREMENTAL_CONTROL(2)
+#define TSU_CONFIG_GEM1_NANOSECOND_LESS_INCREMENTS               TSU_CONFIG_GEM1_TSU_INCREMENTAL_CONTROL(1)
+#define TSU_CONFIG_GEM1_SECONDS_ONLY_OR_SYNC_STROBE_INCREMENTS   TSU_CONFIG_GEM1_TSU_INCREMENTAL_CONTROL(0)
+
+
+/** @brief Bit 4: GEM1 (RGMII) TSU MS (master/slave).
+
+Used with gem_tsu_inc_ctrl to control
+incrementing of the TSU and loading the sync strobe
+register.
+
+Bit values:
+  - '0' - Block is configured as 1588 Slave mode.
+  - '1' - Block is configured as 1588 Master mode (PTP Master).
+*/
+#define TSU_CONFIG_GEM1_PTP_MASTER_SLAVE(x) (x << 4)
+#define TSU_CONFIG_GEM1_PTP_MASTER          TSU_CONFIG_GEM1_PTP_MASTER_SLAVE(1)
+#define TSU_CONFIG_GEM1_PTP_SLAVE           TSU_CONFIG_GEM1_PTP_MASTER_SLAVE(0)
+
+/** @brief Bits 2:1 GEM0 (SGMII/1000BASE-X)TSU Incremental Control.
+
+Used to control incrementing of the TSU and synchronous to tsu_clk.
+
+Bit values:
+  - '11' -  timer register increments as normal.
+  - '01' -  timer register increments by an additional nanosecond.
+  - '10' -  timer increments by a nanosecond less.
+  - '00' -  When gem_tsu_ms = '1', the "nanoseconds" timer register is cleared
+            and the "seconds" timer register is incremented with each clock
+            cycle. \n
+            When gem_tsu_ms = '0', the timer register increments as normal
+            but the timer value is copied to the sync strobe register.
+*/
+#define TSU_CONFIG_GEM0_TSU_INCREMENTAL_CONTROL(x)               (x << 1)
+#define TSU_CONFIG_GEM0_INCREMENTAL_CONTROL_MASK                 TSU_CONFIG_GEM0_TSU_INCREMENTAL_CONTROL(3)
+#define TSU_CONFIG_GEM0_NORMAL_INCREMENTS                        TSU_CONFIG_GEM0_TSU_INCREMENTAL_CONTROL(3)
+#define TSU_CONFIG_GEM0_ADDITIONAL_NANOSECOND_INCREMENTS         TSU_CONFIG_GEM0_TSU_INCREMENTAL_CONTROL(2)
+#define TSU_CONFIG_GEM0_NANOSECOND_LESS_INCREMENTS               TSU_CONFIG_GEM0_TSU_INCREMENTAL_CONTROL(1)
+#define TSU_CONFIG_GEM0_SECONDS_ONLY_OR_SYNC_STROBE_INCREMENTS   TSU_CONFIG_GEM0_TSU_INCREMENTAL_CONTROL(0)
+
+/** @brief Bit 0: GEM0 (SGMII/1000BASE-X) TSU MS (master/slave).
+
+Used with gem_tsu_inc_ctrl to control
+incrementing of the TSU and loading the sync strobe
+register.
+
+Bit values:
+  - '0' - Block is configured as 1588 Slave mode.
+  - '1' - Block is configured as 1588 Master mode (PTP Master).
+*/
+#define TSU_CONFIG_GEM0_PTP_MASTER_SLAVE(x) (x << 0)
+#define TSU_CONFIG_GEM0_PTP_MASTER          TSU_CONFIG_GEM0_PTP_MASTER_SLAVE(1)
+#define TSU_CONFIG_GEM0_PTP_SLAVE           TSU_CONFIG_GEM0_PTP_MASTER_SLAVE(0)
+
+#define TSU_CONFIG_GEM0_MASK         ((0xF << 0) | TSU_CONFIG_PPS_SOURCE_MASK)
+#define TSU_CONFIG_GEM1_MASK         ((0xF << 4) | TSU_CONFIG_PPS_SOURCE_MASK)
+
+#define TSU_CONFIG_MASK (TSU_CONFIG_GEM0_MASK | TSU_CONFIG_GEM1_MASK)
+
+/** @brief Misc. Pin select select register bits for selecting GEM TSU clock input options
+ *
+ * Bit options (binary):
+ *  - "00" - Select GEM NTG    bit-clock to drive TSU clock
+ *  - "01" - Select TSUVCXO    input pin to drive TSU clock
+ *  - "10" - Select FRAME_CLK  input pin to drive TSU clock
+ *  - "11" - Disable TSU clock input
+ *           - NOTE: usefull to synchronize the two TSU clocks, disable clock,
+ *             set value same, enable clock
+ */
+#define TSU_MISC_PIN_REGISTER_TSU_CLOCK_SELECT(x)               (x << 13)
+#define TSU_SELECT_CLOCK_INPUT_INTERNAL_FROM_NTG                TSU_MISC_PIN_REGISTER_TSU_CLOCK_SELECT(0)
+#define TSU_SELECT_CLOCK_INPUT_EXTERNAL_TSUVCXO_PIN             TSU_MISC_PIN_REGISTER_TSU_CLOCK_SELECT(1)
+#define TSU_SELECT_CLOCK_INPUT_EXTERNAL_FRAME_CLK_PIN           TSU_MISC_PIN_REGISTER_TSU_CLOCK_SELECT(2)
+#define TSU_SELECT_CLOCK_INPUT_DISABLED                         TSU_MISC_PIN_REGISTER_TSU_CLOCK_SELECT(3)
+#define TSU_SELECT_CLOCK_INPUT_MASK                             TSU_MISC_PIN_REGISTER_TSU_CLOCK_SELECT(3)
+
+
+/** @brief Misc. Pin select select register bits for selecting GEM TSU clock input options
+ *
+ * Bit options (binary):
+ *  - '0' - Select TSU Timer PPS to drive PPS_CLK pin.  The selection between GEM0 TSU and
+ *          GEM1 TSU is based on TSU_PPS_SEL bit in TSU configuration register
+ *  - '1' - Use PPS_CLK pin as IRQ_REQ_EXT (T3300 feature)
+ */
+#define TSU_MISC_PIN_REGISTER_TSU_PPS_SELECT(x)                 (x << 12)
+#define TSU_SELECT_PPS_CLOCK_OUTPUT_FROM_TSU_TIMER_PPS          TSU_MISC_PIN_REGISTER_TSU_PPS_SELECT(0)
+#define TSU_SELECT_PPS_CLOCK_AS_IRQ_REQ_EXT                     TSU_MISC_PIN_REGISTER_TSU_PPS_SELECT(1)
+#define TSU_SELECT_PPS_CLOCK_OUTPUT_MASK                        TSU_MISC_PIN_REGISTER_TSU_PPS_SELECT(1)
+
+#define TSU_MISC_PIN_REGISTER_MASK  (TSU_SELECT_CLOCK_INPUT_MASK | TSU_SELECT_PPS_CLOCK_OUTPUT_MASK)
+
+
+
+/**
+  @brief NTG Control register [offset 0x20]  (default: 0_1110_0000 b)
+
+Bit 0 *RW Half_enable:
+  - '0' = Sets the divider to use steps of whole high-speed cycles.
+  - '1' = Sets the divider to use steps of  high-speed cycles.
+
+Bits 2:1 *RW Pulse_mode:
+  - "00" = Programmable frame width according to Frame_Pulse_width register.
+  - "01" = 50% of Pulse_frame_out duty cycle.
+  - "10" =  Clock_bit_out cycle pulse.
+  - "11" = reserved.
+
+Bit 3 *RW Frame_count_reset:
+ - '0' = no effect.
+ - '1' = Clears the frame counter to zero. This bit will automatically self-clear to '0' once the frame counter resets to zero.
+
+Bit 4 *RW Clock_bit_inverted:
+It tells DPLL the falling edge of bit clock is to be used.
+  - '0' = Use the rising edge of the bit clock.
+  - '1' = Use the falling edge of the bit clock.
+
+Bit 5 *RW Clock_bit_direction:
+  - '0' = Sets 'clock_bit_out' as an output.
+  - '1' = Disable 'clock_bit_out' as an output.
+
+Bit 6 *RW Pulse_frame_direction:
+  - '0'= Sets 'pulse_frame_out' as an output.
+  - '1' = Disable 'pulse_frame_out' as an output.
+
+Bit 7 RW Register_update:
+Controls the connection between the register space and the divider logic. It affects only those register fields with (*) in 'Type' column.
+  - '0' = No update. Changes to the registers are not relayed to the divider logic and divider status is not updated to the register space.
+  - '1' = Update Enabled. The values are relayed between the registers and the divider logic. (default)
+
+Bit 8 *RW Pulse_frame_inverted:
+  - '0' = No inversion.
+  - '1' = Inverts the output 'pulse_frame_out'.
+
+Bit 9 RW DPLL Enable:
+  - '0' = DPLL not enabled.
+  - '1' = DPLL enabled.
+
+Bit 10 RW DPLL Lock Assertion IRQ Enable:
+  - '1' = Generate IRQ when DPLL Lock assert.
+  - '0' = Disable IRQ generation when DPLL Lock assert.
+
+Bit 11 RW DPLL Lock De-assertion IRQ Enable:
+  - '1' = Generate IRQ when DPLL Lock de-assert.
+  - '0' = Disable IRQ generation when DPLL Lock de-assert.
+31:12  reserved
+*/
+
+#define NTG_CONTROL_HALF_ENABLE                 (1<<0)
+#define NTG_CONTROL_PROGRAMMABLE_FRAME_WIDTH    (0<<1)
+#define NTG_CONTROL_50_PERCENT_PULSE_FRAME_OUT  (1<<1)
+#define NTG_CONTROL_HALF_CLOCK_BIT_OUT_PULSE    (2<<1)
+#define NTG_CONTROL_FRAME_COUNT_RESET           (1<<3)
+#define NTG_CONTROL_CLOCK_BIT_INVERTED          (1<<4)
+#define NTG_CONTROL_CLOCK_BIT_OUT               (0<<5)
+#define NTG_CONTROL_CLOCK_BIT_IN                (1<<5)
+#define NTG_CONTROL_FRAME_PULSE_OUT             (0<<6)
+#define NTG_CONTROL_FRAME_PULSE_IN              (1<<6)
+#define NTG_CONTROL_REGISTER_UPDATE             (1<<7)  /**< Register update       */
+
+
+#define NTG_PLL(x)                      ((x+1)<<1)
+#define NTG_PLL0					    (1<<1)
+#define NTG_PLL1					    (2<<1)
+#define NTG_PLL2					    (3<<1)
+#define NTG_PLL3					    (4<<1)
+#define NTG_PLL4					    (5<<1)
+#define NTG_PLL5					    (6<<1)
+
+// TDMNTG_CLK_DIV_CNTRL & GEMNTG_CLK_DIV_CNTRL bits definition
+
+#define NTG_DIV_RATIO_MASK              (0x1F<<0)
+#define NTG_MAX_DIVISOR                 0x1F
+#define NTG_MIN_DIVISOR                 0x02
+#define NTG_CLK_DIV_BYPASS              (1<<7)
+
+//
+// Configuration values for TDM NTG, change as necessary based on input to NTG and
+// output frequency usage:
+//
+#define TDM_NTG_PLL_NUMBER              1
+
+#define DEFAULT_TDM_NTG_PLL              NTG_PLL(TDM_NTG_PLL_NUMBER)
+
+#define TDM_NTG_OUTPUT_FREQUENCY_8MHZ     8192000  /**< Output speed for 128 x 64 KBPS timeslots */
+#define TDM_NTG_OUTPUT_FREQUENCY_16MHZ   16384000  /**< Output speed for 256 x 64 KBPS timeslots */
+#define TDM_NTG_OUTPUT_FREQUENCY_32MHZ   32768000  /**< Output speed for 512 x 64 KBPS timeslots */
+
+//
+// Configuration values for GEM NTG, change as necessary based on input to NTG and
+// output frequency usage:
+//
+#define GEM_NTG_PLL_NUMBER              1
+
+#define DEFAULT_GEM_NTG_PLL             NTG_PLL(GEM_NTG_PLL_NUMBER)
+
+/* NOTE: Defines for NTG output frequencies below were from T4000 EVM board,
+ * TBD on actual frequencies to be used for AD9548 on T2200/T3300 EVM boards
+ *
+ * Also defines for these functions should be in the Dejitter driver, not
+ * the NTG driver which should be agnostic on output frequencies. So
+ * These defines may be removed in the future from this file.
+ */
+#define GEM_NTG_OUTPUT_FREQUENCY_CPRI    12800000  /**< 12.8 MHz Output speed to Dejitter Circuit for CPRI and IEEE 1588 testing */
+#define GEM_NTG_OUTPUT_FREQUENCY_RADIO   40000000  /**< 40.0 MHz Output speed to Dejitter Circuit for CPRI and IEEE 1588 testing */
+
+
+
+/** T2200/T3300 GEM Core register as structure */
+typedef struct tGEMCOREREGS
+{
+    VUINT32     Cfg;        /**< @brief GEMCORE Configuration Register */
+    VUINT32     MBIST;      /**< @brief GEMCORE MBIST Register */
+    VUINT32     Sense;      /**< @brief GEMCORE Sense Amp Delay Adjustment Register */
+    VUINT32     Delay;      /**< @brief GEMCORE Delay Element Control Register */
+
+} GEMCOREREGS, *PGEMCOREREGS;
+
+/** @brief This structure describes the statistics registers map of Cadence GEM
+    offsets 0x100 through 0x1C4
+    based on the "GEM_UserGuide_d14.pdf" document */
+
+typedef struct tGEMSTATREGS
+{
+    // -----------------------------------------------
+    // TX statistics  (Tx)
+    // -----------------------------------------------
+    VUINT32     TxBytes_0_31;       /**< @brief 100: Octets transmitted [31:0] (in frames without error) */
+    VUINT32     TxBytes_32_47;      /**< @brief 100: Octets transmitted [47:32] (in frames without error) */
+    VUINT32     TxFrames;           /**< @brief 108: Frames           transmitted without error, excluding pause frames */
+    VUINT32     TxBcstFrames;       /**< @brief 10C: Broadcast frames transmitted without error, excluding pause frames */
+    VUINT32     TxMcstFrames;       /**< @brief 110: Multicast frames transmitted without error, excluding pause frames */
+    VUINT32     TxPauseFrames;      /**< @brief 114: Transmitted pause frames (unless sent through external FIFO interface) */
+    VUINT32     Tx64Frames;         /**< @brief 118:           64 byte frames transmitted without error, excluding pause frames */
+    VUINT32     Tx65_127Frames;     /**< @brief 11C:   65 to  127 byte frames transmitted without error */
+    VUINT32     Tx128_255Frames;    /**< @brief 120:  128 to  255 byte frames transmitted without error */
+    VUINT32     Tx256_511Frames;    /**< @brief 124:  256 to  511 byte frames transmitted without error */
+    VUINT32     Tx512_1023Frames;   /**< @brief 128:  512 to 1023 byte frames transmitted without error */
+    VUINT32     Tx1024_1518Frames;  /**< @brief 12C: 1024 to 1518 byte frames transmitted without error */
+    VUINT32     TxGt1518Frames;     /**< @brief 130: Greater than 1518 byte frames transmitted without error */
+    VUINT32     TxUnderrunErr;      /**< @brief 134: Transmit under run errors */
+    VUINT32     TxSingleCollFrames; /**< @brief 138: Single    collision frames */
+    VUINT32     TxMultCollFrames;   /**< @brief 13C: Multiple  collision frames */
+    VUINT32     TxExcCollFrames;    /**< @brief 140: Excessive collisions */
+    VUINT32     TxLateCollFrames;   /**< @brief 144: Late      collisions */
+    VUINT32     TxDeferredFrames;   /**< @brief 148: Deferred transmission frames */
+    VUINT32     TxCarrierSenseErr;  /**< @brief 14C: Carrier sense errors */
+
+    // -----------------------------------------------
+    // RX statistics  (Rx)
+    // -----------------------------------------------
+    VUINT32     RxBytes_0_31;       /**< @brief 150: Octets received [31:0] (in frames without error) */
+    VUINT32     RxBytes_32_47;      /**< @brief 150: Octets received [47:32] (in frames without error) */
+    VUINT32     RxFrames;           /**< @brief 158: Frames received without error, excluding pause frames */
+    VUINT32     RxBcstFrames;       /**< @brief 15C: Broadcast frames received without error, excluding pause frames */
+    VUINT32     RxMcstFrames;       /**< @brief 160: Multicast frames received without error,excluding pause frames */
+    VUINT32     RxPauseFrames;      /**< @brief 164: received pause frames (unless sent through external FIFO interface) */
+    VUINT32     Rx64Frames;         /**< @brief 168:           64 byte frames received without error, excluding pause frames */
+    VUINT32     Rx65_127Frames;     /**< @brief 16C:   65 to  127 byte frames received without error */
+    VUINT32     Rx128_255Frames;    /**< @brief 170:  128 to  255 byte frames received without error */
+    VUINT32     Rx256_511Frames;    /**< @brief 174:  256 to  511 byte frames received without error */
+    VUINT32     Rx512_1023Frames;   /**< @brief 178:  512 to 1023 byte frames received without error */
+    VUINT32     Rx1024_1518Frames;  /**< @brief 17C: 1024 to 1518 byte frames received without error */
+    VUINT32     RxGt1518Frames;     /**< @brief 180: Greater than 1518 byte frames received without error */
+    VUINT32     RxUndersizeFrames;  /**< @brief 184: Undersize frames received */
+    VUINT32     RxOversizeFrames;   /**< @brief 188: Oversize  frames received */
+    VUINT32     RxJabbersFrames;    /**< @brief 18C: Jabbers          received */
+    VUINT32     RxFrmChkSeqErr;     /**< @brief 190: Frame check sequence errors */
+    VUINT32     RxLenErr;           /**< @brief 194: Length field frame errors */
+    VUINT32     RxRecvSymErr;       /**< @brief 198: Receive symbol errors */
+    VUINT32     RxAlignErr;         /**< @brief 19C: Alignment errors */
+    VUINT32     RxRecvResErr;       /**< @brief 1A0: Receive resource errors */
+    VUINT32     RxRecvOvrErr;       /**< @brief 1A4: Receive overrun errors */
+    VUINT32     RxIPCksumErr;       /**< @brief 1A8: IP header checksum errors */
+    VUINT32     RxTCPCksumErr;      /**< @brief 1AC: TCP checksum errors */
+    VUINT32     RxUDPCksumErr;      /**< @brief 1B0: UDP checksum errors */
+    // New for version 25:
+    VUINT32     RxDMABufFlushedPkts;/**< @brief 1B4: Receive DMA Packet Buffer Flushed Packets */
+
+    VUINT32     RxStatsRsvd1B8;     /**< @brief 1B8: Reserved */
+    VUINT32     RxStatsRsvd1BC;     /**< @brief 1BC: Reserved */
+    VUINT32     RxStatsRsvd1C0;     /**< @brief 1C0: Reserved */
+    VUINT32     RxStatsRsvd1C4;     /**< @brief 1C4: Reserved */
+
+} GEMSTATREGS, *PGEMSTATREGS;
+
+
+/** @brief This structure describes the registers map of Cadence GEM
+
+    based on the "GEM_UserGuide_d14.pdf" document */
+
+typedef struct tGEMIPREGS
+{
+    // 0x00 offset
+
+    VUINT32     NetCtrl;            /**< @brief 000 R/W: Network control */
+    VUINT32     NetCfg;             /**< @brief 004 R/W: Network configuration */
+    VUINT32     NetStat;            /**< @brief 008 RO:  Network status, read only register */
+    VUINT32     UserIO;             /**< @brief 00C R/W: User input/output */
+    VUINT32     DMACfg;             /**< @brief 010 R/W: DMA configuration */
+    VUINT32     TxStatus;           /**< @brief 014 R/W: Transmit status */
+    VUINT32     RxQueue;            /**< @brief 018 R/W: Receive buffer queue base address */
+    VUINT32     TxQueue;            /**< @brief 01C R/W: Transmit buffer queue base address */
+    VUINT32     RxStatus;           /**< @brief 020 R/W: Receive status */
+    VUINT32     IntStat;            /**< @brief 024 R/W: Interrupt status */
+    VUINT32     IntEnable;          /**< @brief 028 WO:  Interrupt enable */
+    VUINT32     IntDisable;         /**< @brief 02C WO:  Interrupt disable */
+    VUINT32     IntMask;            /**< @brief 030 RO:  Interrupt mask */
+
+    // 0x34 offset
+    VUINT32     PHY;                /**< @brief 034 R/W: PHY maintenance, GEM0 only, used for MMIO data TX/RX */
+
+    VUINT32     RxPauseQuantum;     /**< @brief 038 RO:  Received pause quantum */
+    VUINT32     TxPauseQuantum;     /**< @brief 03C R/W: Transmit pause quantum */
+
+    // Registers 0x40 & 0x44 new for user guide version 25
+    VUINT32     TxPartialForward;   /**< @brief 040 R/W: Tx partial store and forward */
+    VUINT32     RxPartialForward;    /**< @brief 044 R/W: Rx partial store and forward */
+
+    VUINT32     Res1[14];           /**< @brief 048-07C N/A: Reserved */
+
+    // 0x80 offset
+
+    VUINT32     HashRegBottom;      /**< @brief 080 R/W: Hash register bottom      [31:0]  */
+    VUINT32     HashRegTop;         /**< @brief 084 R/W: Hash register top         [63:32] */
+    VUINT32     SpecAddr1Bottom;    /**< @brief 088 R/W: Specific address 1 bottom [31:0]  */
+    VUINT32     SpecAddr1Top;       /**< @brief 08C R/W: Specific address 1 top    [47:32] */
+    VUINT32     SpecAddr2Bottom;    /**< @brief 090 R/W: Specific address 2 bottom [31:0]  */
+    VUINT32     SpecAddr2Top;       /**< @brief 094 R/W: Specific address 2 top    [47:32] */
+    VUINT32     SpecAddr3Bottom;    /**< @brief 098 R/W: Specific address 3 bottom [31:0]  */
+    VUINT32     SpecAddr3Top;       /**< @brief 09C R/W: Specific address 3 top    [47:32] */
+    VUINT32     SpecAddr4Bottom;    /**< @brief 0A0 R/W: Specific address 4 bottom [31:0]  */
+    VUINT32     SpecAddr4Top;       /**< @brief 0A4 R/W: Specific address 4 top    [47:32] */
+
+    // 0xA8 offset
+    VUINT32     TypeID [4];         /**< @brief 0A4-0B4 R/W: Type ID match 1-4 */
+
+    // 0xB8 offset
+    VUINT32     WakeOnLAN;          /**< @brief 0B8 R/W: Wake on LAN */
+
+    // 0xBC offset
+    VUINT32     IPGStretch;         /**< @brief 0BC R/W: IPG stretch */
+    VUINT32     StackedVLAN;        /**< @brief 0C0 R/W: Stacked VLAN */
+
+    // 0xC4 offset (new for User Guide 25)
+    VUINT32     TransmitPFCPause;   /**< @brief 0C4 R/W: Transmit PFC Pause Register */
+    VUINT32     SpecAddrMaskBottom; /**< @brief 0C8 R/W: Specific address 1 mask bottom [31:0] */
+    VUINT32     SpecAddrMaskTop;    /**< @brief 0C8 R/W: Specific address 1 mask top    [47:32] */
+    VUINT32     RxBufferAddrMask;   /**< @brief 0D0 R/W: AHB Address mask for RX data buffer accesses */
+    VUINT32     PTPRxUnicastAddr;   /**< @brief 0D4 R/W: PTP RX unicast IP destination address */
+    VUINT32     PTPTxUnicastAddr;   /**< @brief 0D8 R/W: PTP TX unicast IP destination address */
+    VUINT32     TimerCompareNanoSec;/**< @brief 0DC R/W: TSU timer comparison value nanoseconds */
+    VUINT32     TimerCompareSec;    /**< @brief 0E0 R/W: TSU timer comparison value seconds */
+
+    VUINT32     Res2 [6];           /**< @brief E4-0F8 */
+
+    // 0xFC offset
+    VUINT32     ModuleID;           /**< @brief 0FC: Module ID */
+
+    /** 0x100-0x1C4 offsets (Statistics), all read only */
+    GEMSTATREGS Stat;
+
+    /**
+     * @details Offset 0x1C8: 1588 Timer Sync Strobe registers
+     * Value of Timer Seconds and Nanoseconds registers
+     * when gem_su_ms and gem_tsu_inc_ctrl are zero
+     */
+    VUINT32     PTPTmrStrobeSecs;   /**< @brief 1C8 R/W: Strobe value seconds */
+    VUINT32     PTPTmrStrobeNSecs;  /**< @brief 1CC R/W: Strobe value nanoseconds */
+
+    // The offset is 0x1D0
+    VUINT32     TimerSec;           /**< @brief 1D0 R/W: 1588 timer seconds */
+    VUINT32     TimerNanosec;       /**< @brief 1D4 R/W: 1588 timer nanoseconds */
+    VUINT32     TimerAdjust;        /**< @brief 1D8 WO:  1588 timer adjust */
+    VUINT32     TimerIncr;          /**< @brief 1DC R/W: 1588 timer increment */
+
+    // The offset is 0x1E0
+    VUINT32     PTPTxSec;           /**< @brief 1E0 RO: PTP event frame transmitted      seconds */
+    VUINT32     PTPTxNanoSec;       /**< @brief 1E4 RO: PTP event frame transmitted      nanoseconds */
+    VUINT32     PTPRxSec;           /**< @brief 1E8 RO: PTP event frame received         seconds */
+    VUINT32     PTPRxNanoSec;       /**< @brief 1EC RO: PTP event frame received         nanoseconds */
+    VUINT32     PTPPeerTxSec;       /**< @brief 1F0 RO: PTP peer event frame transmitted seconds */
+    VUINT32     PTPPeerTxNanoSec;   /**< @brief 1F4 RO: PTP peer event frame transmitted nanoseconds */
+    VUINT32     PTPPeerRxSec;       /**< @brief 1F8 RO: PTP peer event frame received    seconds */
+    VUINT32     PTPPeerRxNanoSec;   /**< @brief 1FC RO: PTP peer event frame received    nanoseconds */
+
+    // The offset is 0x200
+    VUINT32     PCSCtrl;            /**< @brief 200 R/W: PCS control */
+    VUINT32     PCSStat;            /**< @brief 204 RO:  PCS status, read only register */
+    VUINT32     PCSHighPhyID;       /**< @brief 208 RO:  PCS upper PHY identifier */
+    VUINT32     PCSLowPhyID;        /**< @brief 20C RO:  PCS lower PHY identifier */
+    VUINT32     PCSAdv;             /**< @brief 210 R/W: PCS auto-negotiation advertisement */
+    VUINT32     PCSLink;            /**< @brief 214 RO:  PCS auto-negotiation link partner ability, read only for SGMII mode */
+    VUINT32     PCSExp;             /**< @brief 218 RO:  PCS auto-negotiation expansion */
+    VUINT32     PCSNextPage;        /**< @brief 21C R/W: PCS auto-negotiation next page */
+    VUINT32     PCSLinkNextPage;    /**< @brief 220 RO:  PCS auto-negotiation link partner next page */
+
+    VUINT32     Res4[6];            /**< @brief 224-238: Reserved */
+
+    // Offset 0x23C
+    VUINT32     PCSExtStat;         /**< @brief 23C RO: PCS extended status */
+
+    // Offset 0x240
+    VUINT32     Res240[12];         /**< @brief 240-26C N/A: Reserved */
+
+    // Offset 0x270 (new for User Guide version 25)
+    VUINT32     RxLpiTransitions;   /**< @brief 0x270 Received LPI transitions RO 0x0000_0000 */
+    VUINT32     RxLpiTime;          /**< @brief 0x274 Received LPI time        RO 0x0000_0000 */
+    VUINT32     TxLpiTransitions;   /**< @brief 0x278 Transmit LPI transitions RO 0x0000_0000 */
+    VUINT32     TxLpiTime;          /**< @brief 0x27C Transmit LPI time        RO 0x0000_0000 */
+    VUINT32     DesignCfg1;         /**< @brief 0x280 Design Configuration Register 1 RO N/A */
+    VUINT32     DesignCfg2;         /**< @brief 0x284 Design Configuration Register 2 RO N/A */
+    VUINT32     DesignCfg3;         /**< @brief 0x288 Design Configuration Register 3 RO N/A */
+    VUINT32     DesignCfg4;         /**< @brief 0x28C Design Configuration Register 4 RO N/A */
+    VUINT32     DesignCfg5;         /**< @brief 0x290 Design Configuration Register 5 RO N/A */
+    VUINT32     DesignCfg6;         /**< @brief 0x294 Design Configuration Register 6 RO N/A */
+    VUINT32     DesignCfg7;         /**< @brief 0x298 Design Configuration Register 7 RO N/A */
+
+    // Ofset 0x29C
+    VUINT32     Res5[89];           /**< @brief 29C-3FC: Reserved */
+
+    // Offset 0x400 (new for User Guide version 25 and later)
+    VUINT32     IntStatPQ[7];        /**< @brief 400-418 R/W: Interrupt status - Priority Queues 1-7 */
+    VUINT32     Res41C[9];           /**< @brief 41C-43C N/A: Reserved */
+    VUINT32     TxPQueue[7];         /**< @brief 440-458 R/W: Base address  - Tx Priority Queues 1-7 */
+    VUINT32     Res45C[9];           /**< @brief 45C-47C N/A: Reserved */
+    VUINT32     RxPQueue[7];         /**< @brief 480-498 R/W: Base address  - Rx Priority Queues 1-7 */
+    VUINT32     Res49C;              /**< @brief 49C N/A: Reserved */
+    VUINT32     ExtraRxBufSize[7];   /**< @brief 4A0 R/W: Extra Receive Buffer Size Registers, Queues 1-7 0x0000_0002 */
+    VUINT32     Res4BC[17];          /**< @brief 4BC-4FC N/A: Reserved */
+
+    // Offset 0x500 (new for User Guide version 25 and later)
+    /** @brief 0x500 to 0x53c Screening Type 1 Registers (up to 16) RW 0x0000_0000 */
+    VUINT32     ScreeningType1[16];
+
+    /** @brief 0x540 to 0x57c Screening Type 2 Registers (up to 16) RW 0x0000_0000 */
+    VUINT32     ScreeningType2[16];
+    UINT32      Res580[32];
+
+    /** @brief 0x600 to 0x618 Interrupt Enable Registers  Priority Queues 1-7 WO 0x0000_0000 */
+    VUINT32     IntEnablePQ[7];
+    VUINT32     Res61C;
+
+    /** @brief 0x620 to 0x638 Interrupt Disable Registers  Priority Queues 1-7 WO 0x0000_0000 */
+    VUINT32     IntDisablePQ[7];
+    VUINT32     Res63C;
+
+    /** @brief 0x640 to 0x658 Interrupt Mask Registers  Priority Queue 1-7 RW 0x0000_0000 */
+    VUINT32     IntMaskPQ[7];
+    VUINT32     Res65C;
+    VUINT32     Res660[32];
+
+    /** @brief 0x6e0 to 0x6fc Screen Type 2 - Ethertype Registers (up to 8)  RW 0x0000_0000 */
+    VUINT32     ScreeningType2EtherType[8];
+
+    /** @brief 0x6e0 to 0x6fc Screen Type 2 - Compare 0, 1 (up to 32)  RW 0x0000_0000 */
+    VUINT32     ScreeningType2Compare[32][2];
+
+} GEMIPREGS, *PGEMIPREGS;
+
+/** T2200 Ethernet private control data for device driver
+ *
+ * NOTE: Called "c4k" as ethtool is common between T4000/T3000/C4000/C5000 and T2200/T3300
+ */
+struct eth_c4k_priv
+{
+	struct tRXdesc *   RxBase;                     /**< Base Pointer to receive descriptor ring */
+	struct sk_buff*    RxSkbRing[MAX_RX_DESC_NT];  /**< Receive socket buffer ring */
+	u32                RxRingSize;                 /**< Size of receive ring */
+	u32                RxMaxRingSize;              /**< Max size of receive ring */
+	u32                RxtocleanIndex;             /**< Index for receive ring needed to clean */
+	u32                RxtofillIndex;              /**< Index for receive ring to fill (with socket buffers) */
+	spinlock_t         rxlock;                     /**< Receive lock for calls to spin lock */
+
+	spinlock_t         txlock;                     /**< Transmit lock for calls to spin lock */
+	struct tTXdesc *   TxBase;                     /**< Base pointer to GEM transmit descriptor ring */
+	struct tTXextdesc *TxextBase;                  /**< Pointer to IPSEC EXT descriptor ring */
+
+	struct sk_buff*    TxSkbRing[MAX_TX_DESC_NT];  /**< Transmit socket buffer ring (array of socket buffer pointers) */
+	u32                TxRingSize;                 /**< Transmit ring(s) size (note: same size for socket buffer and GEM descriptor ring) */
+	u32                TxMaxRingSize;              /**< Transmit ring(s) max size */
+	u32                Txtosend;                   /**< Transmit ring(s) current "to send" index (index for SW to ring put operation) */
+	u32                Txdone;                     /**< Transmit ring(s) completed index (used for checking used bit and releasing socket buffers) */
+	int                Txavail;                    /**< Number of TX ring slots currently available for use */
+	int                Txqueued;                   /**< Number of TX buffers currently in TX queue */
+
+	struct net_device_stats stats;                 /**< Structure of standard linux device driver statistics */
+	struct net_device *     dev;                   /**< Pointer to standard linux device driver control strucutre */
+	int                     id;                    /**< GEM ID */
+
+	struct transcede_eth_platform_data *einfo;
+
+	int                phys_rx_int;
+
+
+	void *             baseaddr;
+	void *             Descriptors_baseaddr;
+	void *             Descriptors_baseaddr_v;
+	void *             Descriptors_baseaddr_pa;
+	GEM_DEVICE         gemdev; // used for gem AL.c
+	struct napi_struct napi;                      /**< Network (New) API device driver control structure */
+
+#ifdef CONFIG_GLOBAL_POLLING
+	void *periodic_task;
+#endif	/* CONFIG_GLOBAL_POLLING */
+
+	/* PHY stuff */
+	struct phy_device * phydev;                   /**< Pointer to Linux Ethernet PHY device structure */
+	int                 oldspeed;                 /**< Previous Ethernet speed (10, 1000 or 1000) */
+	int                 oldduplex;                /**< Previous Ethernet duplex (full or half) */
+	int                 oldlink;
+
+	/* MII transceiver section. (needed for MDIO bit bang) */
+	u32                 phy_id;     /**< PHY ID */
+	struct mii_bus *    mii_bus;    /**< MDIO bus control */
+
+	enum phy_state link;
+	phy_interface_t phy_interface;
+
+	u32 msg_enable;
+	u32 flags;
+
+
+	struct hwtstamp_config hwts_config;
+	/*struct delayed_work systim_overflow_work; */
+
+	int    valid_tx_ptp;                  /* this bit is 0 or 1;    1 is set by irq routine to indicate there is frame available   0 is cleared by ioctl read from application that  there is no bit availabe */
+	struct timespec   tx_ptp_time;
+       int    hwts_work_counter;
+	 struct timespec   tx_reg_time;
+
+
+	struct sk_buff *tx_hwtstamp_skb;  /* in case we need timestamp packets in the future */
+	struct work_struct tx_hwtstamp_work;
+
+	struct ptp_clock *ptp_clock;
+	struct ptp_clock_info ptp_clock_info;
+
+
+
+};
+
+/*
+ * Exported functions:
+ */
+extern int  t2200_eth_start(struct net_device *dev);
+extern void t2200_eth_stop (struct net_device *dev);
+
+
+#endif /* _T2200_ETH_H */
+
+/* eof t2200_eth.h */
diff --git a/drivers/net/transcede/t3300_reth.c b/drivers/net/transcede/t3300_reth.c
new file mode 100644
index 0000000..228a54f
--- /dev/null
+++ b/drivers/net/transcede/t3300_reth.c
@@ -0,0 +1,633 @@
+/**
+ * @file t2200_reth.c
+ * @brief Remote Ethernet Driver, it's used like a bridge to communicate to remote cluster Ethernet driver
+ *
+ *  File path: linux/drivers/net/transcede/t2200_reth.c
+ *
+ *
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#include <linux/kernel.h>
+#include <linux/kthread.h>
+#include <linux/interrupt.h>
+#include <linux/dma-mapping.h>
+#include <linux/dmapool.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/mii.h>
+#include <linux/phy.h>
+#include <linux/platform_device.h>
+#include <mach/drv_if.h>
+#include "t3300_reth.h"
+
+#define STATE_ENA	(1<<0)
+
+static struct net_device * this_dev = NULL;
+static PDRV_ICC_KDII reth_icc_kdii_fwd_tx = NULL;
+static PDRV_ICC_KDII reth_icc_kdii_fwd_rx = NULL;
+static PDRV_ICC_KDII reth_icc_kdii_fastpath = NULL;
+
+static char _master_mac[6] = { 0x00, 0x15, 0xE1, 0x18, 0xF0, 0x42 };
+static char _slave_mac [6] = { 0x00, 0x15, 0xE1, 0x18, 0xF0, 0x40 };
+static u32  reth_on = 0;
+
+static int reth_device_open(struct net_device *dev);
+static int reth_device_close(struct net_device *dev);
+static int reth_device_xmit(struct sk_buff *skb, struct net_device *dev);
+static int reth_device_poll(struct napi_struct *napi, int budget);
+static struct net_device_stats * reth_device_get_stats(struct net_device *dev);
+static int reth_set_mac_address(struct net_device *dev, void *addr);
+static volatile u32 reth_state = 0;
+static volatile u32 reth_routing_started = 0;
+
+static const struct net_device_ops reth_netdev_ops = {
+        .ndo_open               = reth_device_open,
+        .ndo_start_xmit         = reth_device_xmit,
+        .ndo_stop               = reth_device_close,
+        .ndo_get_stats          = reth_device_get_stats,
+        .ndo_set_mac_address    = reth_set_mac_address,
+};
+
+static void reth_dump(u8 * pPack, u32 nSize)
+{
+    int i;
+
+    if (pPack == NULL)
+        return;
+
+    printk ("R-ETH dump (size:%d):\n", nSize);
+
+    for (i = 0; i < nSize; i++)
+    {
+        if ((i%16)==0)
+            printk ("\n");
+
+        printk (" %02x", pPack[i]);
+    }
+
+    printk ("\n");
+}
+
+static void t3300_reth_callback(unsigned long id)
+{
+	reth_priv* priv;
+
+	if (this_dev == NULL)
+		return;
+
+	priv = netdev_priv(this_dev);
+	if (likely(napi_schedule_prep(&priv->napi)))
+	{
+		__napi_schedule(&priv->napi);
+	}
+}
+
+static int t3300_start_timer(struct net_device * pdev)
+{
+	reth_priv* priv = netdev_priv(pdev);
+	priv->periodic_task = periodic_task_add(0, 1, t3300_reth_callback, 1, "rethRx");
+	return 0;
+}
+
+static int t3300_stop_timer(struct net_device * pdev)
+{
+	reth_priv* priv = netdev_priv(pdev);
+	int err = 0;
+
+	if (priv->periodic_task)
+	{
+		err = periodic_task_remove(priv->periodic_task);
+		priv->periodic_task = NULL;
+	}
+	return err;
+}
+
+static int reth_device_open(struct net_device *dev)
+{
+	reth_priv *priv;
+	if (reth_state & STATE_ENA)
+		return 0;
+
+	priv = (reth_priv *)netdev_priv(dev);
+
+	reth_state |= STATE_ENA;
+	
+	napi_enable(&priv->napi);
+	netif_tx_start_all_queues(dev);
+
+	t3300_start_timer(dev);
+	return 0;
+}
+
+static int reth_device_close(struct net_device *dev)
+{
+	reth_state &= ~STATE_ENA;
+	t3300_stop_timer(dev);
+	return 0;
+}
+
+static int reth_device_tx(struct sk_buff *skb, struct net_device *dev)
+{
+	reth_priv *priv = (reth_priv *)netdev_priv(dev);
+	void * pBlk;
+	u8* pU8Blk;
+	u32 size = skb->len, hdr_size;
+	int rc = 0;
+	u32 i;
+	struct skb_shared_info * pinfo;
+
+	if (!t3300_reth_is_enabled())
+		return 0;
+
+	//printk ("reth_device_xmit: skb->data:0x%x, skb->len:%d\n", (u32)skb->data, skb->len);
+
+	pBlk = reth_icc_kdii_fwd_tx->alloc(reth_icc_kdii_fwd_tx, size);
+	if (pBlk == NULL)
+	{
+		printk ("R-ETH: no memory in ICC service, to drop TX-SKB\n");
+		dev_kfree_skb(skb);
+		return RETH_RC_ICC_NO_MEM;
+	}
+
+	pinfo = skb_shinfo(skb);
+	if (pinfo->map_fr_num > 1)
+	{
+		hdr_size = size;
+		pU8Blk = (u8*)pBlk;
+		for (i = 1; i < pinfo->map_fr_num; i++)
+		{
+			hdr_size -= pinfo->frags[i].size;
+		}
+		memcpy (pU8Blk, skb->data, hdr_size);
+		pU8Blk += hdr_size;
+		for (i = 1; i < pinfo->map_fr_num; i++)
+		{
+			memcpy (pU8Blk, (void*)pinfo->frags[i].page, pinfo->frags[i].size);
+			pU8Blk += pinfo->frags[i].size;
+		}
+		if (reth_icc_kdii_fwd_tx->put(reth_icc_kdii_fwd_tx, pBlk, size, (t3300_slave ? RETH_MASTER_PORT_TX : RETH_SLAVE_PORT_TX), 0) == 0)
+		{
+			reth_icc_kdii_fwd_tx->free (reth_icc_kdii_fwd_tx, pBlk);
+			rc = RETH_RC_ICC_SEND_ERR;
+		}
+	}
+	else
+	{
+		// to memcpy general Linux data to ICC block and to send it to the remote cluster
+		memcpy (pBlk, skb->data, size);
+		if (reth_icc_kdii_fwd_tx->put(reth_icc_kdii_fwd_tx, pBlk, size, (t3300_slave ? RETH_MASTER_PORT_TX : RETH_SLAVE_PORT_TX), 0) == 0)
+		{
+			reth_icc_kdii_fwd_tx->free (reth_icc_kdii_fwd_tx, pBlk);
+			rc = RETH_RC_ICC_SEND_ERR;
+		}
+	}
+
+	if (rc == 0)
+	{
+		priv->stats.tx_packets++;
+		priv->stats.tx_bytes += size;
+		dev->trans_start = jiffies;	/* save the timestamp */
+
+		//reth_dump(skb->data, size);
+		rc = NET_XMIT_SUCCESS;
+	}
+	else
+	{
+		priv->stats.tx_dropped++;
+        	rc = NET_XMIT_DROP;
+	}
+
+	dev_kfree_skb(skb);
+	return rc;
+}
+
+static int reth_device_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+	return reth_device_tx(skb, dev);
+}
+
+static int reth_device_poll(struct napi_struct *napi, int budget)
+{
+	u32 port = 0, msgsize = 0;
+	void * pVPtr;
+	reth_priv *priv;
+	struct sk_buff * skb;
+	int num = 0;
+
+	if (!t3300_reth_is_enabled())
+	{
+		napi_complete(napi);
+		return 0;
+	}
+
+	priv = (reth_priv *)netdev_priv(this_dev);
+
+	while ((pVPtr = reth_icc_kdii_fwd_rx->exget(reth_icc_kdii_fwd_rx, &msgsize, &port)) != NULL)
+	{
+		#if 0
+		if (!t3300_slave)
+		{
+			u8* p = (u8*)pVPtr + (RX_SKB_RESERVE_SZ+NET_SKB_PAD);
+			printk ("Received forwarded rx: (%02x %02x %02x %02x %02x %02x), len:%d\n", 
+				p[0], p[1], p[2], p[3], p[4], p[5], msgsize
+			);
+		}
+		#endif
+
+		priv->stats.rx_packets++;
+		priv->stats.rx_bytes += msgsize;
+
+		skb = alloc_skb_mapped (0, msgsize + 128, pVPtr, GFP_ATOMIC, 0, 0);
+		//skb = alloc_skb (msgsize + 256, GFP_ATOMIC);
+		if (skb == NULL)
+		{
+			printk ("R-ETH: cannot created SKB, drop the remote RX packet\n");
+			reth_icc_kdii_fwd_rx->free (reth_icc_kdii_fwd_rx, pVPtr);
+			continue;
+		}
+		skb_reserve (skb, RX_SKB_RESERVE_SZ+NET_SKB_PAD);
+		skb->dev = this_dev;
+		skb_put(skb, msgsize);
+		//memcpy (skb->head, pVPtr, msgsize);
+		//reth_dump (skb->mac_header, msgsize);
+		skb->ip_summed = CHECKSUM_UNNECESSARY;
+		skb->protocol  = eth_type_trans(skb, this_dev);
+		netif_receive_skb(skb);
+		this_dev->last_rx = jiffies;
+
+		num ++;
+	}
+
+	napi_complete(napi);
+	return num;
+}
+
+static struct net_device_stats * reth_device_get_stats(struct net_device *dev)
+{               	
+	return &(((reth_priv *)netdev_priv(dev))->stats);
+}
+
+static int reth_set_mac_address(struct net_device *dev, void *addr)
+{
+	struct sockaddr *sa = addr;
+
+	if (!is_valid_ether_addr(sa->sa_data))
+		return -EADDRNOTAVAIL;
+
+	memcpy(dev->dev_addr, sa->sa_data, ETH_ALEN);
+	return 0;
+}
+
+static int reth_device_init(void)
+{
+	int rc = 0;
+	struct net_device *dev;
+	reth_priv *priv;
+
+	dev = alloc_etherdev(sizeof(*priv));
+	if (!dev)
+	{
+		printk(KERN_ERR "%s: virtual ethernet %d allocation failed\n",
+			__func__, 0);
+		return -ENODEV;
+	}
+
+	/* Initialize the device structure. */
+	priv = netdev_priv(dev);
+	if (!priv)
+        	return -ENOMEM;
+
+	memset(priv, 0, sizeof(reth_priv));
+
+	spin_lock_init(&priv->lock);
+
+	priv->state = 0;
+	priv->dev = dev;
+
+	dev->netdev_ops = &reth_netdev_ops;
+	dev->mtu = 1500;
+
+	priv = netdev_priv(dev);
+	priv->dev = dev;
+
+	netif_napi_add(dev, &priv->napi, reth_device_poll, 64);
+
+	//dev->flags &= ~IFF_MULTICAST;
+	//dev->flags |= IFF_NOARP;
+
+	memcpy(dev->dev_addr, (!t3300_slave) ? _master_mac : _slave_mac, 6);
+
+	this_dev = dev;
+
+	rc = dev_alloc_name(&this_dev[0], "reth");
+	rc = register_netdev(&this_dev[0]);
+
+	return rc;
+}
+
+static int reth_open_icc_channel(void)
+{
+	u32 master_ports_fwd_tx [2] = {RETH_MASTER_PORT_TX, 0};
+	u32 master_ports_fwd_rx [2] = {RETH_MASTER_PORT_RX, 0};
+
+	u32 slave_ports_fwd_tx  [2] = {RETH_SLAVE_PORT_TX, 0};
+	u32 slave_ports_fwd_rx  [2] = {RETH_SLAVE_PORT_RX, 0};
+
+	u32 fastpath_port       [2] = {RETH_FASTPATH_PORT, 0};
+
+	reth_icc_kdii_fwd_rx = ICC_KDII_Create(RETH_MODULE_NAME, RETH_ICC_QUEUE_SIZE, RETH_ICC_BLK_SIZE, t3300_slave ? slave_ports_fwd_rx : master_ports_fwd_rx, 0);
+	if(reth_icc_kdii_fwd_rx == NULL)
+	{
+		printk ("RETH: it was not possible to open fwd-rx ICC channel\n");
+		return RETH_RC_OPEN_CHANN_ERR;
+	}
+
+	reth_icc_kdii_fwd_tx = ICC_KDII_Create(RETH_MODULE_NAME, RETH_ICC_QUEUE_SIZE, RETH_ICC_BLK_SIZE, t3300_slave ? slave_ports_fwd_tx : master_ports_fwd_tx, 0);
+	if(reth_icc_kdii_fwd_tx == NULL)
+	{
+		printk ("RETH: it was not possible to open fwd-rx ICC channel\n");
+		return RETH_RC_OPEN_CHANN_ERR;
+	}
+
+	reth_icc_kdii_fastpath = ICC_KDII_Create(RETH_MODULE_NAME, RETH_ICC_QUEUE_SIZE, RETH_ICC_BLK_SIZE, fastpath_port, 0);
+	if(reth_icc_kdii_fastpath == NULL)
+	{
+		printk ("RETH: it was not possible to open ICC channel for fastpath stream\n");
+		return RETH_RC_OPEN_CHANN_ERR;
+	}
+	return 0;
+}
+
+void * t3300_reth_get_forwarded_tx (u32 *msgsize_ptr)
+{
+	u32 port = 0;
+	void * pVPtr;
+	reth_priv *priv;
+
+	if (!t3300_reth_is_enabled())
+		return NULL;
+
+	priv = (reth_priv *)netdev_priv(this_dev);
+
+	pVPtr = reth_icc_kdii_fwd_tx->exget(reth_icc_kdii_fwd_tx, msgsize_ptr, &port);
+
+	if (reth_routing_started == 0 && pVPtr != NULL)
+	{
+		asm volatile ("DSB");
+		reth_routing_started = 1;
+		asm volatile ("DMB");
+	}
+
+	//if (pVPtr != NULL)
+	//{
+	//	reth_dump(pVPtr, *msgsize_ptr);
+	//}
+
+	return pVPtr;
+}
+
+fastpath_pack * t3300_reth_get_fastpath_pack(void)
+{
+    u32 port = 0;
+    u32 msgsize = 0;
+    if (reth_icc_kdii_fastpath == NULL)
+	return NULL;
+
+    return (struct fastpath_pack *)reth_icc_kdii_fastpath->exget(reth_icc_kdii_fastpath, &msgsize, &port);
+}
+
+void t3300_reth_get_fastpath_drop_pack(fastpath_pack *p)
+{
+    uint i;
+    if (reth_icc_kdii_fastpath == NULL)
+	return;
+
+    // It starts from <1> because network headers
+    // are located in this <p> packet due to performance
+    // reason, it's prepared by NTL code
+
+    for (i = 1; i < p->fragnum; i++)
+    {
+	reth_icc_kdii_fastpath->free (reth_icc_kdii_fastpath, p->frags[i].data);
+    }
+
+    reth_icc_kdii_fastpath->free (reth_icc_kdii_fastpath, p->frags[0].data);
+}
+
+void t3300_reth_free_fwd_tx (void* ptr)
+{
+	if (reth_icc_kdii_fwd_tx == NULL || ptr == NULL)
+		return;
+
+	reth_icc_kdii_fwd_tx->free (reth_icc_kdii_fwd_tx, ptr);
+}
+
+void t3300_reth_free_fwd_rx (void* ptr)
+{
+	if (reth_icc_kdii_fwd_rx == NULL || ptr == NULL)
+		return;
+
+	reth_icc_kdii_fwd_rx->free (reth_icc_kdii_fwd_rx, ptr);
+}
+
+int t3300_reth_init(struct net_device *dev)
+{
+	int rc = 0;
+
+	if (reth_on == 0)
+		return 0;
+
+	if (ICC_KDII_Create == NULL)
+		return RETH_RC_NO_CTRL_IF;
+
+	if (this_dev != NULL)
+		return rc;
+
+	rc = reth_open_icc_channel ();
+	if (rc != 0)
+	{
+		printk ("RETH opening icc channel error, rc=%d\n", rc);
+		return rc;
+	}
+
+	rc = reth_device_init();
+
+	return rc;
+}
+
+int t3300_reth_is_enabled(void)
+{
+	return (reth_icc_kdii_fwd_tx != NULL) && (reth_icc_kdii_fwd_rx != NULL); // && (reth_state & STATE_ENA);
+}
+
+int t3300_reth_is_routing_enabled (void)
+{
+	if (!t3300_reth_is_enabled ())
+		return 0;
+
+	return reth_routing_started;
+}
+
+int t3300_reth_forward_rx(struct sk_buff *skb, u32 reserved)
+{
+	void * pBlk;
+	u32 size;
+	int rc = 0;
+	reth_priv * priv;
+
+	if (!t3300_reth_is_enabled())
+		return 0;
+
+        priv = (reth_priv *)netdev_priv(this_dev);
+	size = skb->tail - skb->head;
+
+	//printk ("reth_device_forward_rx: skb->head:0x%x, skb->data:0x%x, skb->len:%d\n\n", (u32)skb->head, (u32)skb->data, skb->len);
+
+	pBlk = reth_icc_kdii_fwd_rx->alloc(reth_icc_kdii_fwd_rx, 1532);
+
+	if (pBlk == NULL)
+	{
+		printk ("R-ETH: no memory in ICC service, to drop RX-SKB\n");
+		return RETH_RC_ICC_NO_MEM;
+	}
+
+	// to memcpy general Linux data to ICC block and to send it to the remote cluster
+	memcpy (pBlk, skb->head, size);
+	if (reth_icc_kdii_fwd_rx->put(reth_icc_kdii_fwd_rx, pBlk, size, (t3300_slave ? RETH_MASTER_PORT_RX : RETH_SLAVE_PORT_RX), 0) == 0)
+	{
+		reth_icc_kdii_fwd_rx->free (reth_icc_kdii_fwd_rx, pBlk);
+		rc = RETH_RC_ICC_SEND_ERR;
+	}
+
+	if (rc == 0)
+	{
+		priv->stats.rx_packets++;
+		priv->stats.rx_bytes += size;
+
+		//reth_dump(pBlk, size);
+		rc = NET_XMIT_SUCCESS;
+	}
+	else
+	{
+		priv->stats.rx_dropped++;
+        	rc = NET_XMIT_DROP;
+	}
+	
+	return rc;
+}
+
+int t3300_reth_is_mac (u8* pMac)
+{
+	if (!t3300_reth_is_enabled() || pMac == NULL)
+		return 0;
+
+	// Here <_master_mac> is equal to slave cluster HW GEM MAC address
+	// <_slave_mac> is equal to master HW GEM MAC address
+	return memcmp (pMac, (t3300_slave) ? _master_mac : _slave_mac, 6) == 0;
+}
+
+int t3300_reth_proc_forwarded_rx (void)
+{
+	reth_priv *priv;
+
+	if (!t3300_reth_is_enabled() || !(reth_state & STATE_ENA))
+		return 0;
+
+	priv = (reth_priv *)netdev_priv(this_dev);
+
+	if (likely(napi_schedule_prep(&priv->napi)))
+	{
+		__napi_schedule(&priv->napi);
+	}
+
+	return 0;
+}
+
+int t3300_reth_is_allowed (void)
+{
+	return reth_on;
+}
+
+/**
+ *	This is a kernel command line parameter. The format is as following:
+ *	reth_hwaddress=<M>,<mac address> reth_hwaddress=<S>,<mac address>
+ *	For all the other cases, the MAC address will be given through the ifconfig application
+ *	i.e: ifconfig <interface name> hw ether <mac address>
+ */
+static int __init reth_hwaddress_setup(char *str)
+{
+	int             index = 0;
+	int             i     = 0;
+	unsigned char * pchar = (unsigned char*) str;
+
+	printk ("RETH: mac-addresses parsing: %s\n", str);
+
+	for (i = 0; i < 2; i++) {
+		if (strncmp(pchar, "M", 1) == 0 || strncmp(pchar, "m", 1) == 0) {
+			pchar = strpbrk(pchar,",");
+			++pchar;
+			index = 0;
+			printk ("RETH: parse master MAC: %s\n", pchar);
+			while (pchar && (index < ETH_ALEN)) {
+				if (pchar) {
+					unsigned char tmp = simple_strtol(pchar, NULL, 16);
+					_master_mac [index++] = (unsigned char)tmp;
+					pchar +=3;
+				}
+			}
+		} else if (strncmp(pchar, "S", 1) == 0 || strncmp(pchar, "s", 1) == 0) {
+			pchar = strpbrk(pchar,",");
+			++pchar;
+			index = 0;
+			printk ("RETH: parse slave MAC: %s\n", pchar);
+			while (pchar && (index < ETH_ALEN)) {
+				if (pchar) {
+					unsigned char tmp = simple_strtol(pchar, NULL, 16);
+					_slave_mac [index++] = (unsigned char)tmp;
+					pchar +=3;
+				}
+			}
+		}
+	}
+
+	return 1;
+}
+
+__setup("reth_hwaddress=", reth_hwaddress_setup);
+
+static int __init reth_on_setup(char *str)
+{
+	reth_on = simple_strtol(str, NULL, 10);
+
+	if (reth_on != 0)
+	{
+		printk ("RETH is allowed in the system and may be used when enabled by <ifconfig>\n");
+	}
+	else
+	{
+		printk ("RETH is disabled in the system\n");
+	}
+
+	return 0;
+}
+__setup("reth_on=", reth_on_setup);
diff --git a/drivers/net/transcede/t3300_reth.h b/drivers/net/transcede/t3300_reth.h
new file mode 100644
index 0000000..9d77b7e
--- /dev/null
+++ b/drivers/net/transcede/t3300_reth.h
@@ -0,0 +1,96 @@
+/**
+ * @file t2200_reth.h
+ * @brief Remote Ethernet Driver, it's used like a bridge to communicate to remote cluster Ethernet driver
+ *
+ *  File path: linux/drivers/net/transcede/t2200_reth.h
+ *
+ *
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+
+#ifndef _T3300_RETH_H_
+#define _T3300_RETH_H_
+
+#include "t2200_eth.h"
+
+#define RETH_RC_OK		0
+#define RETH_RC_NO_CTRL_IF	1
+#define RETH_RC_OPEN_CHANN_ERR	2
+#define RETH_RC_ICC_NO_MEM	3
+#define RETH_RC_ICC_SEND_ERR	4
+
+#define RETH_MODULE_NAME	"RETH"
+#define RETH_ICC_QUEUE_SIZE	1024
+#define RETH_ICC_BLK_SIZE	2000
+
+#define RETH_MASTER_PORT_TX	0xE000
+#define RETH_MASTER_PORT_RX	0xE001
+#define RETH_SLAVE_PORT_TX	0xE100
+#define RETH_SLAVE_PORT_RX	0xE101
+
+#define RETH_FASTPATH_PORT	0xE200
+
+#define RETH_TX_PORT_OFFS	0
+#define RETH_RX_PORT_OFFS	1
+
+#define RETH_FASTPATH_MAX_FRAG_NUM	16
+#define RETH_FASTPATH_HDR_RES_SIZE	128
+
+typedef struct _reth_priv_
+{
+	int        state;
+	spinlock_t lock;
+	struct     net_device_stats stats;
+	struct     napi_struct napi;
+	struct     net_device *dev;
+	void *     periodic_task;
+}reth_priv, *preth_priv;
+
+typedef struct _reth_pack_frag_
+{
+    void*	data;		// The pointer to the packet header or payload
+    uint	size;		// The size of this fragment in bytes
+}fastpath_pack_frag;
+
+typedef struct _fastpath_pack_
+{
+    uint			version;		// The packet format version, reserved at this moment
+    uint			res;			// To keep headers alignment
+    uint			size;			// The total size of packet in bytes
+    uint			fragnum;		// The number of fragments in the <frag> array
+    fastpath_pack_frag		frags[RETH_FASTPATH_MAX_FRAG_NUM];	// Fragments container
+    char 			hdrs [RETH_FASTPATH_HDR_RES_SIZE];	// The space reserved for all network headers of this packet
+}fastpath_pack;
+
+int t3300_reth_init(struct net_device *dev);
+void * t3300_reth_get_forwarded_tx (u32 *msgsize_ptr);
+fastpath_pack * t3300_reth_get_fastpath_pack(void);
+void t3300_reth_get_fastpath_drop_pack(fastpath_pack *p);
+int t3300_reth_is_enabled(void);
+int t3300_reth_is_routing_enabled (void);
+int t3300_reth_forward_rx(struct sk_buff *skb, u32 free_skb);
+int t3300_reth_is_mac (u8 *pMac);
+int t3300_reth_proc_forwarded_rx (void);
+void t3300_reth_free_fwd_tx (void* ptr);
+void t3300_reth_free_fwd_rx (void* ptr);
+int t3300_reth_is_allowed (void);
+#endif // _T3300_RETH_H_
diff --git a/drivers/net/transcede/t3300_smi.c b/drivers/net/transcede/t3300_smi.c
new file mode 100644
index 0000000..75b3359
--- /dev/null
+++ b/drivers/net/transcede/t3300_smi.c
@@ -0,0 +1,501 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef __KERNEL__
+#include <unistd.h>
+#include <stdio.h>
+#else
+#include <linux/kernel.h>
+#include <asm/io.h>
+#include <linux/memory.h>
+#endif
+
+#include "t3300_smi.h"
+
+#define SMI_DESC_BASE          shm_desc_base
+#define SMI_DESC_SIZE		2048		// 1536 packet size
+
+unsigned shm_desc_base;
+unsigned shm_part_base;
+unsigned shm_queue_base;
+unsigned shm_sema_base;
+
+unsigned timer_base;
+unsigned intc_base;
+
+// unsigned smi_to_low_arm;
+// unsigned smi_from_low_arm;
+
+unsigned irq_to_peer_cpu;
+unsigned irq_from_peer_cpu;
+
+unsigned smi_sema_base = 0xF4F00000;
+
+#define SMI_PART_BASE	shm_part_base
+#define SMI_QUEUE_BASE	shm_queue_base
+
+/* Those 2 variables should be either global of platform specific */
+#define IPI_TO_QUAD_ARM    (7)     /* QuadCore Interprocessor interrupt number  */
+#define IPI_FROM_QUAD_ARM  (6)
+#define IPI_TO_DUAL_ARM    (32+6)  /* DualCore Interprocessor interrupt number  */
+#define IPI_FROM_DUAL_ARM  (32+7)
+
+#define CMD_LEN                 (2048)
+#define CMD_INBOX_FIFO         	3072
+#define CMD_OUTBOX_FIFO         3072
+#define FIFO_BLOCK_LEN          sizeof(unsigned)
+#define SMI_PART_SIZE           (CMD_INBOX_FIFO*CMD_LEN + CMD_OUTBOX_FIFO*CMD_LEN)
+#define SMI_QUEUE_SIZE          (CMD_INBOX_FIFO*FIFO_BLOCK_LEN + CMD_OUTBOX_FIFO*FIFO_BLOCK_LEN)
+
+
+unsigned long* queue_storage = NULL;
+unsigned long* part_storage = NULL;
+
+struct smi_mailbox_t* mailbox_desc = NULL;
+struct smi_mailbox_t* lower_to_upper_mbox = NULL;
+struct smi_mailbox_t* upper_to_lower_mbox = NULL;
+
+static ved_master = 0;
+
+#define __conv_to_va(va_base, pa_base, addr) ((volatile unsigned*) ((unsigned)(va_base)+((unsigned)(addr)-(pa_base))))
+
+#if 0
+
+#define __part_va(x)   	((struct smi_block_hdr_t*) ((unsigned)(part_storage) + ((unsigned)(x) - SMI_PART_BASE)))
+#define __queue_va(x)  	((volatile unsigned*) ((unsigned)(queue_storage) + ((unsigned)(x) - SMI_QUEUE_BASE)))
+#define __sema_va(x)	((volatile unsigned*) ((unsigned)(sema_base) + ((unsigned)(x) - SEMA_BASE)))
+
+#define __part_pa(x)   	((struct smi_block_hdr_t*) ((unsigned)(SMI_PART_BASE) + ((unsigned)(x) - (long)part_storage)))
+#define __queue_pa(x)  	((volatile unsigned*) ((unsigned)(SMI_QUEUE_BASE) + ((unsigned)(x) - queue_storage)))
+#define __sema_pa(x)	((volatile unsigned*) ((unsigned)(SMI_BASE) + ((unsigned)(x) - sema_base)))
+
+#else
+
+static struct smi_block_hdr_t* __part_va(void* x)
+{
+	//printk("%X %X %X\n", (unsigned)(part_storage),  (unsigned)(x), (unsigned)SMI_PART_BASE);
+	//printk("---------\n");
+
+	if (ved_master)
+		return ((struct smi_block_hdr_t*) ((unsigned)(part_storage) + ((unsigned)(x) - (unsigned)SMI_PART_BASE)));
+	else
+		return ((struct smi_block_hdr_t*) ((unsigned)(part_storage) + ((unsigned)(x)
+		    - (((unsigned)SMI_PART_BASE & 0xFFFFFF) | 0x10000000) )));
+}
+
+static volatile unsigned* __queue_va(void* x)
+{
+	if (ved_master)
+		return ((volatile unsigned*) ((unsigned)(queue_storage) + ((unsigned)(x) - (unsigned)SMI_QUEUE_BASE)));
+	else
+		return ((volatile unsigned*) ((unsigned)(queue_storage) + ((unsigned)(x)
+		    - (((unsigned)SMI_QUEUE_BASE & 0xFFFFFF) | 0x10000000) )));
+}
+
+
+static struct smi_block_hdr_t*  __part_pa(void* x)
+{
+	if (ved_master)
+		return ((struct smi_block_hdr_t*) ((unsigned)(SMI_PART_BASE) + ((unsigned)(x) - (long)part_storage)));
+	else
+		return ((struct smi_block_hdr_t*) ((unsigned)(((unsigned)SMI_PART_BASE&0xFFFFFF) | 0x10000000) + ((unsigned)(x) - (long)part_storage)));
+}
+
+static volatile unsigned* __queue_pa(void* x)
+{
+	if (ved_master)
+		return ((volatile unsigned*) ((unsigned)(SMI_QUEUE_BASE) + ((unsigned)(x) - (long)queue_storage)));
+	else
+		return ((volatile unsigned*) ((unsigned)(((unsigned)SMI_QUEUE_BASE&0xFFFFFF)|0x10000000) + ((unsigned)(x) - (long)queue_storage)));
+}
+
+#if 0
+static void* __sema_pa(void* x)
+{
+	if (ved_master)
+		return ((volatile unsigned*) ((unsigned)(SMI_BASE) + ((unsigned)(x) - sema_base)));
+	else
+		return ((volatile unsigned*) ((unsigned)(SMI_BASE & 0xFFFFFF) | 0x10000000) + ((unsigned)(x) - sema_base));
+}
+#endif
+#endif
+
+#if 0
+void io_wait(void)
+{
+	    int vv = 0;
+	    for (vv=0; vv<300000; vv++)
+		nop();
+}
+#else
+#define io_wait(x)
+#endif
+
+#if 1
+static int smi_lock(unsigned lock)
+{
+	volatile* lock_p = (volatile*)(shm_sema_base + lock);
+	// printk("locking %d %X %X\n", lock, lock_p, *lock_p);
+	while ( *lock_p ) { nop();};
+	// printk("done\n");
+	*lock_p = 1;
+	io_wait();
+	return 0;
+}
+
+static void smi_release(unsigned lock)
+{
+	volatile* lock_p = (volatile*)(shm_sema_base + lock);
+	// printk("releasing lock %x\n", lock_p);
+	*lock_p = 0;
+	io_wait();
+
+	// printk("%x released\n", lock_p);
+}
+#else
+#define smi_lock(x)
+#define smi_release(x)
+#endif
+
+static void* alloc_part(struct smi_part_t *p)
+{
+        volatile struct smi_block_hdr_t *block;
+        if (!p->lock)
+	    return NULL;
+
+	smi_lock( p->lock );
+	//printk("%x %x %x\n", block, p->freeblock, __part_va(block));
+        block = p->freeblock;
+        if (block) {
+               p->freeblock = (struct smi_block_hdr_t*)(__part_va(block)->next);
+        }
+	p->numalloc++;
+	smi_release( p->lock );
+        return (void*) block;
+}
+
+static void free_part(struct smi_part_t* p, struct smi_block_hdr_t* block)
+{
+        if (!p->lock)
+	    return;
+
+	smi_lock( p->lock );
+	__part_va(block)->next = (struct smi_block_hdr_t*)p->freeblock;
+	p->freeblock = block;
+	p->numalloc--;
+	smi_release( p->lock );
+}
+
+static void signal_peer_cpu(void)
+{
+	//printk("send interrupt %x <- %x\n",0xFE070000,irq_to_peer_cpu);
+	*(volatile unsigned*)(0xFE070000) |= 1 << irq_to_peer_cpu;
+	*(volatile unsigned*)(0xFE070000) &= ~(1 << irq_to_peer_cpu);
+}
+
+static int enqueue(struct smi_queue_t* q, unsigned long idx)
+{
+        unsigned put;
+        unsigned overflow = 0;
+
+        if (!q->lock)
+	    return -1;
+#if 0
+	printk("enqueue\n");
+        printk("queue=%X %X\n", q, __queue_pa(q));
+        printk("put=%X\n",q->put);
+        printk("get=%X\n",q->get);
+        printk("size=%X\n",q->size);
+        printk("itemsize=%X\n",q->itemsize);
+        printk("storage=%X\n",q->storage);
+        printk("lock=%X\n", q->lock);
+#endif
+	smi_lock( q->lock );
+	io_wait();
+	//printk("1\n");
+        put = (q->put + 1) % q->size;
+	io_wait();
+	//printk("2\n");
+        if (put != q->get) {
+		////printk("3\n");
+		io_wait();
+                __queue_va(q->storage)[q->put] = idx;
+		io_wait();
+		//printk("4\n");
+                q->put = put;
+                io_wait();
+                //printk("5\n");
+        } else  {
+                overflow = 1;
+                printk("Queue overflow\n");
+        }
+	smi_release( q->lock );
+        return (overflow) ? -1 : 0;
+}
+
+
+static unsigned long dequeue(struct smi_queue_t* q)
+{
+        unsigned get, size;
+        unsigned idx = -1;
+
+#if 0
+	printk("dequeue\n");
+        printk("queue=%X %X\n", q, __queue_pa(q));
+        printk("put=%X\n",q->put);
+        printk("get=%X\n",q->get);
+        printk("size=%X\n",q->size);
+        printk("itemsize=%X\n",q->itemsize);
+        printk("storage=%X\n",q->storage);
+        printk("lock=%X\n",q->lock);
+#endif
+
+        if (!q->lock)
+	    return -1;
+
+	smi_lock( q->lock );
+        get = q->get;
+        io_wait();
+        if (q->put != get) {
+		io_wait();
+                idx = __queue_va(q->storage)[get];
+		io_wait();
+
+		q->get++;
+		io_wait();
+
+		size = q->size;
+		io_wait();
+                if ( q->get >= size ) {
+			io_wait();
+			q->get = 0;
+			io_wait();
+		}
+        }
+	smi_release( q->lock );
+        return idx;
+}
+
+/************************************** SMI public **********************************/
+void* smi_alloc_tx_message()
+{
+	void* part_phys = alloc_part(&upper_to_lower_mbox->part);
+	if (!part_phys)
+	    return NULL;
+
+	// printk("smi_alloc_tx_message = %X %X\n", part_phys, __part_va( part_phys ));
+        return (void*)__part_va( part_phys );
+}
+
+void smi_free_tx_message(void* block)
+{
+	free_part(&upper_to_lower_mbox->part, __part_pa(block));
+}
+
+void smi_free_rx_message(void *block)
+{
+	free_part(&lower_to_upper_mbox->part, __part_pa(block));
+}
+
+
+void smi_send_message(void* blk)
+{
+        int index = ((unsigned)__part_pa(blk) - (unsigned)upper_to_lower_mbox->part.storage) / upper_to_lower_mbox->part.blksize;
+
+	// printk("smi_send_message %d %x %x\n", index, blk, __part_pa(blk));
+        enqueue(&upper_to_lower_mbox->queue, index);
+        signal_peer_cpu();
+}
+
+void* smi_read_message(void)
+{
+        struct smi_part_t *p = &lower_to_upper_mbox->part;
+        struct smi_queue_t *q = &lower_to_upper_mbox->queue;
+	struct smi_block_hdr_t* block;
+
+	unsigned storage;
+
+	// printk("smi_read_message\n");
+	int index = dequeue(q);
+	// printk("smi_read_message done index=%d\n", index);
+	if (index == -1)
+	    return NULL;
+	//while (index == -1) {
+	//    index = dequeue(q);
+        //}
+	storage = p->storage;
+	io_wait();
+	block = (struct smi_block_hdr_t*)(storage + p->blksize * index);
+
+
+	//printk("done\n");
+	// printk("block=%X, va=%X\n", block, __part_va(block));
+	return  (void*)__part_va(block);
+
+	return NULL;
+}
+
+static void smi_create_q(struct smi_queue_t* q, void *storage, u32 blockcount)
+{
+        q->put = 0;
+        q->get = 0;
+        q->size = blockcount;
+        q->itemsize = FIFO_BLOCK_LEN;
+        q->storage = (unsigned long) storage;
+
+        q->lock = smi_sema_base;
+        smi_sema_base += sizeof(u32);
+
+        return 0;
+}
+
+static void smi_create_p(struct smi_part_t *p, void* storage, u32 blksize, u32 blkcnt)
+{
+        volatile struct smi_block_hdr_t *block = (struct smi_block_hdr_t *) storage;
+
+        p->storage = (unsigned long) storage;
+
+        p->blksize = blksize;
+        p->blkcount = blkcnt;
+        p->lock = 0;
+        p->numalloc = 0;
+        p->freeblock = block;
+
+        while ( --blkcnt ) {
+                __part_va(block)->next = (struct smi_block_hdr_t *) (((unsigned char*) block) + blksize);
+                block = __part_va(block)->next;
+        }
+        __part_va(block)->next = 0;
+
+        p->lock = smi_sema_base;
+        smi_sema_base += sizeof(u32);
+}
+
+int smi_init( unsigned pcie_base_address, unsigned irq_to_slave, unsigned irq_from_slave, unsigned master)
+{
+	// printk("VED: smi init: base address = %08X\n", pcie_base_address);
+
+	if ( master )
+	    shm_desc_base = 0x10000000;
+	else
+	    shm_desc_base = 0xF6000000;
+
+	//pcie_base_address;
+	if (master) {
+	        mailbox_desc = map_ram( (shm_desc_base & 0xFFFFFF) | shm_desc_base, SMI_DESC_SIZE);
+	} else {
+	        mailbox_desc =  (shm_desc_base & 0xFFFFFF) | shm_desc_base;
+	}
+
+        if (!mailbox_desc) {
+                return -1;
+        }
+	// printk("mailbox_desc va=%x pa=%x\n", mailbox_desc, (shm_desc_base & 0xFFFFFF) | shm_desc_base);
+
+        lower_to_upper_mbox = &mailbox_desc[0];
+        upper_to_lower_mbox = &mailbox_desc[1];
+
+	shm_part_base = SMI_DESC_BASE + SMI_DESC_SIZE;
+	shm_queue_base = shm_part_base + SMI_PART_SIZE;
+	//printk()
+
+	if (master) {
+		part_storage = map_ram( (shm_part_base & 0xFFFFFF) | shm_desc_base, SMI_PART_SIZE);
+		queue_storage = map_ram( (shm_queue_base & 0xFFFFFF) | shm_desc_base, SMI_QUEUE_SIZE);
+	} else {
+		part_storage = (shm_part_base & 0xFFFFFF) | shm_desc_base;
+		queue_storage = (shm_queue_base & 0xFFFFFF) | shm_desc_base;
+	}
+
+	if (master) {
+		shm_sema_base = map_ram(shm_desc_base + 0x00F00000, PAGE_SIZE);
+	} else {
+		shm_sema_base = shm_desc_base + 0x00FFE000;
+	}
+	memset(shm_sema_base, 0, PAGE_SIZE);
+	smi_sema_base = 4;
+
+	// printk("partition storage va=%x pa=%x\n", part_storage, (shm_part_base & 0xFFFFFF) | shm_desc_base);
+	// printk("queue_storage va=%x pa=%x\n", queue_storage, (shm_queue_base & 0xFFFFFF) | shm_desc_base);
+	// printk("smi_sema_base = %x\n", shm_sema_base);
+
+//	((struct ipi_part_t*)(part_storage))->lock = 0;
+//	((struct ipi_queue_t*)(queue_storage))->lock = 0;
+	if ( master )
+	{
+		ved_master = 1;
+#if 1
+		smi_create_q(&lower_to_upper_mbox->queue, (void*)shm_queue_base, CMD_INBOX_FIFO);
+		smi_create_q(&upper_to_lower_mbox->queue,
+			(void*) (shm_queue_base + FIFO_BLOCK_LEN * CMD_INBOX_FIFO), CMD_OUTBOX_FIFO);
+#endif
+#if 1
+		smi_create_p(&lower_to_upper_mbox->part, (void*)shm_part_base, CMD_LEN, CMD_INBOX_FIFO);
+		smi_create_p(&upper_to_lower_mbox->part,
+			(void*) (shm_part_base + CMD_LEN * CMD_INBOX_FIFO), CMD_LEN, CMD_OUTBOX_FIFO);
+#endif
+	}
+	else {
+		lower_to_upper_mbox = &mailbox_desc[1];
+		upper_to_lower_mbox = &mailbox_desc[0];
+        }
+#if 0
+	printk("lower_to_upper_mbox->queue = %X\n", lower_to_upper_mbox->queue);
+	printk("upper_to_lower_mbox->queue = %X\n", upper_to_lower_mbox->queue);
+
+        printk("lower_to_upper_mbox->queue->storage = %X\n", /*__queue_pa*/(lower_to_upper_mbox->queue.storage));
+        printk("upper_to_lower_mbox->queue->storage = %X\n", /*__queue_pa*/(upper_to_lower_mbox->queue.storage));
+
+        printk("lower_to_upper_mbox->part = %X\n", lower_to_upper_mbox->part);
+        printk("upper_to_lower_mbox->part = %X\n", upper_to_lower_mbox->part);
+
+        printk("lower_to_upper_mbox->part->storage = %X\n", /*__part_pa*/(lower_to_upper_mbox->part.storage));
+        printk("upper_to_lower_mbox->part->storage = %X\n", /*__part_pa*/(upper_to_lower_mbox->part.storage));
+#endif
+
+
+	//upper_to_lower_mbox->part.lock = 0;
+
+        //smi_to_low_arm = IPI_TO_DUAL_ARM;
+        //smi_from_low_arm = IPI_FROM_DUAL_ARM;
+
+	irq_to_peer_cpu = irq_to_slave;
+	irq_from_peer_cpu = irq_from_slave;
+
+        return 0;
+}
+
+void smi_destroy( void )
+{
+	if (ved_master) {
+		unmap_ram((void*) SMI_DESC_BASE);
+
+	//	unmap_ram((void*) TIME_BASE);
+	//	unmap_ram((void*) INTC_BASE);
+
+		unmap_ram((void*) shm_part_base);
+		unmap_ram((void*) shm_queue_base);
+
+		unmap_ram((void*) shm_sema_base);
+	}
+}
+
diff --git a/drivers/net/transcede/t3300_smi.h b/drivers/net/transcede/t3300_smi.h
new file mode 100644
index 0000000..c4951ae
--- /dev/null
+++ b/drivers/net/transcede/t3300_smi.h
@@ -0,0 +1,115 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef _SMI_T3300_H_
+#define _SMI_T3300_H_
+
+struct smi_block_hdr_t {
+        volatile struct ipi_block_hdr_t *next;
+};
+
+struct smi_part_t {
+        unsigned long   	storage;		/* physically continous shared memory pool */
+        volatile struct 	ipi_block_hdr_t* freeblock;
+        volatile unsigned  	blksize;		/* block length in bytes */
+	unsigned             	blkcount;		/* number of blocks in memory pool */
+        volatile unsigned    	lock;			/* interprocessor mutex  */
+        unsigned             	numalloc;
+};
+
+struct smi_queue_t {
+        unsigned long   storage;	/* physical address of fifo pool */
+        volatile short  get;		/* read index */
+        volatile short  put;		/* write index */
+        unsigned short  size;		/* fifo depth */
+        unsigned short  itemsize;	/* should be 4 bytes to carry index */
+        volatile unsigned    lock;	/* interprocessor mutex */
+};
+
+struct smi_mailbox_t {
+        struct smi_queue_t  queue;	/* request fifo */
+        struct smi_part_t   part;	/* shared memory pool descriptor */
+};
+
+int  smi_init( unsigned base_address, unsigned irq_to_slave, unsigned irq_from_slave, unsigned master);
+void smi_destroy( void );
+
+void* smi_alloc_tx_message(void);
+void smi_free_tx_message(void* blk);
+
+void  smi_free_rx_message(void* blk);
+void  smi_send_message(void* blk);
+void* smi_read_message(void);
+// void* smi_read_message_id(unsigned srcID);
+// long  get_ticks(void);
+
+#define MEMID_DDR0    0 /* DDR0 is on region 0 in /dev/uio0 */
+#define MEMID_DDR1    1 /* DDR1 is on region 0 in /dev/uio1 */
+#define MEMID_CRAM    2 /* CRAM is on region 0 in /dev/uio2 */
+#define MEMID_IRAM    3 /* IRAM is on region 0 in /dev/uio3 */
+#define MEMID_JRAM    4 /* DDR1 is on region 0 in /dev/uio4 */
+#define MEMID_TIMER   5 /* XP Timer block for profiling         in /dev/uio5 */
+#define MEMID_SEMA    6 /* H/W Semaphores for Inter-processor   in /dev/uio6 */
+#define MEMID_INTC    7 /* SCU block (don't use w/o permission) in /dev/uio7 */
+#define MEMID_MAX     8
+
+/*
+ * User I/O map
+ */
+#define DDR0_BASE      0x00000000       /* SDRAM (512M) */
+#define DDR0_SIZE      0x20000000
+#define DDR1_BASE      0x20000000       /* SDRAM (512M) */
+#define DDR1_SIZE      0x20000000
+#define JRAM_BASE      0xFA000000       /* JRAM 16K     */
+#define JRAM_SIZE      0x00004000
+#define IRAM_BASE      0xFB000000       /* IRAM 256K    */
+#define IRAM_SIZE      0x00020000
+#define CRAM_BASE      0xFC000000       /* CRAM 3M      */
+#define CRAM_SIZE      0x00300000
+#define TIME_BASE      0xFE050000       /* XP Timer     */
+#define TIME_SIZE      0x00010000
+#define SEMA_BASE      0xFBF00000       /* H/W semaphores */
+#define SEMA_SIZE      0x00010000
+#define INTC_BASE      0xFE430000       /* SCU for GIC access */
+#define INTC_SIZE      0x00010000
+
+#ifndef __KERNEL__
+void* map_ram(unsigned addr, unsigned len);
+void unmap_ram(void* addr);
+#else
+#define map_ram         ioremap_nocache
+#define unmap_ram       iounmap
+#endif
+
+#define map_dram(addr, size)       map_ram(DDR0_BASE+(addr), size)
+#define map_dram1(addr, size)      map_ram(DDR1_BASE+(addr), size)
+#define map_jram(addr, size)       map_ram(JRAM_BASE+(addr), size)
+#define map_iram(addr, size)       map_ram(IRAM_BASE+(addr), size)
+#define map_cram(addr, size)       map_ram(CRAM_BASE+(addr), size)
+#define unmap_dram(addr, size)     unmap_ram(addr)
+#define unmap_dram1(addr, size)    unmap_ram(addr)
+#define unmap_jram(addr, size)     unmap_ram(addr)
+#define unmap_iram(addr, size)     unmap_ram(addr)
+#define unmap_cram(addr, size)     unmap_ram(addr)
+
+
+#endif // _SMI_T3300_H_
diff --git a/drivers/net/transcede/t3300_ved.c b/drivers/net/transcede/t3300_ved.c
new file mode 100644
index 0000000..6069283
--- /dev/null
+++ b/drivers/net/transcede/t3300_ved.c
@@ -0,0 +1,474 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <net/ip.h>
+#include <net/sock.h>
+#include "t3300_ved.h"
+#include "t3300_smi.h"
+//#include "user_ipi.h"
+#include "api.h"
+
+char master_mac[6] = { 0x00, 0x11, 0x22, 0x33, 0x44, 0x77 };
+char slave_mac[6] = { 0x00, 0x11, 0x22, 0x33, 0x44, 0x66 };
+
+#define PKT_BUF_SZ (1540)	/* Size of each rx buffer */
+#define IPI_IRQ (32+7)
+
+int ved_state = 0;
+
+unsigned gpio_rx;
+unsigned gpio_tx;
+
+#define ICCOM_S_GPIO_RX  1
+#define ICCOM_S_GPIO_TX  0
+
+/******************/
+unsigned ved_master = 0;
+/******************/
+unsigned iccom_gpio_in;
+unsigned iccom_gpio_out;
+unsigned pcie_base_address = 0;
+unsigned iccom_irq = 0;
+unsigned iccom_irq_count = 0;
+
+//static struct net_device_stats *ved_get_stats(struct net_device *dev);
+
+int ved_device_open(struct net_device *dev);
+int ved_device_close(struct net_device *dev);
+static int ved_device_xmit(struct sk_buff *skb, struct net_device *dev);
+static int ved_write_packet(struct sk_buff *skb, struct net_device *dev);
+static int ved_device_poll(struct napi_struct *napi, int budget);
+static int ved_set_mac_address(struct net_device *dev, void *addr);
+static int stop_ved(void);
+static struct net_device_stats *ved_device_get_stats(struct net_device *dev);
+
+struct net_device *ctrl_dev;
+
+#define DEBUG
+
+static const struct net_device_ops t3300_netdev_ops = {
+        .ndo_open               = ved_device_open,
+        .ndo_start_xmit         = ved_device_xmit,
+        .ndo_stop               = ved_device_close,
+        .ndo_get_stats          = ved_device_get_stats,
+        .ndo_set_mac_address    = ved_set_mac_address,
+};
+
+/***************************************************/
+static int ved_init(void)
+{
+        struct net_device *dev;
+        struct ved_priv *priv;
+
+        dev = alloc_etherdev(sizeof(*priv));
+        if (!dev) {
+		printk(KERN_ERR "%s: virtual ethernet %d allocation failed\n",
+				__func__, 0);
+		return -ENODEV;
+        }
+
+        /* Initialize the device structure. */
+        priv = netdev_priv(dev);
+        if (!priv)
+                return -ENOMEM;
+
+        memset(priv, 0, sizeof(struct ved_priv));
+
+        spin_lock_init(&priv->lock);
+        // init private section
+        priv->state = 0; /* closed */
+        priv->dev = dev;
+
+        /* Fill in device structure with ethernet-generic values. */
+        //dev->priv = priv;
+        dev->netdev_ops = &t3300_netdev_ops;
+        dev->mtu = 1500;
+
+        priv = netdev_priv(dev);
+	priv->dev = dev;
+#if 0
+        dev->open = ved_device_open;
+        dev->stop = ved_device_release;
+	dev->get_stats = ved_device_get_stats;
+        dev->hard_start_xmit = ved_device_xmit;
+#endif
+        netif_napi_add(dev, &priv->napi, ved_device_poll, 64);
+
+//	dev->flags &= ~IFF_MULTICAST;
+        dev->flags |= IFF_NOARP;
+        memcpy(dev->dev_addr, (ved_master) ? master_mac : slave_mac, 6);
+
+        ctrl_dev = dev;
+        return 0;
+}
+
+
+static irqreturn_t ved_interrupt(int irq, void *context)
+{
+        struct net_device *dev = context;
+        struct ved_priv *priv = netdev_priv(dev);
+        struct napi_struct *napi = &priv->napi;
+
+	//printk("ved_interrupt\n");
+	if (napi_schedule_prep(napi)) {
+		__napi_schedule(napi);
+	}
+
+        // clear GPIO irq
+        *(volatile u32*)TRANSCEDE_GPIO_INT_CLEAR_REG = 1 << iccom_gpio_in;
+	return IRQ_HANDLED;
+}
+
+
+/***************************************************/
+int ved_device_open(struct net_device *dev)
+{
+        struct ved_priv *priv = netdev_priv(dev);
+        int rc = 0;
+
+        if (priv->state == 1)
+                return rc;
+
+        napi_enable(&priv->napi);
+        netif_wake_queue(dev);
+        priv->state = 1; /* open */
+
+        iccom_gpio_in = gpio_rx;
+        iccom_gpio_out = gpio_tx;
+#if 1
+        if (!iccom_irq) {    // locks shold be added to avoid possible races
+		iccom_irq = IRQ_GPIO(iccom_gpio_in);
+		//printk("request_irq %x %x\n", gpio_rx,  IRQ_GPIO(iccom_gpio_in));
+                if (request_irq(iccom_irq, ved_interrupt, IRQF_DISABLED, "ved", dev)) {
+			printk("ved failed to request irq %d\n", iccom_irq);
+			return -ENXIO;
+                }
+        }
+#endif
+        //iccoma_irq = IRQ_GPIO(iccom_gpio_in);
+        iccom_irq_count = 0;
+        return rc;
+}
+
+/***************************************************/
+int ved_device_close(struct net_device *dev)
+{
+        struct ved_priv *priv = netdev_priv(dev);
+
+        /* do nothing if interface is already down */
+        if (priv->state == 1) {
+                napi_disable(&priv->napi);
+                netif_stop_queue(dev);	/* can't transmit any more */
+
+                stop_ved();
+                priv->state = 0;
+		if (iccom_irq) {
+			free_irq(iccom_irq, dev);
+			iccom_irq = 0;
+			iccom_irq_count = 0;
+		}
+        }
+        return 0;
+}
+
+/***************************************************/
+static int ved_device_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+        struct ved_priv *priv = (struct ved_priv *)netdev_priv(dev);
+        unsigned long flags;
+        int rc;
+
+        spin_lock_irqsave(&priv->lock, flags);
+        rc = ved_write_packet(skb, dev);
+        if (rc) {
+                priv->stats.tx_dropped++;
+        } else {
+                priv->stats.tx_packets++;
+                priv->stats.tx_bytes += skb->len;
+                dev->trans_start = jiffies;	/* save the timestamp */
+        }
+        spin_unlock_irqrestore(&priv->lock, flags);
+        return 0;
+}
+
+
+
+/***************************************************/
+static struct net_device_stats *ved_device_get_stats(struct net_device *dev) {
+        return &(((struct ved_priv *)netdev_priv(dev))->stats);
+}
+
+/***************************************************/
+extern unsigned t3300_slave;
+unsigned use_ved = 0;
+
+static int __init init_use_ved(char *unused)
+{
+        use_ved = 1;
+        return 0;
+}
+
+__setup("ved", init_use_ved);
+
+int __init ved_init_module(void)
+{
+        int rc;
+
+	unsigned pci_base_address;
+	volatile unsigned* ved_sync;
+
+	if (!use_ved) {
+		printk("VED is disabled\n");
+		return 0;
+	}
+
+	if (!t3300_slave)
+		ved_master = 1;
+
+	pci_base_address = (!ved_master) ? 0xF6000000 : 0x10000000;
+
+	if (!pci_base_address) {
+		printk("failed to remap pcie memory window\n");
+		return -1;
+	}
+
+	if ( !ved_master ) {
+		gpio_rx = 1;
+		gpio_tx = 0;
+	} else {
+                gpio_rx = 0;
+                gpio_tx = 2;
+	}
+
+	printk("VED Init: master=%d SHM base = %08X, GPIO RX=%d GPIO TX=%d\n",
+		ved_master, pci_base_address, gpio_tx, gpio_rx);
+
+	ved_sync = pci_base_address + 0x820;
+	if (!ved_master) {
+		printk("VED: waiting for master to sync up\n");
+		while ( *ved_sync != 0x77883344 ) { }
+	}
+
+        smi_init( pci_base_address, gpio_tx, gpio_rx, ved_master );
+	if (ved_master) {
+	    ved_sync = ioremap(pci_base_address + 0x820, 0x1000);
+	    printk("VED: sync master signal sent\n");
+	    *ved_sync = 0x77883344;
+	    iounmap( ved_sync );
+	}
+
+        ved_init();
+
+        /* Find a name for this unit */
+        rc = dev_alloc_name(&ctrl_dev[0], CTRL);
+        if (rc < 0) {
+                printk(KERN_ERR "ved_init_module: failed to alloc device name %s\n", CTRL);
+                return rc;
+        }
+
+        rc = register_netdev(&ctrl_dev[0]);
+        if (rc) {
+                printk(KERN_ERR "ved_init_module: failed to register dev %s\n", CTRL);
+                return rc;
+
+        }
+
+        return 0;
+}
+
+/***************************************************/
+static void __exit ved_cleanup_module(void)
+{
+        unregister_netdev(&ctrl_dev[0]);
+        kfree(netdev_priv(&ctrl_dev[0]));
+        smi_destroy();
+}
+
+
+/***************************************************/
+static void  dump(char* p)
+{
+#if 0
+	int i;
+	for (i=0; i<32; i++) {
+		printk("%02X ", *p++);
+	}
+	printk("\n");
+#endif
+}
+
+extern void io_wait(void);
+/***************************************************/
+static void memcpy8(char *dst, char* src, int len)
+{
+	while (len--) {
+		*dst++ = *src++;
+		//printk("%02X ", *dst);
+//		io_wait();
+	}
+}
+
+
+/***************************************************/
+static int ved_read_packet(struct net_device *dev, void *data_addr, int length)
+{
+        struct ved_priv *priv = (struct ved_priv *)netdev_priv(dev);
+        struct sk_buff *skb;
+
+	// printk("ved_read_packet len=%d\n", length);
+	//dump(data_addr);
+	//return 1;
+
+	//printk("1\n");
+        skb = dev_alloc_skb(PKT_BUF_SZ);
+        if (skb) {
+		//printk("2\n");
+                skb_reserve(skb, NET_IP_ALIGN);
+                // printk("3\n");
+		//memcpy8(skb->data, data_addr, length);
+		memcpy(skb->data, data_addr, length);
+		dump(skb->data);
+                // printk("4\n");
+		skb->dev = dev;
+		//printk("5\n");
+                skb_put(skb, length);
+		//printk("6\n");
+                skb->protocol = eth_type_trans(skb, dev);
+
+#if 0 // TBD
+                if (unlikely(skb->tail > skb->end))
+                        skb_over_panic(skb, length, current_text_addr());
+#endif
+
+		//printk("7\n");
+                if ((netif_receive_skb(skb))) {
+                        priv->stats.rx_dropped++;
+                }
+
+
+		//printk("8\n");
+                priv->stats.rx_packets++;
+                priv->stats.rx_bytes += length;
+                dev->last_rx = jiffies;
+
+        } else {
+                priv->stats.rx_dropped++;
+                return 0;
+        }
+        return 1;
+}
+/***************************************************/
+static int ved_device_poll(struct napi_struct *napi, int budget)
+{
+        struct ved_priv *priv = container_of(napi, struct ved_priv, napi);
+        struct net_device *dev = priv->dev;
+        struct api_hdr_t *msg = NULL;
+        int done, length;
+
+	// ved_read_packet(struct net_device *dev, void *data_addr, int length);
+
+	//printk("ved_device_poll\n");
+
+	// printk("smi_read_message done msg=%08X\n", msg);
+#if 1
+        while ( msg = smi_read_message() ) {
+                struct boot_param_t *param = (struct boot_param_t*)msg->payload;
+
+                length = sizeof(struct api_hdr_t) + msg->length;
+                done = ved_read_packet(dev, param->data, length);
+                smi_free_rx_message( msg );
+        }
+#endif
+        napi_complete(napi);
+        return done;
+}
+
+
+/***************************************************/
+static int ved_write_packet(struct sk_buff *skb, struct net_device *dev)
+{
+        struct api_hdr_t *hdr;
+	struct boot_param_t *param;
+        struct ved_priv *priv = (struct ved_priv *)netdev_priv(dev);
+
+        hdr = (struct api_hdr_t*) smi_alloc_tx_message();
+	if (!hdr) {
+		// printk("failed to allocated smi message\n");
+		dev_kfree_skb(skb);
+		return 0;
+	}
+
+#if 1
+	param = (struct boot_param_t*)hdr->payload;
+
+	param->paramID = 3;
+	param->length = skb->len;
+	hdr->length = param->length + 4;
+
+	memcpy(param->data, skb->data, skb->len);
+	memcpy(param->data, (!ved_master) ? master_mac : slave_mac, 6);
+
+	// printk("ved_write_packet\n");
+	dump(param->data);
+
+	smi_send_message(hdr);
+#endif
+	//smi_free_tx_message(hdr);
+        dev_kfree_skb(skb);
+        return 0;
+}
+
+/***************************************************/
+static int stop_ved(void)
+{
+        struct net_device *dev;
+        struct ved_priv *priv;
+
+        dev = &ctrl_dev[0];
+        priv = (struct ved_priv *)netdev_priv(dev);
+
+        return 0;
+}
+
+static int ved_set_mac_address(struct net_device *dev, void *addr)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+	struct sockaddr *sa = addr;
+
+	//printk("%s \n", ved_set_mac_address);
+	if (!is_valid_ether_addr(sa->sa_data))
+		return -EADDRNOTAVAIL;
+
+	memcpy(dev->dev_addr, sa->sa_data, ETH_ALEN);
+
+	//gem_add_arc_entry(&priv->gemdev, dev->dev_addr);
+
+	return 0;
+}
+
+module_init(ved_init_module);
+module_exit(ved_cleanup_module);
+
diff --git a/drivers/net/transcede/t3300_ved.h b/drivers/net/transcede/t3300_ved.h
new file mode 100644
index 0000000..57b0eac
--- /dev/null
+++ b/drivers/net/transcede/t3300_ved.h
@@ -0,0 +1,48 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef _TRANSCEDE_VED_H
+#define _TRANSCEDE_VED_H
+
+#include <linux/netdevice.h>	/* struct device, and other headers */
+
+
+#define VED_SKB_POOL_SIZE	512
+#define VED_TIMEOUT 5		/* In jiffies */
+#define CTRL	"ved0"
+
+
+/*
+ * This structure is private to each device. It is used to pass
+ * packets in and out, so there is place for a packet
+ */
+
+struct ved_priv {
+	struct net_device_stats stats;
+	spinlock_t lock;
+	struct	napi_struct napi;
+	int state;
+	struct net_device *dev;
+};
+
+
+#endif /* _TRANSCEDE_VED_H */
diff --git a/drivers/net/transcede/transcede_ethtool.c b/drivers/net/transcede/transcede_ethtool.c
new file mode 100644
index 0000000..7626bba
--- /dev/null
+++ b/drivers/net/transcede/transcede_ethtool.c
@@ -0,0 +1,430 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+  *
+  * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+  *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+  *
+  * You should have received a copy of the GNU General Public License
+  * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+  */
+
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/string.h>
+#include <linux/errno.h>
+#include <linux/slab.h>
+#include <linux/interrupt.h>
+#include <linux/init.h>
+#include <linux/delay.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/skbuff.h>
+#include <linux/spinlock.h>
+#include <linux/mm.h>
+
+#include <asm/io.h>
+#include <asm/irq.h>
+#include <asm/uaccess.h>
+#include <linux/module.h>
+#include <linux/crc32.h>
+#include <asm/types.h>
+#include <asm/uaccess.h>
+#include <linux/ethtool.h>
+#include <linux/mii.h>
+#include <linux/phy.h>
+
+#if defined(CONFIG_MACH_M84XXX)
+#include "c4000_eth.h"
+#else
+#include "t2200_eth.h"
+#endif
+
+#include "transcede_gemac.h"
+
+
+static void c4k_fill_stats      (struct net_device *dev, struct ethtool_stats *dummy, u64 * buf);
+static void c4k_gstrings        (struct net_device *dev, u32 stringset,                u8 * buf);
+static void c4k_gringparam      (struct net_device *dev, struct ethtool_ringparam *rvals);
+static int  c4k_sringparam      (struct net_device *dev, struct ethtool_ringparam *rvals);
+
+#if 0
+// AKB REMOVING, THESE SHOULD BE IN .H FILES, NOT AS EXTERNS...
+extern int  c4k_eth_start       (struct net_device *dev, int phy);
+extern int  c4k_eth_stop        (struct net_device *dev, int phy);
+#if defined(CONFIG_MACH_M84XXX)
+extern void c4k_eth_start_queues(struct net_device *dev, int queue);
+extern void c4k_eth_stop_queues (struct net_device *dev, int queue);
+#endif
+#endif
+
+/** Ethernet statistics strings, note at time of this implementation
+ * ETH_GSTRING_LEN is 32 (31 printable characters + 1 null character)
+ */
+static char stat_gstrings[][ETH_GSTRING_LEN] = {
+/*	         1         2         3  */
+/*	1234567890123456789012345678901 */
+	"tx- octets (Lower 32-bits).... ",
+	"tx- octets (Upper 16-bits).... ",
+	"tx- total     packets......... ",
+	"tx- broadcast DMAC frames..... ",
+	"tx- multicast packets......... ",
+	"tx- pause frames ............. ",
+	"tx-          64 bytes packets. ",
+	"tx-   64 -  127 bytes packets. ",
+	"tx-  128 -  255 bytes packets. ",
+	"tx-  256 -  511 bytes packets. ",
+	"tx-  512 - 1023 bytes packets. ",
+	"tx- 1024 - 1518 bytes packets. ",
+	"tx-      > 1518 bytes packets. ",
+	"tx- underruns        - errors. ",
+	"tx- single collision - errors. ",
+	"tx- multi  collision - errors. ",
+	"tx- exces. collision - errors. ",
+	"tx- late   collision - errors. ",
+	"tx- deferred frames........... ",
+	"tx- carrier sense - errors.... ",
+	"rx- octets (Lower 32-bits).... ",
+	"rx- octets (Upper 16-bits).... ",
+	"rx- total packets............. ",
+	"rx- broadcast SMAC frames..... ",
+	"rx- multicast SMAC frames..... ",
+	"rx- pause frames.............. ",
+	"rx-          64 bytes packets. ",
+	"rx-  64 -   127 bytes packets. ",
+	"rx-  128 -  255 bytes packets. ",
+	"rx-  256 -  511 bytes packets. ",
+	"rx-  512 - 1023 bytes packets. ",
+	"rx- 1024 - 1518 bytes packets. ",
+	"rx-      > 1518 bytes packets. ",
+	"rx- undersize - errors........ ",
+	"rx- oversize  - errors........ ",
+	"rx- jabbers   - errors........ ",
+	"rx- fcs       - errors........ ",
+	"rx- length    - errors........ ",
+	"rx- symbol    - errors........ ",
+	"rx- align     - errors........ ",
+	"rx- ressource - errors........ ",
+	"rx- overrun   - errors........ ",
+	"rx- IP  cksum - errors........ ",
+	"rx- TCP cksum - errors........ ",
+	"rx- UDP cksum - errors........ "
+};
+
+/* Fill in a buffer with the strings which correspond to the
+ * stats
+ * NOTE assumes that statistics is the only "stringset" supported
+ * and upper level code will only call this if String set count
+ * for given string set is non zero
+ */
+static void  c4k_gstrings(struct net_device *dev, u32 stringset, u8 * buf)
+{
+	memcpy(buf, stat_gstrings, GEMAC_RMON_LEN * ETH_GSTRING_LEN);
+}
+
+/* Fill in an array of 64-bit statistics from various sources.
+ * This array will be appended to the end of the ethtool_stats
+ * structure (after rtnl_link_stats64), and returned to user space
+ */
+static void c4k_fill_stats(struct net_device *dev, struct ethtool_stats *dummy, u64 * buf)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+	//u32 *pstat = (u32*)&priv->gemdev.registers->stats;
+	u32 *pstat = (u32*) (priv->gemdev.registers + GEM_OCT_TX_BOT);
+	int i;
+	for (i=0;i<GEMAC_RMON_LEN;i++)
+		*buf++ = *pstat++;
+//	gem_take_snap(&priv->gemdev);
+//	memcpy(buf, (void*)&priv->gemdev.registers->stats, GEMAC_RMON_LEN*sizeof(u32));
+}
+
+/** Returns number of strings that @get_strings will write */
+static int c4k_sset_count(struct net_device *dev, int sset)
+{
+	switch (sset) {
+	case ETH_SS_STATS:
+		return GEMAC_RMON_LEN;
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+
+/** Fills in the drvinfo structure with some basic info */
+static void c4k_get_drvinfo(struct net_device *dev, struct ethtool_drvinfo *drvinfo)
+{
+	strncpy(drvinfo->driver,     dev->name,            sizeof(drvinfo->driver));
+	strncpy(drvinfo->version,    GEMAC_VERSION_STRING, sizeof(drvinfo->version));
+	strcpy (drvinfo->fw_version, "N/A");
+	strcpy (drvinfo->bus_info,   "N/A");
+	drvinfo->testinfo_len = 0;
+	drvinfo->regdump_len  = 0;
+	drvinfo->eedump_len   = 0;
+}
+
+static int c4k_set_settings(struct net_device *dev, struct ethtool_cmd *cmd)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+	struct phy_device *phydev = priv->phydev;
+
+	if (NULL == phydev)
+		return -EOPNOTSUPP; /* Operation not supported if no PHY device */
+
+	return phy_ethtool_sset(phydev, cmd);
+}
+
+
+/**
+ * Return the current settings in the ethtool_cmd structure
+ *
+ * If Phy device is not present, then get values from GEM if available
+ * or else, based on how the Transcede GEM device features are available.
+ *
+ */
+static int c4k_get_settings(struct net_device *dev, struct ethtool_cmd *cmd)
+{
+	struct eth_c4k_priv * priv   = netdev_priv(dev);
+	struct phy_device *   phydev = priv->phydev;
+	int                   rc     = 0;
+
+	if (phydev != NULL)
+	{
+		/* There is a register PHY device for this GEM,
+		 * Get information from PHY device
+		 */
+		rc = phy_ethtool_gset(phydev,cmd);
+	}
+	else
+	{
+		/*
+		 * No PHY device, force defaults to provide base info to ethtool
+		 */
+		cmd->supported   = SUPPORTED_MII;
+		cmd->speed       = SPEED_1000;
+		cmd->duplex      = DUPLEX_FULL;
+		cmd->port        = PORT_MII;
+		cmd->phy_address = 0;
+		cmd->transceiver = XCVR_EXTERNAL;
+		cmd->autoneg     = AUTONEG_DISABLE;
+	}
+
+	return rc;
+}
+
+/** Return the length of the register structure for GEM registers */
+static int c4k_gemac_reglen(struct net_device *dev)
+{
+	return sizeof (struct gem_reg);
+}
+
+/** Get current values of base GEM registers */
+static void  c4k_gemac_get_regs(struct net_device *dev, struct ethtool_regs *regs, void *regbuf)
+{
+	int i;
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+	u32 *theregs = (u32 __iomem *) priv->gemdev.registers;
+	u32 *buf = (u32 *) regbuf;
+
+	for (i = 0; i < sizeof (struct gem_reg) / sizeof (u32); i++)
+		buf[i] = theregs[i];
+}
+
+/** Get current and maximum settings for TX and RX descriptor ring */
+static void c4k_gringparam(struct net_device *dev, struct ethtool_ringparam *rvals)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+
+	rvals->rx_max_pending        = priv->RxRingSize;
+	rvals->rx_mini_max_pending   = priv->RxRingSize;
+	rvals->rx_jumbo_max_pending  = priv->RxRingSize;
+#if defined(CONFIG_MACH_M84XXX)
+	rvals->tx_max_pending        = priv->tx_queue[0].TxMaxRingSize;
+#else
+	rvals->tx_max_pending        = priv->TxMaxRingSize;
+#endif
+	rvals->rx_pending            = priv->RxRingSize;
+	rvals->rx_mini_pending       = priv->RxRingSize;
+	rvals->rx_jumbo_pending      = priv->RxRingSize;
+#if defined(CONFIG_MACH_M84XXX)
+	rvals->tx_pending            = priv->tx_queue[0].TxRingSize;
+#else
+	rvals->tx_pending            = priv->TxRingSize;
+#endif
+
+}
+
+/**
+ * Change the current ring parameters, stopping the controller if
+ * necessary so that we don't mess things up while we're in
+ * motion.  We wait for the ring to be clean before reallocating
+ * the rings.
+ */
+static int c4k_sringparam(struct net_device *dev, struct ethtool_ringparam *rvals)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+	int err = 0;
+
+	if (rvals->rx_pending > priv->RxRingSize)
+		return -EINVAL;
+
+	if (!is_power_of_2(rvals->rx_pending)) {
+		printk(KERN_ERR "%s: Ring sizes must be a power of 2\n",
+				dev->name);
+		return -EINVAL;
+	}
+#if defined(CONFIG_MACH_M84XXX)
+	if (rvals->tx_pending > priv->tx_queue[0].TxMaxRingSize)
+#else
+        if (rvals->tx_pending > priv->TxMaxRingSize)
+#endif
+		return -EINVAL;
+
+	if (!is_power_of_2(rvals->tx_pending)) {
+		printk(KERN_ERR "%s: Ring sizes must be a power of 2\n",
+				dev->name);
+		return -EINVAL;
+	}
+
+	if (dev->flags & IFF_UP) {
+#if defined(CONFIG_MACH_M84XXX)
+		c4k_eth_stop_queues(dev, 0);
+		c4k_eth_stop(dev);
+#else
+		netif_stop_queue(dev);
+		t2200_eth_stop(dev);
+#endif
+	}
+
+	/* Change the size */
+	priv->RxRingSize= rvals->rx_pending;
+#if defined(CONFIG_MACH_M84XXX)
+	priv->tx_queue[0].TxRingSize = rvals->tx_pending;
+#else
+        priv->TxRingSize = rvals->tx_pending;
+#endif
+
+	/* Rebuild the rings with the new size */
+	if (dev->flags & IFF_UP) {
+
+#if defined(CONFIG_MACH_M84XXX)
+		err = c4k_eth_start(dev);
+		if (err == 0)
+			c4k_eth_start_queues(dev, 0);
+#else
+		err = t2200_eth_start(dev);
+		if (err == 0)
+			netif_start_queue(dev);
+#endif
+	}
+
+	return err;
+}
+
+/** Set RX checksum offload option on/off */
+static int c4k_set_rx_csum(struct net_device *dev, uint32_t data)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+
+	if (data) {
+		gem_enable_rx_checksum_offload(&priv->gemdev);
+		priv->flags |= RX_CSUM_OFFLOAD_ENABLED;
+	} else {
+		gem_disable_rx_checksum_offload(&priv->gemdev);
+		priv->flags &= ~RX_CSUM_OFFLOAD_ENABLED;
+	}
+
+	return 0;
+}
+
+/** Get current RX checksum offload option, on or off */
+static uint32_t c4k_get_rx_csum(struct net_device *dev)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+
+	return (priv->flags & RX_CSUM_OFFLOAD_ENABLED) ? 1 : 0;
+}
+
+/** Set TX checksum offload option, on or off */
+int c4k_set_tx_csum(struct net_device *dev, uint32_t data)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+	unsigned long flags;
+	int rc = 0;
+
+	spin_lock_irqsave(&priv->txlock, flags);
+
+	if (data) {
+		gem_enable_tx_checksum_offload(&priv->gemdev);
+		dev->features |= NETIF_F_IP_CSUM;
+		priv->flags   |= TX_CSUM_OFFLOAD_ENABLED;
+	} else {
+		gem_disable_tx_checksum_offload(&priv->gemdev);
+		dev->features &= ~NETIF_F_IP_CSUM;
+		priv->flags   &= ~TX_CSUM_OFFLOAD_ENABLED;
+	}
+
+	spin_unlock_irqrestore(&priv->txlock, flags);
+
+	return rc;
+}
+
+/** Get current TX checksum offload option, on or off */
+static uint32_t c4k_get_tx_csum(struct net_device *dev)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+
+	return (priv->flags & TX_CSUM_OFFLOAD_ENABLED) ? 1 : 0;
+}
+
+/** Get current driver Network message level (for debug level messages) */
+static uint32_t c4k_get_msglevel(struct net_device *dev)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+
+	return priv->msg_enable;
+}
+
+/** Set current driver Network message level (for debug level messages) */
+static void c4k_set_msglevel(struct net_device *dev, uint32_t data)
+{
+	struct eth_c4k_priv *priv = netdev_priv(dev);
+
+	priv->msg_enable = data;
+}
+
+
+/** ethtool handlers mapping of standard ethtool functions to local driver functions
+ *  NULL if not supported
+ */
+struct ethtool_ops c4k_ethtool_ops = {
+	.get_settings      = c4k_get_settings,
+	.set_settings      = c4k_set_settings,
+	.get_drvinfo       = c4k_get_drvinfo,
+	.get_regs_len      = c4k_gemac_reglen,
+	.get_regs          = c4k_gemac_get_regs,
+	.get_link          = ethtool_op_get_link,
+	.get_ringparam     = c4k_gringparam,
+	.set_ringparam     = c4k_sringparam,
+	.get_strings       = c4k_gstrings,
+	.get_sset_count    = c4k_sset_count,
+	.get_ethtool_stats = c4k_fill_stats,
+	.get_rx_csum       = c4k_get_rx_csum,
+	.get_tx_csum       = c4k_get_tx_csum,
+	.set_rx_csum       = c4k_set_rx_csum,
+	.set_tx_csum       = c4k_set_tx_csum,
+	.get_msglevel      = c4k_get_msglevel,
+	.set_msglevel      = c4k_set_msglevel,
+};
+
+/* eof transcede_ethtool.c */
diff --git a/drivers/net/transcede/transcede_gem_AL.c b/drivers/net/transcede/transcede_gem_AL.c
new file mode 100644
index 0000000..00d57f4
--- /dev/null
+++ b/drivers/net/transcede/transcede_gem_AL.c
@@ -0,0 +1,1929 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#include <mach/hardware.h>
+#include "transcede_gemac.h"
+/******************************************************************************/
+
+
+/* Some functions to set/reset and get specific bits in the MAC registers
+ * Note that all functions operate on a read-modify-write basis
+ */
+
+
+/**
+ ******************************************************************************
+ *
+ * Function to start transmission on the specified device.  The parameter to
+ * this function is simply a pointer to the GEM_DEVICE structure.
+ * This function should be called after the relevant queues and data has been
+ * set up, however it will check if the number of queue elements is zero first.
+ * Note that this function will also enable tx even if it was previously not set
+ *
+ * Return value:
+ *  - 0   :   OK
+ *  - -1  :   Transmit queue not valid.
+ *
+ ******************************************************************************/
+int gem_start_tx (GEM_DEVICE *mac)
+{
+        *(volatile u32*)(mac->registers + GEM_TX_STATUS) = ( GEM_TX_USED);   // clear status
+        *(volatile u32*)(mac->registers + GEM_NET_CONTROL) |= ( GEM_TX_START | GEM_TX_EN );
+        return 0;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Halt transmission after current frame has completed.  This is accomplished
+ * simply by writing to the GEM_TX_HALT bit in the network control register,
+ * which should cause the MAC to complete its current frame then stop.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_stop_tx(GEM_DEVICE *mac)
+{
+        *(volatile u32*)(mac->registers + GEM_NET_CONTROL) |= GEM_TX_HALT;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Abort transmission immediately WITHOUT waiting for completion of any current
+ * frame.
+ * Note that after this operation, the transmit buffer descriptor will be reset
+ * to point to the first buffer in the descriptor list!
+ *
+ * There is no return value.
+ *
+ ******************************************************************************/
+void gem_abort_tx(GEM_DEVICE *mac)
+{
+        *(volatile u32*)(mac->registers + GEM_NET_CONTROL) &= (~GEM_TX_EN);
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Obtains status of transmission circuitry, whether it is transmitting or idle.
+ *
+ * Return value:
+ *   - 0   :   Transmitter is idle.
+ *   - 1   :   Transmitter active.
+ *
+ ******************************************************************************/
+int gem_transmitting(GEM_DEVICE *mac)
+{
+        return ((*(volatile u32*)(mac->registers + GEM_TX_STATUS) & GEM_TX_GO) == GEM_TX_GO);
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Set receive data offset for DMA copying of receive data frames to memory
+ *
+ * There is no return value.
+ *
+ ******************************************************************************/
+int gem_set_rx_offset(GEM_DEVICE *mac, unsigned short offset)
+{
+#if defined(CONFIG_MACH_M84XXX)
+       *(volatile u32*)(mac->registers + GEM_RX_OFFSET) = offset;
+#else
+        *(volatile u32*)(mac->registers + GEM_NET_CONFIG) &= ~(3           << 14); // Clear current offset (if any)
+        *(volatile u32*)(mac->registers + GEM_NET_CONFIG) |= ((offset & 3) << 14); // Set new offset
+#endif
+        return 0;
+}
+
+/**
+ ******************************************************************************
+ *
+ * Enable the receive circuitry.  This should be performed only after the
+ * buffers and the descriptor queues have been set up, otherwise unpredictable
+ * results may occur.
+ *
+ * Return value:
+ *  0   :   OK
+ *  -1  :   Receive queue not valid.
+ *
+ ******************************************************************************/
+int gem_enable_rx(GEM_DEVICE *mac)
+{
+       *(volatile u32*)(mac->registers + GEM_NET_CONTROL) |= GEM_RX_EN;
+        return 0;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Disable the receive circuitry.  This will stop reception of any frame
+ * immediately, note that the descriptor pointer is not changed.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_disable_rx(GEM_DEVICE *mac)
+{
+        *(volatile u32*)(mac->registers + GEM_NET_CONTROL) &= (~GEM_RX_EN);
+}
+/******************************************************************************/
+
+/**
+ ******************************************************************************
+ *
+ * Enable the transmit circuitry.  This should be performed only after the
+ * buffers and the descriptor queues have been set up, otherwise unpredictable
+ * results may occur.
+ *
+ * Return value:
+ *  0   :   OK
+ *  -1  :   Receive queue not valid.
+ *
+ ******************************************************************************/
+int gem_enable_tx(GEM_DEVICE *mac)
+{
+       *(volatile u32*)(mac->registers + GEM_NET_CONTROL) |= GEM_TX_EN;
+        return 0;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Disable the transmit circuitry.  This will stop reception of any frame
+ * immediately, note that the descriptor pointer is not changed.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_disable_tx(GEM_DEVICE *mac)
+{
+        *(volatile u32*)(mac->registers + GEM_NET_CONTROL) &= (~GEM_TX_EN);
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Obtain the status of the receive circuitry, whether it is enabled.
+ *
+ * Return value:
+ *  0   :   Receive circuitry disabled.
+ *  -1  :   Receive circuits enabled.
+ *
+ ******************************************************************************/
+int gem_receive_on(GEM_DEVICE *mac)
+{
+        return ((*(volatile u32*)(mac->registers + GEM_NET_CONTROL) & GEM_RX_EN) == GEM_RX_EN);
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Set the loopback mode of the MAC.  This can be either no loopback for normal
+ * operation, local loopback through MAC internal loopback module or PHY
+ * loopback for external loopback through a PHY.  This asserts the external loop
+ * pin.
+ * The function parameters are a pointer to the device and an enumerated type
+ * specifying the type of loopback required.
+ *
+ * Note: if an invalid loopback mode is specified, loop back will be disabled.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_set_loop
+(
+    GEM_DEVICE  *mac,       /**< Pointer to the device structure. */
+    MAC_LOOP    gem_loop    /**< Loopback mode. */
+)
+{
+    switch (gem_loop) {
+        case LB_LOCAL:
+                *(volatile u32*)(mac->registers + GEM_NET_CONTROL) &= (~GEM_LB_PHY);
+                *(volatile u32*)(mac->registers + GEM_NET_CONTROL) |= ( GEM_LB_MAC);
+            break;
+        case LB_EXT:
+                *(volatile u32*)(mac->registers + GEM_NET_CONTROL) &= (~GEM_LB_MAC);
+                *(volatile u32*)(mac->registers + GEM_NET_CONTROL) |= ( GEM_LB_PHY);
+            break;
+        default:
+                *(volatile u32*)(mac->registers + GEM_NET_CONTROL) &= (~(GEM_LB_MAC | GEM_LB_PHY));
+    }
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Get the loopback mode of the MAC.  This can be either no loopback for normal
+ * operation, local loopback through MAC internal loopback module or PHY
+ * loopback for external loopback through a PHY.  This asserts the external loop
+ * pin.
+ * The function parameters are a pointer to the device.
+ *
+ * Return value:
+ *  LB_LOCAL    :   Local loop back active.
+ *  LB_EXT      :   External loop back active.
+ *  LB_NONE     :   Loop back disabled.
+ *  -1          :   Unknown mode.
+ *
+ ******************************************************************************/
+MAC_LOOP gem_get_loop (GEM_DEVICE  *mac)
+{
+    u32 lb_mode;
+
+	lb_mode = *(volatile u32*)(mac->registers + GEM_NET_CONTROL) & (GEM_LB_PHY | GEM_LB_MAC);
+    switch (lb_mode) {
+        case GEM_LB_MAC:
+            return LB_LOCAL;
+            break;
+        case GEM_LB_PHY:
+            return LB_EXT;
+            break;
+        case 0:
+            return LB_NONE;
+            break;
+        default:
+            return -1;
+    }
+}
+/******************************************************************************/
+
+/**
+ ******************************************************************************
+ *
+ * Enable writing to the statistic registers.  This is for debug purposes only
+ * and should not be active during normal operation.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_stats_wr_en(GEM_DEVICE *mac)
+{
+        *(volatile u32*)(mac->registers + GEM_NET_CONTROL) |= GEM_STATS_WR_EN;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Disable writing to the statistic registers.  Under normal operation this is
+ * not necessary as the writing to statistics registers should be off by default
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_stats_wr_off(GEM_DEVICE *mac)
+{
+        *(volatile u32*)(mac->registers + GEM_NET_CONTROL) &= (~GEM_STATS_WR_EN);
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Increment all the statistic registers by 1.  This is for debug purposes only.
+ * Note that the statistic registers are automatically cleared on read!
+ *
+ * No return value.
+ *
+ ******************************************************************************/
+void gem_stats_inc(GEM_DEVICE *mac)
+{
+        *(volatile u32*)(mac->registers + GEM_NET_CONTROL) |= GEM_STATS_INC;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Clear all the statistic registers.
+ *
+ * No return value.
+ *
+ ******************************************************************************/
+void gem_stats_clr(GEM_DEVICE *mac)
+{
+        *(volatile u32*)(mac->registers + GEM_NET_CONTROL) |= GEM_STATS_CLR;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Enable pause frame reception.  With this enabled, if a valid pause frame is
+ * received, transmission will halt for the specified time after the current
+ * frame has completed transmission.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_enable_pause_rx(GEM_DEVICE *mac)
+{
+        *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) |= GEM_RX_PAUSE_EN;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Disable pause frame reception.  Incoming pause frames are ignored.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_disable_pause_rx(GEM_DEVICE *mac)
+{
+        *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) &= (~GEM_RX_PAUSE_EN);
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Enable Receive Checksum Engine.
+ * With this enabled, Frame with bad IP, TCP or UDP checksums are discarded
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_enable_rx_checksum_offload(GEM_DEVICE *mac)
+{
+        *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) |= GEM_CKSUM_OFFLOAD;
+
+#if defined(CONFIG_MACH_M84XXX)
+        // 84XXX version of GEM also needs to have which checksums are enabled
+        *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) |= GEM_ENABLE_L4_CKSUM | GEM_ENABLE_L3_CKSUM;
+#endif
+
+}
+
+/**
+ ******************************************************************************
+ *
+ * Disable Receive Checksum offload Engine.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_disable_rx_checksum_offload(GEM_DEVICE *mac)
+{
+        *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) &= (~GEM_CKSUM_OFFLOAD);
+
+#if defined(CONFIG_MACH_M84XXX)
+        *(volatile u32*)(mac->registers + GEM_NET_CONFIG) &= ~(GEM_ENABLE_L4_CKSUM | GEM_ENABLE_L3_CKSUM);
+#endif
+}
+
+/**
+ ******************************************************************************
+ *
+ * Enable Transmit Checksum Engine.
+ * With this enabled, Frame IP, TCP or UDP checksums are generated by the GEM
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_enable_tx_checksum_offload(GEM_DEVICE *mac)
+{
+        *(volatile u32*)(mac->registers + GEM_DMA_CONFIG) |= GEM_TX_CSUM_OFFLOAD;
+}
+
+/**
+ ******************************************************************************
+ *
+ * Disable Transmit Checksum Engine.
+ * With this disbled, Frame IP, TCP or UDP checksums are not generated by the GEM
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_disable_tx_checksum_offload(GEM_DEVICE *mac)
+{
+        *(volatile u32*)(mac->registers + GEM_DMA_CONFIG) &= ~GEM_TX_CSUM_OFFLOAD;
+}
+
+/**
+ ******************************************************************************
+ *
+ * Enable copy of received pause frame.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_enable_pause_cpy(GEM_DEVICE *mac)
+{
+        *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) &= (~GEM_RX_NO_PAUSE);
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Disable copy of received pause frame.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_disable_pause_cpy(GEM_DEVICE *mac)
+{
+        *(volatile u32*)(mac->registers + GEM_NET_CONTROL ) |= GEM_RX_NO_PAUSE;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Send a pause frame with zero quantum.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_send_0q_pause(GEM_DEVICE *mac)
+{
+        *(volatile u32*)(mac->registers + GEM_NET_CONTROL ) |= GEM_TX_0Q_PAUSE;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Send a normal pause frame.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_send_pause(GEM_DEVICE *mac)
+{
+        *(volatile u32*)(mac->registers + GEM_NET_CONTROL ) |= GEM_TX_PAUSE;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Set transmit pause quantum.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_set_tx_pause_q(GEM_DEVICE *mac, u32 gem_pause)
+{
+        *(volatile u32*)(mac->registers + GEM_TX_PAUSE_QUANT ) = gem_pause;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Read transmit pause quantum.
+ *
+ * Return values:
+ * 0-65535: transmit pause quantum.
+ *
+ ******************************************************************************/
+u32 gem_get_tx_pause_q(GEM_DEVICE *mac)
+{
+        return *(volatile u32*)(mac->registers + GEM_TX_PAUSE_QUANT );
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Set retry test bit, this is used for debug purposes only to speed up testing.
+ * This should not be enabled for normal operation.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_en_retry_test(GEM_DEVICE *mac)
+{
+        *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) |= GEM_RETRY_TEST;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Disable retry test bit.  For normal operation this bit should not be set.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_dis_retry_test(GEM_DEVICE *mac)
+{
+        *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) &= (~GEM_RETRY_TEST);
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Enable external address match via the eam pin, which when active will copy
+ * the current frame to memory.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_enable_eam(GEM_DEVICE *mac)
+{
+        *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) |= GEM_EAM_EN;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Disable external address match capability.  The MAC will ignore the status of
+ * the eam pin.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_disable_eam(GEM_DEVICE *mac)
+{
+        *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) &= (~GEM_EAM_EN);
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Enable storing of the receive frame check sequence into memory.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_enable_fcs_rx(GEM_DEVICE *mac)
+{
+    *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) &= (~GEM_RX_NO_FCS);
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Disable storing of the receive frame check sequence into memory.  The last 4
+ * bytes from the incoming frame will be checked for valid CRC, however this
+ * will not be stored into memory.  The frame length will be updated
+ * accordingly.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_disable_fcs_rx(GEM_DEVICE *mac)
+{
+    *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) |= GEM_RX_NO_FCS;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Enable reception of long frames up to 1536 bytes in length.
+ * These are not standard IEEE 802.3 frames.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_enable_1536_rx(GEM_DEVICE *mac)
+{
+    *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) |= GEM_FRAME_1536;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Disable reception of frames greater than 1518 bytes in length.
+ * This is normal operation mode for the MAC for compatibility with IEEE 802.3
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_disable_1536_rx(GEM_DEVICE *mac)
+{
+    *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) &= (~GEM_FRAME_1536);
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Enable reception of unicast hashed frames.  The frame will be received when
+ * the 6 bit hash function of the frame's destination address points a bit that
+ * is set in the 64-bit hash register and is signalled as a unicast frame.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_enable_unicast(GEM_DEVICE *mac)
+{
+    *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) |= GEM_UNICAST_EN;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Disable reception of unicast hashed frames.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_disable_unicast(GEM_DEVICE *mac)
+{
+    *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) &= (~GEM_UNICAST_EN);
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Enable reception of multicast hashed frames.  The frame will be received when
+ * the 6 bit hash function of the frame's destination address points a bit that
+ * is set in the 64-bit hash register and is signalled as a multicast frame.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_enable_multicast(GEM_DEVICE *mac)
+{
+    *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) |= GEM_MULTICAST_EN;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Disable reception of multicast hashed frames.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_disable_multicast(GEM_DEVICE *mac)
+{
+    *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) &= (~GEM_MULTICAST_EN);
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Allow reception of broadcast frames (frames with address set to all 1's)
+ * This is normal operating mode for the MAC.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_allow_broadcast(GEM_DEVICE *mac)
+{
+    *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) &= (~GEM_NO_BROADCAST);
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Do not allow reception of broadcast frames, such frames will be ignored.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_no_broadcast(GEM_DEVICE *mac)
+{
+    *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) |= GEM_NO_BROADCAST;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Enable copy all frames.  In this mode, the MAC will copy all valid received
+ * frames to memory regardless of the destination address.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_enable_copy_all(GEM_DEVICE *mac)
+{
+    *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) |= GEM_COPY_ALL;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Do not copy all frames.  Normal operating mode for the MAC, frames will only
+ * be copied to memory if it matches one of the specific or hash addresses.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_disable_copy_all(GEM_DEVICE *mac)
+{
+    *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) &= (~GEM_COPY_ALL);
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Set MAC into full duplex mode.  The crs and col signals will be ignored in
+ * this mode.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_full_duplex(GEM_DEVICE *mac)
+{
+#if defined(CONFIG_MACH_84XXX)
+        *(volatile u32*)(mac->gemac_baseaddr + GEM_CFG)
+                = (*(volatile u32*)(mac->gemac_baseaddr + GEM_CFG) | GEM_CONF_DUPLEX_SEL_GEM ) | GEM_CONF_DUPLEX_GEM_FULL;
+#else
+        *(volatile u32*)(mac->gemac_baseaddr + GEM_CFG)
+                = (*(volatile u32*)(mac->gemac_baseaddr + GEM_CFG) | GEM_CONF_DUPLEX_SEL_GEM ) | GEM_CONF_DUPLEX_GEM_FULL;
+
+        *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) |= GEM_FULL_DUPLEX;
+#endif
+
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Set MAC into half duplex mode.  The crs and col signals are used to detect
+ * collisions and perform deference where necessary.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_half_duplex(GEM_DEVICE *mac)
+{
+#if defined(CONFIG_MACH_84XXX)
+        *(volatile u32*)(mac->gemac_baseaddr + GEM_CFG)
+                = (*(volatile u32*)(mac->gemac_baseaddr + GEM_CFG) | GEM_CONF_DUPLEX_SEL_GEM ) & ~GEM_CONF_DUPLEX_GEM_FULL;
+#else
+        *(volatile u32*)(mac->gemac_baseaddr + GEM_CFG)
+                = (*(volatile u32*)(mac->gemac_baseaddr + GEM_CFG) | GEM_CONF_DUPLEX_SEL_GEM ) & ~GEM_CONF_DUPLEX_GEM_FULL;
+
+        *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) &= ~(GEM_FULL_DUPLEX);
+
+#endif
+
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Set the operating speed of the MAC, for 10 and 100Mb modes
+ *
+ * For 1G modes, this will set the MAC into the appropriate operating mode by
+ * switching to either the GMII or TBI interface depending on required mode.
+ *
+ * NOTE: This code will also force the GEM into manual setting of speed
+ * mode.  This will override "automatic RGMII speed setting" if already
+ * enabled previously.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_set_speed(GEM_DEVICE *mac, MAC_SPEED gem_speed)
+{
+    switch (gem_speed)
+    {
+        case SPEED_10M:
+              *(volatile u32*)(mac->gemac_baseaddr + GEM_CFG)
+                  = (*(volatile u32*)(mac->gemac_baseaddr + GEM_CFG)
+                     & ~GEM_CONF_SPEED_MASK
+                    )
+                  |  GEM_CONF_SPEED_SEL_GEM
+                  |  GEM_CONF_SPEED_GEM_10M;
+#if defined(CONFIG_MACH_M84XXX)
+              *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) &= ~(GEM_PCS_SEL);
+#else
+              *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) &= ~(GEM_PCS_SEL | GEM_GIG_MODE | GEM_SPEED_100);
+#endif
+            break;
+
+        case SPEED_100M:
+              *(volatile u32*)(mac->gemac_baseaddr + GEM_CFG)
+                  = (*(volatile u32*)(mac->gemac_baseaddr + GEM_CFG)
+                     & ~GEM_CONF_SPEED_MASK
+                    )
+                  |  GEM_CONF_SPEED_SEL_GEM
+                  |  GEM_CONF_SPEED_GEM_100M;
+#if defined(CONFIG_MACH_M84XXX)
+              *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) &= ~(GEM_PCS_SEL);
+#else
+              *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) &= ~(GEM_PCS_SEL | GEM_GIG_MODE);
+              *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) |= GEM_SPEED_100;
+#endif
+              break;
+
+        default:
+        case SPEED_1000M:
+              *(volatile u32*)(mac->gemac_baseaddr + GEM_CFG)
+                  = (*(volatile u32*)(mac->gemac_baseaddr + GEM_CFG)
+                     & ~GEM_CONF_SPEED_MASK
+                    )
+                  |  GEM_CONF_SPEED_SEL_GEM
+                  |  GEM_CONF_SPEED_GEM_1G;
+#if defined(CONFIG_MACH_M84XXX)
+              *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) &= ~(GEM_PCS_SEL);
+#else
+              *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) &= ~(GEM_PCS_SEL | GEM_SPEED_100);
+              *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) |= GEM_GIG_MODE;
+#endif
+              break;
+
+        case SPEED_1000M_PCS:
+              *(volatile u32*)(mac->gemac_baseaddr + GEM_CFG)
+                  = (*(volatile u32*)(mac->gemac_baseaddr + GEM_CFG)
+                     & ~GEM_CONF_SPEED_MASK
+                    )
+                  |  GEM_CONF_SPEED_SEL_GEM
+                  |  GEM_CONF_SPEED_GEM_1G;
+#if defined(CONFIG_MACH_M84XXX)
+              *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) |= GEM_PCS_SEL;
+#else
+              *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) &= ~(GEM_SPEED_100);
+              *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) |= (GEM_GIG_MODE | GEM_PCS_SEL);
+#endif
+            break;
+
+    }
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Get the operating speed of the MAC, currently this has no functional effect
+ * on the MAC.
+ *
+ * This function returns an enumerated value cast into an int.  This is for
+ * backward compatibility with the macb driver.
+ *
+ * Return value:
+ *   - SPEED_10M   :       MAC in 10Mb/s mode.
+ *   - SPEED_100M  :       MAC in 100Mb/s mode.
+ *   - SPEED_1000M :       MAC in 1G mode with GMII interface.
+ *   - SPEED_1000M_PCS :   MAC in 1G mode with PCS interface.
+ *
+ ******************************************************************************/
+MAC_SPEED gem_get_speed(GEM_DEVICE *mac)
+{
+
+    /* Check if running in mode where GEM is setting speeds, or automatic
+     * mode where the PHY is setting the speed (note automatic mode is only
+     * supported in RGMII and only with PHYs that support this optional
+     * feature)
+     */
+    if ( *(volatile u32*)(mac->gemac_baseaddr + GEM_CFG) & GEM_CONF_SPEED_SEL_GEM)
+    {
+            if ( *(volatile u32*)(mac->gemac_baseaddr + GEM_CFG) & GEM_CONF_SPEED_GEM_1G)
+         {
+                if (*(volatile u32*)(mac->registers + GEM_NET_CONFIG ) & GEM_PCS_SEL)
+                            return (SPEED_1000M_PCS);
+                 else
+                            return (SPEED_1000M);
+        }
+        else
+        {
+                    if ( *(volatile u32*)(mac->gemac_baseaddr + GEM_CFG) & GEM_CONF_SPEED_GEM_100M)
+                            return (SPEED_100M);
+                 else
+                            return (SPEED_10M);
+        }
+    }
+    else
+    {
+        /* GEM is running in mode where it is manually setting its speed */
+            if ( *(volatile u32*)(mac->gemac_baseaddr + GEM_CFG) & GEM_CONF_SPEED_PHY_1G)
+         {
+                if (*(volatile u32*)(mac->registers + GEM_NET_CONFIG ) & GEM_PCS_SEL)
+                            return (SPEED_1000M_PCS);
+                 else
+                            return (SPEED_1000M);
+        }
+        else
+        {
+                    if ( *(volatile u32*)(mac->gemac_baseaddr + GEM_CFG) & GEM_CONF_SPEED_PHY_100M)
+                            return (SPEED_100M);
+                 else
+                            return (SPEED_10M);
+        }
+    }
+}
+/******************************************************************************/
+
+/**
+ ******************************************************************************
+ *
+ * Enable to read snapshot values of statistic registers.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_enable_rd_snap(GEM_DEVICE *mac)
+{
+    *(volatile u32*)(mac->registers + GEM_NET_CONTROL ) |= GEM_READ_SNAP;
+}
+
+
+/**
+ ******************************************************************************
+ *
+ * Disable to read snapshot values of statistic registers.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_disable_rd_snap(GEM_DEVICE *mac)
+{
+    *(volatile u32*)(mac->registers + GEM_NET_CONTROL ) &= (~GEM_READ_SNAP);
+}
+
+
+/**
+ ******************************************************************************
+ *
+ * Take snapshot of statistic registers. Writing a one will record the current
+ * value of all statistics registers in the snapshot registers and clear the
+ * statistics registers.
+ *
+ * There is no return value for this function.
+ *
+ ******************************************************************************/
+void gem_take_snap(GEM_DEVICE *mac)
+{
+    *(volatile u32*)(mac->registers + GEM_NET_CONTROL ) |= GEM_TAKE_SNAP;
+}
+
+/**
+ ******************************************************************************
+ *
+ * Get the value of the transmit status register.
+ * The return value is an unsigned 32-bit integer containing the contents of the
+ * register.  This should be masked appropriately to obtain the relevant status.
+ *
+ * Return value:
+ *   - Returns current value of transmit status register.
+ *
+ ******************************************************************************/
+u32 gem_get_tx_stat(GEM_DEVICE *mac)
+{
+    return (*(volatile u32*)(mac->registers + GEM_TX_STATUS ));
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Reset the specified bits of the transmit status register.
+ *
+ * There is no return value.
+ *
+ ******************************************************************************/
+void gem_reset_tx_stat
+(
+    GEM_DEVICE  *mac,       /* Pointer to device structure. */
+    u32     rst_status  /* Status to reset. */
+)
+{
+    *(volatile u32*)(mac->registers + GEM_TX_STATUS ) |= rst_status;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Get the value of the receive status register.
+ * The return value is an unsigned 32-bit integer containing the contents of the
+ * register.  This should be masked appropriately to obtain the relevant status.
+ *
+ * Returns current receive status.
+ *
+ ******************************************************************************/
+u32 gem_get_rx_stat(GEM_DEVICE *mac)
+{
+    return (*(volatile u32*)(mac->registers + GEM_RX_STATUS ));
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Reset the specified bits of the receive status register.
+ *
+ * There is no return value.
+ *
+ ******************************************************************************/
+void gem_reset_rx_stat
+(
+    GEM_DEVICE  *mac,       /* Pointer to device structure. */
+    u32     rst_status  /* Status to reset. */
+)
+{
+    *(volatile u32*)(mac->registers + GEM_RX_STATUS ) |= rst_status;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Enable jumbo frames to be accepted.
+ *
+ * There is no return value.
+ *
+ ******************************************************************************/
+void gem_enable_rx_jmb(GEM_DEVICE  *mac)
+{
+    *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) |= GEM_RX_JUMBO;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Disable jumbo frames to be accepted.
+ *
+ * There is no return value.
+ *
+ ******************************************************************************/
+void gem_disable_rx_jmb(GEM_DEVICE  *mac)
+{
+    *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) &= (~GEM_RX_JUMBO);
+}
+/******************************************************************************/
+
+/**
+ ******************************************************************************
+ *
+ * Disable jumbo frames to be accepted and corrects MTU size in GEM-DMA register
+ *
+ * There is no return value.
+ *
+ ******************************************************************************/
+void gem_disable_rx_jmb_ex(GEM_DEVICE *mac, uint mtu_size)
+{
+    uint cfg;
+
+    *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) &= (~GEM_RX_JUMBO);
+
+    cfg = REG32(mac->registers + GEM_DMA_CONFIG);
+    cfg &= ~(0xFF << 16);
+    cfg |= ((mtu_size / 64) + 1) << 16;
+    REG32 (mac->registers + GEM_DMA_CONFIG) = cfg;
+}
+/******************************************************************************/
+
+/**
+ ******************************************************************************
+ *
+ * Enable jumbo frames and set jmb frame size by using GEM-DMA register
+ *
+ * There is no return value.
+ *
+ ******************************************************************************/
+
+void gem_enable_rx_jmb_ex (GEM_DEVICE *mac, uint mtu_size)
+{
+    uint cfg;
+
+    *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) |= GEM_RX_JUMBO;
+
+    cfg = REG32(mac->registers + GEM_DMA_CONFIG);
+    cfg &= ~(0xFF << 16);
+    cfg |= ((mtu_size / 64) + 1) << 16;
+    REG32 (mac->registers + GEM_DMA_CONFIG) = cfg;
+}
+
+/**
+ ******************************************************************************
+ *
+ * Enable only VLAN frames to be accepted, all other frames will be discarded.
+ *
+ * There is no return value.
+ *
+ ******************************************************************************/
+void gem_enable_vlan_only(GEM_DEVICE  *mac)
+{
+    *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) |= GEM_VLAN_ONLY;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Disable VLAN frame only mode. All frames will be accepted.
+ *
+ * There is no return value.
+ *
+ ******************************************************************************/
+void gem_disable_vlan_only(GEM_DEVICE  *mac)
+{
+    *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) &= (~GEM_VLAN_ONLY);
+}
+/******************************************************************************/
+
+
+
+/**
+ ******************************************************************************
+ *
+ * Read the interrupt status register.
+ * This returns an unsigned 32-bit integer with the current interrupt status,
+ * this should be masked appropriately to get the required status.
+ * Note that the interrupt status register is automatically reset on read, so
+ * the returned value should be stored if further processing required.
+ *
+ * Returns the current interrupt status.
+ *
+ ******************************************************************************/
+u32 gem_get_irq_stat(GEM_DEVICE *mac)
+{
+    return (*(volatile u32*)(mac->registers + GEM_IRQ_STATUS ));
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Set specified bits in the interrupt status register.
+ * This can be used for debug purposes to manually activate an interrupt.
+ *
+ * There is no return value.
+ *
+ ******************************************************************************/
+void gem_set_irq_stat(GEM_DEVICE *mac, u32 irq_status)
+{
+    *(volatile u32*)(mac->registers + GEM_IRQ_STATUS ) |= irq_status;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Enable specified interrupts.
+ * The specified interrupt bits are enabled by unmasking them.
+ * Note that this appends to the existing interrupt enable list.
+ *
+ * There is no return value.
+ *
+ ******************************************************************************/
+void gem_enable_irq(GEM_DEVICE *mac, u32 irq_en)
+{
+    *(volatile u32*)(mac->registers + GEM_IRQ_ENABLE ) |= irq_en;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Disable specified interrupts.
+ * The specified interrupts are masked out so that they do not generate an
+ * interrupt.
+ * Note that this appends to the existing interrupt mask list.
+ *
+ * There is no return value.
+ *
+ ******************************************************************************/
+void gem_mask_irq(GEM_DEVICE *mac, u32 irq_mask)
+{
+    *(volatile u32*)(mac->registers + GEM_IRQ_DISABLE ) |= irq_mask;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Obtain the current interrupt mask value.
+ * The return value indicates which interrupts are currently masked out i.e. do
+ * not generate an interrupt.
+ *
+ * Returns the interrupt mask status.
+ *
+ ******************************************************************************/
+u32 gem_get_irq_mask(GEM_DEVICE *mac)
+{
+    return (*(volatile u32*)(mac->registers + GEM_IRQ_MASK ));
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Read the pause time register.
+ *
+ * Returns the current value in the pause time register which will
+ * decrement when the MAC has gone into pause mode.
+ *
+ ******************************************************************************/
+u32 gem_pause_time(GEM_DEVICE *mac)
+{
+    return (*(volatile u32*)(mac->registers + GEM_RX_PAUSE_TIME ));
+}
+/******************************************************************************/
+
+
+#if defined(CONFIG_MACH_M84XXX)
+/**
+ ******************************************************************************
+ *
+ * Set the id-check registers of the MAC.
+ * These registers are used to check the type-id field of the incoming frames,
+ * if matched, the appropriate status bit will be set in word 1 of the receive
+ * descriptor for that frame.
+ * The input parameter is truncated to 16-bits.
+ *
+ * These functions are only supported for T4000/T300 GEM devices
+ *
+ * There is no return value.
+ *
+ ******************************************************************************/
+void gem_set_id_check1
+(
+    GEM_DEVICE  *mac,
+    u32     id_check
+)
+{
+    *(volatile u32*)(mac->registers + GEM_ID_CHECK1 ) = (id_check & 0xFFFF) | 0x80000000;
+}
+void gem_set_id_check2
+(
+    GEM_DEVICE  *mac,
+    u32     id_check
+)
+{
+    *(volatile u32*)(mac->registers + GEM_ID_CHECK2 ) = (id_check & 0xFFFF) | 0x80000000;
+}
+void gem_set_id_check3
+(
+    GEM_DEVICE  *mac,
+    u32     id_check
+)
+{
+    *(volatile u32*)(mac->registers + GEM_ID_CHECK3 ) = (id_check & 0xFFFF) | 0x80000000;
+}
+void gem_set_id_check4
+(
+    GEM_DEVICE  *mac,
+    u32     id_check
+)
+{
+    *(volatile u32*)(mac->registers + GEM_ID_CHECK4 ) = (id_check & 0xFFFF) | 0x80000000;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Get the value of the id-check registers in the MAC.
+ *
+ * These functions are only supported for T4000/T300 GEM devices
+ *
+ * Return value:
+ *  Value of ID check register.
+ *
+ ******************************************************************************/
+u32 gem_get_id_check1(GEM_DEVICE *mac)
+{
+    return (*(volatile u32*)(mac->registers + GEM_ID_CHECK1 ));
+}
+u32 gem_get_id_check2(GEM_DEVICE *mac)
+{
+    return (*(volatile u32*)(mac->registers + GEM_ID_CHECK2 ));
+}
+u32 gem_get_id_check3(GEM_DEVICE *mac)
+{
+    return (*(volatile u32*)(mac->registers + GEM_ID_CHECK3 ));
+}
+u32 gem_get_id_check4(GEM_DEVICE *mac)
+{
+    return (*(volatile u32*)(mac->registers + GEM_ID_CHECK4 ));
+}
+/******************************************************************************/
+#endif // if defined(CONFIG_MACH_M84XXX)
+
+/**
+ ******************************************************************************
+ *
+ * Set the hash register of the MAC.
+ * This register is used for matching unicast and multicast frames.
+ * The parameter of this function should be a pointer to type MAC_ADDR as
+ * defined in the header file.
+ *
+ * There is no return value.
+ *
+ ******************************************************************************/
+void gem_set_hash
+(
+    GEM_DEVICE  *mac,
+    MAC_ADDR    *hash_addr
+)
+{
+        *(volatile u32*)(mac->registers + GEM_HASH_BOT ) = hash_addr->bottom;
+        *(volatile u32*)(mac->registers + GEM_HASH_TOP ) = hash_addr->top;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Get the current value of the hash registers of the MAC.
+ *
+ * This function returns a value of type MAC_ADDR
+ *
+ ******************************************************************************/
+MAC_ADDR gem_get_hash(GEM_DEVICE *mac)
+{
+        MAC_ADDR addr;
+        addr.bottom = *(volatile u32*)(mac->registers + GEM_HASH_BOT ) ;
+        addr.top = *(volatile u32*)(mac->registers + GEM_HASH_TOP ) ;
+        return addr;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Setup all the specific address registers for the MAC.
+ * These registers are matched against incoming frames to determine whether the
+ * frame should be copied to memory.
+ * The input parameter to this function should be a pointer to type SPEC_ADDR
+ * as defined in the header file.
+ *
+ * There is no return value.
+ *
+ ******************************************************************************/
+void gem_set_address(GEM_DEVICE *mac, SPEC_ADDR *spec_addr)
+{
+
+        *(volatile u32*)(mac->registers + GEM_LADDR1_BOT ) = spec_addr->one.bottom;
+        *(volatile u32*)(mac->registers + GEM_LADDR1_TOP ) = spec_addr->one.top;
+        *(volatile u32*)(mac->registers + GEM_LADDR2_BOT ) = spec_addr->two.bottom;
+        *(volatile u32*)(mac->registers + GEM_LADDR2_TOP ) = spec_addr->two.top;
+        *(volatile u32*)(mac->registers + GEM_LADDR3_BOT ) = spec_addr->three.bottom;
+        *(volatile u32*)(mac->registers + GEM_LADDR3_TOP ) = spec_addr->three.top;
+        *(volatile u32*)(mac->registers + GEM_LADDR4_BOT ) = spec_addr->four.bottom;
+        *(volatile u32*)(mac->registers + GEM_LADDR4_TOP ) = spec_addr->four.top;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Get the current set of specific match addresses for the MAC.
+ * Note that a pointer is not returned as this would give direct access to the
+ * MAC address space which may cause unpredictable behaviour if wrongly used.
+ *
+ * Return type is of type SPEC_ADDR as defined in the header file.
+ *
+ ******************************************************************************/
+SPEC_ADDR gem_get_address(GEM_DEVICE *mac)
+{
+        SPEC_ADDR spec_addr;
+        spec_addr.one.bottom   = *(volatile u32*)(mac->registers + GEM_LADDR1_BOT ) ;
+        spec_addr.one.top      = *(volatile u32*)(mac->registers + GEM_LADDR1_TOP ) ;
+        spec_addr.two.bottom   = *(volatile u32*)(mac->registers + GEM_LADDR2_BOT ) ;
+        spec_addr.two.top      = *(volatile u32*)(mac->registers + GEM_LADDR2_TOP ) ;
+        spec_addr.three.bottom = *(volatile u32*)(mac->registers + GEM_LADDR3_BOT ) ;
+        spec_addr.three.top    = *(volatile u32*)(mac->registers + GEM_LADDR3_TOP ) ;
+        spec_addr.four.bottom  = *(volatile u32*)(mac->registers + GEM_LADDR4_BOT ) ;
+        spec_addr.four.top     = *(volatile u32*)(mac->registers + GEM_LADDR4_TOP ) ;
+        return spec_addr;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Set specific local addresses of the MAC.
+ * Rather than setting up all four specific addresses, this function sets them
+ * up individually.  The input parameter should be a pointer to type MAC_ADDR.
+ *
+ * There are no return values.
+ *
+ ******************************************************************************/
+void gem_set_laddr1(GEM_DEVICE *mac, MAC_ADDR *address)
+{
+        *(volatile u32*)(mac->registers + GEM_LADDR1_BOT ) = address->bottom;
+        *(volatile u32*)(mac->registers + GEM_LADDR1_TOP ) = address->top;
+}
+void gem_set_laddr2(GEM_DEVICE *mac, MAC_ADDR *address)
+{
+        *(volatile u32*)(mac->registers + GEM_LADDR2_BOT ) = address->bottom;
+        *(volatile u32*)(mac->registers + GEM_LADDR2_TOP ) = address->top;
+}
+void gem_set_laddr3(GEM_DEVICE *mac, MAC_ADDR *address)
+{
+        *(volatile u32*)(mac->registers + GEM_LADDR3_BOT ) = address->bottom;
+        *(volatile u32*)(mac->registers + GEM_LADDR3_TOP ) = address->top;
+}
+void gem_set_laddr4(GEM_DEVICE *mac, MAC_ADDR *address)
+{
+        *(volatile u32*)(mac->registers + GEM_LADDR4_BOT ) = address->bottom;
+        *(volatile u32*)(mac->registers + GEM_LADDR4_TOP ) = address->top;
+}
+/******************************************************************************/
+
+/**
+ ******************************************************************************
+ *
+ * Disable specific local addresses of the MAC.
+ *
+ ******************************************************************************/
+void gem_clear_laddr1(GEM_DEVICE *mac)
+{
+        *(volatile u32*)(mac->registers + GEM_LADDR1_BOT ) = 0;
+}
+void gem_clear_laddr2(GEM_DEVICE *mac)
+{
+        *(volatile u32*)(mac->registers + GEM_LADDR2_BOT )  = 0;
+}
+void gem_clear_laddr3(GEM_DEVICE *mac)
+{
+        *(volatile u32*)(mac->registers + GEM_LADDR3_BOT ) = 0;
+}
+void gem_clear_laddr4(GEM_DEVICE *mac)
+{
+        *(volatile u32*)(mac->registers + GEM_LADDR4_BOT ) = 0;
+}
+/******************************************************************************/
+
+/**
+ ******************************************************************************
+ *
+ * Get specific local addresses of the MAC.
+ * This allows returning of a single specific address stored in the MAC.
+ *
+ * Return value if of type MAC_ADDR as defined in the header file.
+ *
+ ******************************************************************************/
+MAC_ADDR gem_get_laddr1(GEM_DEVICE *mac)
+{
+        MAC_ADDR addr;
+        addr.bottom = *(volatile u32*)(mac->registers + GEM_LADDR1_BOT ) ;
+        addr.top    = *(volatile u32*)(mac->registers + GEM_LADDR1_TOP ) ;
+        return addr;
+}
+MAC_ADDR gem_get_laddr2(GEM_DEVICE *mac)
+{
+        MAC_ADDR addr;
+        addr.bottom = *(volatile u32*)(mac->registers + GEM_LADDR2_BOT ) ;
+        addr.top    = *(volatile u32*)(mac->registers + GEM_LADDR2_TOP ) ;
+        return addr;
+}
+MAC_ADDR gem_get_laddr3(GEM_DEVICE *mac)
+{
+        MAC_ADDR addr;
+        addr.bottom = *(volatile u32*)(mac->registers + GEM_LADDR3_BOT ) ;
+        addr.top    = *(volatile u32*)(mac->registers + GEM_LADDR3_TOP ) ;
+        return addr;
+}
+MAC_ADDR gem_get_laddr4(GEM_DEVICE *mac)
+{
+        MAC_ADDR addr;
+        addr.bottom = *(volatile u32*)(mac->registers + GEM_LADDR3_BOT ) ;
+        addr.top    = *(volatile u32*)(mac->registers + GEM_LADDR3_TOP ) ;
+        return addr;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Obtain the statistics registers structure.
+ * type GEM_STATS as defined in the header file.
+ * Note that this operation is relatively slow as it is copying all the
+ * statistics register values rather than providing a pointer reference to them.
+ * Note also that the statistics registers will all be automatically cleared
+ * after this operation.
+ *
+ * Returns the entire statistics register block for the MAC in a structure of
+ * type GEM_STATS
+ *
+ ******************************************************************************/
+GEM_STATS gem_get_stats(GEM_DEVICE *mac)
+{
+        return *(GEM_STATS*) (mac->registers + GEM_OCT_TX_BOT);
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Set the values of the statistics registers.
+ * This is for debug only and allows reading and writing to the statistic
+ * registers to verify functionality.
+ *
+ * There is no return value.
+ *
+ ******************************************************************************/
+void gem_set_stats(GEM_DEVICE *mac, GEM_STATS *stats)
+{
+//    mac->registers->stats = *stats;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Read from a specific MAC memory address. The defines in the header file
+ * should be used for this.
+ * Note that the range of the address requested is not checked.
+ *
+ * Returns register contents.
+ *
+ ******************************************************************************/
+u32 gem_register_rd(GEM_DEVICE *mac, u32 reg_addr)
+{
+        return ( *(volatile u32*)(mac->registers + reg_addr ) );
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Reset the MAC device to its default value and load up the MAC address stored
+ * in the header file into specific address 1.  The MAC will be held in
+ * quiescent state.
+ * This function should be called to initialise and check the device prior to
+ * setting up the buffer queues and enabling the MAC.  If it is called mid way
+ * through operation, the MAC is reset to default value and any pending frames
+ * will be lost.
+ * Note that the values in the GEM_DEVICE software structure are not reset, only
+ * the MAC registers are reset.  This is to allow, if necessary to recover the
+ * buffers and reload them into the MAC, however prior to doing this, they
+ * should be cleared first.
+ *
+ * Return value:
+ *   - 0   :   OK
+ *   - -1  :   Error in write/read check on initialisation.
+ *
+ ******************************************************************************/
+int gem_reset(GEM_DEVICE *mac)
+{
+        MAC_ADDR zero_address = {0x00000000, 0x00000000};
+        MAC_ADDR enet_address = zero_address;
+        u32 mdc_div;
+        u32 mdio_enable;
+        int stats_length;
+//    int loop_i;
+
+        stats_length = sizeof(GEM_STATS)/4;
+
+        /* Write to registers and set default values but preserve MDIO settings
+           which are controlled by a different driver */
+        mdio_enable = readl(mac->registers + GEM_NET_CONTROL) & GEM_MDIO_EN;
+        mdc_div     = readl(mac->registers + GEM_NET_CONFIG)  & GEM_MDC_DIV_MASK;
+
+        *(volatile u32*)(mac->registers + GEM_NET_CONTROL ) = GEM_STATS_CLR | mdio_enable;
+        *(volatile u32*)(mac->registers + GEM_NET_CONFIG )  = GEM_DEF_DUPLEX | mdc_div;
+
+        gem_set_loop(mac, GEM_DEF_LOOP);
+        gem_set_speed (mac, GEM_DEF_SPEED);
+
+        *(volatile u32*)(mac->registers + GEM_TX_STATUS )   = 0xFFFFFFFF;
+        *(volatile u32*)(mac->registers + GEM_RX_QPTR )     = 0x00000000;
+
+        *(volatile u32*)(mac->registers + GEM_RX_STATUS )   = 0xFFFFFFFF;
+        *(volatile u32*)(mac->registers + GEM_IRQ_DISABLE ) = 0xFFFFFFFF;
+        *(volatile u32*)(mac->registers + GEM_IRQ_STATUS )  = 0x00000000;
+
+        gem_set_hash(  mac,&zero_address);
+        gem_set_laddr1(mac,&enet_address);
+        gem_set_laddr2(mac,&zero_address);
+        gem_set_laddr3(mac,&zero_address);
+        gem_set_laddr4(mac,&zero_address);
+
+    /* Now read back values and return if not correct. */
+        if (
+        (*(volatile u32*)(mac->registers + GEM_LADDR4_BOT ) != zero_address.bottom) ||
+        (*(volatile u32*)(mac->registers + GEM_LADDR4_TOP ) != zero_address.top)    ||
+        (*(volatile u32*)(mac->registers + GEM_LADDR3_BOT ) != zero_address.bottom) ||
+        (*(volatile u32*)(mac->registers + GEM_LADDR3_TOP ) != zero_address.top)    ||
+        (*(volatile u32*)(mac->registers + GEM_LADDR2_BOT ) != zero_address.bottom) ||
+        (*(volatile u32*)(mac->registers + GEM_LADDR2_TOP)  != zero_address.top)    ||
+        (*(volatile u32*)(mac->registers + GEM_LADDR1_BOT)  != enet_address.bottom) ||
+        (*(volatile u32*)(mac->registers + GEM_LADDR1_TOP)  != enet_address.top)    ||
+        (*(volatile u32*)(mac->registers + GEM_HASH_BOT)    != zero_address.bottom) ||
+        (*(volatile u32*)(mac->registers + GEM_HASH_TOP)    != zero_address.top)    ||
+        (*(volatile u32*)(mac->registers + GEM_IRQ_STATUS ) != 0x00000000)          ||
+        (*(volatile u32*)(mac->registers + GEM_IRQ_MASK )   != 0x0003FFFF)          ||
+        (*(volatile u32*)(mac->registers + GEM_RX_STATUS )  != 0x00000000)          ||
+
+        (*(volatile u32*)(mac->registers + GEM_RX_QPTR )    != 0x00000000)          ||
+        (*(volatile u32*)(mac->registers + GEM_TX_STATUS )  != 0x00000000)
+       )
+        {
+                return -1;
+        }
+        else if (((*(volatile u32*)(mac->registers + GEM_NET_CONFIG ) & GEM_FULL_DUPLEX) !=
+                (0x00000000 | GEM_DEF_DUPLEX)) ||
+                (gem_get_loop(mac) != GEM_DEF_LOOP)
+        )
+        {
+                return -1;
+        }
+    else
+    {
+#if 0
+        for (loop_i=0;loop_i<stats_length;loop_i++)
+        {
+            if ( *( ((u32 *) &mac->registers->stats) + loop_i ) !=
+                        0x00000000 )
+            {
+                return -1;
+            }
+        }
+#endif
+        return 0;
+    }
+}
+/******************************************************************************/
+
+
+
+
+/**
+ ******************************************************************************
+ *
+ * Enable length field checking feature.
+ * The length field check feature automatically discards frames that has a frame
+ * length smaller than that reported in the length field of the header.
+ *
+ * Note that in accordance with the IEEE spec, frames that are longer than that
+ * reported in length field is still accepted as a valid frame.
+ *
+ * This function has no return value.
+ *
+ ******************************************************************************/
+void gem_enable_len_check(GEM_DEVICE *mac)
+{
+    *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) |= GEM_RX_LEN_CHK;
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Disable length field checking feature.
+ *
+ * This function has no return value.
+ *
+ ******************************************************************************/
+void gem_disable_len_check(GEM_DEVICE *mac)
+{
+    *(volatile u32*)(mac->registers + GEM_NET_CONFIG ) &= (~GEM_RX_LEN_CHK);
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Convert standard byte style ethernet address to format compatible with MAC.
+ *
+ * Input    :   Pointer to beginning of 6 byte address.
+ *              Pointer to MAC_ADDR structure.
+ * Return values:
+ *   - 0   :   OK
+ *   - -1  :   Invalid inputs.
+ *
+ ******************************************************************************/
+int gem_enet_addr_byte_mac(u8 * enet_byte_addr, MAC_ADDR *enet_addr)
+{
+    if ((enet_byte_addr == NULL) || (enet_addr == NULL))
+    {
+        return -1;
+    }
+    else
+    {
+        enet_addr->bottom = enet_byte_addr[0] |
+                            (enet_byte_addr[1] << 8) |
+                            (enet_byte_addr[2] << 16) |
+                            (enet_byte_addr[3] << 24);
+        enet_addr->top = enet_byte_addr[4] |
+                         (enet_byte_addr[5] << 8);
+        return 0;
+    }
+}
+/******************************************************************************/
+
+
+/**
+ ******************************************************************************
+ *
+ * Convert MAC type ethernet address to standard byte style ethernet address.
+ *
+ * Input    :   Pointer to beginning of free space for 6 byte address.
+ *              Pointer to MAC_ADDR structure.
+ * Return values:
+ *  0   :   OK
+ *  -1  :   Invalid inputs.
+ *
+ ******************************************************************************/
+int gem_enet_addr_mac_byte(u8 * enet_byte_addr, MAC_ADDR *enet_addr)
+{
+    if ((enet_byte_addr == NULL) || (enet_addr == NULL))
+    {
+        return -1;
+    }
+    else
+    {
+        enet_byte_addr[0] =  enet_addr->bottom & 0xFF;
+        enet_byte_addr[1] = (enet_addr->bottom >>  8) & 0xFF;
+        enet_byte_addr[2] = (enet_addr->bottom >> 16) & 0xFF;
+        enet_byte_addr[3] = (enet_addr->bottom >> 24) & 0xFF;
+
+        enet_byte_addr[4] =  enet_addr->top & 0xFF;
+        enet_byte_addr[5] = (enet_addr->top >> 8) & 0xFF;
+
+        return 0;
+    }
+}
+/******************************************************************************/
+
+/**
+ ******************************************************************************
+ *
+ * Add Mac address to address 1 (main MAC address) entry
+ *
+ * Input    :   Pointer to GEM deice to update
+ *              6 byte MAC address
+ * Return values:
+ *  0   :   OK
+ *  -1  :   Invalid inputs.
+ *
+ ******************************************************************************/
+
+int gem_add_arc_entry(GEM_DEVICE *gemdev, char *MAC_address)
+{
+        MAC_ADDR enet_addr;
+
+        gem_enet_addr_byte_mac(MAC_address, &enet_addr);
+        gem_set_laddr1(gemdev, &enet_addr);
+
+        return 0;
+}
+
+/*adding here for new INTERNAL function calls needed by TSU/1588 functions.  */
+
+s32 gem_get_tsu_sec(GEM_DEVICE *mac)
+{
+     return (*(volatile u32*)(mac->registers + GEM_1588_TIMER_SECONDS ));
+}
+
+/* Note Nano seconds register only has 30 bits value, 
+    we assume the upper 2 bits will always to read as ZERO, 
+    if not, need to add  an operation here &GEM_1588_NANSECONDS_MASK */
+u32 gem_get_tsu_nsec(GEM_DEVICE *mac)
+{
+    return (*(volatile u32*)(mac->registers + GEM_1588_TIMER_NANOSECONDS ));
+}
+
+void gem_set_tsu_sec(GEM_DEVICE *mac, u32 seconds)
+{
+    *(volatile u32*)(mac->registers + GEM_1588_TIMER_SECONDS) = seconds;
+}
+
+/* note the user should guarantee that only 30 bits of this register has value, 
+bit 30, bit 31 can only be ZERO */
+
+void gem_set_tsu_nsec(GEM_DEVICE *mac, u32 ns)
+{
+    *(volatile u32*)(mac->registers + GEM_1588_TIMER_NANOSECONDS) = ns;
+}
+
+void gem_set_register (GEM_DEVICE *mac, u32 offset, u32 value) 
+
+{
+   *(volatile u32*)(mac->registers + offset) = value;
+}
+
+u32 gem_get_register (GEM_DEVICE *mac,  u32 offset) 
+{
+   return (*(volatile u32*)(mac->registers + offset ));
+}
+
+s32 gem_get_tsu_ptp_tx_sec(GEM_DEVICE *mac)
+{
+     return (*(volatile u32*)(mac->registers + GEM_1588_PTP_EVENT_TX_SECONDS ));
+}
+
+/* Note Nano seconds register only has 30 bits value, 
+    we assume the upper 2 bits will always to read as ZERO, 
+    if not, need to add  an operation here &GEM_1588_NANSECONDS_MASK */
+s32 gem_get_tsu_ptp_tx_nsec(GEM_DEVICE *mac)
+{
+    return (*(volatile u32*)(mac->registers + GEM_1588_PTP_EVENT_TX_NANOSECONDS ));
+}
+
+s32 gem_get_tsu_ptp_peer_tx_sec(GEM_DEVICE *mac)
+{
+     return (*(volatile u32*)(mac->registers + GEM_1588_PTP_PEER_EVENT_TX_SECONDS ));
+}
+
+/* Note Nano seconds register only has 30 bits value, 
+we assume the upper 2 bits will always to read as ZERO, if not, need to add  an operation here &GEM_1588_NANSECONDS_MASK */
+s32 gem_get_tsu_ptp_peer_tx_nsec(GEM_DEVICE *mac)
+{
+    return (*(volatile u32*)(mac->registers + GEM_1588_PTP_PEER_EVENT_RX_NANOSECONDS ));
+}
+
+/* eof transcede_gem_AL.c */
diff --git a/drivers/net/transcede/transcede_gemac.h b/drivers/net/transcede/transcede_gemac.h
new file mode 100644
index 0000000..64d01a5
--- /dev/null
+++ b/drivers/net/transcede/transcede_gemac.h
@@ -0,0 +1,1046 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+  *
+  * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+  *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+  *
+  * You should have received a copy of the GNU General Public License
+  * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+  */
+
+#ifndef _TRANSCEDE_GEMAC_H
+#define _TRANSCEDE_GEMAC_H
+
+#include <asm/io.h>
+#include <mach/hardware.h>
+
+/* TODO: VIVA clenup this */
+#if defined(CONFIG_MACH_M84XXX)
+#include <mach/transcede-4000.h>
+#else
+#include <mach/transcede-2200.h>
+#endif
+
+#define CONFIG_TRANSCEDE_GEMAC	1
+
+
+#ifndef __MAC_LOOP_ENUM__
+#define __MAC_LOOP_ENUM__
+/**
+ * An enumerated type for loopback values.  This can be one of three values, no
+ * loopback -normal operation, local loopback with internal loopback module of
+ * MAC or PHY loopback which is through the external PHY.
+ */
+typedef enum {LB_NONE, LB_EXT, LB_LOCAL} MAC_LOOP;
+#endif
+
+
+#ifndef __MAC_SPEED_ENUM__
+#define __MAC_SPEED_ENUM__
+/**
+ * The possible operating speeds of the MAC, currently supporting 10, 100 and
+ * 1000Mb modes.
+ */
+typedef enum {SPEED_10M, SPEED_100M, SPEED_1000M, SPEED_1000M_PCS} MAC_SPEED;
+#endif
+
+#define GEM_HASH_REG_BITS	64
+/**
+ * Specify whether the MDIO should be available, this is set so that for reset
+ * function, appropriate options are setup.  To disable, use 0.
+ */
+#define GEM_MDIO_ENABLED (GEM_MDIO_EN)
+
+/** Specify default duplex mode, for half duplex - specify 0. */
+#ifndef GEM_DEF_DUPLEX
+/* Allow the value to be specified as compiler option
+   0 - half-duplex,
+   GEM_FULL_DUPLEX - full-duplex
+*/
+#define GEM_DEF_DUPLEX (GEM_FULL_DUPLEX)
+#endif
+
+
+#ifndef GEM_DEF_SPEED
+/* Allow the value to be specified as compiler option */
+/**
+ * Specify default operating speed, 1 for 100Mb.  Note that this is left
+ * shifted.  Also note that this simply asserts a signal to the PHY and has no
+ * effect on the operation of the MAC.
+ *   - For 10Mb/s mode, specify         SPEED_10M
+ *   - For 100Mb/s mode, specify        SPEED_100M
+ *   - For 1Gb/s mode, specify          SPEED_1000M
+ *   - For 1Gb/s with PCS mode, specify SPEED_1000M_PCS
+ */
+#define GEM_DEF_SPEED (SPEED_1000M)
+#endif
+
+/* Specify default loopback mode.  LB_NONE for no loopback, other values are LB_MAC
+ * and LB_PHY
+ */
+#define GEM_DEF_LOOP (LB_NONE)
+
+#if defined(CONFIG_MACH_M84XXX)
+////////////////////////////////////////////////////////////
+//	GEMAC IP wrapper 										    //
+////////////////////////////////////////////////////////////
+#define GEM_ADM_BLOCK		0x4000
+// admittance block
+#define	ADM_STATUS 		0x0
+#define	ADM_PKTDQ		0x4
+#define	ADM_CNFG		0x8
+#define	ADM_CONTROL		0xC
+#define ADM_QUEUEDEPTH 		0x80
+#define	ADM_QFULLTHR		0x88
+#define	ADM_QDROPMAXTHR		0x8c
+#define	ADM_QDROPMINTHR		0x90
+#define	ADM_DECAYTIMER		0x98
+#define ADM_BATCHINTRPKTCNT	0x100
+#define	ADM_BATCHINTRPKTTHRES	0x104
+#define	ADM_BATCHINTRTIMER	0x108
+#define	ADM_BATCHINTRTIMERINIT	0x10C
+#define	ADM_BATCHINTRSTAT	0x110
+
+
+#define GEM_SCH_BLOCK		0x8000
+
+#define SCH_CONTROL		0x04
+#define SCH_PACKET_QUEUED 	0x08
+#define SCH_STATUS	 	0x00
+
+#if 0
+#define SCH_PORT_BYTE_COUNTER	 	0x0C
+#define SCH_PORT_PACKET_COUNTER		0x10
+#define SCH_PACKET_OVERHEAD	 	0x14
+#define SCH_HW_FAULT_STATUS	 	0x18
+#define SCH_HW_FAULT_MASK	 	0x1C
+#define SCH_PORT_SHAPER_QBYTES		0x40
+#define SCH_PORT_SHAPER_QPACKETS	0x44
+#define SCH_PORT_SHAPER_RATE	 	0x50
+#define SCH_PORT_SHAPER_MAx_CREDIT	0x54
+#define SCH_PORT_SHAPER_CREDIT		0x58
+#define SCH_PORT_SHAPER_CONTROL		0x5C
+#define SCH_GROUP_SHAPER_QBYTES		0x60
+#define SCH_GROUP_SHAPER_QPACKETS	0x64
+#define SCH_GROUP_SHAPER_RATE	 	0x70
+#define SCH_GROUP_SHAPER_MAx_CREDIT	0x74
+#define SCH_GROUP_SHAPER_CREDIT		0x78
+#define SCH_GROUP_SHAPER_CONTROL	0x7C
+#define SCH_IOB_BASE			0x80
+
+// scheduler Queues
+#define SCH_BYTES_Q0			0x80
+#define SCH_PACKETS_Q0			0x84
+#define SCH_IDLE_Q0			0x88
+#define SCH_BYTES_Q1			0xa0
+#define SCH_PACKETS_Q1			0xa4
+#define SCH_IDLE_Q1			0xa8
+#define SCH_BYTES_Q2			0xc0
+#define SCH_PACKETS_Q2			0xc4
+#define SCH_IDLE_Q2			0xc8
+#define SCH_BYTES_Q3			0xe0
+#define SCH_PACKETS_Q3			0xe4
+#define SCH_IDLE_Q3			0xe8
+#define SCH_BYTES_Q4			0x100
+#define SCH_PACKETS_Q4			0x104
+#define SCH_IDLE_Q4			0x108
+#define SCH_BYTES_Q5			0x120
+#define SCH_PACKETS_Q5			0x124
+#define SCH_IDLE_Q5			0x128
+#define SCH_BYTES_Q6			0x140
+#define SCH_PACKETS_Q6			0x144
+#define SCH_IDLE_Q6			0x148
+#define SCH_BYTES_Q7			0x160
+#define SCH_PACKETS_Q7			0x164
+#define SCH_IDLE_Q7			0x168
+
+//#endif
+
+#endif //0
+#endif //CONFIG_MACH_M84XXX
+
+#define GEM_IP				0xE000
+#define GEM_CFG				0xF000
+#define GEM_TX_CTRL			0xF004
+#define GEM_TX_COLL			0xF008
+
+
+/**
+ Bit 0: Interface Mode Select
+   - 0: PIN Mode - the selection is done based on GEM#_MODE pins
+   - 1: GEM mode - the selection is done based on bits [3:1]
+*/
+#define GEM_CONF_MODE_SEL_PIN		(0 << 0)
+#define GEM_CONF_MODE_SEL_GEM		(1 << 0)
+
+/**
+Bits 3:1 GEM mode
+
+When MODE_SEL = 1, bits 3:1 used to configure Ethernet Interface protocol.
+
+NOTE: Interface type supported varies based on per chip and per pot
+Transcede GEM capabilities.  Not all types are supported on all GEM
+ports.
+*/
+#define GEM_CONF_MODE_GEM_MASK		(7 << 1)
+#define GEM_CONF_MODE_GEM_RGMII		(0 << 1)
+#define GEM_CONF_MODE_GEM_RMII		(1 << 1)
+#define GEM_CONF_MODE_GEM_MII		(2 << 1)
+#define GEM_CONF_MODE_GEM_GMII		(3 << 1)
+#define GEM_CONF_MODE_GEM_SGMII         (4 << 1)
+#define GEM_CONF_MODE_GEM_S3MII         (6 << 1)
+
+/**
+ Bits 6:4 Pin strap mode
+
+ When MODE_SEL = 0 use GEM#_MODE[2:0] pins to determine the
+ Ethernet Interface protocol as setup by external pin strapping.
+
+ Bits indicate the value of these pins.
+*/
+#define GEM_CONF_MODE_PIN_MASK		(7 << 4)
+#define GEM_CONF_MODE_PIN_RGMII		(0 << 4)
+#define GEM_CONF_MODE_PIN_RMII		(1 << 4)
+#define GEM_CONF_MODE_PIN_MII		(2 << 4)
+#define GEM_CONF_MODE_PIN_GMII		(3 << 4)
+
+/** Half duplex flow control enable */
+#define GEM_CONF_HDX_FC_ENABLED         (1 << 7)
+#define GEM_CONF_HDX_FC_DISABLED        (0 << 7)
+
+/**
+  Bit 8: Duplex Mux Select
+    - 0: PHY Duplex control
+    - 1: GEM Duplex control
+*/
+#define GEM_CONF_DUPLEX_SEL_PHY		(0 << 8)
+#define GEM_CONF_DUPLEX_SEL_GEM		(1 << 8)
+
+/**
+Bit 9: GEM Duplex control (for setting duplex)
+  - 0: Half duplex
+  - 1: Full Duplex
+*/
+#define GEM_CONF_DUPLEX_GEM_HALF	(0 << 9)
+#define GEM_CONF_DUPLEX_GEM_FULL	(1 << 9)
+
+/**
+Bit 10: PHY Duplex
+
+PHY Duplex from PHY's side-band or in-band signal (NOT for MII/GMII in which the value is fixed and will be '0'). For RMII/RGMII:
+  - 0: Half duplex
+  - 1: Full Duplex
+*/
+#define GEM_CONF_DUPLEX_PHY_HALF	(0 << 10)
+#define GEM_CONF_DUPLEX_PHY_FULL	(1 << 10)
+
+/**
+Bit 11: Speed Mux Select
+  - 0: PHY Speed control
+  - 1: GEM Speed control
+*/
+#define GEM_CONF_SPEED_SEL_PHY		(0 << 11)
+#define GEM_CONF_SPEED_SEL_GEM		(1 << 11)
+
+/**
+Bits 13:12 GEM speed control (for setting by SW)
+  - 00: 10 Mbps     (RGMII,S3MII, MII*,RMII)
+  - 01: 100 Mbps   (RGMII, S3MII, MII*, RMII)
+  - 10: 1000 Mbps (GMII, RGMII, SGMII)
+  - 11:  reserved
+*/
+#define GEM_CONF_SPEED_MASK		(3 << 12)
+#define GEM_CONF_SPEED_GEM_10M		(0 << 12)
+#define GEM_CONF_SPEED_GEM_100M		(1 << 12)
+#define GEM_CONF_SPEED_GEM_1G		(2 << 12)
+
+/**
+Bits 15:14 PHY Speed status For RGMII (for PHYs that support optional in-band or side band RGMII speed reporting):
+  - 00: 10 Mbps
+  - 01: 100 Mbps
+  - 10: 1000 Mbps
+  - 11: reserved
+*/
+#define GEM_CONF_SPEED_PHY_10M		(0 << 14)
+#define GEM_CONF_SPEED_PHY_100M		(1 << 14)
+#define GEM_CONF_SPEED_PHY_1G		(2 << 14)
+
+/**
+Bit 16: PHY Link from PHY's side-band or in-band signal For RGMII (for PHYs that support this optional feature):
+  - 0: Link down
+  - 1: Link up
+*/
+
+#define GEM_CONF_PHY_LINK_DOWN		(0 << 16)
+#define GEM_CONF_PHY_LINK_UP		(1 << 16)
+
+/**
+Bit 17: GEMCORE internal interface level loopback. Requires interface clock. Test Mode feature.
+  - 0: disable
+  - 1: enable
+*/
+#define GEM_CONF_GEM_LOOPBACK		(1 << 17)
+
+
+/* Define some bit positions for registers. */
+
+/*
+ * Bit positions for network control register, Offset 0x000
+ */
+#if defined(CONFIG_MACH_M822XX)
+
+/** Store TCP/UDP offset to memory (stored to CRC field of RX frames) */
+#define GEM_STORE_TCP_UDP_OFFSET_TO_MEMORY      (1<<22)
+
+/** Alternative SGMII mode */
+#define GEM_ALT_SGMII_MODE                      (1<<21)
+
+/** Enable detection of PTP unicast frames */
+#define GEM_PTP_UNICASE_ENABLE                  (1<<20)
+
+/** Enable LPI transmission */
+#define GEM_LPI_TX_ENABLE                       (1<<19)
+
+/** Flush the next packet from the external RX DPRAM. */
+#define GEM_FLUSH_RX_DPRAM                      (1<<18)
+
+#define GEM_TX_PFC_PAUSE                        (1<<17)
+#define GEM_PFC_PAUSE_ENABLE                    (1<<16)
+
+/**
+ * Store receive timestamp to memory.  Setting this bit to
+ * one will ause the CRC of every received frame to be replaced
+ * with the value of the nanoseconds field of the 1588 timer that
+ * was captured as the receive frame passed the message
+ * timestamp point.  Set to zero for "normal" operation
+ */
+#define GEM_STORE_RX_TIMESTAMP_TO_MEMORY        (1<<15)
+
+#endif //#if defined(CONFIG_MACH_M822XX)
+
+#define GEM_READ_SNAP	    (1<<14)     /* Read snapshot register */
+#define GEM_TAKE_SNAP       (1<<13)     /* Take a snapshot */
+#define GEM_TX_0Q_PAUSE     (1<<12)     /* Transmit zero quantum pause frame */
+#define GEM_TX_PAUSE        (1<<11)     /* Transmit pause frame */
+#define GEM_TX_HALT         (1<<10)     /* Halt transmission after curr frame */
+#define GEM_TX_START        (1<<9)      /* Start tx (tx_go) */
+#define GEM_STATS_WR_EN     (1<<7)      /* Enable writing to stat registers */
+#define GEM_STATS_INC       (1<<6)      /* Increment statistic registers */
+#define GEM_STATS_CLR       (1<<5)      /* Clear statistic registers */
+#define GEM_MDIO_EN         (1<<4)      /* Enable MDIO port */
+#define GEM_TX_EN           (1<<3)      /* Enable transmit circuits */
+#define GEM_RX_EN           (1<<2)      /* Enable receive circuits */
+#define GEM_LB_MAC          (1<<1)      /* Perform local loopback at MAC */
+#define GEM_LB_PHY          (1<<0)      /* Perform ext loopback through PHY */
+
+/*
+ * Bit positions for network configuration register, Offset 0x004
+ */
+
+#if defined(CONFIG_MACH_M822XX)
+#define GEM_UNI_DIR_ENABLE  (1<<31)     /* Uni direction enable. Controls PCS behavior on link up or down */
+#define GEM_IGNORE_IPG_ERR  (1<<30)     /* Ignore Inter-packet gap error */
+#endif
+
+#define GEM_RX_BAD_PREAMBLE (1<<29)     /* Receive frames with bad preamble */
+
+#if defined(CONFIG_MACH_M822XX)
+#define GEM_IPG_STRETCH_EN  (1<<28)     /* IPG STRETCH ENABLE */
+#define GEM_SGMII_MODE      (1<<27)     /* SGMII mode enable, changes auto-neg behavior */
+#define GEM_IGNORE_RX_FCS   (1<<26)     /* Allow reception and transfer of frames with bad CRC/FCS */
+#define GEM_HDX_RX_ALWAYS   (1<<25)     /* Enable RX frames to be received while transmitting in half duplex mode */
+#endif
+
+#define GEM_CKSUM_OFFLOAD   (1<<24)     /* Enable Checksum Engine*/
+#define GEM_RX_NO_PAUSE     (1<<23)     /* Do not copy pause frames to memory */
+
+#if defined(CONFIG_MACH_M84XXX)
+#define GEM_ENABLE_L4_DROP  (1<<22)     /* Drop packets if error in L4 checksum */
+#define GEM_ENABLE_L4_CKSUM (1<<21)     /* Enable L4 checksum check */
+#endif
+
+#if defined(CONFIG_MACH_M822XX)
+/**
+ *  For T2200/T3300 GEM, bits 22 and 21 control data bus width
+ *  Only 64 bit mode is support, other values to program to this register
+ *  are invalid, so they are not defined here.
+ */
+#define GEM_64_BIT_BUS      (1<<21)     /* 64 bit AXI data bus width */
+#endif
+
+#define GEM_MDC_DIV_MASK    (0x7 << 18) /* PCLK divisor for MDC */
+#define GEM_RX_NO_FCS       (1<<17)     /* Discard FCS from received frames. */
+#define GEM_RX_LEN_CHK      (1<<16)     /* Receive length check. */
+
+#if defined(CONFIG_MACH_M84XXX)
+#define GEM_ENABLE_L3_DROP  (1<<15)     /* Drop packets if error in L3 checksum */
+#define GEM_ENABLE_L3_CKSUM (1<<14)     /* Enable L3 checksum check */
+#endif
+
+#if defined(CONFIG_MACH_M822XX)
+/**
+ *  For T2200/T3300 GEM, bits 15 and 14 control receive offset.
+ *  This is used to set the number of bytes by which the received
+ *  data is offset from the start of the receive buffer.
+ *
+ *  NOTE: For the receive descriptor, it must be set to a RX pointer
+ *  aligned to 32 bits.  The offset will move the data by n where
+ *  n is 0 to 3 bytes from that base address set in the descriptor
+ */
+#define GEM_RX_BUF_OFFSET(x)(x<<14)   /* 64 bit AXI data bus width */
+#endif
+
+#define GEM_RX_PAUSE_EN     (1<<13)     /* Enable pause reception */
+#define GEM_RETRY_TEST      (1<<12)     /* Retry test for speeding up debug */
+#define GEM_PCS_SEL         (1<<11)     /* Select PCS */
+#define GEM_GIG_MODE        (1<<10)     /* Gigabit mode enable */
+#define GEM_EAM_EN          (1<<9)      /* External address match enable */
+#define GEM_FRAME_1536      (1<<8)      /* Enable 1536 byte frames reception */
+#define GEM_UNICAST_EN      (1<<7)      /* Receive unicast hash frames */
+#define GEM_MULTICAST_EN    (1<<6)      /* Receive multicast hash frames */
+#define GEM_NO_BROADCAST    (1<<5)      /* Do not receive broadcast frames */
+#define GEM_COPY_ALL        (1<<4)      /* Copy all frames */
+#define GEM_RX_JUMBO        (1<<3)      /* Allow jumbo frame reception */
+#define GEM_VLAN_ONLY       (1<<2)      /* Receive only VLAN frames */
+#define GEM_FULL_DUPLEX     (1<<1)      /* Enable full duplex */
+#define GEM_SPEED_100       (1<<0)      /* Set to 100Mb mode */
+
+/*
+ * Bit positions for network status register
+ */
+#define GEM_PHY_IDLE        (1<<2)      /* PHY management is idle */
+#define GEM_MDIO_IN         (1<<1)      /* Status of mdio_in pin */
+#define GEM_LINK_STATUS     (1<<0)      /* Status of link pin */
+
+/*
+ * Bit positions for dma configuration register
+ */
+#define GEM_TX_CSUM_OFFLOAD (1 << 11)
+
+ /*
+ * Bit positions for transmit status register
+ */
+#define GEM_TX_HRESP        (1<<8)      /* Transmit hresp not OK */
+#define GEM_LATE_COL        (1<<7)      /* Late collision */
+#define GEM_TX_URUN         (1<<6)      /* Transmit underrun occurred */
+#define GEM_TX_COMPLETE     (1<<5)      /* Transmit completed OK */
+#define GEM_TX_BUF_EXH      (1<<4)      /* Transmit buffs exhausted mid frame */
+#define GEM_TX_GO           (1<<3)      /* Status of tx_go internal variable */
+#define GEM_TX_RETRY_EXC    (1<<2)      /* Retry limit exceeded */
+#define GEM_TX_COL          (1<<1)      /* Collision occurred during frame tx */
+#define GEM_TX_USED         (1<<0)      /* Used bit read in tx buffer */
+
+/*
+ * Bit positions for receive status register
+ */
+#define GEM_RX_HRESP        (1<<3)      /* Receive hresp not OK */
+#define GEM_RX_ORUN         (1<<2)      /* Receive overrun occurred */
+#define GEM_RX_DONE         (1<<1)      /* Frame successfully received */
+#define GEM_RX_BUF_USED     (1<<0)      /* Receive buffer used bit read */
+
+/*
+ * Bit positions for interrupts
+ */
+#define GEM_IRQ_PCS_AN      (1<<16)     /* PCS autonegotiation complete */
+#define GEM_IRQ_EXT_INT     (1<<15)     /* External interrupt pin triggered */
+#define GEM_IRQ_PAUSE_TX    (1<<14)     /* Pause frame transmitted */
+#define GEM_IRQ_PAUSE_0     (1<<13)     /* Pause time has reached zero */
+#define GEM_IRQ_PAUSE_RX    (1<<12)     /* Pause frame received */
+#define GEM_IRQ_HRESP       (1<<11)     /* hresp not ok */
+#define GEM_IRQ_RX_ORUN     (1<<10)     /* Receive overrun occurred */
+#define GEM_IRQ_PCS_LINK    (1<<9)      /* Status of PCS link changed */
+#define GEM_IRQ_TX_DONE     (1<<7)      /* Frame transmitted ok */
+#define GEM_IRQ_TX_ERROR    (1<<6)      /* Transmit err occurred or no buffers*/
+#define GEM_IRQ_RETRY_EXC   (1<<5)      /* Retry limit exceeded */
+#define GEM_IRQ_TX_URUN     (1<<4)      /* Transmit underrun occurred */
+#define GEM_IRQ_TX_USED     (1<<3)      /* Tx buffer used bit read */
+#define GEM_IRQ_RX_USED     (1<<2)      /* Rx buffer used bit read */
+#define GEM_IRQ_RX_DONE     (1<<1)      /* Frame received ok */
+#define GEM_IRQ_MAN_DONE    (1<<0)      /* PHY management operation complete */
+#define GEM_IRQ_ALL         (0xFFFFFFFF)/* Everything! */
+
+/*
+ * Define some memory offsets for easier direct access to memory map.
+ */
+#define GEM_NET_CONTROL         (0x00)  /**<@brief Offset 0x000 Access:R/W Default: 0x0000_0000 Network control register */
+#define GEM_NET_CONFIG          (0x04)  /**<@brief Offset 0x004 Access:R/W Default: 0x0008_0000 Network configuration register */
+#define GEM_NET_STATUS          (0x08)
+#define GEM_USER_IO             (0x0C)
+#define GEM_DMA_CONFIG		(0x10)
+#define GEM_TX_STATUS           (0x14)
+#define GEM_RX_QPTR             (0x18)
+
+#if defined(CONFIG_MACH_M84XXX)
+
+/**
+ * @brief Offset 0x01C Access:R/W Default: 0x0000_0000
+ * Receive Buffer Offset register (bits 7:0, offset)
+ */
+#define	GEM_RX_OFFSET		(0x1C)
+
+#endif
+
+#if defined(CONFIG_MACH_M822XX)
+
+// Registers added specifically for T2200/T3300
+#define GEM_RX_QUEUE0           (0x18)
+#define GEM_TX_QUEUE0           (0x1C)
+
+#endif
+
+#define GEM_RX_STATUS           (0x20)
+#define GEM_IRQ_STATUS          (0x24)
+#define GEM_IRQ_ENABLE          (0x28)
+#define GEM_IRQ_DISABLE         (0x2C)
+#define GEM_IRQ_MASK            (0x30)
+#define GEM_PHY_MAN             (0x34)
+#define GEM_RX_PAUSE_TIME       (0x38)
+#define GEM_TX_PAUSE_QUANT      (0x3C)
+
+#if defined(CONFIG_MACH_M822XX)
+#define GEM_TX_PART_STORE_FORW  (0x40)
+#define GEM_RX_PART_STORE_FORW  (0x44)
+#endif
+
+#define GEM_HASH_BOT            (0x80)
+#define GEM_HASH_TOP            (0x84)
+#define GEM_LADDR1_BOT          (0x88)
+#define GEM_LADDR1_TOP          (0x8C)
+#define GEM_LADDR2_BOT          (0x90)
+#define GEM_LADDR2_TOP          (0x94)
+#define GEM_LADDR3_BOT          (0x98)
+#define GEM_LADDR3_TOP          (0x9C)
+#define GEM_LADDR4_BOT          (0xA0)
+#define GEM_LADDR4_TOP          (0xA4)
+
+#if defined(CONFIG_MACH_M822XX)
+#define GEM_TYPE_ID_MATCH_1     (0xA8)
+#define GEM_TYPE_ID_MATCH_2     (0xAC)
+#define GEM_TYPE_ID_MATCH_3     (0xB0)
+#define GEM_TYPE_ID_MATCH_4     (0xB4)
+#define GEM_WAKE_ON_LAN         (0xB8)
+#define GEM_IPG_STRETCH         (0xBC)
+#endif
+
+
+#define GEM_REV_ID              (0xFC)
+
+
+#define GEM_OCT_TX_BOT          (0x100)
+#define GEM_OCT_TX_TOP          (0x104)
+#define GEM_STATS_FRAMES_TX     (0x108)
+#define GEM_BROADCAST_TX        (0x10C)
+#define GEM_MULTICAST_TX        (0x110)
+#define GEM_STATS_PAUSE_TX      (0x114)
+#define GEM_FRAME64_TX          (0x118)
+#define GEM_FRAME65_TX          (0x11C)
+#define GEM_FRAME128_TX         (0x120)
+#define GEM_FRAME256_TX         (0x124)
+#define GEM_FRAME512_TX         (0x128)
+#define GEM_FRAME1024_TX        (0x12C)
+#define GEM_FRAME1519_TX        (0x130)
+#define GEM_STATS_TX_URUN       (0x134)
+#define GEM_STATS_SINGLE_COL    (0x138)
+#define GEM_STATS_MULTI_COL     (0x13C)
+#define GEM_STATS_EXCESS_COL    (0x140)
+#define GEM_STATS_LATE_COL      (0x144)
+#define GEM_STATS_DEF_TX        (0x148)
+#define GEM_STATS_CRS_ERRORS    (0x14C)
+#define GEM_OCT_RX_BOT          (0x150)
+#define GEM_OCT_RX_TOP          (0x154)
+#define GEM_STATS_FRAMES_RX     (0x158)
+#define GEM_BROADCAST_RX        (0x15C)
+#define GEM_MULTICAST_RX        (0x160)
+#define GEM_STATS_PAUSE_RX      (0x164)
+#define GEM_FRAME64_RX          (0x168)
+#define GEM_FRAME65_RX          (0x16C)
+#define GEM_FRAME128_RX         (0x170)
+#define GEM_FRAME256_RX         (0x174)
+#define GEM_FRAME512_RX         (0x178)
+#define GEM_FRAME1024_RX        (0x17C)
+#define GEM_FRAME1519_RX        (0x180)
+#define GEM_STATS_USIZE_FRAMES  (0x184)
+#define GEM_STATS_EXCESS_LEN    (0x188)
+#define GEM_STATS_JABBERS       (0x18C)
+#define GEM_STATS_FCS_ERRORS    (0x190)
+#define GEM_STATS_LENGTH_ERRORS (0x194)
+#define GEM_STATS_RX_SYM_ERR    (0x198)
+#define GEM_STATS_ALIGN_ERRORS  (0x19C)
+#define GEM_STATS_RX_RES_ERR    (0x1a0)
+#define GEM_STATS_RX_ORUN       (0x1a4)
+
+
+// Registers added specifically for T4000/T3000
+
+#if defined(CONFIG_MACH_M84XXX)
+#define	GEM_QUEUE_BASE0		(0x300)
+#define	GEM_QUEUE_BASE1		(0x304)
+#define	GEM_QUEUE_BASE2		(0x308)
+#define	GEM_QUEUE_BASE3		(0x30C)
+#define	GEM_QUEUE_BASE4		(0x310)
+#define	GEM_QUEUE_BASE5		(0x314)
+#define	GEM_QUEUE_BASE6		(0x318)
+#define	GEM_QUEUE_BASE7		(0x31C)
+#define GEM_ID_CHECK1           (0x320)
+#define GEM_ID_CHECK2           (0x324)
+#define GEM_ID_CHECK3           (0x328)
+#define GEM_ID_CHECK4           (0x32C)
+#define GEM_ID_CHECK5           (0x320)
+#define GEM_ID_CHECK6           (0x324)
+#define GEM_ID_CHECK7           (0x328)
+#define GEM_ID_CHECK8           (0x32C)
+#endif
+
+
+
+// Registers added specifically for T2200/T3300
+
+#if defined(CONFIG_MACH_M822XX)
+#define GEM_IRQ_STATUS_PQUEUE(i) (0x400 + 4*((i)-1)) /* i = 1..7 */
+#define GEM_TX_PQUEUE(i)         (0x440 + 4*((i)-1)) /* i = 1..7 */
+#define GEM_RX_PQUEUE(i)         (0x480 + 4*((i)-1)) /* i = 1..7 */
+#define GEM_RX_BUF_SIZE_PQUEUE(i)(0x4A0 + 4*((i)-1)) /* i = 1..7 */
+#define GEM_IRQ_ENABLE_PQUEUE(i) (0x600 + 4*((i)-1)) /* i = 1..7 */
+#define GEM_IRQ_DISABLE_PQUEUE(i)(0x620 + 4*((i)-1)) /* i = 1..7 */
+#define GEM_IRQ_MASK_PQUEUE(i)   (0x640 + 4*((i)-1)) /* i = 1..7 */
+#endif
+
+/*
+ * 1588 related
+ */
+#define GEM_STATS_BASE				0
+#define GEM_1588_UNICAST_IP_ADDR_RX   (GEM_STATS_BASE + 0xd4)
+#define GEM_1588_UNICAST_IP_ADDR_TX   (GEM_STATS_BASE + 0xd8)
+#define GEM_1588_TIMER_COMPARE_NANOSECONDS   (GEM_STATS_BASE + 0xdc)
+#define GEM_1588_TIMER_COMPARE_SECONDS   (GEM_STATS_BASE + 0xe0)
+#define GEM_1588_TIMER_SECONDS			(GEM_STATS_BASE+0x1D0)
+#define GEM_1588_TIMER_NANOSECONDS		(GEM_STATS_BASE+0x1D4)
+#define GEM_1588_TIMER_ADJUST			(GEM_STATS_BASE+0x1D8)
+#define GEM_1588_TIMER_INCREMENT		(GEM_STATS_BASE+0x1DC)
+#define GEM_1588_PTP_EVENT_TX_SECONDS		(GEM_STATS_BASE+0x1E0)
+#define GEM_1588_PTP_EVENT_TX_NANOSECONDS	(GEM_STATS_BASE+0x1E4)
+#define GEM_1588_PTP_EVENT_RX_SECONDS		(GEM_STATS_BASE+0x1E8)
+#define GEM_1588_PTP_EVENT_RX_NANOSECONDS	(GEM_STATS_BASE+0x1EC)
+#define GEM_1588_PTP_PEER_EVENT_TX_SECONDS	(GEM_STATS_BASE+0x1F0)
+#define GEM_1588_PTP_PEER_EVENT_TX_NANOSECONDS	(GEM_STATS_BASE+0x1F4)
+#define GEM_1588_PTP_PEER_EVENT_RX_SECONDS	(GEM_STATS_BASE+0x1F8)
+#define GEM_1588_PTP_PEER_EVENT_RX_NANOSECONDS	(GEM_STATS_BASE+0x1FC)
+#define GEM_1588_TSYNC_STROBE_SECONDS		(GEM_STATS_BASE+0x1C8)
+#define GEM_1588_TSYNC_STROBE_NANOSECONDS	(GEM_STATS_BASE+0x1CC)
+#define GEM_1588_INT_STATUS			(GEM_STATS_BASE+0x334)
+#define GEM_1588_INT_ENABLE			(GEM_STATS_BASE+0x338)
+#define GEM_1588_INT_DISABLE			(GEM_STATS_BASE+0x33C)
+#define GEM_1588_INT_MASK			(GEM_STATS_BASE+0x340)
+
+
+#if defined(CONFIG_MACH_M84XXX)
+#define GEM_REG_TOP             (0x23C)
+#endif
+
+
+/* Define some types for using with the HAL.  These types correspond to the
+ * memory map and programming structure of the MAC device.
+ * All structures are 'volatile' to indicate they can be changed by some non-
+ * programming means - i.e. by the hardware itself.  This prevents the compiler
+ * from making false assumptions on how to optimise the code.  Some elements
+ * are also defined as 'const' to enforce some checks on the programmer.  These
+ * are only for register fields that can only be changed by the hardware and are
+ * not writable.
+ */
+
+
+
+#ifndef __MAC_ADDR_DEF__
+#define __MAC_ADDR_DEF__
+/**
+ * The Address organisation for the MAC device.  All addresses are split into
+ * two 32-bit register fields.  The first one (bottom) is the lower 32-bits of
+ * the address and the other field are the high order bits - this may be 16-bits
+ * in the case of MAC addresses, or 32-bits for the hash address.
+ * In terms of memory storage, the first item (bottom) is assumed to be at a
+ * lower address location than 'top'. i.e. top should be at address location of
+ * 'bottom' + 4 bytes.
+ */
+typedef struct {
+   u32  bottom;     /* Lower 32-bits of address. */
+   u32  top;        /* Upper 32-bits of address. */
+} volatile MAC_ADDR;
+#endif
+
+#define MAC_ADDR_LEN 6
+
+
+
+#ifndef __SPEC_ADDR_DEF__
+#define __SPEC_ADDR_DEF__
+/**
+ * The following is the organisation of the address filters section of the MAC
+ * registers.  The Cadence MAC contains four possible specific address match
+ * addresses, if an incoming frame corresponds to any one of these four
+ * addresses then the frame will be copied to memory.
+ * It is not necessary for all four of the address match registers to be
+ * programmed, this is application dependant.
+ */
+typedef struct {
+    MAC_ADDR    one;        /**< Specific address register 1. */
+    MAC_ADDR    two;        /**< Specific address register 2. */
+    MAC_ADDR    three;      /**< Specific address register 3. */
+    MAC_ADDR    four;       /**< Specific address register 4. */
+} volatile SPEC_ADDR;
+#endif
+
+/**
+ * The set of statistics registers implemented in the Cadence MAC.
+ * The statistics registers implemented are a subset of all the statistics
+ * available, but contains all the compulsory ones.
+ * For full descriptions on the registers, refer to the Cadence MAC programmers
+ * guide or the IEEE 802.3 specifications.
+ */
+typedef struct gem_stats{
+    u32 octets_tx_bot;      /**< @brief Lower 32-bits for number of octets tx'd */
+    u32 octets_tx_top;      /**< @brief Upper 16-bits for number of octets tx'd */
+    u32 frames_tx;          /**< @brief Number of frames transmitted OK */
+    u32 broadcast_tx;       /**< @brief Number of broadcast frames transmitted */
+    u32 multicast_tx;       /**< @brief Number of multicast frames transmitted */
+    u32 pause_tx;           /**< @brief Number of pause frames transmitted. */
+    u32 frame64_tx;         /**< @brief Number of 64byte frames transmitted */
+    u32 frame65_127_tx;     /**< @brief Number of 65-127 byte frames transmitted */
+    u32 frame128_255_tx;    /**< @brief Number of 128-255 byte frames transmitted */
+    u32 frame256_511_tx;    /**< @brief Number of 256-511 byte frames transmitted */
+    u32 frame512_1023_tx;   /**< @brief Number of 512-1023 byte frames transmitted */
+    u32 frame1024_1518_tx;  /**< @brief Number of 1024-1518 byte frames transmitted*/
+    u32 frame1519_tx;       /**< @brief Number of frames greater than 1518 bytes tx*/
+    u32 tx_urun;            /**< @brief Transmit underrun errors due to DMA */
+    u32 single_col;         /**< @brief Number of single collision frames */
+    u32 multi_col;          /**< @brief Number of multi collision frames */
+    u32 excess_col;         /**< @brief Number of excessive collision frames. */
+    u32 late_col;           /**< @brief Collisions occuring after slot time */
+    u32 def_tx;             /**< @brief Frames deferred due to crs */
+    u32 crs_errors;         /**< @brief Errors caused by crs not being asserted. */
+    u32 octets_rx_bot;      /**< @brief Lower 32-bits for number of octets rx'd */
+    u32 octets_rx_top;      /**< @brief Upper 16-bits for number of octets rx'd */
+    u32 frames_rx;          /**< @brief Number of frames received OK */
+    u32 broadcast_rx;       /**< @brief Number of broadcast frames received */
+    u32 multicast_rx;       /**< @brief Number of multicast frames received */
+    u32 pause_rx;           /**< @brief Number of pause frames received. */
+    u32 frame64_rx;         /**< @brief Number of 64byte frames received */
+    u32 frame65_127_rx;     /**< @brief Number of 65-127 byte frames received */
+    u32 frame128_255_rx;    /**< @brief Number of 128-255 byte frames received */
+    u32 frame256_511_rx;    /**< @brief Number of 256-511 byte frames received */
+    u32 frame512_1023_rx;   /**< @brief Number of 512-1023 byte frames received */
+    u32 frame1024_1518_rx;  /**< @brief Number of 1024-1518 byte frames received*/
+    u32 frame1519_rx;       /**< @brief Number of frames greater than 1518 bytes rx*/
+    u32 usize_frames;       /**< @brief Frames received less than min of 64 bytes */
+    u32 excess_length;      /**< @brief Number of excessive length frames rx */
+    u32 jabbers;            /**< @brief Excessive length + crc or align errors. */
+    u32 fcs_errors;         /**< @brief Number of frames received with crc errors */
+    u32 length_check_errors;/**< @brief Number of frames with incorrect length */
+    u32 rx_symbol_errors;   /**< @brief Number of times rx_er asserted during rx */
+    u32 align_errors;       /**< @brief Frames received without integer no. bytes */
+    u32 rx_res_errors;      /**< @brief Number of times buffers ran out during rx */
+    u32 rx_orun;            /**< @brief Receive overrun errors due to DMA */
+    u32 ip_cksum;           /**< @brief IP header checksum errors */
+    u32 tcp_cksum;          /**< @brief TCP checksum errors */
+    u32 udp_cksum;          /**< @brief UDP checksum errors */
+} volatile GEM_STATS;
+
+
+/**
+ * This is the memory map for the Cadence Enhanced MAC device.
+ * For full descriptions on the registers, refer to the Cadence MAC programmers
+ * guide or the IEEE 802.3 specifications.
+ */
+typedef struct gem_reg{
+    u32                net_control;             /**< @brief 0x000: Network control      */
+    u32                net_config;              /**< @brief 0x004: Network config       */
+    const volatile u32 net_status;              /**< @brief 0x008: Network status, RO   */
+    volatile u32       user_io;                 /**< @brief 0x00C: User IO              */
+    const volatile u32 rsvd1;                   /**< @brief 0x010: reserved             */
+    u32                tx_status;               /**< @brief 0x014: Transmit status 0x14 */
+    u32                rx_qptr;                 /**< @brief 0x018: Receive queue pointer 0x18 */
+    u32                rx_global_offset;        /**< @brief 0x01C: Transmit queue pointer 0x1C */
+    u32                rx_status;               /**< @brief 0x020: Receive status       */
+    u32                irq_status;              /**< @brief 0x024: Interrupt status     */
+    u32                irq_enable;              /**< @brief 0x028: Interrupt enable     */
+    u32                irq_disable;             /**< @brief 0x02C: Interrupt disable    */
+    const volatile u32 irq_mask;                /**< @brief 0x030: Interrupt mask, RO   */
+    u32                phy_man;                 /**< @brief 0x034: PHY management       */
+    const volatile u32 pause_time;              /**< @brief 0x038: Pause time register  */
+    u32                tx_pause_quant;          /**< @brief 0x03C: Transmit pause quantum      */
+#if defined(CONFIG_MACH_M822XX)
+    u32                tx_partial_store_and_fw; /**< @brief 0x040: Transmit partial store and forward */
+    u32                rx_partial_store_and_fw; /**< @brief 0x044: Receive  partial store and forward 0x44 */
+    u32                reserved[14];            /**< @brief Reserved 0x48-0x7C */
+#else
+    u32                reserved[16];            /**< @brief Reserved 0x40-0x7C */
+#endif
+    MAC_ADDR           hash_addr;               /**< @brief Hash address 0x80 - 0x84 */
+    SPEC_ADDR          address;                 /**< @brief Specific addresses 0x88 - 0xA4 */
+    u32                id_check1;               /**< @brief 0x0A8: Type ID check1  */
+    u32                id_check2;               /**< @brief 0x0AC: Type ID check2  */
+    u32                id_check3;               /**< @brief 0x0B0: Type ID check3  */
+    u32                id_check4;               /**< @brief 0x0B4: Type ID check4  */
+#if defined(CONFIG_MACH_M822XX)
+    u32                wake_on_lan;             /**< @brief 0x0B8: Wake on LAN  */
+    u32                ipg_stretch;             /**< @brief 0x0BC: Inter-Packet(frame) Gap stretch */
+    u32                stacked_vlan;            /**< @brief 0x0C0: Stacked VLAN  */
+    u32                tx_pfc_pause;            /**< @brief 0x0C4: Transmit PFC Pause register */
+    u32                spec_addr1_match_bot;    /**< @brief 0x0C8: Specific Address 1 match bottom [31:00]  */
+    u32                spec_addr1_match_top;    /**< @brief 0x0CC: Specific Address 1 match top    [47:32]  */
+    u32                rx_data_buf_addr_mask;   /**< @brief 0x0D0: AHB/AXI Address mask for RX data buffer accesses */
+    u32                ptp_rx_unicast_ip_addr;  /**< @brief 0x0D4: PTP RX unicast IP destination address    */
+    u32                ptp_tx_unicast_ip_addr;  /**< @brief 0x0D8: PTP TX unicast IP destination address    */
+    u32                tsu_timer_compare_nsecs; /**< @brief 0x0DC: TSU timer comparison value, nanoseconds  */
+    u32                tsu_timer_compare_secs;  /**< @brief 0x0E0: TSU timer comparison value, seconds      */
+    u32                reserved_e4;
+    u32                reserved_e8;
+    u32                reserved_ec;
+    u32                reserved_f0;
+    u32                reserved_f4;
+    u32                reserved_f8;
+#else
+    u32                rsvd2[17];               /**< @brief 0x0B8-0x0F8: Reserved */
+#endif
+    u32                rev_id;                  /**< @brief 0x0FC: Device Revision ID */
+    struct             gem_stats stats;         /**< @brief MAC statistics 0x100 - 0x1A4 */
+}   GEM_REG;
+
+
+
+#define GEMAC_REG_SPACE	sizeof(struct gem_reg)
+#define GEMAC_RMON_LEN (sizeof(struct gem_stats)/sizeof(u32))
+
+
+#if 0
+struct gemac_info_struct {
+	unsigned int gemacbaseaddr;
+	unsigned int idmaaddr;
+	unsigned int phyaddr;
+	u32 flags;
+	u32 phyflags;			// to indicate if gigabit supported or not
+	unsigned int phyregidx;
+};
+#endif
+
+/**
+ * This is a structure that will be passed and used for all HAL operations, it
+ * consists of pointers to the various MAC structures such as the MAC register
+ * block and the first descriptor element for the rx and tx buffer queues.
+ * Other internal variables declared for use in function calls and to keep track
+ * of where things are.
+ */
+typedef struct {
+	void	*registers;	  /**< @brief Pointer (base pointer plus offset) to the GEM IP registers. */
+	void	*gemac_baseaddr;  /**< @brief Base pointer to GEM (does not include offset to registers) */
+} GEM_DEVICE;
+
+
+/******************************************************************************/
+/*
+ * Prototypes for functions of HAL
+*/
+/******************************************************************************/
+
+
+/** Re-initialise device and check reset values. */
+int gem_reset             (GEM_DEVICE *mac);
+
+/* Device setup. */
+
+void gem_set_loop         (GEM_DEVICE *mac, MAC_LOOP gem_loop);
+MAC_LOOP gem_get_loop     (GEM_DEVICE *mac);
+
+
+void gem_enable_eam       (GEM_DEVICE *mac);
+void gem_disable_eam      (GEM_DEVICE *mac);
+
+void gem_enable_fcs_rx    (GEM_DEVICE *mac);
+void gem_disable_fcs_rx   (GEM_DEVICE *mac);
+
+void gem_enable_1536_rx   (GEM_DEVICE *mac);
+void gem_disable_1536_rx  (GEM_DEVICE *mac);
+
+void gem_full_duplex      (GEM_DEVICE *mac);
+void gem_half_duplex      (GEM_DEVICE *mac);
+
+void gem_set_speed        (GEM_DEVICE *mac, MAC_SPEED gem_speed);
+MAC_SPEED gem_get_speed   (GEM_DEVICE *mac);
+
+/* Pause control. */
+void gem_enable_pause_rx  (GEM_DEVICE *mac);
+void gem_disable_pause_rx (GEM_DEVICE *mac);
+
+void gem_enable_rx_checksum_offload (GEM_DEVICE *mac);
+void gem_disable_rx_checksum_offload(GEM_DEVICE *mac);
+void gem_enable_tx_checksum_offload (GEM_DEVICE *mac);
+void gem_disable_tx_checksum_offload(GEM_DEVICE *mac);
+
+u32 gem_pause_time(GEM_DEVICE *mac);
+
+void gem_enable_pause_cpy (GEM_DEVICE *mac);
+void gem_disable_pause_cpy(GEM_DEVICE *mac);
+
+void gem_send_0q_pause    (GEM_DEVICE *mac);
+void gem_send_pause       (GEM_DEVICE *mac);
+
+void gem_set_tx_pause_q   (GEM_DEVICE *mac, u32 gem_pause);
+u32 gem_get_tx_pause_q    (GEM_DEVICE *mac);
+
+/* Address setup and control. */
+void gem_enable_unicast   (GEM_DEVICE *mac);
+void gem_disable_unicast  (GEM_DEVICE *mac);
+
+void gem_enable_multicast (GEM_DEVICE *mac);
+void gem_disable_multicast(GEM_DEVICE *mac);
+
+void gem_allow_broadcast  (GEM_DEVICE *mac);
+void gem_no_broadcast     (GEM_DEVICE *mac);
+
+void gem_enable_copy_all  (GEM_DEVICE *mac);
+void gem_disable_copy_all (GEM_DEVICE *mac);
+
+void gem_set_hash         (GEM_DEVICE *mac, MAC_ADDR *hash_addr);
+MAC_ADDR gem_get_hash     (GEM_DEVICE *mac);
+
+void gem_set_address      (GEM_DEVICE *mac, SPEC_ADDR *spec_addr);
+SPEC_ADDR gem_get_address (GEM_DEVICE *mac);
+
+int gem_add_arc_entry     (GEM_DEVICE *gemdev, char *MAC_address);
+
+
+/* Functions to convert between address formats. */
+int gem_enet_addr_byte_mac(u8 * enet_byte_addr, MAC_ADDR *enet_addr);
+int gem_enet_addr_mac_byte(u8 * enet_byte_addr, MAC_ADDR *enet_addr);
+
+void gem_set_laddr1       (GEM_DEVICE *mac, MAC_ADDR *address);
+void gem_set_laddr2       (GEM_DEVICE *mac, MAC_ADDR *address);
+void gem_set_laddr3       (GEM_DEVICE *mac, MAC_ADDR *address);
+void gem_set_laddr4       (GEM_DEVICE *mac, MAC_ADDR *address);
+void gem_clear_laddr1     (GEM_DEVICE *mac);
+void gem_clear_laddr2     (GEM_DEVICE *mac);
+void gem_clear_laddr3     (GEM_DEVICE *mac);
+void gem_clear_laddr4     (GEM_DEVICE *mac);
+
+MAC_ADDR gem_get_laddr1   (GEM_DEVICE *mac);
+MAC_ADDR gem_get_laddr2   (GEM_DEVICE *mac);
+MAC_ADDR gem_get_laddr3   (GEM_DEVICE *mac);
+MAC_ADDR gem_get_laddr4   (GEM_DEVICE *mac);
+
+int gem_add_arc_entry(GEM_DEVICE *gemdev, char *MAC_address);
+
+#if defined(CONFIG_MACH_84XXX)
+/* These functions are only supported on GEM devices for T4000/T3000 */
+void gem_set_id_check1    (GEM_DEVICE *mac, u32 id_check);
+void gem_set_id_check2    (GEM_DEVICE *mac, u32 id_check);
+void gem_set_id_check3    (GEM_DEVICE *mac, u32 id_check);
+void gem_set_id_check4    (GEM_DEVICE *mac, u32 id_check);
+u32 gem_get_id_check1     (GEM_DEVICE *mac);
+u32 gem_get_id_check2     (GEM_DEVICE *mac);
+u32 gem_get_id_check3     (GEM_DEVICE *mac);
+u32 gem_get_id_check4     (GEM_DEVICE *mac);
+#endif
+
+void gem_enable_len_check (GEM_DEVICE *mac);
+void gem_disable_len_check(GEM_DEVICE *mac);
+
+/* Interrupt handling and masking. */
+void gem_set_irq_stat     (GEM_DEVICE *mac, u32 irq_status);
+u32 gem_get_irq_stat      (GEM_DEVICE *mac);
+
+void gem_enable_irq       (GEM_DEVICE *mac, u32 irq_en);
+void gem_mask_irq         (GEM_DEVICE *mac, u32 irq_mask);
+u32 gem_get_irq_mask      (GEM_DEVICE *mac);
+
+/* Transmit control. */
+int gem_enable_tx         (GEM_DEVICE *mac);
+void gem_disable_tx       (GEM_DEVICE *mac);
+int gem_start_tx          (GEM_DEVICE *mac);
+void gem_stop_tx          (GEM_DEVICE *mac) ;
+void gem_abort_tx         (GEM_DEVICE *mac);
+int gem_restart_tx        (GEM_DEVICE *mac);
+int gem_transmitting      (GEM_DEVICE *mac);
+u32 gem_get_tx_stat       (GEM_DEVICE *mac);
+void gem_reset_tx_stat    (GEM_DEVICE *mac, u32 rst_status);
+void gem_reset_tx_q       (GEM_DEVICE *mac);
+
+/* Receive control. */
+int gem_enable_rx         (GEM_DEVICE *mac);
+void gem_disable_rx       (GEM_DEVICE *mac);
+//#if defined(CONFIG_ARCH_M83XXX)
+int gem_set_rx_offset     (GEM_DEVICE *mac, unsigned short offset);
+//#endif
+void gem_reset_rx_q       (GEM_DEVICE *mac);
+
+int gem_receive_on        (GEM_DEVICE *mac);
+u32 gem_get_rx_stat       (GEM_DEVICE *mac);
+void gem_reset_rx_stat    (GEM_DEVICE *mac, u32 rst_status);
+
+void gem_enable_rx_jmb    (GEM_DEVICE *mac);
+void gem_disable_rx_jmb   (GEM_DEVICE *mac);
+void gem_enable_rx_jmb_ex (GEM_DEVICE *mac, uint mtu_size);
+void gem_disable_rx_jmb_ex(GEM_DEVICE *mac, uint mtu_size);
+
+void gem_enable_vlan_only (GEM_DEVICE *mac);
+void gem_disable_vlan_only(GEM_DEVICE *mac);
+
+/* Snapshot of statistic registers */
+void gem_enable_rd_snap   (GEM_DEVICE *mac);
+void gem_disable_rd_snap  (GEM_DEVICE *mac);
+void gem_take_snap        (GEM_DEVICE *mac);
+
+/* Debug options. */
+void gem_stats_wr_en      (GEM_DEVICE *mac);
+void gem_stats_wr_off     (GEM_DEVICE *mac);
+void gem_stats_inc        (GEM_DEVICE *mac);
+void gem_stats_clr        (GEM_DEVICE *mac);
+
+void gem_enable_bp        (GEM_DEVICE *mac);
+void gem_disable_bp       (GEM_DEVICE *mac);
+
+void gem_en_retry_test    (GEM_DEVICE *mac);
+void gem_dis_retry_test   (GEM_DEVICE *mac);
+
+GEM_STATS gem_get_stats   (GEM_DEVICE *mac);
+void gem_set_stats        (GEM_DEVICE *mac, GEM_STATS *stats);
+
+/** Generic register access interface. */
+u32 gem_register_rd(GEM_DEVICE *mac, u32 reg_addr);
+
+/* Below are operations related to TSU/1588 */
+s32 gem_get_tsu_sec(GEM_DEVICE *mac);
+
+u32 gem_get_tsu_nsec(GEM_DEVICE *mac);
+
+void gem_set_tsu_sec(GEM_DEVICE *mac, u32 seconds); 
+
+void gem_set_tsu_nsec(GEM_DEVICE *mac, u32 ns);
+
+s32 gem_get_tsu_ptp_tx_sec(GEM_DEVICE *mac);
+
+s32 gem_get_tsu_ptp_tx_nsec(GEM_DEVICE *mac);
+
+s32 gem_get_tsu_ptp_peer_tx_sec(GEM_DEVICE *mac);
+
+s32 gem_get_tsu_ptp_peer_tx_nsec(GEM_DEVICE *mac);
+
+void gem_set_register (GEM_DEVICE *mac, u32 offset, u32 value) ;
+
+u32 gem_get_register (GEM_DEVICE *mac,  u32 offset) ;
+
+
+/******************************************************************************/
+
+
+#endif /* __GEMAC_H__ */
diff --git a/drivers/net/transcede/transcede_mii.c b/drivers/net/transcede/transcede_mii.c
new file mode 100644
index 0000000..8505646
--- /dev/null
+++ b/drivers/net/transcede/transcede_mii.c
@@ -0,0 +1,655 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+  *
+  * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+  *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+  *
+  * You should have received a copy of the GNU General Public License
+  * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+  */
+
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/string.h>
+#include <linux/errno.h>
+#include <linux/unistd.h>
+#include <linux/slab.h>
+#include <linux/interrupt.h>
+#include <linux/init.h>
+#include <linux/delay.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/skbuff.h>
+#include <linux/spinlock.h>
+#include <linux/mm.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/crc32.h>
+#include <linux/mii.h>
+#include <linux/phy.h>
+#include <linux/gpio.h>
+
+#include <asm/io.h>
+#include <asm/irq.h>
+#include <asm/uaccess.h>
+
+#include "transcede_gemac.h"
+#include "transcede_mii.h"
+
+#if defined(CONFIG_MACH_M822XX)
+/** \brief Define to compile in MDIO bit bang support for transcede (transcede chip bug workaround) */
+#define CONFIG_ENABLE_MDIO_BIT_BANG
+#endif
+
+#ifdef CONFIG_ENABLE_MDIO_BIT_BANG
+
+/* Include GPIO macros for MDIO bit bang via GPIO */
+/* Added to support MDIO bitbang instead of GEM MDIO for T2200/T3300 Revision A1 chip bug */
+#include <mach/gpio.h>
+#include <linux/mdio-bitbang.h>
+
+/* Map defines from working/ported diagnostic code to Linux defines for GPIO */
+
+/** \brief GPIO output enable register address */
+#define GPIO_OE_REG                     TRANSCEDE_GPIO_OE_REG
+
+/** \brief GPIO output register */
+#define GPIO_OUTPUT_REG                 TRANSCEDE_GPIO_OUTPUT_REG
+
+/** \brief GPIO input register */
+#define GPIO_INPUT_REG                  TRANSCEDE_GPIO_INPUT_REG
+
+/** \brief GPIO pin select register to selecting GPIO versus other multiplexed functions for pins */
+#define GPIO_31_16_PIN_SELECT_REG       TRANSCEDE_GPIO_PIN_SELECT1
+
+//
+// T2200/T3300 MDIO bit bang defines (mapping/use of GPIO for MDIO/MDC operations
+// to issue read/write commands to Ethernet MDIO devices using two GPIO
+// pins and Linux kernel mdio bit bang data
+//
+/** \brief mapping for MDIO Data (MDIO) GPIO for T2200 EVM board, also mapped to debug LED 1 (0 for on) */
+#define MDIO_GPIO             20
+
+/** \brief mapping for MDIO Clock (MDC) GPIO for T2200 EVM board */
+#define MDC_GPIO              21
+
+#define MDIO_GPIO_SELECT_MASK (1 << ((MDIO_GPIO - 16) * 2))
+#define MDIO_GPIO_SELECT_REG  GPIO_31_16_PIN_SELECT_REG
+
+#define MDC_GPIO_SELECT_MASK  (1 << ((MDC_GPIO - 16) * 2))
+#define MDC_GPIO_SELECT_REG   GPIO_31_16_PIN_SELECT_REG
+
+#define MDIO_GPIO_BIT         (1<<MDIO_GPIO)
+#define MDC_GPIO_BIT          (1<<MDC_GPIO)
+
+/**
+ * \brief Transcede MDIO global variable for whether running MDIO bit bang
+ * workaround (TRUE) or not (FALSE)
+ *
+ */
+int transcede_use_mdio_bit_bang = 0; // Assume not using Bit bang unless enabled and GEM MDIO fails
+
+//
+// Transcede GPIO to bit bang I/O routines
+//
+/* Set the Management Data Clock high if level is one,
+ * low if level is zero.
+ */
+void SetMdc(struct mdiobb_ctrl *ctrl, int level)
+{
+	if (level)
+	{
+		gpio_set_value(MDC_GPIO,1);
+	}
+	else
+	{
+		gpio_set_value(MDC_GPIO,0);
+	}
+}
+
+/**
+ * \brief Configure the Management Data I/O pin as an input if
+ * "output" is zero, or an output if "output" is one.
+ */
+void SetMdioDir(struct mdiobb_ctrl *ctrl, int output)
+{
+	//
+	// Set MDIO GPIO as output
+	// Use this opportunity to also make sure clock (MDC) is properly set as output
+	//
+	if (output)
+	{
+		gpio_direction_output(MDIO_GPIO,1);
+	}
+	else
+	{
+		gpio_direction_input(MDIO_GPIO);
+	}
+	gpio_direction_output(MDC_GPIO,1);
+
+#ifdef MDIO_GPIO_SELECT_MASK
+	//
+	// If MDIO GPIO is multiplexed, also setup as GPIO instead of alternate function
+	//
+	REG32(MDIO_GPIO_SELECT_REG) |= MDIO_GPIO_SELECT_MASK;
+#endif
+
+#ifdef MDC_GPIO_SELECT_MASK
+	//
+	// If MDIO GPIO is multiplexed, also setup as GPIO instead of alternate function
+	//
+	REG32(MDC_GPIO_SELECT_REG) |= MDC_GPIO_SELECT_MASK;
+#endif
+}
+
+/**
+ * \brief Set the Management Data I/O pin high if value is one,
+ * low if "value" is zero.  This may only be called
+ * when the MDIO pin is configured as an output.
+ */
+static void SetMdioData(struct mdiobb_ctrl *ctrl, int value)
+{
+	if (value)
+	{
+		gpio_set_value(MDIO_GPIO, 1);
+	}
+	else
+	{
+		gpio_set_value(MDIO_GPIO, 0);
+	}
+}
+
+/** \brief Retrieve the state of the Management Data I/O pin. */
+static int GetMdioData(struct mdiobb_ctrl *ctrl)
+{
+    if (REG32(GPIO_INPUT_REG) & MDIO_GPIO_BIT)
+    {
+        return 1;
+    }
+    else
+    {
+        return 0;
+    }
+}
+
+/** \brief MDIO bit bang operations mapping table */
+static struct mdiobb_ops MdioBitBangBusOpsData =
+{
+    .owner = THIS_MODULE, // struct module *owner;
+
+    /* Set the Management Data Clock high if level is one,
+     * low if level is zero.
+     */
+    .set_mdc = SetMdc, //void (*set_mdc)(struct mdiobb_ctrl *ctrl, int level);
+
+    /* Configure the Management Data I/O pin as an input if
+     * "output" is zero, or an output if "output" is one.
+     */
+    .set_mdio_dir = SetMdioDir, //void (*set_mdio_dir)(struct mdiobb_ctrl *ctrl, int output);
+
+    /* Set the Management Data I/O pin high if value is one,
+     * low if "value" is zero.  This may only be called
+     * when the MDIO pin is configured as an output.
+     */
+    .set_mdio_data = SetMdioData, // void (*set_mdio_data)(struct mdiobb_ctrl *ctrl, int value);
+
+    /* Retrieve the state Management Data I/O pin. */
+    .get_mdio_data = GetMdioData  // int (*get_mdio_data)(struct mdiobb_ctrl *ctrl);
+};
+
+static struct mdiobb_ctrl MdioBitBangBusOpsPointer =
+{
+	&MdioBitBangBusOpsData
+};
+
+/** \brief MDIO bus release function */
+static int t2200_mdiobb_release(struct device *mdev)
+{
+	struct mii_bus *bus = dev_get_drvdata(mdev);
+
+	/* unregister mdio bus */
+	mdiobus_unregister(bus);
+
+	/* remove mdio bus info from net_device */
+	dev_set_drvdata(mdev, NULL);
+
+	/* free interrupts memory */
+	kfree(bus->irq);
+
+	/* free bitbang info */
+	free_mdio_bitbang(bus);
+	gpio_free(MDIO_GPIO);
+	gpio_free(MDC_GPIO);
+
+	return 0;
+}
+
+/** \brief MDIO bus init function */
+static int t2200_mdiobb_init(struct device *mdev,
+                             int id)
+{
+	int                   ret;
+	int                   i;
+	struct mii_bus *      bus;
+	if (gpio_request(MDIO_GPIO, "gpio-mdio")) {
+		printk(KERN_ERR "error: failed to request MDIO GPIO\n");
+	}
+	if (gpio_request(MDC_GPIO, "gpio-mdc")) {
+		printk(KERN_ERR "error: failed to request MDC GPIO\n");
+	}
+	/* MII controller setting */
+	bus = alloc_mdio_bitbang(&MdioBitBangBusOpsPointer);
+	if (!bus)
+	{
+		printk(KERN_ERR "%s: Failed alloc MDIO bit bang bus ID:%d\n", __func__, id);
+		ret = -ENOMEM;
+		goto out_free_bitbang;
+	}
+
+	/* Set up MDIO Bus name and ID number */
+	bus->name   = "Transcede_mdiobb";
+	snprintf(bus->id, MII_BUS_ID_SIZE, "%x", id);
+
+	/* PHY IRQ (currently IRQ not used or supported, set table to poll mode */
+	bus->irq = kmalloc(sizeof(int)*PHY_MAX_ADDR, GFP_KERNEL);
+	if (!bus->irq)
+	{
+		printk(KERN_ERR "%s: Failed kmalloc MDIO bit bang bus ID:%d\n", __func__, id);
+		ret = -ENOMEM;
+		goto out_free_bus;
+	}
+
+	for (i = 0; i < PHY_MAX_ADDR; i++)
+	{
+		bus->irq[i] = PHY_POLL;
+	}
+
+	/* register mdio bus, continue if OK, exit if error */
+	ret = mdiobus_register(bus);
+	if (ret)
+	{
+		printk(KERN_ERR "%s: Failed MDIO bit bus register ID:%d\n", __func__, id);
+		goto out_free_irq;
+	}
+
+	dev_set_drvdata(mdev, bus);
+
+	return 0;
+
+out_free_irq:
+	kfree(bus->irq);
+
+out_free_bus:
+	free_mdio_bitbang(bus);
+
+out_free_bitbang:
+	return ret;
+}
+
+#endif //#ifdef CONFIG_ENABLE_MDIO_BIT_BANG
+
+/** \brief Function to set MDC clock divisor based on value passed */
+static void transcede_gem_set_mdc_div(struct transcede_mii *mii, int mdc_div)
+{
+	u32 val = readl(mii->baseaddr + GEM_NET_CONFIG) & ~GEM_MDC_DIV_MASK;
+	u32 div;
+
+	switch (mdc_div) {
+	case 8:
+		div = 0;
+		break;
+
+	case 16:
+		div = 1;
+		break;
+
+	case 32:
+		div = 2;
+		break;
+
+	case 48:
+		div = 3;
+		break;
+
+	default:
+	case 64:
+		div = 4;
+		break;
+
+	case 96:
+		div = 5;
+		break;
+
+	case 128:
+		div = 6;
+		break;
+
+	case 224:
+		div = 7;
+		break;
+	}
+
+	val |= div << 18;
+
+	__raw_writel(val, mii->baseaddr + GEM_NET_CONFIG);
+}
+
+/** \brief Function to do an MDIO write to a specified bus, PHY ID on the bus, register in the PHY, value for the register */
+static int transcede_mdio_write(struct mii_bus *bus, int mii_id, int regnum, u16 value)
+{
+	struct transcede_mii * mii = (struct transcede_mii *)bus->priv;
+	u32                    write_data;
+	u32                    net_control;
+
+	//printk (KERN_INFO "mii write %s %02d %02d 0x%04X\n", bus->id, mii_id, regnum, value);
+	/* Save current value of network control register */
+	net_control = __raw_readl(mii->baseaddr + GEM_NET_CONTROL);
+
+	/* Make sure MDIO enabled for this GEM (note one GEM controls MDIO for both Ethernet ports on Transcede ) */
+	__raw_writel((net_control | GEM_MDIO_EN), mii->baseaddr + GEM_NET_CONTROL);
+
+	/* Setup PHY control regiswter for MDIO write operation based on PHY address to write and data to write */
+	write_data = 0x50020000;
+	write_data |= ((mii_id << 23) | (regnum << 18) | value);
+	__raw_writel(write_data, mii->baseaddr + GEM_PHY_MAN);
+
+	/* Wait for MDIO operation to complete */
+	while(!(__raw_readl(mii->baseaddr + GEM_NET_STATUS) & GEM_PHY_IDLE)) ;
+
+	/* Restore Network control register value */
+	__raw_writel(net_control, mii->baseaddr + GEM_NET_CONTROL);
+
+	return 0;
+}
+
+/** \brief Function to do an MDIO read from a specified bus, PHY ID on the bus, register in the PHY
+ *
+ *  \return value for the register
+ */
+static int transcede_mdio_read(struct mii_bus *bus, int mii_id, int regnum)
+{
+	struct transcede_mii * mii = (struct transcede_mii *)bus->priv;
+	u16                    value;
+	u32                    write_data;
+	u32                    net_control;
+
+	/* Save current value of network control register */
+	net_control = __raw_readl(mii->baseaddr + GEM_NET_CONTROL);
+
+	/* Make sure MDIO enabled for this GEM (note one GEM controls MDIO for both Ethernet ports on Transcede ) */
+	__raw_writel((net_control | GEM_MDIO_EN), mii->baseaddr + GEM_NET_CONTROL);
+
+	/* Setup PHY control regiswter for MDIO read operation based on PHY address to write and data to write */
+	write_data = 0x60020000;
+	write_data |= ((mii_id << 23) | (regnum << 18));
+	__raw_writel(write_data, mii->baseaddr + GEM_PHY_MAN);
+
+	/* Wait for MDIO read operation to complete */
+	while(!(__raw_readl(mii->baseaddr + GEM_NET_STATUS) & GEM_PHY_IDLE)) ;
+
+	/* Get data read from MDIO read operation (note if MDIO failure, or device not present
+	 * should return 0xFFFF assuming MDIO signal is propely pulled high on the on
+	 * the HW design
+	 */
+	value = __raw_readl(mii->baseaddr + GEM_PHY_MAN) & 0xFFFF;
+
+	//printk (KERN_INFO "mii read  %s %02d %02d 0x%04X\n", bus->id, mii_id, regnum, value);
+
+	/* Restore Network control register value */
+	__raw_writel(net_control, mii->baseaddr + GEM_NET_CONTROL);
+
+	return value;
+}
+
+
+/** \brief Function to do an MDIO read from a specified bus, PHY ID on the bus, register in the PHY
+ *
+ *  \return value read from the register
+ */
+static int transcede_mdio_reset(struct mii_bus *bus)
+{
+	struct transcede_mii * mii = (struct transcede_mii *)bus->priv;
+	volatile int           delay_count;
+
+
+	mutex_lock(&bus->mdio_lock);
+
+	// get GEMAC out of reset
+//HL: Needs to be re-worked when platform info is ready
+//	__raw_writel(__raw_readl(TRANSCEDE_BLOCK_RESET_REG) | (GEMAC0_RST << bus->id), TRANSCEDE_BLOCK_RESET_REG);
+	// 20 ops delay
+	delay_count = 20;
+	while(delay_count--);
+
+	/* Setup the MII Mgmt clock speed */
+	transcede_gem_set_mdc_div(mii, mii->mdc_div);
+
+	/* Reset the management interface */
+	__raw_writel(__raw_readl(mii->baseaddr + GEM_NET_CONTROL) | GEM_MDIO_EN, mii->baseaddr + GEM_NET_CONTROL);
+
+	/* Wait until the bus is free */
+	while(!(__raw_readl(mii->baseaddr + GEM_NET_STATUS) & GEM_PHY_IDLE));
+
+	mutex_unlock(&bus->mdio_lock);
+
+	return 0;
+}
+
+/** \brief Probe function to allocate memory and create a Transcede GEM MDIO driver */
+static int transcede_mdio_probe(struct platform_device *pdev)
+{
+	struct device *              dev = &pdev->dev;
+	struct transcede_mdio_data * pdata;
+	struct transcede_mii *       mii;
+	struct mii_bus *             bus;
+	struct resource *            r;
+	int                          rc;
+	int                          val;
+	int                          addr;
+
+	/* Allocate new MDIO bus driver for Linux kernel, pointer to newly
+	 * allocated standard Linux mii_bus structure number returned
+	 * if allocated OK, else returns NULL pointer if failed
+	 */
+	bus = mdiobus_alloc();
+	if (!bus) {
+		printk(KERN_ERR "transcede mdio %d: mdiobus_alloc() failed\n", pdev->id);
+		rc = -ENOMEM;
+		goto err0;
+	}
+
+	/* Allocate memory for Transcede MII structure (includes mii_bus structure memory) */
+	mii = kzalloc(sizeof(struct transcede_mii), GFP_KERNEL);
+	if (!mii) {
+		printk(KERN_ERR "transcede mdio %d: kzalloc() failed\n", pdev->id);
+		rc = -ENOMEM;
+		goto err1;
+	}
+
+	/* Setup MDIO bus driver fields for string and call back functions */
+	bus->name  = "Transcede MDIO Bus";
+	bus->read  = &transcede_mdio_read;
+	bus->write = &transcede_mdio_write;
+	bus->reset = &transcede_mdio_reset;
+
+	/* Use GEM ID number for MDIO bus ID */
+	snprintf(bus->id, MII_BUS_ID_SIZE, "%x", pdev->id);
+
+	bus->priv = mii;
+
+	/* Setup private data pointer to pointer to Transcede specific MDIO structure
+	 * newly allocated data pointer
+	 */
+	pdata = (struct transcede_mdio_data *)pdev->dev.platform_data;
+	if (!pdata) {
+		printk(KERN_ERR "transcede mdio %d: missing platform data\n", pdev->id);
+		rc = -EINVAL;
+		goto err2;
+	}
+
+	/* Get MII bus PHY valid mask data from platform data */
+	bus->phy_mask = pdata->phy_mask;
+
+	/* Get GEMAC MDC divisor (takes internal GEMAC AXI bus speed and divides to get MDC clock rate */
+	mii->mdc_div  = pdata->mdc_div;
+
+	/* Make sure MDC divisor is set to something (value tested OK on MSPD EVMs
+	 * if config data is 0
+	 */
+	if (!mii->mdc_div)
+		mii->mdc_div = 64;
+
+	/* Get pointer to Transcede MDIO platform resource data */
+	r = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!r) {
+		printk(KERN_ERR "transcede mdio %d: missing platform resource\n", pdev->id);
+		rc = -EINVAL;
+		goto err2;
+	}
+
+	/* Set the MDIO base address (GEM base address that is connected to MDIO bus) */
+	mii->baseaddr = (void *)(r->start);
+	if (!mii->baseaddr) {
+		printk(KERN_ERR "transcede mdio %d: ioremap() failed\n", pdev->id);
+		rc = -ENOMEM;
+		goto err2;
+	}
+
+	/* Setup MDIO bus IRQ table for all MDIO PHY devices in polled mode
+	 * (no interrupts)
+	 */
+	bus->irq = pdata->irq;
+	memset(bus->irq, 0, 32 * sizeof(int)); // we don't have specific mdio irqs
+
+	//	bus->dev = dev;    //maybe an issue?
+	/* Store a pointer to Driver state with dev_set_drvdata() */
+	dev_set_drvdata(dev, bus);
+
+	// Make sure MDIO bus speed is setup OK:
+	transcede_mdio_reset(bus);
+
+#ifdef CONFIG_ENABLE_MDIO_BIT_BANG
+	/* Newer revision of T2200 and T3300 now also support GEMAC controller based MDIO
+	 * (revision X1, this did not work).  Before enabling MDIO bit bang option, try using
+	 * GEM MDIO to read some registers (example code below for MSPD boards)
+	 * to see if GEM MDIO is working.  If not, then enable MDIO bit bang instead.
+	 * On Transcede EVMs, PHY IDs used to autodetect are 0 (Atheros Switch), 5 (Atheros
+	 * RGMII PHY) and 6 (Atheros SGMII PHY).
+	 */
+	transcede_use_mdio_bit_bang = 0;
+
+	addr = 0;
+	val = transcede_mdio_read(bus, addr, 3);
+	if ((val == 0x0000) || (val == 0xFFFF))
+	{
+		/* PHY 0 address read failed, try address 5 */
+		addr = 5;
+		val = transcede_mdio_read(bus, addr, 3);
+		if ((val == 0x0000) || (val == 0xFFFF))
+		{
+			/* PHy 5 address read failed, try address 6 */
+			addr = 6;
+			val = transcede_mdio_read(bus, addr, 3);
+			if ((val == 0x0000) || (val == 0xFFFF))
+			{
+				/* PHy 6 address read failed, assume MDIO bit bang bus instead of GEMAC bus */
+				transcede_use_mdio_bit_bang = 1;
+			}
+		}
+	}
+
+	if (transcede_use_mdio_bit_bang == 0)
+	{
+		/* Setup for GEMAC based MDIO instead of Bit bang */
+		printk(KERN_INFO "%s: GEMAC MDIO OK driver %d, PHY 0x%02X, reg 3 = 0x%04X\n", __func__, pdev->id, addr, val);
+	}
+	else
+	{
+		/* Setup for MDIO bit bang instead of GEMAC based MDIO */
+		kfree(mii);
+		kfree(bus);
+		transcede_use_mdio_bit_bang = 1;
+		printk(KERN_INFO "%s: GEMAC MDIO, no devices found, setting up MDIO bit bang driver %d\n", __func__, pdev->id);
+		rc = t2200_mdiobb_init(dev, pdev->id);
+		return rc;
+	}
+#endif
+
+	/* Regiser the MDIO bus driver */
+	rc = mdiobus_register(bus);
+	if (rc) {
+		printk (KERN_ERR "%s: Cannot register as MDIO bus\n", bus->name);
+		goto err2;
+	}
+
+	return 0;
+
+err2:
+	kfree(mii);
+
+err1:
+	kfree(bus);
+
+err0:
+	return rc;
+}
+
+/** /brief Transcede MDIO driver remove function */
+static int transcede_mdio_remove(struct platform_device *pdev)
+{
+	struct device  * dev = &pdev->dev;
+	struct mii_bus * bus = dev_get_drvdata(dev);
+
+#ifdef CONFIG_ENABLE_MDIO_BIT_BANG
+	/* Test if running MDIO bit bang (for workaround for First revision T2200/T3300 chips) */
+	if (transcede_use_mdio_bit_bang)
+	{
+            return(t2200_mdiobb_release(dev));
+	}
+#endif
+
+	mdiobus_unregister(bus);
+
+	dev_set_drvdata(dev, NULL);
+
+	kfree(bus->priv);
+	kfree(bus);
+
+	return 0;
+}
+
+/** \brief Platform data structure for a Transcede MDIO device driver */
+static struct platform_driver transcede_mdio_driver = {
+	.probe = transcede_mdio_probe,
+	.remove = transcede_mdio_remove,
+	.driver	= {
+		.name = "transcede-mdio",
+	},
+};
+
+/** \brief Transcede MDIO driver initialization: platform driver register function */
+int __init transcede_mdio_init(void)
+{
+	return platform_driver_register(&transcede_mdio_driver);
+}
+
+/** \brief Transcede MDIO driver exit: platform driver unregister function */
+void  __exit transcede_mdio_exit(void)
+{
+	platform_driver_unregister(&transcede_mdio_driver);
+}
+
+module_init(transcede_mdio_init);
+module_exit(transcede_mdio_exit);
diff --git a/drivers/net/transcede/transcede_mii.h b/drivers/net/transcede/transcede_mii.h
new file mode 100644
index 0000000..fa14ab4
--- /dev/null
+++ b/drivers/net/transcede/transcede_mii.h
@@ -0,0 +1,40 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+  *
+  * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+  *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+  *
+  * You should have received a copy of the GNU General Public License
+  * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+  */
+
+#ifndef __TRANSCEDE_MII_H
+#define __TRANSCEDE_MII_H
+
+#define TRANSCEDE_SUPPORTED (SUPPORTED_10baseT_Half \
+		| SUPPORTED_10baseT_Full \
+		| SUPPORTED_100baseT_Half \
+		| SUPPORTED_100baseT_Full \
+		| SUPPORTED_Autoneg \
+		| SUPPORTED_MII)
+
+struct transcede_mii {
+	void *baseaddr;
+	int mdc_div;
+};
+
+int __init transcede_mdio_init(void);
+void __exit transcede_mdio_exit(void);
+#endif
diff --git a/drivers/net/transcede/userio.c b/drivers/net/transcede/userio.c
new file mode 100644
index 0000000..94a8773
--- /dev/null
+++ b/drivers/net/transcede/userio.c
@@ -0,0 +1,124 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef __KERNEL__
+
+#include <sys/types.h>
+#include <sys/wait.h>
+#include <stdio.h>
+#include <unistd.h>
+#include <stdlib.h>
+#include <limits.h>
+#include <limits.h>
+#include <unistd.h>
+#include <stdio.h>
+#include <sys/mman.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include "userio.h"
+
+
+struct region {
+        unsigned base;
+        unsigned size;
+        int	fd;
+        int	ref_cnt;
+        void*	iomem;
+};
+
+static struct region maps[MEMID_MAX] = {
+        { DDR0_BASE, DDR0_SIZE, 0, 0, NULL},
+        { DDR1_BASE, DDR1_SIZE, 0, 0, NULL},
+        { CRAM_BASE, CRAM_SIZE, 0, 0, NULL},
+        { IRAM_BASE, IRAM_SIZE, 0, 0, NULL},
+        { JRAM_BASE, JRAM_SIZE, 0, 0, NULL},
+        { TIME_BASE, TIME_SIZE, 0, 0, NULL},
+        { SEMA_BASE, SEMA_SIZE, 0, 0, NULL},
+        { INTC_BASE, INTC_SIZE, 0, 0, NULL},
+};
+
+/************************************************************************/
+static int request_uio(unsigned addr, unsigned len)
+{
+        int id;
+        struct region* map = maps;
+
+        for (id = 0; id < MEMID_MAX; id++, map++) {
+                if (addr >= map->base && (addr + len) < (map->base + map->size)) {
+                        return id;
+                }
+        }
+        return -1;
+}
+
+/************************************************************************/
+void* map_ram(unsigned addr, unsigned len)
+{
+        char dev_name[20];
+        struct region* map;
+
+        int uio_id = request_uio(addr, len);
+        if (uio_id < 0 ) {
+                printf("bad uio_id\n");
+                return NULL;
+        }
+        map = &maps[uio_id];
+        if ( !map->fd ) {
+                sprintf(dev_name, "/dev/uio%d", uio_id);
+                map->fd = open(dev_name, O_RDWR);
+                if ( map->fd <= 0 ) {
+                        return 0;
+                }
+                map->iomem = mmap(0, map->size,
+                                  PROT_READ | PROT_WRITE,
+                                  MAP_SHARED, map->fd, 0);
+                if (map->iomem <= NULL) {
+                        close ( map->fd );
+                        return NULL;
+                }
+        }
+        map->ref_cnt++;
+        return (void*)( ((int)map->iomem) + (addr - map->base));
+}
+
+/************************************************************************/
+void unmap_ram(void* ptr)
+{
+        int uio_id = request_uio((unsigned)ptr, 0);
+        if (uio_id >= 0) {
+                struct region* map = &maps[uio_id];
+                map->ref_cnt--;
+                if ( !map->ref_cnt ) {
+                        if ( ptr ) {
+                                munmap(map->iomem, map->size);
+                                map->iomem = NULL;
+                        }
+
+                        if ( map->fd ) {
+                                close( map->fd );
+                                map->fd = 0;
+                        }
+                }
+        }
+}
+
+#endif /* __KERNEL__ */
diff --git a/drivers/net/transcede/userio.h b/drivers/net/transcede/userio.h
new file mode 100644
index 0000000..815681e
--- /dev/null
+++ b/drivers/net/transcede/userio.h
@@ -0,0 +1,75 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+
+#ifndef _T4K_USER_IO_
+#define _T4K_USER_IO_
+
+#define MEMID_DDR0    0 /* DDR0 is on region 0 in /dev/uio0 */
+#define MEMID_DDR1    1 /* DDR1 is on region 0 in /dev/uio1 */
+#define MEMID_CRAM    2 /* CRAM is on region 0 in /dev/uio2 */
+#define MEMID_IRAM    3 /* IRAM is on region 0 in /dev/uio3 */
+#define MEMID_JRAM    4 /* DDR1 is on region 0 in /dev/uio4 */
+#define MEMID_TIMER   5 /* XP Timer block for profiling	        in /dev/uio5 */
+#define MEMID_SEMA    6 /* H/W Semaphores for Inter-processor   in /dev/uio6 */
+#define MEMID_INTC    7 /* SCU block (don't use w/o permission) in /dev/uio7 */
+#define MEMID_MAX     8
+
+/*
+ * User I/O map
+ */
+#define DDR0_BASE      0x00000000	/* SDRAM (512M) */
+#define DDR0_SIZE      0x20000000
+#define DDR1_BASE      0x20000000	/* SDRAM (512M) */
+#define DDR1_SIZE      0x20000000
+#define JRAM_BASE      0xFA000000	/* JRAM 16K     */
+#define JRAM_SIZE      0x00004000
+#define IRAM_BASE      0xFB000000	/* IRAM 256K    */
+#define IRAM_SIZE      0x00020000
+#define CRAM_BASE      0xFC000000	/* CRAM 3M      */
+#define CRAM_SIZE      0x00300000
+#define TIME_BASE      0xFE050000	/* XP Timer 	*/
+#define TIME_SIZE      0x00010000
+#define SEMA_BASE      0xFBF00000	/* H/W semaphores */
+#define SEMA_SIZE      0x00010000
+#define INTC_BASE      0xFE430000	/* SCU for GIC access */
+#define INTC_SIZE      0x00010000
+
+#ifndef __KERNEL__
+void* map_ram(unsigned addr, unsigned len);
+void unmap_ram(void* addr);
+#else
+#define map_ram 	ioremap_nocache
+#define unmap_ram 	iounmap
+#endif
+
+#define map_dram(addr, size)       map_ram(DDR0_BASE+(addr), size)
+#define map_dram1(addr, size)      map_ram(DDR1_BASE+(addr), size)
+#define map_jram(addr, size)       map_ram(JRAM_BASE+(addr), size)
+#define map_iram(addr, size)       map_ram(IRAM_BASE+(addr), size)
+#define map_cram(addr, size)       map_ram(CRAM_BASE+(addr), size)
+#define unmap_dram(addr, size)     unmap_ram(addr)
+#define unmap_dram1(addr, size)    unmap_ram(addr)
+#define unmap_jram(addr, size)     unmap_ram(addr)
+#define unmap_iram(addr, size)     unmap_ram(addr)
+#define unmap_cram(addr, size)     unmap_ram(addr)
+
+#endif // _T4K_USER_IO_
diff --git a/drivers/net/usb/asix.c b/drivers/net/usb/asix.c
index e2a988c..6729585 100644
--- a/drivers/net/usb/asix.c
+++ b/drivers/net/usb/asix.c
@@ -35,9 +35,10 @@
 #include <linux/crc32.h>
 #include <linux/usb/usbnet.h>
 #include <linux/slab.h>
+#include <linux/if_vlan.h>
 
-#define DRIVER_VERSION "14-Jun-2006"
-static const char driver_name [] = "asix";
+#define DRIVER_VERSION "08-Nov-2011"
+#define DRIVER_NAME "asix"
 
 /* ASIX AX8817X based USB 2.0 Ethernet Devices */
 
@@ -115,28 +116,27 @@ static const char driver_name [] = "asix";
 #define AX88178_MEDIUM_DEFAULT	\
 	(AX_MEDIUM_PS | AX_MEDIUM_FD | AX_MEDIUM_AC | \
 	 AX_MEDIUM_RFC | AX_MEDIUM_TFC | AX_MEDIUM_JFE | \
-	 AX_MEDIUM_RE )
+	 AX_MEDIUM_RE)
 
 #define AX88772_MEDIUM_DEFAULT	\
 	(AX_MEDIUM_FD | AX_MEDIUM_RFC | \
 	 AX_MEDIUM_TFC | AX_MEDIUM_PS | \
-	 AX_MEDIUM_AC | AX_MEDIUM_RE )
+	 AX_MEDIUM_AC | AX_MEDIUM_RE)
 
 /* AX88772 & AX88178 RX_CTL values */
-#define AX_RX_CTL_SO			0x0080
-#define AX_RX_CTL_AP			0x0020
-#define AX_RX_CTL_AM			0x0010
-#define AX_RX_CTL_AB			0x0008
-#define AX_RX_CTL_SEP			0x0004
-#define AX_RX_CTL_AMALL			0x0002
-#define AX_RX_CTL_PRO			0x0001
-#define AX_RX_CTL_MFB_2048		0x0000
-#define AX_RX_CTL_MFB_4096		0x0100
-#define AX_RX_CTL_MFB_8192		0x0200
-#define AX_RX_CTL_MFB_16384		0x0300
-
-#define AX_DEFAULT_RX_CTL	\
-	(AX_RX_CTL_SO | AX_RX_CTL_AB )
+#define AX_RX_CTL_SO		0x0080
+#define AX_RX_CTL_AP		0x0020
+#define AX_RX_CTL_AM		0x0010
+#define AX_RX_CTL_AB		0x0008
+#define AX_RX_CTL_SEP		0x0004
+#define AX_RX_CTL_AMALL		0x0002
+#define AX_RX_CTL_PRO		0x0001
+#define AX_RX_CTL_MFB_2048	0x0000
+#define AX_RX_CTL_MFB_4096	0x0100
+#define AX_RX_CTL_MFB_8192	0x0200
+#define AX_RX_CTL_MFB_16384	0x0300
+
+#define AX_DEFAULT_RX_CTL	(AX_RX_CTL_SO | AX_RX_CTL_AB)
 
 /* GPIO 0 .. 2 toggles */
 #define AX_GPIO_GPO0EN		0x01	/* GPIO0 Output enable */
@@ -164,6 +164,8 @@ static const char driver_name [] = "asix";
 #define MARVELL_CTRL_TXDELAY	0x0002
 #define MARVELL_CTRL_RXDELAY	0x0080
 
+#define	PHY_MODE_RTL8211CL	0x000C
+
 /* This structure cannot exceed sizeof(unsigned long [5]) AKA 20 bytes */
 struct asix_data {
 	u8 multi_filter[AX_MCAST_FILTER_SIZE];
@@ -268,12 +270,15 @@ asix_write_cmd_async(struct usbnet *dev, u8 cmd, u16 value, u16 index,
 
 	netdev_dbg(dev->net, "asix_write_cmd_async() cmd=0x%02x value=0x%04x index=0x%04x size=%d\n",
 		   cmd, value, index, size);
-	if ((urb = usb_alloc_urb(0, GFP_ATOMIC)) == NULL) {
+
+	urb = usb_alloc_urb(0, GFP_ATOMIC);
+	if (!urb) {
 		netdev_err(dev->net, "Error allocating URB in write_cmd_async!\n");
 		return;
 	}
 
-	if ((req = kmalloc(sizeof(struct usb_ctrlrequest), GFP_ATOMIC)) == NULL) {
+	req = kmalloc(sizeof(struct usb_ctrlrequest), GFP_ATOMIC);
+	if (!req) {
 		netdev_err(dev->net, "Failed to allocate memory for control request\n");
 		usb_free_urb(urb);
 		return;
@@ -290,7 +295,8 @@ asix_write_cmd_async(struct usbnet *dev, u8 cmd, u16 value, u16 index,
 			     (void *)req, data, size,
 			     asix_async_cmd_callback, req);
 
-	if((status = usb_submit_urb(urb, GFP_ATOMIC)) < 0) {
+	status = usb_submit_urb(urb, GFP_ATOMIC);
+	if (status < 0) {
 		netdev_err(dev->net, "Error submitting the control message: status=%d\n",
 			   status);
 		kfree(req);
@@ -343,7 +349,7 @@ static int asix_rx_fixup(struct usbnet *dev, struct sk_buff *skb)
 			return 2;
 		}
 
-		if (size > dev->net->mtu + ETH_HLEN) {
+		if (size > dev->net->mtu + ETH_HLEN + VLAN_HLEN) {
 			netdev_err(dev->net, "asix_rx_fixup() Bad RX Length %d\n",
 				   size);
 			return 0;
@@ -531,11 +537,11 @@ static u16 asix_read_medium_status(struct usbnet *dev)
 	if (ret < 0) {
 		netdev_err(dev->net, "Error reading Medium Status register: %02x\n",
 			   ret);
-		goto out;
+		return ret;	/* TODO: callers not checking for error ret */
 	}
-	ret = le16_to_cpu(v);
-out:
-	return ret;
+
+	return le16_to_cpu(v);
+
 }
 
 static int asix_write_medium_mode(struct usbnet *dev, u16 mode)
@@ -647,9 +653,17 @@ static u32 asix_get_phyid(struct usbnet *dev)
 {
 	int phy_reg;
 	u32 phy_id;
+	int i;
 
-	phy_reg = asix_mdio_read(dev->net, dev->mii.phy_id, MII_PHYSID1);
-	if (phy_reg < 0)
+	/* Poll for the rare case the FW or phy isn't ready yet.  */
+	for (i = 0; i < 100; i++) {
+		phy_reg = asix_mdio_read(dev->net, dev->mii.phy_id, MII_PHYSID1);
+		if (phy_reg != 0 && phy_reg != 0xFFFF)
+			break;
+		mdelay(1);
+	}
+
+	if (phy_reg <= 0 || phy_reg == 0xFFFF)
 		return 0;
 
 	phy_id = (phy_reg & 0xffff) << 16;
@@ -676,12 +690,6 @@ asix_get_wol(struct net_device *net, struct ethtool_wolinfo *wolinfo)
 	}
 	wolinfo->supported = WAKE_PHY | WAKE_MAGIC;
 	wolinfo->wolopts = 0;
-	if (opt & AX_MONITOR_MODE) {
-		if (opt & AX_MONITOR_LINK)
-			wolinfo->wolopts |= WAKE_PHY;
-		if (opt & AX_MONITOR_MAGIC)
-			wolinfo->wolopts |= WAKE_MAGIC;
-	}
 }
 
 static int
@@ -694,8 +702,6 @@ asix_set_wol(struct net_device *net, struct ethtool_wolinfo *wolinfo)
 		opt |= AX_MONITOR_LINK;
 	if (wolinfo->wolopts & WAKE_MAGIC)
 		opt |= AX_MONITOR_MAGIC;
-	if (opt != 0)
-		opt |= AX_MONITOR_MODE;
 
 	if (asix_write_cmd(dev, AX_CMD_WRITE_MONITOR_MODE,
 			      opt, 0, 0, NULL) < 0)
@@ -744,7 +750,7 @@ static void asix_get_drvinfo (struct net_device *net,
 
 	/* Inherit standard device info */
 	usbnet_get_drvinfo(net, info);
-	strncpy (info->driver, driver_name, sizeof info->driver);
+	strncpy (info->driver, DRIVER_NAME, sizeof info->driver);
 	strncpy (info->version, DRIVER_VERSION, sizeof info->version);
 	info->eedump_len = data->eeprom_len;
 }
@@ -872,7 +878,7 @@ static const struct net_device_ops ax88172_netdev_ops = {
 	.ndo_set_mac_address 	= eth_mac_addr,
 	.ndo_validate_addr	= eth_validate_addr,
 	.ndo_do_ioctl		= asix_ioctl,
-	.ndo_set_multicast_list = ax88172_set_multicast,
+	.ndo_set_rx_mode	= ax88172_set_multicast,
 };
 
 static int ax88172_bind(struct usbnet *dev, struct usb_interface *intf)
@@ -889,19 +895,20 @@ static int ax88172_bind(struct usbnet *dev, struct usb_interface *intf)
 
 	/* Toggle the GPIOs in a manufacturer/model specific way */
 	for (i = 2; i >= 0; i--) {
-		if ((ret = asix_write_cmd(dev, AX_CMD_WRITE_GPIOS,
-					(gpio_bits >> (i * 8)) & 0xff, 0, 0,
-					NULL)) < 0)
+		ret = asix_write_cmd(dev, AX_CMD_WRITE_GPIOS,
+				(gpio_bits >> (i * 8)) & 0xff, 0, 0, NULL);
+		if (ret < 0)
 			goto out;
 		msleep(5);
 	}
 
-	if ((ret = asix_write_rx_ctl(dev, 0x80)) < 0)
+	ret = asix_write_rx_ctl(dev, 0x80);
+	if (ret < 0)
 		goto out;
 
 	/* Get the MAC address */
-	if ((ret = asix_read_cmd(dev, AX88172_CMD_READ_NODE_ID,
-				0, 0, ETH_ALEN, buf)) < 0) {
+	ret = asix_read_cmd(dev, AX88172_CMD_READ_NODE_ID, 0, 0, ETH_ALEN, buf);
+	if (ret < 0) {
 		dbg("read AX_CMD_READ_NODE_ID failed: %d", ret);
 		goto out;
 	}
@@ -966,117 +973,96 @@ static int ax88772_link_reset(struct usbnet *dev)
 	return 0;
 }
 
-static const struct net_device_ops ax88772_netdev_ops = {
-	.ndo_open		= usbnet_open,
-	.ndo_stop		= usbnet_stop,
-	.ndo_start_xmit		= usbnet_start_xmit,
-	.ndo_tx_timeout		= usbnet_tx_timeout,
-	.ndo_change_mtu		= usbnet_change_mtu,
-	.ndo_set_mac_address 	= asix_set_mac_address,
-	.ndo_validate_addr	= eth_validate_addr,
-	.ndo_do_ioctl		= asix_ioctl,
-	.ndo_set_multicast_list = asix_set_multicast,
-};
-
-static int ax88772_bind(struct usbnet *dev, struct usb_interface *intf)
+static int ax88772_reset(struct usbnet *dev)
 {
+	struct asix_data *data = (struct asix_data *)&dev->data;
 	int ret, embd_phy;
 	u16 rx_ctl;
-	struct asix_data *data = (struct asix_data *)&dev->data;
-	u8 buf[ETH_ALEN];
-	u32 phyid;
-
-	data->eeprom_len = AX88772_EEPROM_LEN;
 
-	usbnet_get_endpoints(dev,intf);
-
-	if ((ret = asix_write_gpio(dev,
-			AX_GPIO_RSE | AX_GPIO_GPO_2 | AX_GPIO_GPO2EN, 5)) < 0)
+	ret = asix_write_gpio(dev,
+			AX_GPIO_RSE | AX_GPIO_GPO_2 | AX_GPIO_GPO2EN, 5);
+	if (ret < 0)
 		goto out;
 
-	/* 0x10 is the phy id of the embedded 10/100 ethernet phy */
 	embd_phy = ((asix_get_phy_addr(dev) & 0x1f) == 0x10 ? 1 : 0);
-	if ((ret = asix_write_cmd(dev, AX_CMD_SW_PHY_SELECT,
-				embd_phy, 0, 0, NULL)) < 0) {
+
+	ret = asix_write_cmd(dev, AX_CMD_SW_PHY_SELECT, embd_phy, 0, 0, NULL);
+	if (ret < 0) {
 		dbg("Select PHY #1 failed: %d", ret);
 		goto out;
 	}
 
-	if ((ret = asix_sw_reset(dev, AX_SWRESET_IPPD | AX_SWRESET_PRL)) < 0)
+	ret = asix_sw_reset(dev, AX_SWRESET_IPPD | AX_SWRESET_PRL);
+	if (ret < 0)
 		goto out;
 
 	msleep(150);
-	if ((ret = asix_sw_reset(dev, AX_SWRESET_CLEAR)) < 0)
+
+	ret = asix_sw_reset(dev, AX_SWRESET_CLEAR);
+	if (ret < 0)
 		goto out;
 
 	msleep(150);
+
 	if (embd_phy) {
-		if ((ret = asix_sw_reset(dev, AX_SWRESET_IPRL)) < 0)
+		ret = asix_sw_reset(dev, AX_SWRESET_IPRL);
+		if (ret < 0)
 			goto out;
-	}
-	else {
-		if ((ret = asix_sw_reset(dev, AX_SWRESET_PRTE)) < 0)
+	} else {
+		ret = asix_sw_reset(dev, AX_SWRESET_PRTE);
+		if (ret < 0)
 			goto out;
 	}
 
 	msleep(150);
 	rx_ctl = asix_read_rx_ctl(dev);
 	dbg("RX_CTL is 0x%04x after software reset", rx_ctl);
-	if ((ret = asix_write_rx_ctl(dev, 0x0000)) < 0)
+	ret = asix_write_rx_ctl(dev, 0x0000);
+	if (ret < 0)
 		goto out;
 
 	rx_ctl = asix_read_rx_ctl(dev);
 	dbg("RX_CTL is 0x%04x setting to 0x0000", rx_ctl);
 
-	/* Get the MAC address */
-	if ((ret = asix_read_cmd(dev, AX_CMD_READ_NODE_ID,
-				0, 0, ETH_ALEN, buf)) < 0) {
-		dbg("Failed to read MAC address: %d", ret);
-		goto out;
-	}
-	memcpy(dev->net->dev_addr, buf, ETH_ALEN);
-
-	/* Initialize MII structure */
-	dev->mii.dev = dev->net;
-	dev->mii.mdio_read = asix_mdio_read;
-	dev->mii.mdio_write = asix_mdio_write;
-	dev->mii.phy_id_mask = 0x1f;
-	dev->mii.reg_num_mask = 0x1f;
-	dev->mii.phy_id = asix_get_phy_addr(dev);
-
-	phyid = asix_get_phyid(dev);
-	dbg("PHYID=0x%08x", phyid);
-
-	if ((ret = asix_sw_reset(dev, AX_SWRESET_PRL)) < 0)
+	ret = asix_sw_reset(dev, AX_SWRESET_PRL);
+	if (ret < 0)
 		goto out;
 
 	msleep(150);
 
-	if ((ret = asix_sw_reset(dev, AX_SWRESET_IPRL | AX_SWRESET_PRL)) < 0)
+	ret = asix_sw_reset(dev, AX_SWRESET_IPRL | AX_SWRESET_PRL);
+	if (ret < 0)
 		goto out;
 
 	msleep(150);
 
-	dev->net->netdev_ops = &ax88772_netdev_ops;
-	dev->net->ethtool_ops = &ax88772_ethtool_ops;
-
 	asix_mdio_write(dev->net, dev->mii.phy_id, MII_BMCR, BMCR_RESET);
 	asix_mdio_write(dev->net, dev->mii.phy_id, MII_ADVERTISE,
 			ADVERTISE_ALL | ADVERTISE_CSMA);
 	mii_nway_restart(&dev->mii);
 
-	if ((ret = asix_write_medium_mode(dev, AX88772_MEDIUM_DEFAULT)) < 0)
+	ret = asix_write_medium_mode(dev, AX88772_MEDIUM_DEFAULT);
+	if (ret < 0)
 		goto out;
 
-	if ((ret = asix_write_cmd(dev, AX_CMD_WRITE_IPG0,
+	ret = asix_write_cmd(dev, AX_CMD_WRITE_IPG0,
 				AX88772_IPG0_DEFAULT | AX88772_IPG1_DEFAULT,
-				AX88772_IPG2_DEFAULT, 0, NULL)) < 0) {
+				AX88772_IPG2_DEFAULT, 0, NULL);
+	if (ret < 0) {
 		dbg("Write IPG,IPG1,IPG2 failed: %d", ret);
 		goto out;
 	}
 
+	/* Rewrite MAC address */
+	memcpy(data->mac_addr, dev->net->dev_addr, ETH_ALEN);
+	ret = asix_write_cmd(dev, AX_CMD_WRITE_NODE_ID, 0, 0, ETH_ALEN,
+							data->mac_addr);
+	if (ret < 0)
+		goto out;
+
 	/* Set RX_CTL to default values with 2k buffer, and enable cactus */
-	if ((ret = asix_write_rx_ctl(dev, AX_DEFAULT_RX_CTL)) < 0)
+	ret = asix_write_rx_ctl(dev, AX_DEFAULT_RX_CTL);
+	if (ret < 0)
 		goto out;
 
 	rx_ctl = asix_read_rx_ctl(dev);
@@ -1085,16 +1071,90 @@ static int ax88772_bind(struct usbnet *dev, struct usb_interface *intf)
 	rx_ctl = asix_read_medium_status(dev);
 	dbg("Medium Status is 0x%04x after all initializations", rx_ctl);
 
+	return 0;
+
+out:
+	return ret;
+
+}
+
+static const struct net_device_ops ax88772_netdev_ops = {
+	.ndo_open		= usbnet_open,
+	.ndo_stop		= usbnet_stop,
+	.ndo_start_xmit		= usbnet_start_xmit,
+	.ndo_tx_timeout		= usbnet_tx_timeout,
+	.ndo_change_mtu		= usbnet_change_mtu,
+	.ndo_set_mac_address 	= asix_set_mac_address,
+	.ndo_validate_addr	= eth_validate_addr,
+	.ndo_do_ioctl		= asix_ioctl,
+	.ndo_set_rx_mode        = asix_set_multicast,
+};
+
+static int ax88772_bind(struct usbnet *dev, struct usb_interface *intf)
+{
+	int ret, embd_phy;
+	struct asix_data *data = (struct asix_data *)&dev->data;
+	u8 buf[ETH_ALEN];
+	u32 phyid;
+
+	data->eeprom_len = AX88772_EEPROM_LEN;
+
+	usbnet_get_endpoints(dev,intf);
+
+	/* Get the MAC address */
+	ret = asix_read_cmd(dev, AX_CMD_READ_NODE_ID, 0, 0, ETH_ALEN, buf);
+	if (ret < 0) {
+		dbg("Failed to read MAC address: %d", ret);
+		return ret;
+	}
+	memcpy(dev->net->dev_addr, buf, ETH_ALEN);
+
+	/* Initialize MII structure */
+	dev->mii.dev = dev->net;
+	dev->mii.mdio_read = asix_mdio_read;
+	dev->mii.mdio_write = asix_mdio_write;
+	dev->mii.phy_id_mask = 0x1f;
+	dev->mii.reg_num_mask = 0x1f;
+	dev->mii.phy_id = asix_get_phy_addr(dev);
+
+	dev->net->netdev_ops = &ax88772_netdev_ops;
+	dev->net->ethtool_ops = &ax88772_ethtool_ops;
+
+	embd_phy = ((dev->mii.phy_id & 0x1f) == 0x10 ? 1 : 0);
+
+	/* Reset the PHY to normal operation mode */
+	ret = asix_write_cmd(dev, AX_CMD_SW_PHY_SELECT, embd_phy, 0, 0, NULL);
+	if (ret < 0) {
+		dbg("Select PHY #1 failed: %d", ret);
+		return ret;
+	}
+
+	ret = asix_sw_reset(dev, AX_SWRESET_IPPD | AX_SWRESET_PRL);
+	if (ret < 0)
+		return ret;
+
+	msleep(150);
+
+	ret = asix_sw_reset(dev, AX_SWRESET_CLEAR);
+	if (ret < 0)
+		return ret;
+
+	msleep(150);
+
+	ret = asix_sw_reset(dev, embd_phy ? AX_SWRESET_IPRL : AX_SWRESET_PRTE);
+
+	/* Read PHYID register *AFTER* the PHY was reset properly */
+	phyid = asix_get_phyid(dev);
+	dbg("PHYID=0x%08x", phyid);
+
 	/* Asix framing packs multiple eth frames into a 2K usb bulk transfer */
 	if (dev->driver_info->flags & FLAG_FRAMING_AX) {
 		/* hard_mtu  is still the default - the device does not support
 		   jumbo eth frames */
 		dev->rx_urb_size = 2048;
 	}
-	return 0;
 
-out:
-	return ret;
+	return 0;
 }
 
 static struct ethtool_ops ax88178_ethtool_ops = {
@@ -1143,6 +1203,27 @@ static int marvell_phy_init(struct usbnet *dev)
 	return 0;
 }
 
+static int rtl8211cl_phy_init(struct usbnet *dev)
+{
+	struct asix_data *data = (struct asix_data *)&dev->data;
+
+	netdev_dbg(dev->net, "rtl8211cl_phy_init()\n");
+
+	asix_mdio_write (dev->net, dev->mii.phy_id, 0x1f, 0x0005);
+	asix_mdio_write (dev->net, dev->mii.phy_id, 0x0c, 0);
+	asix_mdio_write (dev->net, dev->mii.phy_id, 0x01,
+		asix_mdio_read (dev->net, dev->mii.phy_id, 0x01) | 0x0080);
+	asix_mdio_write (dev->net, dev->mii.phy_id, 0x1f, 0);
+
+	if (data->ledmode == 12) {
+		asix_mdio_write (dev->net, dev->mii.phy_id, 0x1f, 0x0002);
+		asix_mdio_write (dev->net, dev->mii.phy_id, 0x1a, 0x00cb);
+		asix_mdio_write (dev->net, dev->mii.phy_id, 0x1f, 0);
+	}
+
+	return 0;
+}
+
 static int marvell_led_status(struct usbnet *dev, u16 speed)
 {
 	u16 reg = asix_mdio_read(dev->net, dev->mii.phy_id, MARVELL_LED_MANUAL);
@@ -1169,6 +1250,95 @@ static int marvell_led_status(struct usbnet *dev, u16 speed)
 	return 0;
 }
 
+static int ax88178_reset(struct usbnet *dev)
+{
+	struct asix_data *data = (struct asix_data *)&dev->data;
+	int ret;
+	__le16 eeprom;
+	u8 status;
+	int gpio0 = 0;
+	u32 phyid;
+
+	asix_read_cmd(dev, AX_CMD_READ_GPIOS, 0, 0, 1, &status);
+	dbg("GPIO Status: 0x%04x", status);
+
+	asix_write_cmd(dev, AX_CMD_WRITE_ENABLE, 0, 0, 0, NULL);
+	asix_read_cmd(dev, AX_CMD_READ_EEPROM, 0x0017, 0, 2, &eeprom);
+	asix_write_cmd(dev, AX_CMD_WRITE_DISABLE, 0, 0, 0, NULL);
+
+	dbg("EEPROM index 0x17 is 0x%04x", eeprom);
+
+	if (eeprom == cpu_to_le16(0xffff)) {
+		data->phymode = PHY_MODE_MARVELL;
+		data->ledmode = 0;
+		gpio0 = 1;
+	} else {
+		data->phymode = le16_to_cpu(eeprom) & 0x7F;
+		data->ledmode = le16_to_cpu(eeprom) >> 8;
+		gpio0 = (le16_to_cpu(eeprom) & 0x80) ? 0 : 1;
+	}
+	dbg("GPIO0: %d, PhyMode: %d", gpio0, data->phymode);
+
+	/* Power up external GigaPHY through AX88178 GPIO pin */
+	asix_write_gpio(dev, AX_GPIO_RSE | AX_GPIO_GPO_1 | AX_GPIO_GPO1EN, 40);
+	if ((le16_to_cpu(eeprom) >> 8) != 1) {
+		asix_write_gpio(dev, 0x003c, 30);
+		asix_write_gpio(dev, 0x001c, 300);
+		asix_write_gpio(dev, 0x003c, 30);
+	} else {
+		dbg("gpio phymode == 1 path");
+		asix_write_gpio(dev, AX_GPIO_GPO1EN, 30);
+		asix_write_gpio(dev, AX_GPIO_GPO1EN | AX_GPIO_GPO_1, 30);
+	}
+
+	/* Read PHYID register *AFTER* powering up PHY */
+	phyid = asix_get_phyid(dev);
+	dbg("PHYID=0x%08x", phyid);
+
+	/* Set AX88178 to enable MII/GMII/RGMII interface for external PHY */
+	asix_write_cmd(dev, AX_CMD_SW_PHY_SELECT, 0, 0, 0, NULL);
+
+	asix_sw_reset(dev, 0);
+	msleep(150);
+
+	asix_sw_reset(dev, AX_SWRESET_PRL | AX_SWRESET_IPPD);
+	msleep(150);
+
+	asix_write_rx_ctl(dev, 0);
+
+	if (data->phymode == PHY_MODE_MARVELL) {
+		marvell_phy_init(dev);
+		msleep(60);
+	} else if (data->phymode == PHY_MODE_RTL8211CL)
+		rtl8211cl_phy_init(dev);
+
+	asix_mdio_write(dev->net, dev->mii.phy_id, MII_BMCR,
+			BMCR_RESET | BMCR_ANENABLE);
+	asix_mdio_write(dev->net, dev->mii.phy_id, MII_ADVERTISE,
+			ADVERTISE_ALL | ADVERTISE_CSMA | ADVERTISE_PAUSE_CAP);
+	asix_mdio_write(dev->net, dev->mii.phy_id, MII_CTRL1000,
+			ADVERTISE_1000FULL);
+
+	mii_nway_restart(&dev->mii);
+
+	ret = asix_write_medium_mode(dev, AX88178_MEDIUM_DEFAULT);
+	if (ret < 0)
+		return ret;
+
+	/* Rewrite MAC address */
+	memcpy(data->mac_addr, dev->net->dev_addr, ETH_ALEN);
+	ret = asix_write_cmd(dev, AX_CMD_WRITE_NODE_ID, 0, 0, ETH_ALEN,
+							data->mac_addr);
+	if (ret < 0)
+		return ret;
+
+	ret = asix_write_rx_ctl(dev, AX_DEFAULT_RX_CTL);
+	if (ret < 0)
+		return ret;
+
+	return 0;
+}
+
 static int ax88178_link_reset(struct usbnet *dev)
 {
 	u16 mode;
@@ -1270,67 +1440,26 @@ static const struct net_device_ops ax88178_netdev_ops = {
 	.ndo_tx_timeout		= usbnet_tx_timeout,
 	.ndo_set_mac_address 	= asix_set_mac_address,
 	.ndo_validate_addr	= eth_validate_addr,
-	.ndo_set_multicast_list = asix_set_multicast,
+	.ndo_set_rx_mode	= asix_set_multicast,
 	.ndo_do_ioctl 		= asix_ioctl,
 	.ndo_change_mtu 	= ax88178_change_mtu,
 };
 
 static int ax88178_bind(struct usbnet *dev, struct usb_interface *intf)
 {
-	struct asix_data *data = (struct asix_data *)&dev->data;
 	int ret;
 	u8 buf[ETH_ALEN];
-	__le16 eeprom;
-	u8 status;
-	int gpio0 = 0;
-	u32 phyid;
-
-	usbnet_get_endpoints(dev,intf);
-
-	asix_read_cmd(dev, AX_CMD_READ_GPIOS, 0, 0, 1, &status);
-	dbg("GPIO Status: 0x%04x", status);
-
-	asix_write_cmd(dev, AX_CMD_WRITE_ENABLE, 0, 0, 0, NULL);
-	asix_read_cmd(dev, AX_CMD_READ_EEPROM, 0x0017, 0, 2, &eeprom);
-	asix_write_cmd(dev, AX_CMD_WRITE_DISABLE, 0, 0, 0, NULL);
-
-	dbg("EEPROM index 0x17 is 0x%04x", eeprom);
-
-	if (eeprom == cpu_to_le16(0xffff)) {
-		data->phymode = PHY_MODE_MARVELL;
-		data->ledmode = 0;
-		gpio0 = 1;
-	} else {
-		data->phymode = le16_to_cpu(eeprom) & 7;
-		data->ledmode = le16_to_cpu(eeprom) >> 8;
-		gpio0 = (le16_to_cpu(eeprom) & 0x80) ? 0 : 1;
-	}
-	dbg("GPIO0: %d, PhyMode: %d", gpio0, data->phymode);
-
-	asix_write_gpio(dev, AX_GPIO_RSE | AX_GPIO_GPO_1 | AX_GPIO_GPO1EN, 40);
-	if ((le16_to_cpu(eeprom) >> 8) != 1) {
-		asix_write_gpio(dev, 0x003c, 30);
-		asix_write_gpio(dev, 0x001c, 300);
-		asix_write_gpio(dev, 0x003c, 30);
-	} else {
-		dbg("gpio phymode == 1 path");
-		asix_write_gpio(dev, AX_GPIO_GPO1EN, 30);
-		asix_write_gpio(dev, AX_GPIO_GPO1EN | AX_GPIO_GPO_1, 30);
-	}
-
-	asix_sw_reset(dev, 0);
-	msleep(150);
+	struct asix_data *data = (struct asix_data *)&dev->data;
 
-	asix_sw_reset(dev, AX_SWRESET_PRL | AX_SWRESET_IPPD);
-	msleep(150);
+	data->eeprom_len = AX88772_EEPROM_LEN;
 
-	asix_write_rx_ctl(dev, 0);
+	usbnet_get_endpoints(dev,intf);
 
 	/* Get the MAC address */
-	if ((ret = asix_read_cmd(dev, AX_CMD_READ_NODE_ID,
-				0, 0, ETH_ALEN, buf)) < 0) {
+	ret = asix_read_cmd(dev, AX_CMD_READ_NODE_ID, 0, 0, ETH_ALEN, buf);
+	if (ret < 0) {
 		dbg("Failed to read MAC address: %d", ret);
-		goto out;
+		return ret;
 	}
 	memcpy(dev->net->dev_addr, buf, ETH_ALEN);
 
@@ -1346,28 +1475,12 @@ static int ax88178_bind(struct usbnet *dev, struct usb_interface *intf)
 	dev->net->netdev_ops = &ax88178_netdev_ops;
 	dev->net->ethtool_ops = &ax88178_ethtool_ops;
 
-	phyid = asix_get_phyid(dev);
-	dbg("PHYID=0x%08x", phyid);
-
-	if (data->phymode == PHY_MODE_MARVELL) {
-		marvell_phy_init(dev);
-		msleep(60);
-	}
-
-	asix_mdio_write(dev->net, dev->mii.phy_id, MII_BMCR,
-			BMCR_RESET | BMCR_ANENABLE);
-	asix_mdio_write(dev->net, dev->mii.phy_id, MII_ADVERTISE,
-			ADVERTISE_ALL | ADVERTISE_CSMA | ADVERTISE_PAUSE_CAP);
-	asix_mdio_write(dev->net, dev->mii.phy_id, MII_CTRL1000,
-			ADVERTISE_1000FULL);
-
-	mii_nway_restart(&dev->mii);
-
-	if ((ret = asix_write_medium_mode(dev, AX88178_MEDIUM_DEFAULT)) < 0)
-		goto out;
+	/* Blink LEDS so users know driver saw dongle */
+	asix_sw_reset(dev, 0);
+	msleep(150);
 
-	if ((ret = asix_write_rx_ctl(dev, AX_DEFAULT_RX_CTL)) < 0)
-		goto out;
+	asix_sw_reset(dev, AX_SWRESET_PRL | AX_SWRESET_IPPD);
+	msleep(150);
 
 	/* Asix framing packs multiple eth frames into a 2K usb bulk transfer */
 	if (dev->driver_info->flags & FLAG_FRAMING_AX) {
@@ -1375,10 +1488,8 @@ static int ax88178_bind(struct usbnet *dev, struct usb_interface *intf)
 		   jumbo eth frames */
 		dev->rx_urb_size = 2048;
 	}
-	return 0;
 
-out:
-	return ret;
+	return 0;
 }
 
 static const struct driver_info ax8817x_info = {
@@ -1426,7 +1537,7 @@ static const struct driver_info ax88772_info = {
 	.bind = ax88772_bind,
 	.status = asix_status,
 	.link_reset = ax88772_link_reset,
-	.reset = ax88772_link_reset,
+	.reset = ax88772_reset,
 	.flags = FLAG_ETHER | FLAG_FRAMING_AX | FLAG_LINK_INTR,
 	.rx_fixup = asix_rx_fixup,
 	.tx_fixup = asix_tx_fixup,
@@ -1437,7 +1548,7 @@ static const struct driver_info ax88178_info = {
 	.bind = ax88178_bind,
 	.status = asix_status,
 	.link_reset = ax88178_link_reset,
-	.reset = ax88178_link_reset,
+	.reset = ax88178_reset,
 	.flags = FLAG_ETHER | FLAG_FRAMING_AX | FLAG_LINK_INTR,
 	.rx_fixup = asix_rx_fixup,
 	.tx_fixup = asix_tx_fixup,
@@ -1578,7 +1689,7 @@ static const struct usb_device_id	products [] = {
 MODULE_DEVICE_TABLE(usb, products);
 
 static struct usb_driver asix_driver = {
-	.name =		"asix",
+	.name =		DRIVER_NAME,
 	.id_table =	products,
 	.probe =	usbnet_probe,
 	.suspend =	usbnet_suspend,
@@ -1600,6 +1711,7 @@ static void __exit asix_exit(void)
 module_exit(asix_exit);
 
 MODULE_AUTHOR("David Hollis");
+MODULE_VERSION(DRIVER_VERSION);
 MODULE_DESCRIPTION("ASIX AX8817X based USB 2.0 Ethernet Devices");
 MODULE_LICENSE("GPL");
 
diff --git a/drivers/net/usb/usbnet.c b/drivers/net/usb/usbnet.c
index dd225fc..749cf84 100644
--- a/drivers/net/usb/usbnet.c
+++ b/drivers/net/usb/usbnet.c
@@ -238,6 +238,10 @@ void usbnet_skb_return (struct usbnet *dev, struct sk_buff *skb)
 	netif_dbg(dev, rx_status, dev->net, "< rx, len %zu, type 0x%x\n",
 		  skb->len + sizeof (struct ethhdr), skb->protocol);
 	memset (skb->cb, 0, sizeof (struct skb_data));
+
+	if (skb_defer_rx_timestamp(skb))
+		return;
+
 	status = netif_rx (skb);
 	if (status != NET_RX_SUCCESS)
 		netif_dbg(dev, rx_err, dev->net,
@@ -1086,6 +1090,9 @@ netdev_tx_t usbnet_start_xmit (struct sk_buff *skb,
 	unsigned long		flags;
 	int retval;
 
+	if (skb)
+		skb_tx_timestamp(skb);
+
 	// some devices want funky USB-level framing, for
 	// win32 driver (usually) and/or hardware quirks
 	if (info->tx_fixup) {
@@ -1149,6 +1156,7 @@ netdev_tx_t usbnet_start_xmit (struct sk_buff *skb,
 		usb_anchor_urb(urb, &dev->deferred);
 		/* no use to process more packets */
 		netif_stop_queue(net);
+		usb_put_urb(urb);
 		spin_unlock_irqrestore(&dev->txq.lock, flags);
 		netdev_dbg(dev->net, "Delaying transmission for resumption\n");
 		goto deferred;
@@ -1290,6 +1298,8 @@ void usbnet_disconnect (struct usb_interface *intf)
 
 	cancel_work_sync(&dev->kevent);
 
+	usb_scuttle_anchored_urbs(&dev->deferred);
+
 	if (dev->driver_info->unbind)
 		dev->driver_info->unbind (dev, intf);
 
diff --git a/drivers/spi/Kconfig b/drivers/spi/Kconfig
index de35c3a..0d26222 100644
--- a/drivers/spi/Kconfig
+++ b/drivers/spi/Kconfig
@@ -395,6 +395,22 @@ config SPI_TOPCLIFF_PCH
 	  SPI driver for the Topcliff PCH (Platform Controller Hub) SPI bus
 	  used in some x86 embedded processors.
 
+config SPI_TRANSCEDE
+	tristate "Transcede SPI controller"
+	depends on SPI_MASTER && ARCH_TRANSCEDE
+	help
+	  This option enables Transcede SPI controller support.
+
+	  This driver can also be built as a module.  If so, the module
+	  will be called comcerto-spi.
+
+config SPI_TRANSCEDE_CS_GPIO
+	bool "Use GPIO21..24 to control chip selects"
+	depends on SPI_TRANSCEDE
+	default y if MACH_M84XXX
+	help
+	  This option enables GPIO lines to control chip selects.
+
 config SPI_TXX9
 	tristate "Toshiba TXx9 SPI controller"
 	depends on GENERIC_GPIO && CPU_TX49XX
@@ -474,4 +490,14 @@ endif # SPI_MASTER
 
 # (slave support would go here)
 
+config SPI_CDCE62005
+	tristate "SPI Clock Generator featuring low output jitter"
+	default m
+	depends on SPI_TRANSCEDE
+	help
+          Read/write support for TI CDCE62005 device.
+
+	  This driver can also be built as a module.  If so, the module
+	  will be called cdce62005.
+
 endif # SPI
diff --git a/drivers/spi/Makefile b/drivers/spi/Makefile
index 0f8c69b..47a3c6e 100644
--- a/drivers/spi/Makefile
+++ b/drivers/spi/Makefile
@@ -48,6 +48,7 @@ obj-$(CONFIG_SPI_S3C64XX)		+= spi_s3c64xx.o
 obj-$(CONFIG_SPI_TEGRA)			+= spi_tegra.o
 obj-$(CONFIG_SPI_TI_SSP)		+= ti-ssp-spi.o
 obj-$(CONFIG_SPI_TOPCLIFF_PCH)		+= spi_topcliff_pch.o
+obj-$(CONFIG_SPI_TRANSCEDE)		+= spi_transcede.o
 obj-$(CONFIG_SPI_TXX9)			+= spi_txx9.o
 obj-$(CONFIG_SPI_XILINX)		+= xilinx_spi.o
 obj-$(CONFIG_SPI_SH)			+= spi_sh.o
@@ -72,3 +73,4 @@ obj-$(CONFIG_SPI_TLE62X0)	+= tle62x0.o
 
 # SPI slave drivers (protocol for that link)
 # 	... add above this line ...
+obj-$(CONFIG_SPI_CDCE62005)	+= cdce62005.o
diff --git a/drivers/spi/cdce62005.c b/drivers/spi/cdce62005.c
new file mode 100644
index 0000000..c3713a7
--- /dev/null
+++ b/drivers/spi/cdce62005.c
@@ -0,0 +1,501 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+/*#ifdef MODULE*/
+
+#include <linux/bitrev.h>
+#include <linux/fs.h>
+
+#include <linux/hwmon.h>
+#include <linux/init.h>
+#include <linux/err.h>
+#include <linux/delay.h>
+#include <linux/input.h>
+#include <linux/interrupt.h>
+#include <linux/slab.h>
+#include <linux/gpio.h>
+#include <linux/device.h>
+#include <linux/spi/spi.h>
+#include <linux/spi/cdce62005.h>
+#include <asm/irq.h>
+#include <linux/module.h>
+
+#define CDCE62005_DEBUG_INFO
+
+#if defined (CDCE62005_DEBUG_INFO)
+#define PRINTK(f...)		printk (f)
+#else
+#define PRINTK(f...)
+#endif
+
+void cdce62005_lock(struct cdce62005 *cdce62005)
+{
+	if (!mutex_trylock(&cdce62005->lock)) {
+		dev_dbg(&cdce62005->spidev->dev, "wait for %s from %pf\n",
+				__func__, __builtin_return_address(0));
+
+		mutex_lock(&cdce62005->lock);
+
+		//printk(KERN_INFO "%s: in mutex_trylock.\r\n", __FUNCTION__ );
+	}
+	dev_dbg(&cdce62005->spidev->dev, "%s from %pf\n",
+			__func__, __builtin_return_address(0));
+
+	//printk(KERN_INFO "%s: leave: ->lock.count=0x%x.\r\n", __FUNCTION__, cdce62005->lock.count);
+}
+EXPORT_SYMBOL(cdce62005_lock);
+
+void cdce62005_unlock(struct cdce62005 *cdce62005)
+{
+	dev_dbg(&cdce62005->spidev->dev, "%s from %pf\n",
+			__func__, __builtin_return_address(0));
+
+	mutex_unlock(&cdce62005->lock);
+
+	//printk(KERN_INFO "%s: leave: ->lock.count=0x%x.\r\n", __FUNCTION__, cdce62005->lock.count);
+}
+EXPORT_SYMBOL(cdce62005_unlock);
+
+#define CDCE62005_REGOFFSET_SHIFT		0x4
+#define CDCE62005_REGREAD_ADDRFIELD_MARKER 		0xE
+int cdce62005_reg_read(struct cdce62005 *cdce62005, unsigned int offset, u32 *val)
+{
+	struct spi_message	*m;
+	struct spi_transfer		*x;
+	int ret;
+
+	u32    writeData, writeData2;
+	u32    readData, readData2;
+	u8 command[4], data[4];
+
+	u32    writeNone = 0xFFFFFFFF;
+
+	BUG_ON(!mutex_is_locked(&cdce62005->lock));
+
+	dev_vdbg(&cdce62005->spidev->dev, "[0x%02x] read \n", offset);
+
+	if (offset > CDCE62005_MAXREGS)
+		return -EINVAL;
+
+	*val = ( offset << CDCE62005_REGOFFSET_SHIFT ) | CDCE62005_REGREAD_ADDRFIELD_MARKER;
+		PRINTK(KERN_INFO "%s__1: *val = 0x%x.\r\n", __FUNCTION__, *val );
+
+	writeData = *val;
+		command[0] = bitrev8((u8) (writeData&0xFF)); //bit 0 : bit7
+		command[1] = bitrev8((u8)((writeData>>8) &0xFF)); //bit8  : bit15
+		command[2] = bitrev8((u8)((writeData>>16)&0xFF)); //bit16 : bit23
+		command[3] = bitrev8((u8)((writeData>>24)&0xFF)); //bit24 : bit31
+	writeData2 = ((u32)command[3] << 24) | ((u32)command[2] << 16) | ((u32)command[1] << 8 ) | ((u32)command[0]) ;
+
+	*val = writeData2;
+		//PRINTK(KERN_INFO "%s__1: *val swap_bit = 0x%x.\r\n", __FUNCTION__, *val );
+
+
+	/* set up the transfers to read touchscreen state; this assumes we
+	 * use formula #2 for pressure, not #3.
+	 */
+	m = &cdce62005->msg;
+	x = cdce62005->xfer;
+
+	spi_message_init(m);
+
+	memset(x, 0, sizeof(*x));
+	x->tx_buf = val;
+	x->len = sizeof(u32);	/* 4 bytes length */
+	x->cs_change = 1;
+	x->bits_per_word = CDCE62005_BITWIDTH;
+	x->speed_hz = CDCE62005_CLOCK_FREQ;
+	x->delay_usecs = CDCE62005_DELAY_USEC;
+	spi_message_add_tail(x, m);
+
+	x ++;
+	memset(x, 0, sizeof(*x));
+		x->tx_buf = &writeNone;	/* write invalid tx_FIFO to trigger IRQ */
+	x->rx_buf = val;
+	x->len = sizeof(u32);	/* 4 bytes length */
+	x->bits_per_word = CDCE62005_BITWIDTH;
+	x->speed_hz = CDCE62005_CLOCK_FREQ;
+	x->delay_usecs = CDCE62005_DELAY_USEC;
+	//x->cs_change = 0;	/* trigger cs_swap in giveback() */
+	spi_message_add_tail(x, m);
+
+	ret = spi_sync(cdce62005->spidev, m);
+
+	/* error in message.status implies error return from spi_sync */
+	BUG_ON(!ret && m->status);
+
+	if (ret)
+		return ret;
+
+
+	readData = *val;
+		data[0] = bitrev8((u8) (readData&0xFF)); //bit 0 : bit7
+		data[1] = bitrev8((u8)((readData>>8) &0xFF)); //bit8  : bit15
+		data[2] = bitrev8((u8)((readData>>16)&0xFF)); //bit16 : bit23
+		data[3] = bitrev8((u8)((readData>>24)&0xFF)); //bit24 : bit31
+	readData2 = ((u32)data[3] << 24) | ((u32)data[2] << 16) | ((u32)data[1] << 8 ) | ((u32)data[0]) ;
+
+		//PRINTK(KERN_INFO "%s__2: *val = 0x%x, readData2 = 0x%x.\r\n", __FUNCTION__, *val, readData2 );
+
+	//May need to ignore 4 lowest bits
+	*val = readData2 >> 4;
+	//*val &= 0xffffff;
+
+	dev_vdbg(&cdce62005->spidev->dev, "[0x%02x] -> 0x%06x\n", offset, *val);
+
+		PRINTK(KERN_INFO "%s__2: *val swapbit = 0x%x.\r\n", __FUNCTION__, *val );
+
+	return 0;
+}
+EXPORT_SYMBOL(cdce62005_reg_read);
+
+int cdce62005_reg_write(struct cdce62005 *cdce62005, unsigned int offset, u32 val)
+{
+	struct spi_message	*m;
+	struct spi_transfer		*x;
+	int ret;
+	u32 buf;
+
+	u32    writeData, writeData2;
+	u8 command[4];
+
+	BUG_ON(!mutex_is_locked(&cdce62005->lock));
+
+	dev_vdbg(&cdce62005->spidev->dev, "[0x%02x] <- 0x%06x\n", offset, val);
+
+	/* data fields are only 28-bits wide */
+	if (offset > CDCE62005_MAXREGS || val > 0xFFFFFFFF)
+		return -EINVAL;
+
+	/*--------------------------------------
+	   REGISTER_X		|	DEFAULT SETTING
+	   --------------------------------------
+		REG0000			|		0x8184032
+		REG0001			|		0x8184030
+		REG0002			|		0x8186030
+		REG0003			|		0xEB86030
+		REG0004			|		0x0186031
+		REG0005			|		0x101C0BE
+		REG0006			|		0x04BE19A
+		REG0007			|		0xBD0037F
+		REG0008			|		0x80005DD
+	   --------------------------------------
+	*/
+	// buf = 1 << 31 | offset << CDCE62005_REGOFFSET_SHIFT | val;
+
+	buf = val << CDCE62005_REGOFFSET_SHIFT | offset;
+		//PRINTK(KERN_INFO "%s__1: buf = 0x%x.\r\n", __FUNCTION__, buf );
+
+	writeData = buf;
+		command[0] = bitrev8((u8) (writeData&0xFF)); //bit 0 : bit7
+		command[1] = bitrev8((u8)((writeData>>8) &0xFF)); //bit8  : bit15
+		command[2] = bitrev8((u8)((writeData>>16)&0xFF)); //bit16 : bit23
+		command[3] = bitrev8((u8)((writeData>>24)&0xFF)); //bit24 : bit31
+	writeData2 = ((u32)command[3] << 24) | ((u32)command[2] << 16) | ((u32)command[1] << 8 ) | ((u32)command[0]) ;
+
+	buf = writeData2;
+		PRINTK(KERN_INFO "%s__1: command[0][1][2][3]=0x%x,0x%x,0x%x,0x%x. buf_swap = 0x%x. \r\n",
+			__FUNCTION__, command[0], command[1], command[2], command[3], buf );
+
+	/* set up the transfers to read touchscreen state; this assumes we
+	 * use formula #2 for pressure, not #3.
+	 */
+	m = &cdce62005->msg;
+	x = cdce62005->xfer;
+
+	spi_message_init(m);
+
+	memset(x, 0, sizeof(*x));
+	x->tx_buf = &buf;
+	x->len = sizeof(u32);	/* 4 bytes length */
+	x->bits_per_word = CDCE62005_BITWIDTH;
+	x->speed_hz = CDCE62005_CLOCK_FREQ;
+	x->delay_usecs = CDCE62005_DELAY_USEC;
+	// x->cs_change = 0;
+	spi_message_add_tail(x, m);
+
+	ret = spi_sync(cdce62005->spidev, m);
+
+	BUG_ON(!ret && m->status);
+
+	if (ret)
+		return ret;
+
+	return 0;
+}
+EXPORT_SYMBOL(cdce62005_reg_write);
+
+int cdce62005_reg_rmw(struct cdce62005 *cdce62005, unsigned int offset,
+		u32 mask, u32 val)
+{
+	int ret;
+	u32 valread;
+
+	BUG_ON(val & ~mask);
+
+	ret = cdce62005_reg_read(cdce62005, offset, &valread);
+	if (ret)
+		return ret;
+
+	valread = (valread & ~mask) | val;
+
+	return cdce62005_reg_write(cdce62005, offset, valread);
+}
+EXPORT_SYMBOL(cdce62005_reg_rmw);
+
+static int cdce62005_detect_device(struct cdce62005 *cdce62005)
+{
+	u8	idx, startRegIdx = CDCE62005_REG0008;
+	u32 	rev_id;
+
+	// -ENXIO
+	{
+		idx = startRegIdx;
+
+		cdce62005_reg_read(cdce62005, idx, &rev_id);
+
+		printk(KERN_INFO "%s_done: CDCE62005 REG %d Read Result 0x%x.\r\n", __FUNCTION__, idx, rev_id );
+	}
+
+	return 0;
+}
+
+static ssize_t
+cdce62005_sysfs_operation(
+	struct cdce62005	*cdce62005,
+	char			*buf,
+	//unsigned		offset,
+	loff_t		offset,
+	size_t		count,
+	u8			isread
+)
+{
+	u8	idx;
+	u8	woffset_start = offset/(sizeof(u32));
+	u8	woffset_end = (offset+count-1)/(sizeof(u32));
+
+	u32* wbuf = (u32*)buf;
+
+	if  ( (woffset_start < CDCE62005_REG0000) ||
+		(woffset_end > CDCE62005_REG0008) )
+	   return -EINVAL;
+
+	cdce62005_lock(cdce62005);
+
+		printk(KERN_INFO "%s__1: offset = %lld, count = %d, woffset_start = 0x%x, woffset_end = 0x%x. isread=%d. \r\n",
+			__FUNCTION__, offset, count, woffset_start, woffset_end, isread );
+
+	for ( idx = woffset_start; idx <= woffset_end; idx ++)
+	{
+		if( isread )
+		{	// reading operation
+			cdce62005_reg_read(cdce62005, idx, wbuf );
+
+			printk(KERN_INFO "%s__3: CDCE62005 REG %d Read Result 0x%x.\r\n", __FUNCTION__, idx, *wbuf);
+		}
+		else
+		{	// writing operation
+			printk(KERN_INFO "%s__2: CDCE62005 REG %d Write 0x%x.\r\n", __FUNCTION__, idx, *wbuf );
+
+			cdce62005_reg_write(cdce62005, idx, *wbuf);
+		}
+
+		wbuf ++;		// databuf pointer increase ..
+	}
+
+	printk(KERN_INFO "%s__1: count = 0x%x. \r\n", __FUNCTION__, count );
+
+	cdce62005_unlock(cdce62005);
+	return count;
+}
+
+static ssize_t
+cdce62005_bin_read(struct file *file, struct kobject *kobj, struct bin_attribute *bin_attr,
+	      char *buf, loff_t off, size_t count)
+{
+	struct device *dev;
+	struct cdce62005 *cdce62005;
+
+	dev = container_of(kobj, struct device, kobj);
+	cdce62005 = dev_get_drvdata(dev);
+
+		//printk(KERN_INFO "%s__1: buf = 0x%x, offset=%lld, count=%d. \r\n", __FUNCTION__, buf, off, count );
+
+#if 0
+	off = 0;
+	count = 4;
+#endif
+
+	/* [offset, count] need to be aligned to u32, otherwise cdce62005 register access would be invalid */
+	if ( off % sizeof(u32) )
+		return 0;
+	if ( count % sizeof(u32) )
+		return 0;
+
+		//printk(KERN_INFO "%s__1: pass parameter checking. buf = 0x%x, off = %lld, count = %d. \r\n", __FUNCTION__, buf, off, count );
+
+	// block size checking ..
+	if (unlikely(off >= cdce62005->bin.size))
+		return 0;
+	if (count >= cdce62005->bin.size)
+		count = cdce62005->bin.size;
+	if ((off + count) > cdce62005->bin.size)
+		count = cdce62005->bin.size - off;
+	if (unlikely(!count))
+		return count;
+
+		printk(KERN_INFO "%s__1: reading to enter cdce62005_sysfs_operation . buf = 0x%p, off = %lld, count = %d. \r\n", __FUNCTION__, buf, off, count );
+
+	return cdce62005_sysfs_operation(cdce62005, buf, off, count, 1 );
+}
+
+static ssize_t
+cdce62005_bin_write(struct file *file, struct kobject *kobj, struct bin_attribute *bin_attr,
+	       char *buf, loff_t off, size_t count)
+{
+	struct device *dev;
+	struct cdce62005 *cdce62005;
+
+	dev = container_of(kobj, struct device, kobj);
+	cdce62005 = dev_get_drvdata(dev);
+
+		printk(KERN_INFO "%s__1: buf = 0x%p, off = 0x%llx, count = 0x%x. \r\n", __FUNCTION__, buf, off, count );
+
+	/* [offset, count] need to be aligned to u32, otherwise cdce62005 register access would be invalid */
+	if ( off % sizeof(u32) )
+		return 0;
+	if ( count % sizeof(u32) )
+		return 0;
+
+		printk(KERN_INFO "%s__1: pass parameter checking. \r\n", __FUNCTION__ );
+
+	if (unlikely(off >= cdce62005->bin.size))
+		return -EFBIG;
+	if (count >= cdce62005->bin.size)
+		count = cdce62005->bin.size;
+	if ((off + count) > cdce62005->bin.size)
+		count = cdce62005->bin.size - off;
+	if (unlikely(!count))
+		return count;
+
+		printk(KERN_INFO "%s__1: writing to enter cdce62005_sysfs_operation . \r\n", __FUNCTION__ );
+
+	return cdce62005_sysfs_operation(cdce62005, buf, off, count, 0 );
+}
+
+static int cdce62005_probe(struct spi_device *spi)
+{
+	struct cdce62005 *cdce62005;
+	int ret;
+
+	//PRINTK(KERN_INFO "%s__1: spi= 0x%x. \r\n", __FUNCTION__, spi );
+	cdce62005 = kzalloc(sizeof(*cdce62005), GFP_KERNEL);
+	if (!cdce62005)
+		return -ENOMEM;
+
+	dev_set_drvdata(&spi->dev, cdce62005);
+	spi->mode = SPI_MODE_0;	/* SPI_MODE_0 | SPI_CS_HIGH; */
+	spi->bits_per_word = 16;
+
+	/* dw_spi_setup */
+	ret = spi_setup(spi);
+	if (ret)
+		goto err_revision;
+
+	cdce62005->spidev = spi;
+
+	mutex_init(&cdce62005->lock);
+
+	cdce62005_lock(cdce62005);
+
+#if 1
+	ret = cdce62005_detect_device(cdce62005);	/* Ping the chip ... the status register */
+	if (ret)
+		goto err_revision;
+#endif
+
+
+	/* Export the CDCE62005 device through sysfs, since that's convenient,  Default to root-only access to . */
+	cdce62005->bin.attr.name = "cdce62005";
+	cdce62005->bin.attr.mode = 0664;
+
+	cdce62005->bin.read = cdce62005_bin_read;
+	cdce62005->bin.write = cdce62005_bin_write;
+	cdce62005->bin.size = CDCE62005_REGISTER_BLOCKSIZE;	// default setting to 36 bytes
+
+	ret = sysfs_create_bin_file(&spi->dev.kobj, &cdce62005->bin);
+	if (ret) {
+		dev_err(&spi->dev, "failed to register sysfs node\n");
+		goto err_revision;
+	}
+
+	cdce62005_unlock(cdce62005);
+
+	return 0;
+
+err_revision:
+	mutex_unlock(&cdce62005->lock);
+	dev_set_drvdata(&spi->dev, NULL);
+	kfree(cdce62005);
+	return ret;
+
+}
+
+static int __devexit cdce62005_remove(struct spi_device *spi)
+{
+	struct cdce62005 *cdce62005 = dev_get_drvdata(&spi->dev);
+	dev_set_drvdata(&spi->dev, NULL);
+
+	sysfs_remove_bin_file(&spi->dev.kobj, &cdce62005->bin);
+	kfree(cdce62005);
+
+	dev_dbg(&spi->dev, "unregistered cdce62005\n");
+	return 0;
+}
+
+static struct spi_driver cdce62005_driver = {
+	.driver = {
+		.name = CDCE62005_DRIVER_NAME,
+		.bus = &spi_bus_type,
+		.owner = THIS_MODULE,
+	},
+	.probe = cdce62005_probe,
+	.remove = __devexit_p(cdce62005_remove),
+	/* FIXME:  investigate suspend and resume... */
+};
+
+static int __init cdce62005_init(void)
+{
+	return spi_register_driver(&cdce62005_driver);
+}
+module_init(cdce62005_init);
+
+static void __exit cdce62005_exit(void)
+{
+	spi_unregister_driver(&cdce62005_driver);
+}
+module_exit(cdce62005_exit);
+
+MODULE_DESCRIPTION("Device Driver for TI CDCE62005 DeJitter");
+MODULE_AUTHOR("Intel");
+MODULE_LICENSE("GPL v2");
+
+/*#endif	 MODULE */
diff --git a/drivers/spi/spi_transcede.c b/drivers/spi/spi_transcede.c
new file mode 100644
index 0000000..adee02d
--- /dev/null
+++ b/drivers/spi/spi_transcede.c
@@ -0,0 +1,937 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+#include <linux/version.h>
+#include <linux/interrupt.h>
+#include <linux/platform_device.h>
+#include <linux/spi/spi.h>
+#include <asm/delay.h>
+#include <asm/fiq.h>
+#include <asm/io.h>
+#include <mach/spi.h>
+
+#if defined(CONFIG_MACH_M84XXX) && !defined(CONFIG_SPI_TRANSCEDE_CS_GPIO)
+#error  CONFIG_SPI_TRANSCEDE_CS_GPIO should be defined for MACH_M84XXX platform
+#endif
+
+MODULE_AUTHOR("Intel");
+MODULE_DESCRIPTION("SPI bus driver");
+MODULE_LICENSE("GPL");
+
+
+#define SPI_VERSION             "1.1.0"
+
+#define SPI_BAUDR_MIN		2
+#define SPI_BAUDR_MAX		0xFFFE
+#define SPI_SPEED_MAX		(4*1000*1000)
+#define SPI_SPEED_MIN		(TRANSCEDE_AHBCLK_HZ / SPI_BAUDR_MAX)
+#define SPI_FRAME_SIZE_MIN	4
+#define SPI_FRAME_SIZE_MAX	16
+#define SPI_CHIP_SELECT_MAX	15
+
+
+#define spi_err(dev, fmt, args...)	dev_err(dev, "%s: " fmt, __FUNCTION__, ##args)
+#define spi_dbg(dev, fmt, args...)	dev_dbg(dev, "%s: " fmt, __FUNCTION__, ##args)
+#define spi_warn(dev, fmt, args...)	dev_warn(dev, "%s: " fmt, __FUNCTION__, ##args)
+
+
+static int loopback = 0;
+module_param(loopback, bool, S_IRUGO);
+MODULE_PARM_DESC(loopback, "Force loopback mode: 0=normal mode, 1=loopback mode");
+static int polling = 0;
+module_param(polling, bool, S_IRUGO);
+MODULE_PARM_DESC(polling, "Force polling mode: 0=interrupt mode (if possible), 1=polling mode");
+
+
+struct transcede_spi
+{
+	ulong			membase;
+	int			irq;
+	struct device		*dev;
+	struct spi_master	*master;
+
+	spinlock_t		lock;
+
+	struct spi_message	*message;	/* current message */
+	struct spi_transfer	*rx_tr;		/* the first transfer that was put into buffer, needed to update rx_buf when buffer is completed */
+	struct spi_transfer	*tx_tr;		/* the last transfer that was put into buffer, needed to walk through the message */
+	int			rx_len;		/* number of bytes read */
+	int			tx_len;		/* number of bytes written */
+	u8			stopped;
+	u8			single;
+
+	/* core parameters, to avoid reading them from regs */
+	u8			chip_select;
+	u8			mode;
+	u8			bits_per_word;
+	u8			bytes_per_word;
+	u8			ssienr;
+	u32			ctrlr0;
+	u32			speed_hz;
+
+#ifdef CONFIG_SPI_TRANSCEDE_FIQ
+	u32			**fiq_rx_buf;
+	u32			**fiq_tx_buf;
+	u32			*fiq_tx_count;
+	u32			fiq_mask;
+	u32			fiq_mask_reg;
+	struct tasklet_struct	queue_tasklet;
+#endif
+	int			len;		/* buffer length, bytes */
+	u8			*buf;		/* linearized transfer buffer */
+	u8			buf_space[1];
+};
+
+
+#ifdef CONFIG_SPI_TRANSCEDE_FIQ
+extern u32 transcede_spi_intc_fiq_mask;
+extern u32 *transcede_spi0_fiq_rx_buf;
+extern u32 *transcede_spi0_fiq_tx_buf;
+extern u32 transcede_spi0_fiq_tx_count;
+extern u32 *transcede_spi1_fiq_rx_buf;
+extern u32 *transcede_spi1_fiq_tx_buf;
+extern u32 transcede_spi1_fiq_tx_count;
+#endif
+
+#ifdef CONFIG_MACH_M84XXX
+static DEFINE_SPINLOCK(gpio_lock);
+
+static unsigned long gpio_lock_flags;
+
+static inline void __gpio_lock(void)
+{
+	spin_lock_irqsave(&gpio_lock, gpio_lock_flags);
+}
+
+static inline void __gpio_unlock(void)
+{
+	spin_unlock_irqrestore(&gpio_lock, gpio_lock_flags);
+}
+#endif
+
+#ifdef CONFIG_SPI_TRANSCEDE_CS_GPIO
+static inline void gpio_set_cs(u8 cs)
+{
+	u32 out;
+
+	__gpio_lock();
+
+	/* CS is active low, GPIO lines must be inverse of CS value */
+	out = __raw_readl(TRANSCEDE_GPIO_OUTPUT_REG);
+	out |= 15 << 21;
+	cs &= 15;
+	__raw_writel(out & ~((u32)cs << 21), TRANSCEDE_GPIO_OUTPUT_REG);
+
+	__gpio_unlock();
+}
+#endif
+
+static inline void spi_set_cs(struct transcede_spi *adapter, u8 cs)
+{
+#ifdef CONFIG_SPI_TRANSCEDE_CS_GPIO
+	gpio_set_cs(cs);
+#endif
+	__raw_writel(cs, adapter->membase + TRANSCEDE_SPI_SER);	/* set SER anyway since transmit logic depends on it */
+}
+
+static inline int fifo_rx_used(struct transcede_spi *adapter)
+{
+	return __raw_readl(adapter->membase + TRANSCEDE_SPI_RXFLR) * adapter->bytes_per_word;
+}
+
+static inline int fifo_tx_used(struct transcede_spi *adapter)
+{
+	return __raw_readl(adapter->membase + TRANSCEDE_SPI_TXFLR) * adapter->bytes_per_word;
+}
+
+static inline int fifo_tx_free(struct transcede_spi *adapter)
+{
+	return TRANSCEDE_SPI_FIFO_DEPTH*adapter->bytes_per_word - fifo_tx_used(adapter);
+}
+
+static void spi_set_ssienr(struct transcede_spi *adapter, u8 ssienr)
+{
+	__raw_writel(ssienr, adapter->membase + TRANSCEDE_SPI_SSIENR);
+	adapter->ssienr = ssienr;
+}
+
+static void spi_set_speed(struct transcede_spi *adapter, u32 speed_hz)
+{
+	u32 baudr;
+	u32 real_speed_hz;
+
+	if (speed_hz > 0) {
+		/* calculate divisor, it must be even */
+		baudr = (TRANSCEDE_AHBCLK_HZ/speed_hz + 1) & ~1;
+
+		if (unlikely(baudr < SPI_BAUDR_MIN))
+			baudr = SPI_BAUDR_MIN;
+		if (unlikely(baudr > SPI_BAUDR_MAX))
+			baudr = SPI_BAUDR_MAX;
+
+		real_speed_hz = TRANSCEDE_AHBCLK_HZ / baudr;
+	} else
+		baudr = 0;
+
+	if (baudr)
+		spi_dbg(adapter->dev, "requested %uHz (real %uHz), divisor %#x\n",
+			speed_hz, real_speed_hz, baudr);
+	else
+		spi_dbg(adapter->dev, "disable serial clock\n");
+	__raw_writel(baudr, adapter->membase + TRANSCEDE_SPI_BAUDR);
+
+	adapter->speed_hz = speed_hz;	/* note that we save requested speed */
+}
+
+static void spi_set_control(struct transcede_spi *adapter, u8 mode, u8 bits_per_word)
+{
+	u32 ctrlr0 = adapter->ctrlr0;
+
+	/* mask out bits we may touch */
+	ctrlr0 &= ~(SPI_CTRLR0_SCPOL | SPI_CTRLR0_SCPH | SPI_CTRLR0_DFS_MASK | SPI_CTRLR0_SRL);
+
+	if (mode & SPI_CPOL)
+		ctrlr0 |= SPI_CTRLR0_SCPOL;
+
+	if (mode & SPI_CPHA)
+		ctrlr0 |= SPI_CTRLR0_SCPH;
+
+	if (mode & SPI_LOOP)
+		ctrlr0 |= SPI_CTRLR0_SRL;
+
+	ctrlr0 |= (bits_per_word - 1) & 15;
+
+	__raw_writel(ctrlr0, adapter->membase + TRANSCEDE_SPI_CTRLR0);
+
+	adapter->ctrlr0 = ctrlr0;
+	adapter->mode = mode;
+	adapter->bits_per_word = bits_per_word;
+	adapter->bytes_per_word = bits_per_word/9 + 1;
+}
+
+static void spi_reset(struct transcede_spi *adapter)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&adapter->lock, flags);
+
+	__raw_writel(0, adapter->membase + TRANSCEDE_SPI_IMR);	/* mask all IRQs */
+
+	spi_set_ssienr(adapter, 0);	/* disable operations: halt transfers, clear FIFOs */
+	spi_set_speed(adapter, 0);	/* disable serial clock */
+
+	/* set defaults
+	 * ---
+	 * 15:12 CFS	0000 control frame size (not supported in Motorola SPI mode)
+	 *    11 SRL	   0 shift register loop (normal mode operation)
+	 *    10 SLV_OE	   0 slave output enable (not used for master device)
+	 *  9: 8 TMOD	   0 transfer mode (read-write)
+	 *  5: 4 FRF	  00 frame format (Motorola SPI)
+	 */
+
+	/* this call will setup SCPOL, SCPH and DFS fields and write value to register */
+	spi_set_control(adapter, SPI_CPOL | SPI_CPHA, 8);
+
+	spin_unlock_irqrestore(&adapter->lock, flags);
+}
+
+static int __spi_message_setup(struct transcede_spi *adapter, struct spi_device *spi, struct spi_transfer *tr)
+{
+	u32 speed_hz = spi->max_speed_hz;
+	u8 bits_per_word = spi->bits_per_word;
+
+	/* we at the ::bits_per_word and ::speed_hz parameters of the first transfer,
+	 * it's possible to override device parameters at the start of series of transfers
+	 * which use continous chip select
+	 */
+	if (tr->bits_per_word) {
+		bits_per_word = tr->bits_per_word;
+		if (bits_per_word < SPI_FRAME_SIZE_MIN || bits_per_word > SPI_FRAME_SIZE_MAX)
+			return -EINVAL;
+	}
+
+	if (tr->speed_hz) {
+		speed_hz = tr->speed_hz;
+		if (speed_hz < SPI_SPEED_MIN || speed_hz > SPI_SPEED_MAX)
+			return -EINVAL;
+	}
+
+	spi_set_ssienr(adapter, 0);	/* SSI disable to allow programming registers below and reset FIFOs */
+
+	spi_set_cs(adapter, 0);		/* zero to chip select to disable transfer start before we filled FIFO */
+	adapter->chip_select = spi->chip_select;
+
+	if (speed_hz != adapter->speed_hz)
+		spi_set_speed(adapter, speed_hz);
+
+	if (spi->mode != adapter->mode || bits_per_word != adapter->bits_per_word)
+		spi_set_control(adapter, spi->mode, bits_per_word);
+
+	return 0;
+}
+
+static inline void fifo_read(struct transcede_spi *adapter, int len)
+{
+	int n = len;
+
+	if (likely(adapter->bits_per_word <= 8)) {
+		u8 *buf = adapter->buf + adapter->rx_len;
+
+		while (n--)
+			*buf++ = __raw_readl(adapter->membase + TRANSCEDE_SPI_DR);
+	} else {
+		u16 *buf = (u16*)(adapter->buf + adapter->rx_len);
+
+		n >>= 1;
+		while (n--)
+			*buf++ = __raw_readl(adapter->membase + TRANSCEDE_SPI_DR);
+	}
+
+	adapter->rx_len += len;
+}
+
+static inline void fifo_write(struct transcede_spi *adapter, int len)
+{
+	int n = len;
+
+	if (likely(adapter->bits_per_word <= 8)) {
+		u8 *buf = adapter->buf + adapter->tx_len;
+
+		while (n--)
+			__raw_writel(*buf++, adapter->membase + TRANSCEDE_SPI_DR);
+	} else {
+		u16 *buf = (u16*)(adapter->buf + adapter->tx_len);
+
+		n >>= 1;
+		while (n--)
+			__raw_writel(*buf++, adapter->membase + TRANSCEDE_SPI_DR);
+	}
+	/* if chip select is zero transfer won't start */
+	if (__raw_readl(adapter->membase + TRANSCEDE_SPI_SER) == 0)
+		spi_set_cs(adapter, adapter->chip_select);
+
+	adapter->tx_len += len;
+}
+
+static int buffer_fill(struct transcede_spi *adapter)
+{
+	struct spi_message *msg = adapter->message;
+	struct spi_transfer *tr = adapter->tx_tr;
+	int stop;
+
+	adapter->len = 0;
+	adapter->rx_len = 0;
+	adapter->tx_len = 0;
+
+	while (1) {
+		if (adapter->len + tr->len > PAGE_SIZE)
+			return -E2BIG;
+
+		if (adapter->bits_per_word > 8 && (tr->len & 1)) {
+			spi_err(adapter->dev, "odd transfer length for > 8 bits frame size\n");
+			return -EINVAL;
+		}
+
+		if (tr->tx_buf) {
+			memcpy(adapter->buf + adapter->len, tr->tx_buf, tr->len);
+			msg->actual_length += tr->len;
+		} else
+			memset(adapter->buf + adapter->len, 0, tr->len);
+
+		adapter->len += tr->len;
+
+		if (list_is_last(&tr->transfer_list, &msg->transfers))
+			break;
+
+		stop = tr->delay_usecs || tr->cs_change;
+
+		tr = list_entry(tr->transfer_list.next, struct spi_transfer, transfer_list);
+
+		if (stop)
+			break;
+	}
+
+	adapter->tx_tr = tr;
+
+	return 0;
+}
+
+static void buffer_empty(struct transcede_spi *adapter)
+{
+	int len;
+	struct spi_message *msg = adapter->message;
+	struct spi_transfer *tr = adapter->rx_tr, *tr_last;
+
+	len = 0;
+
+	while (1) {
+		if (tr->rx_buf) {
+			memcpy(tr->rx_buf, adapter->buf + len, tr->len);
+			msg->actual_length += tr->len;
+		}
+
+		len += tr->len;
+
+		tr_last = tr;
+
+		if (list_is_last(&tr->transfer_list, &msg->transfers))
+			break;
+
+		tr = list_entry(tr->transfer_list.next, struct spi_transfer, transfer_list);
+
+		if (tr == adapter->tx_tr)
+			break;
+	}
+
+	adapter->rx_tr = tr;
+
+	/* process cs_change and delay_usecs here since we move to next rx transfer unconditionally soon */
+	if (tr_last->delay_usecs)
+		udelay(tr->delay_usecs);
+
+	if (tr_last->cs_change)
+		spi_set_cs(adapter, 0);
+
+	adapter->stopped = tr_last->delay_usecs || tr_last->cs_change;
+}
+
+static void buffer_process(struct transcede_spi *adapter)
+{
+	int len;
+
+	spi_set_ssienr(adapter, 1);
+
+	do {
+		len = fifo_rx_used(adapter);
+		if (len)
+			fifo_read(adapter, len);
+
+		if (adapter->tx_len < adapter->len) {
+			/* take in account shifter reg, if we fill FIFO up when shifter is filled
+			 * we may get rx overrun if something delays reception path
+			 */
+			len = fifo_tx_free(adapter) - adapter->bytes_per_word;
+			if (len > 0) {
+				if (len > (adapter->len - adapter->tx_len))
+					len = adapter->len - adapter->tx_len;
+				fifo_write(adapter, len);
+			}
+		}
+	} while (adapter->rx_len < adapter->len);
+}
+
+#ifdef CONFIG_SPI_TRANSCEDE_FIQ
+static void buffer_start(struct transcede_spi *adapter)
+{
+	int len;
+
+	spi_set_ssienr(adapter, 1);
+
+	len = fifo_tx_free(adapter) - adapter->bytes_per_word;
+	if (len > (adapter->len - adapter->tx_len))
+		len = adapter->len - adapter->tx_len;
+	fifo_write(adapter, len);
+}
+
+static void spi_int_setup(struct transcede_spi *adapter)
+{
+	buffer_start(adapter);
+
+	if (adapter->tx_len >= adapter->len) {
+		/* everything is in TX FIFO, no FIQ is needed, unmask IRQ */
+
+		*adapter->fiq_rx_buf = (u32*) adapter->buf;	/* this field is used by IRQ handler */
+
+		__raw_writel(0, adapter->membase + TRANSCEDE_SPI_TXFTLR);
+		__raw_writel(1, adapter->membase + TRANSCEDE_SPI_IMR);
+
+		__raw_writel(__raw_readl(adapter->fiq_mask_reg-4) | adapter->fiq_mask, adapter->fiq_mask_reg-4);
+		__raw_writel(__raw_readl(adapter->fiq_mask_reg) & ~adapter->fiq_mask, adapter->fiq_mask_reg);
+	} else {
+		/* unmask FIQ, we need to refill TX FIFO */
+
+		*adapter->fiq_rx_buf = (u32*) adapter->buf;
+		*adapter->fiq_tx_buf = (u32*) ((u8*)adapter->buf + adapter->tx_len);
+		*adapter->fiq_tx_count = adapter->len - adapter->tx_len;
+
+		__raw_writel(8, adapter->membase + TRANSCEDE_SPI_TXFTLR);
+		__raw_writel(1, adapter->membase + TRANSCEDE_SPI_IMR);
+
+		__raw_writel(__raw_readl(adapter->fiq_mask_reg-4) & ~adapter->fiq_mask, adapter->fiq_mask_reg-4);
+		__raw_writel(__raw_readl(adapter->fiq_mask_reg) | adapter->fiq_mask, adapter->fiq_mask_reg);
+	}
+}
+#endif
+
+/*
+ * Here we get to proceed the last transfer in multi-transfer message, when
+ * the process was stopped on previous transfer.
+ *
+ * Return value indicates the status:
+ * < 0 - message failed, error status updated
+ * = 0 - message completed ok
+ * > 0 - control transferred to FIQ/IRQ, IRQ handler will schedule tasklet for completion
+ */
+static int __spi_message_finish(struct transcede_spi *adapter)
+{
+	int rc = 0;
+	struct spi_message *msg = adapter->message;
+
+	rc = __spi_message_setup(adapter, msg->spi, adapter->rx_tr);
+	if (rc)
+		return rc;
+
+	rc = buffer_fill(adapter);
+	if (rc)
+		return rc;
+
+	buffer_process(adapter);
+	buffer_empty(adapter);
+
+	return rc;
+}
+
+/*
+ * Here we get after either from IRQ after completing tx part of buffer in FIQ or from
+ * the transfer() function after filling the buffer, we need to finish processing
+ * completely and switch to the rest of transfers. If all transfers are completed -
+ * message is completed as well. Switch to the next message is handled in upper
+ * level code.
+ *
+ * Return value indicates the status:
+ * < 0 - message failed, error status updated
+ * = 0 - message completed ok
+ * > 0 - control transferred to FIQ/IRQ, IRQ handler will schedule tasklet for completion
+ */
+static int __spi_message_continue(struct transcede_spi *adapter)
+{
+	int rc = 0;
+	struct spi_message *msg = adapter->message;
+
+	while (1) {
+		buffer_process(adapter);
+		buffer_empty(adapter);
+
+		if (list_is_last(&adapter->rx_tr->transfer_list, &msg->transfers)) {
+			/* if we stopped on previous transfer - go to another state to avoid endless loop */
+			if (adapter->stopped && !adapter->single)
+				rc = __spi_message_finish(adapter);
+			break;
+		}
+
+		rc = __spi_message_setup(adapter, msg->spi, adapter->rx_tr);
+		if (rc)
+			break;
+
+		rc = buffer_fill(adapter);
+		if (rc)
+			break;
+
+#ifdef CONFIG_SPI_TRANSCEDE_FIQ
+		if (polling <= 0 && adapter->len > 8) {
+			/* transfer is long enough to go via FIQ */
+			spi_int_setup(adapter);
+			return 1;	/* indicate that we went into FIQ */
+		}
+#endif
+	}
+
+	spi_set_ssienr(adapter, 0);
+
+	msg->status = rc;
+
+	return rc;
+}
+
+static int __spi_message_start(struct transcede_spi *adapter, struct spi_message *msg)
+{
+	int rc;
+
+	msg->status = -EINPROGRESS;
+	msg->actual_length = 0;
+
+	adapter->message = msg;
+	adapter->rx_tr = list_first_entry(&msg->transfers, struct spi_transfer, transfer_list);
+	adapter->tx_tr = adapter->rx_tr;
+	adapter->stopped = 0;
+	adapter->single = list_is_last(&adapter->rx_tr->transfer_list, &msg->transfers);
+
+	rc = __spi_message_setup(adapter, msg->spi, adapter->rx_tr);
+	if (rc)
+		goto err;
+
+	rc = buffer_fill(adapter);
+	if (rc)
+		goto err;
+
+#ifdef CONFIG_SPI_TRANSCEDE_FIQ
+	if (polling <= 0 && adapter->len > 8) {
+		/* transfer is long enough to go via interrupt */
+		spi_int_setup(adapter);
+		goto fiq;
+	}
+#endif
+	rc = __spi_message_continue(adapter);
+	if (rc > 0)
+		goto fiq;		/* control was transferred to FIQ, just return */
+
+	spi_set_cs(adapter, 0);
+	spi_set_ssienr(adapter, 0);
+
+err:
+	msg->status = rc;
+	msg->complete(msg->context);
+
+	adapter->message = NULL;	/* completed, mark we don't have queued messages */
+	return rc;
+
+fiq:
+	return 1;
+}
+
+static int transcede_spi_transfer(struct spi_device *spi, struct spi_message *msg)
+{
+	struct transcede_spi *adapter = spi_master_get_devdata(spi->master);
+	int rc = 0;
+	unsigned long flags;
+
+	if (list_empty(&msg->transfers))
+		return -EINVAL;
+
+	spin_lock_irqsave(&adapter->lock, flags);
+
+#ifdef CONFIG_SPI_TRANSCEDE_FIQ
+	INIT_LIST_HEAD(&msg->queue);
+
+	if (adapter->message) {
+		list_add_tail(&msg->queue, &adapter->message->queue);
+		spin_unlock_irqrestore(&adapter->lock, flags);
+		return 0;
+	}
+#endif
+
+	rc = __spi_message_start(adapter, msg);
+	if (rc > 0)
+		rc = 0;
+
+	spin_unlock_irqrestore(&adapter->lock, flags);
+
+	return rc;
+}
+
+static int transcede_spi_setup(struct spi_device *spi)
+{
+	int rc = 0;
+
+	spi_dbg(&spi->dev, "bits per word %u, max speed %uHz, mode %#x\n",
+		spi->bits_per_word, spi->max_speed_hz, spi->mode);
+
+	if (!spi->bits_per_word)
+		spi->bits_per_word = 8;
+
+	if (spi->bits_per_word < SPI_FRAME_SIZE_MIN || spi->bits_per_word > SPI_FRAME_SIZE_MAX) {
+		spi_err(&spi->dev, "bits per word (frame size) %u out of range %u..%u\n",
+			spi->max_speed_hz, SPI_FRAME_SIZE_MIN, SPI_FRAME_SIZE_MAX);
+		rc = -EINVAL;
+		goto err;
+	}
+
+	if (spi->max_speed_hz < SPI_SPEED_MIN) {
+		spi_err(&spi->dev, "such low speed %u isn't supported, min is %u\n",
+			spi->max_speed_hz, SPI_SPEED_MIN);
+		rc = -EINVAL;
+		goto err;
+	}
+
+	if (spi->max_speed_hz > SPI_SPEED_MAX) {
+		spi_warn(&spi->dev, "decreasing speed %u to max supported %u\n",
+			spi->max_speed_hz, SPI_SPEED_MAX);
+		spi->max_speed_hz = SPI_SPEED_MAX;
+	}
+
+	if (!spi->chip_select || spi->chip_select > SPI_CHIP_SELECT_MAX) {
+		spi_err(&spi->dev, "chip select %u out of range 1..%u\n",
+			spi->max_speed_hz, SPI_CHIP_SELECT_MAX);
+		rc = -EINVAL;
+		goto err;
+	}
+
+	if (spi->mode & SPI_CS_HIGH) {
+		spi_err(&spi->dev, "chip select active high isn't supported\n");
+		rc = -EINVAL;
+		goto err;
+	}
+
+	if (spi->mode & SPI_LSB_FIRST) {
+		spi_err(&spi->dev, "LSB first mode isn't supported\n");
+		rc = -EINVAL;
+		goto err;
+	}
+
+err:
+	return rc;
+}
+
+void transcede_spi_set_loopback(struct spi_master *master, int loopback)
+{
+	struct transcede_spi *adapter = spi_master_get_devdata(master);
+	unsigned long flags;
+
+	spin_lock_irqsave(&adapter->lock, flags);
+
+	if (loopback)
+		adapter->ctrlr0 |= SPI_CTRLR0_SRL;
+	else
+		adapter->ctrlr0 &= ~SPI_CTRLR0_SRL;
+
+	__raw_writel(adapter->ctrlr0, adapter->membase + TRANSCEDE_SPI_CTRLR0);
+
+	spin_unlock_irqrestore(&adapter->lock, flags);
+}
+
+#ifdef CONFIG_SPI_TRANSCEDE_FIQ
+/*
+ * Current adapter::message is completed, call completion and check whether we have
+ * anything else to do.
+ */
+static void spi_message_next(ulong arg)
+{
+	struct transcede_spi *adapter = (void*) arg;
+	struct spi_message *msg = adapter->message;
+	unsigned long flags;
+
+	spin_lock_irqsave(&adapter->lock, flags);
+
+	msg->complete(msg->context);
+
+	if (!list_empty(&msg->queue)) {
+		struct spi_message *msg_next;
+
+		msg_next = list_entry(msg->queue.next, struct spi_message, queue);
+
+		list_del(&msg->queue);
+
+		__spi_message_start(adapter, msg_next);
+	} else
+		adapter->message = NULL;
+
+	spin_unlock_irqrestore(&adapter->lock, flags);
+}
+
+/*
+ * IRQ handler is called when FIQ completed tx part of transfer.
+ * It finishes rx part and tries to complete the message. Next message
+ * can be picked by tasklet function spi_message_next().
+ */
+static irqreturn_t spi_interrupt(int irq, void *dev_id)
+{
+	int rc = 0;
+	struct transcede_spi *adapter = dev_id;
+	struct spi_message *msg = adapter->message;
+
+	__raw_writel(0, adapter->membase + TRANSCEDE_SPI_IMR);		/* disable IRQ */
+
+	/* here we get after completing tx part of buffer, we need to finish rx
+	 * part completely and switch to another part of message
+	 */
+	adapter->tx_len = adapter->len;					/* no tx required */
+	adapter->rx_len = (u8*)(*adapter->fiq_rx_buf) - adapter->buf;	/* calculate how much we have processed on rx path */
+
+	rc = __spi_message_continue(adapter);
+	if (rc > 0)
+		goto fiq;		/* positive return value: control was transferred to FIQ again */
+
+	/* message either completed or failed, schedule tasklet to finish it and pick the next if any */
+	spi_set_ssienr(adapter, 0);
+	msg->status = rc;
+	tasklet_schedule(&adapter->queue_tasklet);
+
+fiq:
+	return IRQ_HANDLED;
+}
+#endif
+
+static int __devinit transcede_spi_probe(struct platform_device *pdev)
+{
+	struct spi_master *master;
+	struct transcede_spi *adapter;
+	struct resource *res;
+	int rc = -EINVAL;
+
+	spi_dbg(&pdev->dev, "registering device\n");
+
+	master = spi_alloc_master(&pdev->dev, sizeof(struct transcede_spi) + PAGE_SIZE + 32);
+	if (!master) {
+		spi_err(&pdev->dev, "couldn't allocate memory\n");
+		rc = -ENOMEM;
+		goto err0;
+	}
+
+	adapter = spi_master_get_devdata(master);
+	adapter->dev = &master->dev;
+	adapter->master = master;
+	adapter->buf = (void*) (((u32)&adapter->buf_space[0] + 31) & ~31);
+
+	spin_lock_init(&adapter->lock);
+
+#ifdef CONFIG_SPI_TRANSCEDE_FIQ
+	if (polling <= 0) {
+		extern u8 transcede_spi_fiq_start, transcede_spi_fiq_end;
+
+		adapter->irq = platform_get_irq(pdev, 0);
+		if (adapter->irq < 0) {
+			spi_err(&pdev->dev, "no IRQ resource\n");
+			rc = -EINVAL;
+			goto err1;
+		}
+
+		if (request_irq(adapter->irq, spi_interrupt, IRQF_DISABLED, pdev->name, adapter)) {
+			spi_err(&pdev->dev, "failed to request IRQ%d\n", adapter->irq);
+			goto err1;
+		}
+
+		if (pdev->id == 0) {
+			adapter->fiq_rx_buf = (void*) (0xFFFF001C + (u32) &transcede_spi0_fiq_rx_buf - (u32) &transcede_spi_fiq_start);
+			adapter->fiq_tx_buf = (void*) (0xFFFF001C + (u32) &transcede_spi0_fiq_tx_buf - (u32) &transcede_spi_fiq_start);
+			adapter->fiq_tx_count = (void*) (0xFFFF001C + (u32) &transcede_spi0_fiq_tx_count - (u32) &transcede_spi_fiq_start);
+			adapter->fiq_mask = 1 << IRQ_SPI0;
+		} else {
+			adapter->fiq_rx_buf = (void*) (0xFFFF001C + (u32) &transcede_spi1_fiq_rx_buf - (u32) &transcede_spi_fiq_start);
+			adapter->fiq_tx_buf = (void*) (0xFFFF001C + (u32) &transcede_spi1_fiq_tx_buf - (u32) &transcede_spi_fiq_start);
+			adapter->fiq_tx_count = (void*) (0xFFFF001C + (u32) &transcede_spi1_fiq_tx_count - (u32) &transcede_spi_fiq_start);
+			adapter->fiq_mask = 1 << IRQ_SPI1;
+		}
+
+		if (transcede_arm1_is_running()) {
+			adapter->fiq_mask_reg = TRANSCEDE_INTC_ARM1_FIQMASK0;
+			transcede_spi_intc_fiq_mask = TRANSCEDE_INTC_ARM1_FIQMASK0;
+		} else {
+			adapter->fiq_mask_reg = TRANSCEDE_INTC_ARM0_FIQMASK0;
+			transcede_spi_intc_fiq_mask = TRANSCEDE_INTC_ARM0_FIQMASK0;
+		}
+
+		set_fiq_handler(&transcede_spi_fiq_start, &transcede_spi_fiq_end - &transcede_spi_fiq_start);
+	}
+
+	tasklet_init(&adapter->queue_tasklet, spi_message_next, (ulong) adapter);
+#endif
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res) {
+		spi_err(&pdev->dev, "no memory resource\n");
+		rc = -EINVAL;
+		goto err1;
+	}
+
+	/* should be mapped in arch/arm/mach-transcede/transcede-xxx.c */
+	adapter->membase = res->start;
+
+	spi_reset(adapter);
+	transcede_spi_set_loopback(master, loopback);
+
+#ifdef CONFIG_MACH_M84XXX
+	/* be shure  GPIO21..26 lines controled by SPI */
+	__gpio_lock();
+	__raw_writel(__raw_readl(TRANSCEDE_GPIO_PIN_SELECT) &  ~(0x3F << 21), TRANSCEDE_GPIO_PIN_SELECT);
+	__gpio_unlock();
+#endif
+
+#ifdef CONFIG_SPI_TRANSCEDE_CS_GPIO
+	/* select GPIO21..24 lines control instead of SSI SS_0..3 chip selects */
+	__gpio_lock();
+	__raw_writel(__raw_readl(TRANSCEDE_GPIO_PIN_SELECT) | (15 << 21), TRANSCEDE_GPIO_PIN_SELECT);
+	__gpio_unlock();
+#endif
+
+	master->bus_num		= pdev->id;
+	master->num_chipselect	= 16;
+	master->setup		= transcede_spi_setup;
+	master->transfer	= transcede_spi_transfer;
+	master->cleanup		= NULL;
+	master->mode_bits = SPI_CPOL | SPI_CPHA | SPI_LOOP;
+
+	platform_set_drvdata(pdev, master);
+
+	rc = spi_register_master(master);
+        if (rc != 0) {
+		spi_err(&pdev->dev, "error registering SPI master\n");
+		goto err1;
+	}
+
+	return 0;
+
+err1:
+	spi_master_put(master);
+
+err0:
+	return rc;
+}
+
+static int transcede_spi_remove(struct platform_device *pdev)
+{
+	struct spi_master *master = platform_get_drvdata(pdev);
+	struct transcede_spi *adapter = spi_master_get_devdata(master);
+
+#ifdef CONFIG_SPI_TRANSCEDE_FIQ
+	if (polling <= 0)
+		free_irq(adapter->irq, adapter);
+#endif
+
+	spi_reset(adapter);
+
+	spi_unregister_master(master);
+
+	return 0;
+}
+
+static struct platform_driver transcede_spi_driver = {
+	.driver	= {
+		.name	= "transcede_spi",
+		.owner	= THIS_MODULE,
+	},
+	.probe	= transcede_spi_probe,
+	.remove	= __devexit_p(transcede_spi_remove),
+};
+
+static int __init transcede_spi_init(void)
+{
+	printk(KERN_INFO "transcede-spi: %s: loaded version %s\n", __FUNCTION__, SPI_VERSION);
+
+	if (platform_driver_register(&transcede_spi_driver)) {
+		printk(KERN_ERR "transcede-spi: %s: error registering driver\n", __FUNCTION__);
+		goto err0;
+	}
+
+	return 0;
+
+err0:
+	return -1;
+}
+
+static void __exit transcede_spi_exit(void)
+{
+	platform_driver_unregister(&transcede_spi_driver);
+}
+
+EXPORT_SYMBOL(transcede_spi_set_loopback);
+
+module_init(transcede_spi_init);
+module_exit(transcede_spi_exit);
diff --git a/drivers/spi/spidev.c b/drivers/spi/spidev.c
index d9fd862..ac71da3 100644
--- a/drivers/spi/spidev.c
+++ b/drivers/spi/spidev.c
@@ -37,6 +37,10 @@
 
 #include <asm/uaccess.h>
 
+#ifdef CONFIG_MACH_M822XX
+#include <asm/io.h>
+#endif
+
 
 /*
  * This supports access to SPI devices using normal userspace I/O calls.
@@ -98,33 +102,15 @@ MODULE_PARM_DESC(bufsiz, "data bytes in biggest supported SPI message");
  * We can't use the standard synchronous wrappers for file I/O; we
  * need to protect against async removal of the underlying spi_device.
  */
-static void spidev_complete(void *arg)
-{
-	complete(arg);
-}
-
 static ssize_t
 spidev_sync(struct spidev_data *spidev, struct spi_message *message)
 {
-	DECLARE_COMPLETION_ONSTACK(done);
 	int status;
 
-	message->complete = spidev_complete;
-	message->context = &done;
-
 	spin_lock_irq(&spidev->spi_lock);
-	if (spidev->spi == NULL)
-		status = -ESHUTDOWN;
-	else
-		status = spi_async(spidev->spi, message);
+	status = spi_sync(spidev->spi, message);
 	spin_unlock_irq(&spidev->spi_lock);
 
-	if (status == 0) {
-		wait_for_completion(&done);
-		status = message->status;
-		if (status == 0)
-			status = message->actual_length;
-	}
 	return status;
 }
 
@@ -132,6 +118,7 @@ static inline ssize_t
 spidev_sync_write(struct spidev_data *spidev, size_t len)
 {
 	struct spi_transfer	t = {
+			.rx_buf = NULL,
 			.tx_buf		= spidev->buffer,
 			.len		= len,
 		};
@@ -146,6 +133,7 @@ static inline ssize_t
 spidev_sync_read(struct spidev_data *spidev, size_t len)
 {
 	struct spi_transfer	t = {
+			.tx_buf = NULL,
 			.rx_buf		= spidev->buffer,
 			.len		= len,
 		};
@@ -482,6 +470,29 @@ spidev_compat_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
 #define spidev_compat_ioctl NULL
 #endif /* CONFIG_COMPAT */
 
+#ifdef CONFIG_MACH_M822XX
+/*
+ * @brief Takes HW lock assigned for SPI. (SPI is also used from CEVA)
+ *
+ * @return      0       On timeout error.
+ * @return      >0      On success.
+ */
+static int transcede_spi_lock(unsigned int bus_num)
+{
+	unsigned int timeout = TRANSCEDE_SYSLOCK_SPI_TIMEOUT;
+	while (--timeout && readl(TRANSCEDE_SEMAARMCTRL(TRANSCEDE_SYSLOCKID_SPI(bus_num))));
+	return timeout;
+}
+
+/*
+ * @brief Releases HW lock assigned for SPI. (SPI is also used from CEVA)
+ */
+static void transcede_spi_unlock(unsigned int bus_num)
+{
+	writel(0, TRANSCEDE_SEMAARMCTRL(TRANSCEDE_SYSLOCKID_SPI(bus_num)));
+}
+#endif
+
 static int spidev_open(struct inode *inode, struct file *filp)
 {
 	struct spidev_data	*spidev;
@@ -495,6 +506,14 @@ static int spidev_open(struct inode *inode, struct file *filp)
 			break;
 		}
 	}
+
+#ifdef CONFIG_MACH_M822XX
+	if (status == 0 && !transcede_spi_lock(spidev->spi->master->bus_num)) {
+		pr_err("Timeout (%u) reached when trying to acquire SPI HW lock. Looks like CEVA already took over SPI.\n", TRANSCEDE_SYSLOCK_SPI_TIMEOUT);
+		status = -EBUSY;
+	}
+#endif
+
 	if (status == 0) {
 		if (!spidev->buffer) {
 			spidev->buffer = kmalloc(bufsiz, GFP_KERNEL);
@@ -508,6 +527,11 @@ static int spidev_open(struct inode *inode, struct file *filp)
 			filp->private_data = spidev;
 			nonseekable_open(inode, filp);
 		}
+#ifdef CONFIG_MACH_M822XX
+		if (status != 0) {
+			transcede_spi_unlock(spidev->spi->master->bus_num);
+		}
+#endif
 	} else
 		pr_debug("spidev: nothing for minor %d\n", iminor(inode));
 
@@ -542,6 +566,10 @@ static int spidev_release(struct inode *inode, struct file *filp)
 	}
 	mutex_unlock(&device_list_lock);
 
+#ifdef CONFIG_MACH_M822XX
+	transcede_spi_unlock(spidev->spi->master->bus_num);
+#endif
+
 	return status;
 }
 
diff --git a/drivers/tty/serial/8250.c b/drivers/tty/serial/8250.c
index b89144e..6c31830 100644
--- a/drivers/tty/serial/8250.c
+++ b/drivers/tty/serial/8250.c
@@ -2942,7 +2942,11 @@ serial8250_console_write(struct console *co, const char *s, unsigned int count)
 static int __init serial8250_console_setup(struct console *co, char *options)
 {
 	struct uart_port *port;
+#ifdef CONFIG_ARCH_TRANSCEDE
+	int baud = 115200;
+#else
 	int baud = 9600;
+#endif
 	int bits = 8;
 	int parity = 'n';
 	int flow = 'n';
diff --git a/drivers/tty/serial/8250_early.c b/drivers/tty/serial/8250_early.c
index eaafb98..a71d44f 100644
--- a/drivers/tty/serial/8250_early.c
+++ b/drivers/tty/serial/8250_early.c
@@ -131,6 +131,7 @@ static unsigned int __init probe_baud(struct uart_port *port)
 
 static void __init init_port(struct early_serial8250_device *device)
 {
+#ifndef CONFIG_ARCH_TRANSCEDE
 	struct uart_port *port = &device->port;
 	unsigned int divisor;
 	unsigned char c;
@@ -146,6 +147,7 @@ static void __init init_port(struct early_serial8250_device *device)
 	serial_out(port, UART_DLL, divisor & 0xff);
 	serial_out(port, UART_DLM, (divisor >> 8) & 0xff);
 	serial_out(port, UART_LCR, c & ~UART_LCR_DLAB);
+#endif
 }
 
 static int __init parse_options(struct early_serial8250_device *device,
diff --git a/drivers/usb/Kconfig b/drivers/usb/Kconfig
index 48f1781..9485281 100644
--- a/drivers/usb/Kconfig
+++ b/drivers/usb/Kconfig
@@ -168,4 +168,6 @@ source "drivers/usb/gadget/Kconfig"
 
 source "drivers/usb/otg/Kconfig"
 
+source "drivers/usb/dwc_otg/Kconfig"
+
 endif # USB_SUPPORT
diff --git a/drivers/usb/Makefile b/drivers/usb/Makefile
index 30ddf8d..96e2ac7 100644
--- a/drivers/usb/Makefile
+++ b/drivers/usb/Makefile
@@ -47,6 +47,8 @@ obj-$(CONFIG_EARLY_PRINTK_DBGP)	+= early/
 obj-$(CONFIG_USB_ATM)		+= atm/
 obj-$(CONFIG_USB_SPEEDTOUCH)	+= atm/
 
+obj-$(CONFIG_DWC_OTG)       += dwc_otg/
+
 obj-$(CONFIG_USB_MUSB_HDRC)	+= musb/
 obj-$(CONFIG_USB_RENESAS_USBHS)	+= renesas_usbhs/
 obj-$(CONFIG_USB_OTG_UTILS)	+= otg/
diff --git a/drivers/usb/dwc_otg/Kconfig b/drivers/usb/dwc_otg/Kconfig
new file mode 100644
index 0000000..4d99b0b
--- /dev/null
+++ b/drivers/usb/dwc_otg/Kconfig
@@ -0,0 +1,37 @@
+config DWC_OTG
+        tristate "Synopsis DWC_OTG support"
+        depends on USB_SUPPORT
+        help
+          This driver supports Synopsis DWC_OTG IP core
+		  embebbed on many SOCs (ralink, infineon, etc)
+
+choice
+        prompt "USB Operation Mode"
+        depends on DWC_OTG
+        default DWC_OTG_HOST_ONLY
+
+config DWC_OTG_HOST_ONLY
+        bool "HOST ONLY MODE"
+        depends on DWC_OTG
+
+config DWC_OTG_DEVICE_ONLY
+        bool "DEVICE ONLY MODE"
+        depends on DWC_OTG
+endchoice
+
+choice
+        prompt "Platform"
+        depends on DWC_OTG
+        default DWC_OTG_TRANSCEDE
+
+config DWC_OTG_TRANSCEDE
+        bool "Transcede 2200/3300"
+        depends on ARCH_TRANSCEDE
+        help
+          USB 2.0 USB Host Controller
+		  platform support
+endchoice
+
+config DWC_OTG_DEBUG
+        bool "Enable debug mode"
+        depends on DWC_OTG
diff --git a/drivers/usb/dwc_otg/Makefile b/drivers/usb/dwc_otg/Makefile
new file mode 100644
index 0000000..f13cd53
--- /dev/null
+++ b/drivers/usb/dwc_otg/Makefile
@@ -0,0 +1,30 @@
+#
+# Makefile for DWC_otg Highspeed USB controller driver
+#
+
+ifeq ($(CONFIG_DWC_OTG_DEBUG),y)
+# EXTRA_CFLAGS   += -DDEBUG
+endif
+
+# Use one of the following flags to compile the software in host-only or
+# device-only mode.
+ifeq ($(CONFIG_DWC_OTG_HOST_ONLY),y)
+EXTRA_CFLAGS   += -DDWC_HOST_ONLY
+EXTRA_CFLAGS   += -DDWC_EN_ISOC
+EXTRA_CFLAGS   += -DDWC_LINUX
+# EXTRA_CFLAGS   += -DDWC_LIBMODULE
+# EXTRA_CFLAGS   += -DDEBUG
+endif
+
+ifeq ($(CONFIG_DWC_OTG_DEVICE_ONLY),y)
+EXTRA_CFLAGS   += -DDWC_DEVICE_ONLY
+endif
+
+obj-$(CONFIG_DWC_OTG)	:= dwc_otg.o
+
+dwc_otg-objs	:= dwc_otg_driver.o dwc_otg_attr.o
+dwc_otg-objs	+= dwc_otg_cil.o dwc_otg_cil_intr.o
+dwc_otg-objs	+= dwc_otg_hcd_linux.o dwc_otg_adp.o dwc_otg_cfi.o
+dwc_otg-objs	+= dwc_otg_hcd_ddma.o
+dwc_otg-objs	+= dwc_otg_hcd.o dwc_otg_hcd_intr.o dwc_otg_hcd_queue.o
+dwc_otg-objs	+= dwc_common_linux.o dwc_mem.o
diff --git a/drivers/usb/dwc_otg/dwc_cc.c b/drivers/usb/dwc_otg/dwc_cc.c
new file mode 100644
index 0000000..327b21e
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_cc.c
@@ -0,0 +1,532 @@
+/* =========================================================================
+ * $File: //dwh/usb_iip/dev/software/dwc_common_port_2/dwc_cc.c $
+ * $Revision: #4 $
+ * $Date: 2010/11/04 $
+ * $Change: 1621692 $
+ *
+ * Synopsys Portability Library Software and documentation
+ * (hereinafter, "Software") is an Unsupported proprietary work of
+ * Synopsys, Inc. unless otherwise expressly agreed to in writing
+ * between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product
+ * under any End User Software License Agreement or Agreement for
+ * Licensed Product with Synopsys or any supplement thereto. You are
+ * permitted to use and redistribute this Software in source and binary
+ * forms, with or without modification, provided that redistributions
+ * of source code must retain this notice. You may not view, use,
+ * disclose, copy or distribute this file or any information contained
+ * herein except pursuant to this license grant from Synopsys. If you
+ * do not agree with this notice, including the disclaimer below, then
+ * you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS"
+ * BASIS AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+ * FOR A PARTICULAR PURPOSE ARE HEREBY DISCLAIMED. IN NO EVENT SHALL
+ * SYNOPSYS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
+ * USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================= */
+#ifdef DWC_CCLIB
+
+#include "dwc_cc.h"
+
+typedef struct dwc_cc
+{
+	uint32_t uid;
+	uint8_t chid[16];
+	uint8_t cdid[16];
+	uint8_t ck[16];
+	uint8_t *name;
+	uint8_t length;
+        DWC_CIRCLEQ_ENTRY(dwc_cc) list_entry;
+} dwc_cc_t;
+
+DWC_CIRCLEQ_HEAD(context_list, dwc_cc);
+
+/** The main structure for CC management.  */
+struct dwc_cc_if
+{
+	dwc_mutex_t *mutex;
+	char *filename;
+
+	unsigned is_host:1;
+
+	dwc_notifier_t *notifier;
+
+	struct context_list list;
+};
+
+#ifdef DEBUG
+static inline void dump_bytes(char *name, uint8_t *bytes, int len)
+{
+	int i;
+	DWC_PRINTF("%s: ", name);
+	for (i=0; i<len; i++) {
+		DWC_PRINTF("%02x ", bytes[i]);
+	}
+	DWC_PRINTF("\n");
+}
+#else
+#define dump_bytes(x...)
+#endif
+
+static dwc_cc_t *alloc_cc(void *mem_ctx, uint8_t *name, uint32_t length)
+{
+	dwc_cc_t *cc = dwc_alloc(mem_ctx, sizeof(dwc_cc_t));
+	if (!cc) {
+		return NULL;
+	}
+	DWC_MEMSET(cc, 0, sizeof(dwc_cc_t));
+
+	if (name) {
+		cc->length = length;
+		cc->name = dwc_alloc(mem_ctx, length);
+		if (!cc->name) {
+			dwc_free(mem_ctx, cc);
+			return NULL;
+		}
+
+		DWC_MEMCPY(cc->name, name, length);
+	}
+
+	return cc;
+}
+
+static void free_cc(void *mem_ctx, dwc_cc_t *cc)
+{
+	if (cc->name) {
+		dwc_free(mem_ctx, cc->name);
+	}
+	dwc_free(mem_ctx, cc);
+}
+
+static uint32_t next_uid(dwc_cc_if_t *cc_if)
+{
+	uint32_t uid = 0;
+	dwc_cc_t *cc;
+	DWC_CIRCLEQ_FOREACH(cc, &cc_if->list, list_entry) {
+		if (cc->uid > uid) {
+			uid = cc->uid;
+		}
+	}
+
+	if (uid == 0) {
+		uid = 255;
+	}
+
+	return uid + 1;
+}
+
+static dwc_cc_t *cc_find(dwc_cc_if_t *cc_if, uint32_t uid)
+{
+	dwc_cc_t *cc;
+	DWC_CIRCLEQ_FOREACH(cc, &cc_if->list, list_entry) {
+		if (cc->uid == uid) {
+			return cc;
+		}
+	}
+	return NULL;
+}
+
+static unsigned int cc_data_size(dwc_cc_if_t *cc_if)
+{
+	unsigned int size = 0;
+	dwc_cc_t *cc;
+	DWC_CIRCLEQ_FOREACH(cc, &cc_if->list, list_entry) {
+		size += (48 + 1);
+		if (cc->name) {
+			size += cc->length;
+		}
+	}
+	return size;
+}
+
+static uint32_t cc_match_chid(dwc_cc_if_t *cc_if, uint8_t *chid)
+{
+	uint32_t uid = 0;
+	dwc_cc_t *cc;
+
+	DWC_CIRCLEQ_FOREACH(cc, &cc_if->list, list_entry) {
+		if (DWC_MEMCMP(cc->chid, chid, 16) == 0) {
+			uid = cc->uid;
+			break;
+		}
+	}
+	return uid;
+}
+static uint32_t cc_match_cdid(dwc_cc_if_t *cc_if, uint8_t *cdid)
+{
+	uint32_t uid = 0;
+	dwc_cc_t *cc;
+
+	DWC_CIRCLEQ_FOREACH(cc, &cc_if->list, list_entry) {
+		if (DWC_MEMCMP(cc->cdid, cdid, 16) == 0) {
+			uid = cc->uid;
+			break;
+		}
+	}
+	return uid;
+}
+
+/* Internal cc_add */
+static int32_t cc_add(void *mem_ctx, dwc_cc_if_t *cc_if, uint8_t *chid,
+		      uint8_t *cdid, uint8_t *ck, uint8_t *name, uint8_t length)
+{
+	dwc_cc_t *cc;
+	uint32_t uid;
+
+	if (cc_if->is_host) {
+		uid = cc_match_cdid(cc_if, cdid);
+	}
+	else {
+		uid = cc_match_chid(cc_if, chid);
+	}
+
+	if (uid) {
+		DWC_DEBUG("Replacing previous connection context id=%d name=%p name_len=%d", uid, name, length);
+		cc = cc_find(cc_if, uid);
+	}
+	else {
+		cc = alloc_cc(mem_ctx, name, length);
+		cc->uid = next_uid(cc_if);
+		DWC_CIRCLEQ_INSERT_TAIL(&cc_if->list, cc, list_entry);
+	}
+
+	DWC_MEMCPY(&(cc->chid[0]), chid, 16);
+	DWC_MEMCPY(&(cc->cdid[0]), cdid, 16);
+	DWC_MEMCPY(&(cc->ck[0]), ck, 16);
+
+	DWC_DEBUG("Added connection context id=%d name=%p name_len=%d", cc->uid, name, length);
+	dump_bytes("CHID", cc->chid, 16);
+	dump_bytes("CDID", cc->cdid, 16);
+	dump_bytes("CK", cc->ck, 16);
+	return cc->uid;
+}
+
+/* Internal cc_clear */
+static void cc_clear(void *mem_ctx, dwc_cc_if_t *cc_if)
+{
+	while (!DWC_CIRCLEQ_EMPTY(&cc_if->list)) {
+		dwc_cc_t *cc = DWC_CIRCLEQ_FIRST(&cc_if->list);
+		DWC_CIRCLEQ_REMOVE_INIT(&cc_if->list, cc, list_entry);
+		free_cc(mem_ctx, cc);
+	}
+}
+
+dwc_cc_if_t *dwc_cc_if_alloc(void *mem_ctx, void *mtx_ctx, 
+			     dwc_notifier_t *notifier, unsigned is_host)
+{
+	dwc_cc_if_t *cc_if = NULL;
+
+	/* Allocate a common_cc_if structure */
+	cc_if = dwc_alloc(mem_ctx, sizeof(dwc_cc_if_t));
+
+	if (!cc_if)
+		return NULL;
+
+#if (defined(DWC_LINUX) && defined(CONFIG_DEBUG_MUTEXES))
+	DWC_MUTEX_ALLOC_LINUX_DEBUG(cc_if->mutex);
+#else
+	cc_if->mutex = dwc_mutex_alloc(mtx_ctx);
+#endif
+	if (!cc_if->mutex) {
+		dwc_free(mem_ctx, cc_if);
+		return NULL;
+	}
+
+	DWC_CIRCLEQ_INIT(&cc_if->list);
+	cc_if->is_host = is_host;
+	cc_if->notifier = notifier;
+	return cc_if;
+}
+
+void dwc_cc_if_free(void *mem_ctx, void *mtx_ctx, dwc_cc_if_t *cc_if)
+{
+#if (defined(DWC_LINUX) && defined(CONFIG_DEBUG_MUTEXES))
+	DWC_MUTEX_FREE(cc_if->mutex);
+#else
+	dwc_mutex_free(mtx_ctx, cc_if->mutex);
+#endif
+	cc_clear(mem_ctx, cc_if);
+	dwc_free(mem_ctx, cc_if);
+}
+
+static void cc_changed(dwc_cc_if_t *cc_if)
+{
+	if (cc_if->notifier) {
+		dwc_notify(cc_if->notifier, DWC_CC_LIST_CHANGED_NOTIFICATION, cc_if);
+	}
+}
+
+void dwc_cc_clear(void *mem_ctx, dwc_cc_if_t *cc_if)
+{
+	DWC_MUTEX_LOCK(cc_if->mutex);
+	cc_clear(mem_ctx, cc_if);
+	DWC_MUTEX_UNLOCK(cc_if->mutex);
+	cc_changed(cc_if);
+}
+
+int32_t dwc_cc_add(void *mem_ctx, dwc_cc_if_t *cc_if, uint8_t *chid,
+		   uint8_t *cdid, uint8_t *ck, uint8_t *name, uint8_t length)
+{
+	uint32_t uid;
+
+	DWC_MUTEX_LOCK(cc_if->mutex);
+	uid = cc_add(mem_ctx, cc_if, chid, cdid, ck, name, length);
+	DWC_MUTEX_UNLOCK(cc_if->mutex);
+	cc_changed(cc_if);
+
+	return uid;
+}
+
+void dwc_cc_change(void *mem_ctx, dwc_cc_if_t *cc_if, int32_t id, uint8_t *chid,
+		   uint8_t *cdid, uint8_t *ck, uint8_t *name, uint8_t length)
+{
+	dwc_cc_t* cc;
+
+	DWC_DEBUG("Change connection context %d", id);
+
+	DWC_MUTEX_LOCK(cc_if->mutex);
+	cc = cc_find(cc_if, id);
+	if (!cc) {
+		DWC_ERROR("Uid %d not found in cc list\n", id);
+		DWC_MUTEX_UNLOCK(cc_if->mutex);
+		return;
+	}
+
+	if (chid) {
+		DWC_MEMCPY(&(cc->chid[0]), chid, 16);
+	}
+	if (cdid) {
+		DWC_MEMCPY(&(cc->cdid[0]), cdid, 16);
+	}
+	if (ck) {
+		DWC_MEMCPY(&(cc->ck[0]), ck, 16);
+	}
+
+	if (name) {
+		if (cc->name) {
+			dwc_free(mem_ctx, cc->name);
+		}
+		cc->name = dwc_alloc(mem_ctx, length);
+		if (!cc->name) {
+			DWC_ERROR("Out of memory in dwc_cc_change()\n");
+			DWC_MUTEX_UNLOCK(cc_if->mutex);
+			return;
+		}
+		cc->length = length;
+		DWC_MEMCPY(cc->name, name, length);
+	}
+
+	DWC_MUTEX_UNLOCK(cc_if->mutex);
+
+	cc_changed(cc_if);
+
+	DWC_DEBUG("Changed connection context id=%d\n", id);
+	dump_bytes("New CHID", cc->chid, 16);
+	dump_bytes("New CDID", cc->cdid, 16);
+	dump_bytes("New CK", cc->ck, 16);
+}
+
+void dwc_cc_remove(void *mem_ctx, dwc_cc_if_t *cc_if, int32_t id)
+{
+	dwc_cc_t *cc;
+
+	DWC_DEBUG("Removing connection context %d", id);
+
+	DWC_MUTEX_LOCK(cc_if->mutex);
+	cc = cc_find(cc_if, id);
+	if (!cc) {
+		DWC_ERROR("Uid %d not found in cc list\n", id);
+		DWC_MUTEX_UNLOCK(cc_if->mutex);
+		return;
+	}
+
+	DWC_CIRCLEQ_REMOVE_INIT(&cc_if->list, cc, list_entry);
+	DWC_MUTEX_UNLOCK(cc_if->mutex);
+	free_cc(mem_ctx, cc);
+
+	cc_changed(cc_if);
+}
+
+uint8_t *dwc_cc_data_for_save(void *mem_ctx, dwc_cc_if_t *cc_if, unsigned int *length)
+{
+	uint8_t *buf, *x;
+	uint8_t zero = 0;
+	dwc_cc_t *cc;
+
+	DWC_MUTEX_LOCK(cc_if->mutex);
+	*length = cc_data_size(cc_if);
+	if (!(*length)) {
+		DWC_MUTEX_UNLOCK(cc_if->mutex);
+		return NULL;
+	}
+
+	DWC_DEBUG("Creating data for saving (length=%d)", *length);
+
+	buf = dwc_alloc(mem_ctx, *length);
+	if (!buf) {
+		*length = 0;
+		DWC_MUTEX_UNLOCK(cc_if->mutex);
+		return NULL;
+	}
+
+	x = buf;
+	DWC_CIRCLEQ_FOREACH(cc, &cc_if->list, list_entry) {
+		DWC_MEMCPY(x, cc->chid, 16);
+		x += 16;
+		DWC_MEMCPY(x, cc->cdid, 16);
+		x += 16;
+		DWC_MEMCPY(x, cc->ck, 16);
+		x += 16;
+		if (cc->name) {
+			DWC_MEMCPY(x, &cc->length, 1);
+			x += 1;
+			DWC_MEMCPY(x, cc->name, cc->length);
+			x += cc->length;
+		}
+		else {
+			DWC_MEMCPY(x, &zero, 1);
+			x += 1;
+		}
+	}
+	DWC_MUTEX_UNLOCK(cc_if->mutex);
+
+	return buf;
+}
+
+void dwc_cc_restore_from_data(void *mem_ctx, dwc_cc_if_t *cc_if, uint8_t *data, uint32_t length)
+{
+	uint8_t name_length;
+	uint8_t *name;
+	uint8_t *chid;
+	uint8_t *cdid;
+	uint8_t *ck;
+	uint32_t i = 0;
+
+	DWC_MUTEX_LOCK(cc_if->mutex);
+	cc_clear(mem_ctx, cc_if);
+
+	while (i < length) {
+		chid = &data[i];
+		i += 16;
+		cdid = &data[i];
+		i += 16;
+		ck = &data[i];
+		i += 16;
+
+		name_length = data[i];
+		i ++;
+
+		if (name_length) {
+			name = &data[i];
+			i += name_length;
+		}
+		else {
+			name = NULL;
+		}
+
+		/* check to see if we haven't overflown the buffer */
+		if (i > length) {
+			DWC_ERROR("Data format error while attempting to load CCs "
+				  "(nlen=%d, iter=%d, buflen=%d).\n", name_length, i, length);
+			break;
+		}
+
+		cc_add(mem_ctx, cc_if, chid, cdid, ck, name, name_length);
+	}
+	DWC_MUTEX_UNLOCK(cc_if->mutex);
+
+	cc_changed(cc_if);
+}
+
+uint32_t dwc_cc_match_chid(dwc_cc_if_t *cc_if, uint8_t *chid)
+{
+	uint32_t uid = 0;
+
+	DWC_MUTEX_LOCK(cc_if->mutex);
+	uid = cc_match_chid(cc_if, chid);
+	DWC_MUTEX_UNLOCK(cc_if->mutex);
+	return uid;
+}
+uint32_t dwc_cc_match_cdid(dwc_cc_if_t *cc_if, uint8_t *cdid)
+{
+	uint32_t uid = 0;
+
+	DWC_MUTEX_LOCK(cc_if->mutex);
+	uid = cc_match_cdid(cc_if, cdid);
+	DWC_MUTEX_UNLOCK(cc_if->mutex);
+	return uid;
+}
+
+uint8_t *dwc_cc_ck(dwc_cc_if_t *cc_if, int32_t id)
+{
+	uint8_t *ck = NULL;
+	dwc_cc_t *cc;
+
+	DWC_MUTEX_LOCK(cc_if->mutex);
+	cc = cc_find(cc_if, id);
+	if (cc) {
+		ck = cc->ck;
+	}
+	DWC_MUTEX_UNLOCK(cc_if->mutex);
+
+	return ck;
+
+}
+
+uint8_t *dwc_cc_chid(dwc_cc_if_t *cc_if, int32_t id)
+{
+	uint8_t *retval = NULL;
+	dwc_cc_t *cc;
+
+	DWC_MUTEX_LOCK(cc_if->mutex);
+	cc = cc_find(cc_if, id);
+	if (cc) {
+		retval = cc->chid;
+	}
+	DWC_MUTEX_UNLOCK(cc_if->mutex);
+
+	return retval;
+}
+
+uint8_t *dwc_cc_cdid(dwc_cc_if_t *cc_if, int32_t id)
+{
+	uint8_t *retval = NULL;
+	dwc_cc_t *cc;
+
+	DWC_MUTEX_LOCK(cc_if->mutex);
+	cc = cc_find(cc_if, id);
+	if (cc) {
+		retval = cc->cdid;
+	}
+	DWC_MUTEX_UNLOCK(cc_if->mutex);
+
+	return retval;
+}
+
+uint8_t *dwc_cc_name(dwc_cc_if_t *cc_if, int32_t id, uint8_t *length)
+{
+	uint8_t *retval = NULL;
+	dwc_cc_t *cc;
+
+	DWC_MUTEX_LOCK(cc_if->mutex);
+	*length = 0;
+	cc = cc_find(cc_if, id);
+	if (cc) {
+		*length = cc->length;
+		retval = cc->name;
+	}
+	DWC_MUTEX_UNLOCK(cc_if->mutex);
+
+	return retval;
+}
+
+#endif	/* DWC_CCLIB */
diff --git a/drivers/usb/dwc_otg/dwc_cc.h b/drivers/usb/dwc_otg/dwc_cc.h
new file mode 100644
index 0000000..6b2bbaf
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_cc.h
@@ -0,0 +1,225 @@
+/* =========================================================================
+ * $File: //dwh/usb_iip/dev/software/dwc_common_port_2/dwc_cc.h $
+ * $Revision: #4 $
+ * $Date: 2010/09/28 $
+ * $Change: 1596182 $
+ *
+ * Synopsys Portability Library Software and documentation
+ * (hereinafter, "Software") is an Unsupported proprietary work of
+ * Synopsys, Inc. unless otherwise expressly agreed to in writing
+ * between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product
+ * under any End User Software License Agreement or Agreement for
+ * Licensed Product with Synopsys or any supplement thereto. You are
+ * permitted to use and redistribute this Software in source and binary
+ * forms, with or without modification, provided that redistributions
+ * of source code must retain this notice. You may not view, use,
+ * disclose, copy or distribute this file or any information contained
+ * herein except pursuant to this license grant from Synopsys. If you
+ * do not agree with this notice, including the disclaimer below, then
+ * you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS"
+ * BASIS AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+ * FOR A PARTICULAR PURPOSE ARE HEREBY DISCLAIMED. IN NO EVENT SHALL
+ * SYNOPSYS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
+ * USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================= */
+#ifndef _DWC_CC_H_
+#define _DWC_CC_H_
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/** @file
+ *
+ * This file defines the Context Context library.
+ *
+ * The main data structure is dwc_cc_if_t which is returned by either the
+ * dwc_cc_if_alloc function or returned by the module to the user via a provided
+ * function. The data structure is opaque and should only be manipulated via the
+ * functions provied in this API.
+ *
+ * It manages a list of connection contexts and operations can be performed to
+ * add, remove, query, search, and change, those contexts.  Additionally,
+ * a dwc_notifier_t object can be requested from the manager so that
+ * the user can be notified whenever the context list has changed.
+ */
+
+#include "dwc_os.h"
+#include "dwc_list.h"
+#include "dwc_notifier.h"
+
+
+/* Notifications */
+#define DWC_CC_LIST_CHANGED_NOTIFICATION "DWC_CC_LIST_CHANGED_NOTIFICATION"
+
+struct dwc_cc_if;
+typedef struct dwc_cc_if dwc_cc_if_t;
+
+
+/** @name Connection Context Operations */
+/** @{ */
+
+/** This function allocates memory for a dwc_cc_if_t structure, initializes
+ * fields to default values, and returns a pointer to the structure or NULL on
+ * error. */
+extern dwc_cc_if_t *dwc_cc_if_alloc(void *mem_ctx, void *mtx_ctx,
+				    dwc_notifier_t *notifier, unsigned is_host);
+
+/** Frees the memory for the specified CC structure allocated from
+ * dwc_cc_if_alloc(). */
+extern void dwc_cc_if_free(void *mem_ctx, void *mtx_ctx, dwc_cc_if_t *cc_if);
+
+/** Removes all contexts from the connection context list */
+extern void dwc_cc_clear(void *mem_ctx, dwc_cc_if_t *cc_if);
+
+/** Adds a connection context (CHID, CK, CDID, Name) to the connection context list.
+ * If a CHID already exists, the CK and name are overwritten.  Statistics are
+ * not overwritten.
+ *
+ * @param cc_if The cc_if structure.
+ * @param chid A pointer to the 16-byte CHID.  This value will be copied.
+ * @param ck A pointer to the 16-byte CK.  This value will be copied.
+ * @param cdid A pointer to the 16-byte CDID.  This value will be copied.
+ * @param name An optional host friendly name as defined in the association model
+ * spec.  Must be a UTF16-LE unicode string.  Can be NULL to indicated no name.
+ * @param length The length othe unicode string.
+ * @return A unique identifier used to refer to this context that is valid for
+ * as long as this context is still in the list. */
+extern int32_t dwc_cc_add(void *mem_ctx, dwc_cc_if_t *cc_if, uint8_t *chid,
+			  uint8_t *cdid, uint8_t *ck, uint8_t *name,
+			  uint8_t length);
+
+/** Changes the CHID, CK, CDID, or Name values of a connection context in the
+ * list, preserving any accumulated statistics.  This would typically be called
+ * if the host decideds to change the context with a SET_CONNECTION request.
+ *
+ * @param cc_if The cc_if structure.
+ * @param id The identifier of the connection context.
+ * @param chid A pointer to the 16-byte CHID.  This value will be copied.  NULL
+ * indicates no change.
+ * @param cdid A pointer to the 16-byte CDID.  This value will be copied.  NULL
+ * indicates no change.
+ * @param ck A pointer to the 16-byte CK.  This value will be copied.  NULL
+ * indicates no change.
+ * @param name Host friendly name UTF16-LE.  NULL indicates no change.
+ * @param length Length of name. */
+extern void dwc_cc_change(void *mem_ctx, dwc_cc_if_t *cc_if, int32_t id,
+			  uint8_t *chid, uint8_t *cdid, uint8_t *ck,
+			  uint8_t *name, uint8_t length);
+
+/** Remove the specified connection context.
+ * @param cc_if The cc_if structure.
+ * @param id The identifier of the connection context to remove. */
+extern void dwc_cc_remove(void *mem_ctx, dwc_cc_if_t *cc_if, int32_t id);
+
+/** Get a binary block of data for the connection context list and attributes.
+ * This data can be used by the OS specific driver to save the connection
+ * context list into non-volatile memory.
+ *
+ * @param cc_if The cc_if structure.
+ * @param length Return the length of the data buffer.
+ * @return A pointer to the data buffer.  The memory for this buffer should be
+ * freed with DWC_FREE() after use. */
+extern uint8_t *dwc_cc_data_for_save(void *mem_ctx, dwc_cc_if_t *cc_if,
+				     unsigned int *length);
+
+/** Restore the connection context list from the binary data that was previously
+ * returned from a call to dwc_cc_data_for_save.  This can be used by the OS specific
+ * driver to load a connection context list from non-volatile memory.
+ *
+ * @param cc_if The cc_if structure.
+ * @param data The data bytes as returned from dwc_cc_data_for_save.
+ * @param length The length of the data. */
+extern void dwc_cc_restore_from_data(void *mem_ctx, dwc_cc_if_t *cc_if,
+				     uint8_t *data, unsigned int length);
+
+/** Find the connection context from the specified CHID.
+ *
+ * @param cc_if The cc_if structure.
+ * @param chid A pointer to the CHID data.
+ * @return A non-zero identifier of the connection context if the CHID matches.
+ * Otherwise returns 0. */
+extern uint32_t dwc_cc_match_chid(dwc_cc_if_t *cc_if, uint8_t *chid);
+
+/** Find the connection context from the specified CDID.
+ *
+ * @param cc_if The cc_if structure.
+ * @param cdid A pointer to the CDID data.
+ * @return A non-zero identifier of the connection context if the CHID matches.
+ * Otherwise returns 0. */
+extern uint32_t dwc_cc_match_cdid(dwc_cc_if_t *cc_if, uint8_t *cdid);
+
+/** Retrieve the CK from the specified connection context.
+ *
+ * @param cc_if The cc_if structure.
+ * @param id The identifier of the connection context.
+ * @return A pointer to the CK data.  The memory does not need to be freed. */
+extern uint8_t *dwc_cc_ck(dwc_cc_if_t *cc_if, int32_t id);
+
+/** Retrieve the CHID from the specified connection context.
+ *
+ * @param cc_if The cc_if structure.
+ * @param id The identifier of the connection context.
+ * @return A pointer to the CHID data.  The memory does not need to be freed. */
+extern uint8_t *dwc_cc_chid(dwc_cc_if_t *cc_if, int32_t id);
+
+/** Retrieve the CDID from the specified connection context.
+ *
+ * @param cc_if The cc_if structure.
+ * @param id The identifier of the connection context.
+ * @return A pointer to the CDID data.  The memory does not need to be freed. */
+extern uint8_t *dwc_cc_cdid(dwc_cc_if_t *cc_if, int32_t id);
+
+extern uint8_t *dwc_cc_name(dwc_cc_if_t *cc_if, int32_t id, uint8_t *length);
+
+/** Checks a buffer for non-zero.
+ * @param id A pointer to a 16 byte buffer. 
+ * @return true if the 16 byte value is non-zero. */
+static inline unsigned dwc_assoc_is_not_zero_id(uint8_t *id) {
+	int i;
+	for (i=0; i<16; i++) {
+		if (id[i]) return 1;
+	}
+	return 0;
+}
+
+/** Checks a buffer for zero.
+ * @param id A pointer to a 16 byte buffer. 
+ * @return true if the 16 byte value is zero. */
+static inline unsigned dwc_assoc_is_zero_id(uint8_t *id) {
+	return !dwc_assoc_is_not_zero_id(id);
+}
+
+/** Prints an ASCII representation for the 16-byte chid, cdid, or ck, into
+ * buffer. */
+static inline int dwc_print_id_string(char *buffer, uint8_t *id) {
+	char *ptr = buffer;
+	int i;
+	for (i=0; i<16; i++) {
+		ptr += DWC_SPRINTF(ptr, "%02x", id[i]);
+		if (i < 15) {
+			ptr += DWC_SPRINTF(ptr, " ");
+		}
+	}
+	return ptr - buffer;
+}
+
+/** @} */
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* _DWC_CC_H_ */
+
diff --git a/drivers/usb/dwc_otg/dwc_cfi_common.h b/drivers/usb/dwc_otg/dwc_cfi_common.h
new file mode 100644
index 0000000..be56af4
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_cfi_common.h
@@ -0,0 +1,142 @@
+/* ==========================================================================
+ * Synopsys HS OTG Linux Software Driver and documentation (hereinafter,
+ * "Software") is an Unsupported proprietary work of Synopsys, Inc. unless
+ * otherwise expressly agreed to in writing between Synopsys and you.
+ * 
+ * The Software IS NOT an item of Licensed Software or Licensed Product under
+ * any End User Software License Agreement or Agreement for Licensed Product
+ * with Synopsys or any supplement thereto. You are permitted to use and
+ * redistribute this Software in source and binary forms, with or without
+ * modification, provided that redistributions of source code must retain this
+ * notice. You may not view, use, disclose, copy or distribute this file or
+ * any information contained herein except pursuant to this license grant from
+ * Synopsys. If you do not agree with this notice, including the disclaimer
+ * below, then you are not authorized to use the Software.
+ * 
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS" BASIS
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE HEREBY DISCLAIMED. IN NO EVENT SHALL SYNOPSYS BE LIABLE FOR ANY DIRECT,
+ * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================== */
+
+#if !defined(__DWC_CFI_COMMON_H__)
+#define __DWC_CFI_COMMON_H__
+
+//#include <linux/types.h>
+
+/**
+ * @file 
+ *
+ * This file contains the CFI specific common constants, interfaces
+ * (functions and macros) and structures for Linux. No PCD specific
+ * data structure or definition is to be included in this file.
+ *
+ */
+
+/** This is a request for all Core Features */
+#define VEN_CORE_GET_FEATURES		0xB1
+
+/** This is a request to get the value of a specific Core Feature */
+#define VEN_CORE_GET_FEATURE		0xB2
+
+/** This command allows the host to set the value of a specific Core Feature */
+#define VEN_CORE_SET_FEATURE		0xB3
+
+/** This command allows the host to set the default values of 
+ * either all or any specific Core Feature 
+ */
+#define VEN_CORE_RESET_FEATURES		0xB4
+
+/** This command forces the PCD to write the deferred values of a Core Features */
+#define VEN_CORE_ACTIVATE_FEATURES	0xB5
+
+/** This request reads a DWORD value from a register at the specified offset */
+#define VEN_CORE_READ_REGISTER		0xB6
+
+/** This request writes a DWORD value into a register at the specified offset */
+#define VEN_CORE_WRITE_REGISTER		0xB7
+
+/** This structure is the header of the Core Features dataset returned to 
+ *  the Host
+ */
+struct cfi_all_features_header {
+/** The features header structure length is */
+#define CFI_ALL_FEATURES_HDR_LEN		8
+	/**
+	 * The total length of the features dataset returned to the Host 
+	 */
+	uint16_t wTotalLen;
+
+	/**
+	 * CFI version number inBinary-Coded Decimal (i.e., 1.00 is 100H).
+	 * This field identifies the version of the CFI Specification with which 
+	 * the device is compliant.
+	 */
+	uint16_t wVersion;
+
+	/** The ID of the Core */
+	uint16_t wCoreID;
+#define CFI_CORE_ID_UDC		1
+#define CFI_CORE_ID_OTG		2
+#define CFI_CORE_ID_WUDEV	3
+
+	/** Number of features returned by VEN_CORE_GET_FEATURES request */
+	uint16_t wNumFeatures;
+} UPACKED;
+
+typedef struct cfi_all_features_header cfi_all_features_header_t;
+
+/** This structure is a header of the Core Feature descriptor dataset returned to 
+ *  the Host after the VEN_CORE_GET_FEATURES request
+ */
+struct cfi_feature_desc_header {
+#define CFI_FEATURE_DESC_HDR_LEN	8
+
+	/** The feature ID */
+	uint16_t wFeatureID;
+
+	/** Length of this feature descriptor in bytes - including the
+	 * length of the feature name string
+	 */
+	uint16_t wLength;
+
+	/** The data length of this feature in bytes */
+	uint16_t wDataLength;
+
+	/** 
+	 * Attributes of this features 
+	 * D0: Access rights
+	 * 0 - Read/Write
+	 * 1 - Read only
+	 */
+	uint8_t bmAttributes;
+#define CFI_FEATURE_ATTR_RO		1
+#define CFI_FEATURE_ATTR_RW		0
+
+	/** Length of the feature name in bytes */
+	uint8_t bNameLen;
+
+	/** The feature name buffer */
+	//uint8_t *name;
+} UPACKED;
+
+typedef struct cfi_feature_desc_header cfi_feature_desc_header_t;
+
+/**
+ * This structure describes a NULL terminated string referenced by its id field.
+ * It is very similar to usb_string structure but has the id field type set to 16-bit.
+ */
+struct cfi_string {
+	uint16_t id;
+	const uint8_t *s;
+};
+typedef struct cfi_string cfi_string_t;
+
+#endif
diff --git a/drivers/usb/dwc_otg/dwc_common_fbsd.c b/drivers/usb/dwc_otg/dwc_common_fbsd.c
new file mode 100644
index 0000000..6dd04b5
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_common_fbsd.c
@@ -0,0 +1,1308 @@
+#include "dwc_os.h"
+#include "dwc_list.h"
+
+#ifdef DWC_CCLIB
+# include "dwc_cc.h"
+#endif
+
+#ifdef DWC_CRYPTOLIB
+# include "dwc_modpow.h"
+# include "dwc_dh.h"
+# include "dwc_crypto.h"
+#endif
+
+#ifdef DWC_NOTIFYLIB
+# include "dwc_notifier.h"
+#endif
+
+/* OS-Level Implementations */
+
+/* This is the FreeBSD 7.0 kernel implementation of the DWC platform library. */
+
+
+/* MISC */
+
+void *DWC_MEMSET(void *dest, uint8_t byte, uint32_t size)
+{
+	return memset(dest, byte, size);
+}
+
+void *DWC_MEMCPY(void *dest, void const *src, uint32_t size)
+{
+	return memcpy(dest, src, size);
+}
+
+void *DWC_MEMMOVE(void *dest, void *src, uint32_t size)
+{
+	bcopy(src, dest, size);
+	return dest;
+}
+
+int DWC_MEMCMP(void *m1, void *m2, uint32_t size)
+{
+	return memcmp(m1, m2, size);
+}
+
+int DWC_STRNCMP(void *s1, void *s2, uint32_t size)
+{
+	return strncmp(s1, s2, size);
+}
+
+int DWC_STRCMP(void *s1, void *s2)
+{
+	return strcmp(s1, s2);
+}
+
+int DWC_STRLEN(char const *str)
+{
+	return strlen(str);
+}
+
+char *DWC_STRCPY(char *to, char const *from)
+{
+	return strcpy(to, from);
+}
+
+char *DWC_STRDUP(char const *str)
+{
+	int len = DWC_STRLEN(str) + 1;
+	char *new = DWC_ALLOC_ATOMIC(len);
+
+	if (!new) {
+		return NULL;
+	}
+
+	DWC_MEMCPY(new, str, len);
+	return new;
+}
+
+int DWC_ATOI(char *str, int32_t *value)
+{
+	char *end = NULL;
+
+	*value = strtol(str, &end, 0);
+	if (*end == '\0') {
+		return 0;
+	}
+
+	return -1;
+}
+
+int DWC_ATOUI(char *str, uint32_t *value)
+{
+	char *end = NULL;
+
+	*value = strtoul(str, &end, 0);
+	if (*end == '\0') {
+		return 0;
+	}
+
+	return -1;
+}
+
+
+#ifdef DWC_UTFLIB
+/* From usbstring.c */
+
+int DWC_UTF8_TO_UTF16LE(uint8_t const *s, uint16_t *cp, unsigned len)
+{
+	int	count = 0;
+	u8	c;
+	u16	uchar;
+
+	/* this insists on correct encodings, though not minimal ones.
+	 * BUT it currently rejects legit 4-byte UTF-8 code points,
+	 * which need surrogate pairs.  (Unicode 3.1 can use them.)
+	 */
+	while (len != 0 && (c = (u8) *s++) != 0) {
+		if (unlikely(c & 0x80)) {
+			// 2-byte sequence:
+			// 00000yyyyyxxxxxx = 110yyyyy 10xxxxxx
+			if ((c & 0xe0) == 0xc0) {
+				uchar = (c & 0x1f) << 6;
+
+				c = (u8) *s++;
+				if ((c & 0xc0) != 0xc0)
+					goto fail;
+				c &= 0x3f;
+				uchar |= c;
+
+			// 3-byte sequence (most CJKV characters):
+			// zzzzyyyyyyxxxxxx = 1110zzzz 10yyyyyy 10xxxxxx
+			} else if ((c & 0xf0) == 0xe0) {
+				uchar = (c & 0x0f) << 12;
+
+				c = (u8) *s++;
+				if ((c & 0xc0) != 0xc0)
+					goto fail;
+				c &= 0x3f;
+				uchar |= c << 6;
+
+				c = (u8) *s++;
+				if ((c & 0xc0) != 0xc0)
+					goto fail;
+				c &= 0x3f;
+				uchar |= c;
+
+				/* no bogus surrogates */
+				if (0xd800 <= uchar && uchar <= 0xdfff)
+					goto fail;
+
+			// 4-byte sequence (surrogate pairs, currently rare):
+			// 11101110wwwwzzzzyy + 110111yyyyxxxxxx
+			//     = 11110uuu 10uuzzzz 10yyyyyy 10xxxxxx
+			// (uuuuu = wwww + 1)
+			// FIXME accept the surrogate code points (only)
+			} else
+				goto fail;
+		} else
+			uchar = c;
+		put_unaligned (cpu_to_le16 (uchar), cp++);
+		count++;
+		len--;
+	}
+	return count;
+fail:
+	return -1;
+}
+
+#endif	/* DWC_UTFLIB */
+
+
+/* dwc_debug.h */
+
+dwc_bool_t DWC_IN_IRQ(void)
+{
+//	return in_irq();
+	return 0;
+}
+
+dwc_bool_t DWC_IN_BH(void)
+{
+//	return in_softirq();
+	return 0;
+}
+
+void DWC_VPRINTF(char *format, va_list args)
+{
+	vprintf(format, args);
+}
+
+int DWC_VSNPRINTF(char *str, int size, char *format, va_list args)
+{
+	return vsnprintf(str, size, format, args);
+}
+
+void DWC_PRINTF(char *format, ...)
+{
+	va_list args;
+
+	va_start(args, format);
+	DWC_VPRINTF(format, args);
+	va_end(args);
+}
+
+int DWC_SPRINTF(char *buffer, char *format, ...)
+{
+	int retval;
+	va_list args;
+
+	va_start(args, format);
+	retval = vsprintf(buffer, format, args);
+	va_end(args);
+	return retval;
+}
+
+int DWC_SNPRINTF(char *buffer, int size, char *format, ...)
+{
+	int retval;
+	va_list args;
+
+	va_start(args, format);
+	retval = vsnprintf(buffer, size, format, args);
+	va_end(args);
+	return retval;
+}
+
+void __DWC_WARN(char *format, ...)
+{
+	va_list args;
+
+	va_start(args, format);
+	DWC_VPRINTF(format, args);
+	va_end(args);
+}
+
+void __DWC_ERROR(char *format, ...)
+{
+	va_list args;
+
+	va_start(args, format);
+	DWC_VPRINTF(format, args);
+	va_end(args);
+}
+
+void DWC_EXCEPTION(char *format, ...)
+{
+	va_list args;
+
+	va_start(args, format);
+	DWC_VPRINTF(format, args);
+	va_end(args);
+//	BUG_ON(1);	???
+}
+
+#ifdef DEBUG
+void __DWC_DEBUG(char *format, ...)
+{
+	va_list args;
+
+	va_start(args, format);
+	DWC_VPRINTF(format, args);
+	va_end(args);
+}
+#endif
+
+
+/* dwc_mem.h */
+
+#if 0
+dwc_pool_t *DWC_DMA_POOL_CREATE(uint32_t size,
+				uint32_t align,
+				uint32_t alloc)
+{
+	struct dma_pool *pool = dma_pool_create("Pool", NULL,
+						size, align, alloc);
+	return (dwc_pool_t *)pool;
+}
+
+void DWC_DMA_POOL_DESTROY(dwc_pool_t *pool)
+{
+	dma_pool_destroy((struct dma_pool *)pool);
+}
+
+void *DWC_DMA_POOL_ALLOC(dwc_pool_t *pool, uint64_t *dma_addr)
+{
+//	return dma_pool_alloc((struct dma_pool *)pool, GFP_KERNEL, dma_addr);
+	return dma_pool_alloc((struct dma_pool *)pool, M_WAITOK, dma_addr);
+}
+
+void *DWC_DMA_POOL_ZALLOC(dwc_pool_t *pool, uint64_t *dma_addr)
+{
+	void *vaddr = DWC_DMA_POOL_ALLOC(pool, dma_addr);
+	memset(..);
+}
+
+void DWC_DMA_POOL_FREE(dwc_pool_t *pool, void *vaddr, void *daddr)
+{
+	dma_pool_free(pool, vaddr, daddr);
+}
+#endif
+
+static void dmamap_cb(void *arg, bus_dma_segment_t *segs, int nseg, int error)
+{
+	if (error)
+		return;
+	*(bus_addr_t *)arg = segs[0].ds_addr;
+}
+
+void *__DWC_DMA_ALLOC(void *dma_ctx, uint32_t size, dwc_dma_t *dma_addr)
+{
+	dwc_dmactx_t *dma = (dwc_dmactx_t *)dma_ctx;
+	int error;
+
+	error = bus_dma_tag_create(
+#if __FreeBSD_version >= 700000
+			bus_get_dma_tag(dma->dev),	/* parent */
+#else
+			NULL,				/* parent */
+#endif
+			4, 0,				/* alignment, bounds */
+			BUS_SPACE_MAXADDR_32BIT,	/* lowaddr */
+			BUS_SPACE_MAXADDR,		/* highaddr */
+			NULL, NULL,			/* filter, filterarg */
+			size,				/* maxsize */
+			1,				/* nsegments */
+			size,				/* maxsegsize */
+			0,				/* flags */
+			NULL,				/* lockfunc */
+			NULL,				/* lockarg */
+			&dma->dma_tag);
+	if (error) {
+		device_printf(dma->dev, "%s: bus_dma_tag_create failed: %d\n",
+			      __func__, error);
+		goto fail_0;
+	}
+
+	error = bus_dmamem_alloc(dma->dma_tag, &dma->dma_vaddr,
+				 BUS_DMA_NOWAIT | BUS_DMA_COHERENT, &dma->dma_map);
+	if (error) {
+		device_printf(dma->dev, "%s: bus_dmamem_alloc(%ju) failed: %d\n",
+			      __func__, (uintmax_t)size, error);
+		goto fail_1;
+	}
+
+	dma->dma_paddr = 0;
+	error = bus_dmamap_load(dma->dma_tag, dma->dma_map, dma->dma_vaddr, size,
+				dmamap_cb, &dma->dma_paddr, BUS_DMA_NOWAIT);
+	if (error || dma->dma_paddr == 0) {
+		device_printf(dma->dev, "%s: bus_dmamap_load failed: %d\n",
+			      __func__, error);
+		goto fail_2;
+	}
+
+	*dma_addr = dma->dma_paddr;
+	return dma->dma_vaddr;
+
+fail_2:
+	bus_dmamap_unload(dma->dma_tag, dma->dma_map);
+fail_1:
+	bus_dmamem_free(dma->dma_tag, dma->dma_vaddr, dma->dma_map);
+	bus_dma_tag_destroy(dma->dma_tag);
+fail_0:
+	dma->dma_map = NULL;
+	dma->dma_tag = NULL;
+
+	return NULL;
+}
+
+void __DWC_DMA_FREE(void *dma_ctx, uint32_t size, void *virt_addr, dwc_dma_t dma_addr)
+{
+	dwc_dmactx_t *dma = (dwc_dmactx_t *)dma_ctx;
+
+	if (dma->dma_tag == NULL)
+		return;
+	if (dma->dma_map != NULL) {
+		bus_dmamap_sync(dma->dma_tag, dma->dma_map,
+				BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE);
+		bus_dmamap_unload(dma->dma_tag, dma->dma_map);
+		bus_dmamem_free(dma->dma_tag, dma->dma_vaddr, dma->dma_map);
+		dma->dma_map = NULL;
+	}
+
+	bus_dma_tag_destroy(dma->dma_tag);
+	dma->dma_tag = NULL;
+}
+
+void *__DWC_ALLOC(void *mem_ctx, uint32_t size)
+{
+	return malloc(size, M_DEVBUF, M_WAITOK | M_ZERO);
+}
+
+void *__DWC_ALLOC_ATOMIC(void *mem_ctx, uint32_t size)
+{
+	return malloc(size, M_DEVBUF, M_NOWAIT | M_ZERO);
+}
+
+void __DWC_FREE(void *mem_ctx, void *addr)
+{
+	free(addr, M_DEVBUF);
+}
+
+
+#ifdef DWC_CRYPTOLIB
+/* dwc_crypto.h */
+
+void DWC_RANDOM_BYTES(uint8_t *buffer, uint32_t length)
+{
+	get_random_bytes(buffer, length);
+}
+
+int DWC_AES_CBC(uint8_t *message, uint32_t messagelen, uint8_t *key, uint32_t keylen, uint8_t iv[16], uint8_t *out)
+{
+	struct crypto_blkcipher *tfm;
+	struct blkcipher_desc desc;
+	struct scatterlist sgd;
+	struct scatterlist sgs;
+
+	tfm = crypto_alloc_blkcipher("cbc(aes)", 0, CRYPTO_ALG_ASYNC);
+	if (tfm == NULL) {
+		printk("failed to load transform for aes CBC\n");
+		return -1;
+	}
+
+	crypto_blkcipher_setkey(tfm, key, keylen);
+	crypto_blkcipher_set_iv(tfm, iv, 16);
+
+	sg_init_one(&sgd, out, messagelen);
+	sg_init_one(&sgs, message, messagelen);
+
+	desc.tfm = tfm;
+	desc.flags = 0;
+
+	if (crypto_blkcipher_encrypt(&desc, &sgd, &sgs, messagelen)) {
+		crypto_free_blkcipher(tfm);
+		DWC_ERROR("AES CBC encryption failed");
+		return -1;
+	}
+
+	crypto_free_blkcipher(tfm);
+	return 0;
+}
+
+int DWC_SHA256(uint8_t *message, uint32_t len, uint8_t *out)
+{
+	struct crypto_hash *tfm;
+	struct hash_desc desc;
+	struct scatterlist sg;
+
+	tfm = crypto_alloc_hash("sha256", 0, CRYPTO_ALG_ASYNC);
+	if (IS_ERR(tfm)) {
+		DWC_ERROR("Failed to load transform for sha256: %ld", PTR_ERR(tfm));
+		return 0;
+	}
+	desc.tfm = tfm;
+	desc.flags = 0;
+
+	sg_init_one(&sg, message, len);
+	crypto_hash_digest(&desc, &sg, len, out);
+	crypto_free_hash(tfm);
+
+	return 1;
+}
+
+int DWC_HMAC_SHA256(uint8_t *message, uint32_t messagelen,
+		    uint8_t *key, uint32_t keylen, uint8_t *out)
+{
+	struct crypto_hash *tfm;
+	struct hash_desc desc;
+	struct scatterlist sg;
+
+	tfm = crypto_alloc_hash("hmac(sha256)", 0, CRYPTO_ALG_ASYNC);
+	if (IS_ERR(tfm)) {
+		DWC_ERROR("Failed to load transform for hmac(sha256): %ld", PTR_ERR(tfm));
+		return 0;
+	}
+	desc.tfm = tfm;
+	desc.flags = 0;
+
+	sg_init_one(&sg, message, messagelen);
+	crypto_hash_setkey(tfm, key, keylen);
+	crypto_hash_digest(&desc, &sg, messagelen, out);
+	crypto_free_hash(tfm);
+
+	return 1;
+}
+
+#endif	/* DWC_CRYPTOLIB */
+
+
+/* Byte Ordering Conversions */
+
+uint32_t DWC_CPU_TO_LE32(uint32_t *p)
+{
+#ifdef __LITTLE_ENDIAN
+	return *p;
+#else
+	uint8_t *u_p = (uint8_t *)p;
+
+	return (u_p[3] | (u_p[2] << 8) | (u_p[1] << 16) | (u_p[0] << 24));
+#endif
+}
+
+uint32_t DWC_CPU_TO_BE32(uint32_t *p)
+{
+#ifdef __BIG_ENDIAN
+	return *p;
+#else
+	uint8_t *u_p = (uint8_t *)p;
+
+	return (u_p[3] | (u_p[2] << 8) | (u_p[1] << 16) | (u_p[0] << 24));
+#endif
+}
+
+uint32_t DWC_LE32_TO_CPU(uint32_t *p)
+{
+#ifdef __LITTLE_ENDIAN
+	return *p;
+#else
+	uint8_t *u_p = (uint8_t *)p;
+
+	return (u_p[3] | (u_p[2] << 8) | (u_p[1] << 16) | (u_p[0] << 24));
+#endif
+}
+
+uint32_t DWC_BE32_TO_CPU(uint32_t *p)
+{
+#ifdef __BIG_ENDIAN
+	return *p;
+#else
+	uint8_t *u_p = (uint8_t *)p;
+
+	return (u_p[3] | (u_p[2] << 8) | (u_p[1] << 16) | (u_p[0] << 24));
+#endif
+}
+
+uint16_t DWC_CPU_TO_LE16(uint16_t *p)
+{
+#ifdef __LITTLE_ENDIAN
+	return *p;
+#else
+	uint8_t *u_p = (uint8_t *)p;
+	return (u_p[1] | (u_p[0] << 8));
+#endif
+}
+
+uint16_t DWC_CPU_TO_BE16(uint16_t *p)
+{
+#ifdef __BIG_ENDIAN
+	return *p;
+#else
+	uint8_t *u_p = (uint8_t *)p;
+	return (u_p[1] | (u_p[0] << 8));
+#endif
+}
+
+uint16_t DWC_LE16_TO_CPU(uint16_t *p)
+{
+#ifdef __LITTLE_ENDIAN
+	return *p;
+#else
+	uint8_t *u_p = (uint8_t *)p;
+	return (u_p[1] | (u_p[0] << 8));
+#endif
+}
+
+uint16_t DWC_BE16_TO_CPU(uint16_t *p)
+{
+#ifdef __BIG_ENDIAN
+	return *p;
+#else
+	uint8_t *u_p = (uint8_t *)p;
+	return (u_p[1] | (u_p[0] << 8));
+#endif
+}
+
+
+/* Registers */
+
+uint32_t DWC_READ_REG32(void *io_ctx, uint32_t volatile *reg)
+{
+	dwc_ioctx_t *io = (dwc_ioctx_t *)io_ctx;
+	bus_size_t ior = (bus_size_t)reg;
+
+	return bus_space_read_4(io->iot, io->ioh, ior);
+}
+
+#if 0
+uint64_t DWC_READ_REG64(void *io_ctx, uint64_t volatile *reg)
+{
+	dwc_ioctx_t *io = (dwc_ioctx_t *)io_ctx;
+	bus_size_t ior = (bus_size_t)reg;
+
+	return bus_space_read_8(io->iot, io->ioh, ior);
+}
+#endif
+
+void DWC_WRITE_REG32(void *io_ctx, uint32_t volatile *reg, uint32_t value)
+{
+	dwc_ioctx_t *io = (dwc_ioctx_t *)io_ctx;
+	bus_size_t ior = (bus_size_t)reg;
+
+	bus_space_write_4(io->iot, io->ioh, ior, value);
+}
+
+#if 0
+void DWC_WRITE_REG64(void *io_ctx, uint64_t volatile *reg, uint64_t value)
+{
+	dwc_ioctx_t *io = (dwc_ioctx_t *)io_ctx;
+	bus_size_t ior = (bus_size_t)reg;
+
+	bus_space_write_8(io->iot, io->ioh, ior, value);
+}
+#endif
+
+void DWC_MODIFY_REG32(void *io_ctx, uint32_t volatile *reg, uint32_t clear_mask,
+		      uint32_t set_mask)
+{
+	dwc_ioctx_t *io = (dwc_ioctx_t *)io_ctx;
+	bus_size_t ior = (bus_size_t)reg;
+
+	bus_space_write_4(io->iot, io->ioh, ior,
+			  (bus_space_read_4(io->iot, io->ioh, ior) &
+			   ~clear_mask) | set_mask);
+}
+
+#if 0
+void DWC_MODIFY_REG64(void *io_ctx, uint64_t volatile *reg, uint64_t clear_mask,
+		      uint64_t set_mask)
+{
+	dwc_ioctx_t *io = (dwc_ioctx_t *)io_ctx;
+	bus_size_t ior = (bus_size_t)reg;
+
+	bus_space_write_8(io->iot, io->ioh, ior,
+			  (bus_space_read_8(io->iot, io->ioh, ior) &
+			   ~clear_mask) | set_mask);
+}
+#endif
+
+
+/* Locking */
+
+dwc_spinlock_t *DWC_SPINLOCK_ALLOC(void)
+{
+	struct mtx *sl = DWC_ALLOC(sizeof(*sl));
+
+	if (!sl) {
+		DWC_ERROR("Cannot allocate memory for spinlock");
+		return NULL;
+	}
+
+	mtx_init(sl, "dw3spn", NULL, MTX_SPIN);
+	return (dwc_spinlock_t *)sl;
+}
+
+void DWC_SPINLOCK_FREE(dwc_spinlock_t *lock)
+{
+	struct mtx *sl = (struct mtx *)lock;
+
+	mtx_destroy(sl);
+	DWC_FREE(sl);
+}
+
+void DWC_SPINLOCK(dwc_spinlock_t *lock)
+{
+	mtx_lock_spin((struct mtx *)lock);	// ???
+}
+
+void DWC_SPINUNLOCK(dwc_spinlock_t *lock)
+{
+	mtx_unlock_spin((struct mtx *)lock);	// ???
+}
+
+void DWC_SPINLOCK_IRQSAVE(dwc_spinlock_t *lock, dwc_irqflags_t *flags)
+{
+	mtx_lock_spin((struct mtx *)lock);
+}
+
+void DWC_SPINUNLOCK_IRQRESTORE(dwc_spinlock_t *lock, dwc_irqflags_t flags)
+{
+	mtx_unlock_spin((struct mtx *)lock);
+}
+
+dwc_mutex_t *DWC_MUTEX_ALLOC(void)
+{
+	struct mtx *m;
+	dwc_mutex_t *mutex = (dwc_mutex_t *)DWC_ALLOC(sizeof(struct mtx));
+
+	if (!mutex) {
+		DWC_ERROR("Cannot allocate memory for mutex");
+		return NULL;
+	}
+
+	m = (struct mtx *)mutex;
+	mtx_init(m, "dw3mtx", NULL, MTX_DEF);
+	return mutex;
+}
+
+#if (defined(DWC_LINUX) && defined(CONFIG_DEBUG_MUTEXES))
+#else
+void DWC_MUTEX_FREE(dwc_mutex_t *mutex)
+{
+	mtx_destroy((struct mtx *)mutex);
+	DWC_FREE(mutex);
+}
+#endif
+
+void DWC_MUTEX_LOCK(dwc_mutex_t *mutex)
+{
+	struct mtx *m = (struct mtx *)mutex;
+
+	mtx_lock(m);
+}
+
+int DWC_MUTEX_TRYLOCK(dwc_mutex_t *mutex)
+{
+	struct mtx *m = (struct mtx *)mutex;
+
+	return mtx_trylock(m);
+}
+
+void DWC_MUTEX_UNLOCK(dwc_mutex_t *mutex)
+{
+	struct mtx *m = (struct mtx *)mutex;
+
+	mtx_unlock(m);
+}
+
+
+/* Timing */
+
+void DWC_UDELAY(uint32_t usecs)
+{
+	DELAY(usecs);
+}
+
+void DWC_MDELAY(uint32_t msecs)
+{
+	do {
+		DELAY(1000);
+	} while (--msecs);
+}
+
+void DWC_MSLEEP(uint32_t msecs)
+{
+	struct timeval tv;
+
+	tv.tv_sec = msecs / 1000;
+	tv.tv_usec = (msecs - tv.tv_sec * 1000) * 1000;
+	pause("dw3slp", tvtohz(&tv));
+}
+
+uint32_t DWC_TIME(void)
+{
+	struct timeval tv;
+
+	microuptime(&tv);	// or getmicrouptime? (less precise, but faster)
+	return tv.tv_sec * 1000 + tv.tv_usec / 1000;
+}
+
+
+/* Timers */
+
+struct dwc_timer {
+	struct callout t;
+	char *name;
+	dwc_spinlock_t *lock;
+	dwc_timer_callback_t cb;
+	void *data;
+};
+
+dwc_timer_t *DWC_TIMER_ALLOC(char *name, dwc_timer_callback_t cb, void *data)
+{
+	dwc_timer_t *t = DWC_ALLOC(sizeof(*t));
+
+	if (!t) {
+		DWC_ERROR("Cannot allocate memory for timer");
+		return NULL;
+	}
+
+	callout_init(&t->t, 1);
+
+	t->name = DWC_STRDUP(name);
+	if (!t->name) {
+		DWC_ERROR("Cannot allocate memory for timer->name");
+		goto no_name;
+	}
+
+	t->lock = DWC_SPINLOCK_ALLOC();
+	if (!t->lock) {
+		DWC_ERROR("Cannot allocate memory for lock");
+		goto no_lock;
+	}
+
+	t->cb = cb;
+	t->data = data;
+
+	return t;
+
+ no_lock:
+	DWC_FREE(t->name);
+ no_name:
+	DWC_FREE(t);
+
+	return NULL;
+}
+
+void DWC_TIMER_FREE(dwc_timer_t *timer)
+{
+	callout_stop(&timer->t);
+	DWC_SPINLOCK_FREE(timer->lock);
+	DWC_FREE(timer->name);
+	DWC_FREE(timer);
+}
+
+void DWC_TIMER_SCHEDULE(dwc_timer_t *timer, uint32_t time)
+{
+	struct timeval tv;
+
+	tv.tv_sec = time / 1000;
+	tv.tv_usec = (time - tv.tv_sec * 1000) * 1000;
+	callout_reset(&timer->t, tvtohz(&tv), timer->cb, timer->data);
+}
+
+void DWC_TIMER_CANCEL(dwc_timer_t *timer)
+{
+	callout_stop(&timer->t);
+}
+
+
+/* Wait Queues */
+
+struct dwc_waitq {
+	struct mtx lock;
+	int abort;
+};
+
+dwc_waitq_t *DWC_WAITQ_ALLOC(void)
+{
+	dwc_waitq_t *wq = DWC_ALLOC(sizeof(*wq));
+
+	if (!wq) {
+		DWC_ERROR("Cannot allocate memory for waitqueue");
+		return NULL;
+	}
+
+	mtx_init(&wq->lock, "dw3wtq", NULL, MTX_DEF);
+	wq->abort = 0;
+
+	return wq;
+}
+
+void DWC_WAITQ_FREE(dwc_waitq_t *wq)
+{
+	mtx_destroy(&wq->lock);
+	DWC_FREE(wq);
+}
+
+int32_t DWC_WAITQ_WAIT(dwc_waitq_t *wq, dwc_waitq_condition_t cond, void *data)
+{
+//	intrmask_t ipl;
+	int result = 0;
+
+	mtx_lock(&wq->lock);
+//	ipl = splbio();
+
+	/* Skip the sleep if already aborted or triggered */
+	if (!wq->abort && !cond(data)) {
+//		splx(ipl);
+		result = msleep(wq, &wq->lock, PCATCH, "dw3wat", 0); // infinite timeout
+//		ipl = splbio();
+	}
+
+	if (result == ERESTART) {	// signaled - restart
+		result = -DWC_E_RESTART;
+
+	} else if (result == EINTR) {	// signaled - interrupt
+		result = -DWC_E_ABORT;
+
+	} else if (wq->abort) {
+		result = -DWC_E_ABORT;
+
+	} else {
+		result = 0;
+	}
+
+	wq->abort = 0;
+//	splx(ipl);
+	mtx_unlock(&wq->lock);
+	return result;
+}
+
+int32_t DWC_WAITQ_WAIT_TIMEOUT(dwc_waitq_t *wq, dwc_waitq_condition_t cond,
+			       void *data, int32_t msecs)
+{
+	struct timeval tv, tv1, tv2;
+//	intrmask_t ipl;
+	int result = 0;
+
+	tv.tv_sec = msecs / 1000;
+	tv.tv_usec = (msecs - tv.tv_sec * 1000) * 1000;
+
+	mtx_lock(&wq->lock);
+//	ipl = splbio();
+
+	/* Skip the sleep if already aborted or triggered */
+	if (!wq->abort && !cond(data)) {
+//		splx(ipl);
+		getmicrouptime(&tv1);
+		result = msleep(wq, &wq->lock, PCATCH, "dw3wto", tvtohz(&tv));
+		getmicrouptime(&tv2);
+//		ipl = splbio();
+	}
+
+	if (result == 0) {			// awoken
+		if (wq->abort) {
+			result = -DWC_E_ABORT;
+		} else {
+			tv2.tv_usec -= tv1.tv_usec;
+			if (tv2.tv_usec < 0) {
+				tv2.tv_usec += 1000000;
+				tv2.tv_sec--;
+			}
+
+			tv2.tv_sec -= tv1.tv_sec;
+			result = tv2.tv_sec * 1000 + tv2.tv_usec / 1000;
+			result = msecs - result;
+			if (result <= 0)
+				result = 1;
+		}
+	} else if (result == ERESTART) {	// signaled - restart
+		result = -DWC_E_RESTART;
+
+	} else if (result == EINTR) {		// signaled - interrupt
+		result = -DWC_E_ABORT;
+
+	} else {				// timed out
+		result = -DWC_E_TIMEOUT;
+	}
+
+	wq->abort = 0;
+//	splx(ipl);
+	mtx_unlock(&wq->lock);
+	return result;
+}
+
+void DWC_WAITQ_TRIGGER(dwc_waitq_t *wq)
+{
+	wakeup(wq);
+}
+
+void DWC_WAITQ_ABORT(dwc_waitq_t *wq)
+{
+//	intrmask_t ipl;
+
+	mtx_lock(&wq->lock);
+//	ipl = splbio();
+	wq->abort = 1;
+	wakeup(wq);
+//	splx(ipl);
+	mtx_unlock(&wq->lock);
+}
+
+
+/* Threading */
+
+struct dwc_thread {
+	struct proc *proc;
+	int abort;
+};
+
+dwc_thread_t *DWC_THREAD_RUN(dwc_thread_function_t func, char *name, void *data)
+{
+	int retval;
+	dwc_thread_t *thread = DWC_ALLOC(sizeof(*thread));
+
+	if (!thread) {
+		return NULL;
+	}
+
+	thread->abort = 0;
+	retval = kthread_create((void (*)(void *))func, data, &thread->proc,
+				RFPROC | RFNOWAIT, 0, "%s", name);
+	if (retval) {
+		DWC_FREE(thread);
+		return NULL;
+	}
+
+	return thread;
+}
+
+int DWC_THREAD_STOP(dwc_thread_t *thread)
+{
+	int retval;
+
+	thread->abort = 1;
+	retval = tsleep(&thread->abort, 0, "dw3stp", 60 * hz);
+
+	if (retval == 0) {
+		/* DWC_THREAD_EXIT() will free the thread struct */
+		return 0;
+	}
+
+	/* NOTE: We leak the thread struct if thread doesn't die */
+
+	if (retval == EWOULDBLOCK) {
+		return -DWC_E_TIMEOUT;
+	}
+
+	return -DWC_E_UNKNOWN;
+}
+
+dwc_bool_t DWC_THREAD_SHOULD_STOP(dwc_thread_t *thread)
+{
+	return thread->abort;
+}
+
+void DWC_THREAD_EXIT(dwc_thread_t *thread)
+{
+	wakeup(&thread->abort);
+	DWC_FREE(thread);
+	kthread_exit(0);
+}
+
+
+/* tasklets
+ - Runs in interrupt context (cannot sleep)
+ - Each tasklet runs on a single CPU [ How can we ensure this on FreeBSD? Does it matter? ]
+ - Different tasklets can be running simultaneously on different CPUs [ shouldn't matter ]
+ */
+struct dwc_tasklet {
+	struct task t;
+	dwc_tasklet_callback_t cb;
+	void *data;
+};
+
+static void tasklet_callback(void *data, int pending)	// what to do with pending ???
+{
+	dwc_tasklet_t *task = (dwc_tasklet_t *)data;
+
+	task->cb(task->data);
+}
+
+dwc_tasklet_t *DWC_TASK_ALLOC(char *name, dwc_tasklet_callback_t cb, void *data)
+{
+	dwc_tasklet_t *task = DWC_ALLOC(sizeof(*task));
+
+	if (task) {
+		task->cb = cb;
+		task->data = data;
+		TASK_INIT(&task->t, 0, tasklet_callback, task);
+	} else {
+		DWC_ERROR("Cannot allocate memory for tasklet");
+	}
+
+	return task;
+}
+
+void DWC_TASK_FREE(dwc_tasklet_t *task)
+{
+	taskqueue_drain(taskqueue_fast, &task->t);	// ???
+	DWC_FREE(task);
+}
+
+void DWC_TASK_SCHEDULE(dwc_tasklet_t *task)
+{
+	/* Uses predefined system queue */
+	taskqueue_enqueue_fast(taskqueue_fast, &task->t);
+}
+
+
+/* workqueues
+ - Runs in process context (can sleep)
+ */
+typedef struct work_container {
+	dwc_work_callback_t cb;
+	void *data;
+	dwc_workq_t *wq;
+	char *name;
+	int hz;
+
+#ifdef DEBUG
+	DWC_CIRCLEQ_ENTRY(work_container) entry;
+#endif
+	struct task task;
+} work_container_t;
+
+#ifdef DEBUG
+DWC_CIRCLEQ_HEAD(work_container_queue, work_container);
+#endif
+
+struct dwc_workq {
+	struct taskqueue *taskq;
+	dwc_spinlock_t *lock;
+	dwc_waitq_t *waitq;
+	int pending;
+
+#ifdef DEBUG
+	struct work_container_queue entries;
+#endif
+};
+
+static void do_work(void *data, int pending)	// what to do with pending ???
+{
+	work_container_t *container = (work_container_t *)data;
+	dwc_workq_t *wq = container->wq;
+	dwc_irqflags_t flags;
+
+	if (container->hz) {
+		pause("dw3wrk", container->hz);
+	}
+
+	container->cb(container->data);
+	DWC_DEBUG("Work done: %s, container=%p", container->name, container);
+
+	DWC_SPINLOCK_IRQSAVE(wq->lock, &flags);
+
+#ifdef DEBUG
+	DWC_CIRCLEQ_REMOVE(&wq->entries, container, entry);
+#endif
+	if (container->name)
+		DWC_FREE(container->name);
+	DWC_FREE(container);
+	wq->pending--;
+	DWC_SPINUNLOCK_IRQRESTORE(wq->lock, flags);
+	DWC_WAITQ_TRIGGER(wq->waitq);
+}
+
+static int work_done(void *data)
+{
+	dwc_workq_t *workq = (dwc_workq_t *)data;
+
+	return workq->pending == 0;
+}
+
+int DWC_WORKQ_WAIT_WORK_DONE(dwc_workq_t *workq, int timeout)
+{
+	return DWC_WAITQ_WAIT_TIMEOUT(workq->waitq, work_done, workq, timeout);
+}
+
+dwc_workq_t *DWC_WORKQ_ALLOC(char *name)
+{
+	dwc_workq_t *wq = DWC_ALLOC(sizeof(*wq));
+
+	if (!wq) {
+		DWC_ERROR("Cannot allocate memory for workqueue");
+		return NULL;
+	}
+
+	wq->taskq = taskqueue_create(name, M_NOWAIT, taskqueue_thread_enqueue, &wq->taskq);
+	if (!wq->taskq) {
+		DWC_ERROR("Cannot allocate memory for taskqueue");
+		goto no_taskq;
+	}
+
+	wq->pending = 0;
+
+	wq->lock = DWC_SPINLOCK_ALLOC();
+	if (!wq->lock) {
+		DWC_ERROR("Cannot allocate memory for spinlock");
+		goto no_lock;
+	}
+
+	wq->waitq = DWC_WAITQ_ALLOC();
+	if (!wq->waitq) {
+		DWC_ERROR("Cannot allocate memory for waitqueue");
+		goto no_waitq;
+	}
+
+	taskqueue_start_threads(&wq->taskq, 1, PWAIT, "%s taskq", "dw3tsk");
+
+#ifdef DEBUG
+	DWC_CIRCLEQ_INIT(&wq->entries);
+#endif
+	return wq;
+
+ no_waitq:
+	DWC_SPINLOCK_FREE(wq->lock);
+ no_lock:
+	taskqueue_free(wq->taskq);
+ no_taskq:
+	DWC_FREE(wq);
+
+	return NULL;
+}
+
+void DWC_WORKQ_FREE(dwc_workq_t *wq)
+{
+#ifdef DEBUG
+	dwc_irqflags_t flags;
+
+	DWC_SPINLOCK_IRQSAVE(wq->lock, &flags);
+
+	if (wq->pending != 0) {
+		struct work_container *container;
+
+		DWC_ERROR("Destroying work queue with pending work");
+
+		DWC_CIRCLEQ_FOREACH(container, &wq->entries, entry) {
+			DWC_ERROR("Work %s still pending", container->name);
+		}
+	}
+
+	DWC_SPINUNLOCK_IRQRESTORE(wq->lock, flags);
+#endif
+	DWC_WAITQ_FREE(wq->waitq);
+	DWC_SPINLOCK_FREE(wq->lock);
+	taskqueue_free(wq->taskq);
+	DWC_FREE(wq);
+}
+
+void DWC_WORKQ_SCHEDULE(dwc_workq_t *wq, dwc_work_callback_t cb, void *data,
+			char *format, ...)
+{
+	dwc_irqflags_t flags;
+	work_container_t *container;
+	static char name[128];
+	va_list args;
+
+	va_start(args, format);
+	DWC_VSNPRINTF(name, 128, format, args);
+	va_end(args);
+
+	DWC_SPINLOCK_IRQSAVE(wq->lock, &flags);
+	wq->pending++;
+	DWC_SPINUNLOCK_IRQRESTORE(wq->lock, flags);
+	DWC_WAITQ_TRIGGER(wq->waitq);
+
+	container = DWC_ALLOC_ATOMIC(sizeof(*container));
+	if (!container) {
+		DWC_ERROR("Cannot allocate memory for container");
+		return;
+	}
+
+	container->name = DWC_STRDUP(name);
+	if (!container->name) {
+		DWC_ERROR("Cannot allocate memory for container->name");
+		DWC_FREE(container);
+		return;
+	}
+
+	container->cb = cb;
+	container->data = data;
+	container->wq = wq;
+	container->hz = 0;
+
+	DWC_DEBUG("Queueing work: %s, container=%p", container->name, container);
+
+	TASK_INIT(&container->task, 0, do_work, container);
+
+#ifdef DEBUG
+	DWC_CIRCLEQ_INSERT_TAIL(&wq->entries, container, entry);
+#endif
+	taskqueue_enqueue_fast(wq->taskq, &container->task);
+}
+
+void DWC_WORKQ_SCHEDULE_DELAYED(dwc_workq_t *wq, dwc_work_callback_t cb,
+				void *data, uint32_t time, char *format, ...)
+{
+	dwc_irqflags_t flags;
+	work_container_t *container;
+	static char name[128];
+	struct timeval tv;
+	va_list args;
+
+	va_start(args, format);
+	DWC_VSNPRINTF(name, 128, format, args);
+	va_end(args);
+
+	DWC_SPINLOCK_IRQSAVE(wq->lock, &flags);
+	wq->pending++;
+	DWC_SPINUNLOCK_IRQRESTORE(wq->lock, flags);
+	DWC_WAITQ_TRIGGER(wq->waitq);
+
+	container = DWC_ALLOC_ATOMIC(sizeof(*container));
+	if (!container) {
+		DWC_ERROR("Cannot allocate memory for container");
+		return;
+	}
+
+	container->name = DWC_STRDUP(name);
+	if (!container->name) {
+		DWC_ERROR("Cannot allocate memory for container->name");
+		DWC_FREE(container);
+		return;
+	}
+
+	container->cb = cb;
+	container->data = data;
+	container->wq = wq;
+
+	tv.tv_sec = time / 1000;
+	tv.tv_usec = (time - tv.tv_sec * 1000) * 1000;
+	container->hz = tvtohz(&tv);
+
+	DWC_DEBUG("Queueing work: %s, container=%p", container->name, container);
+
+	TASK_INIT(&container->task, 0, do_work, container);
+
+#ifdef DEBUG
+	DWC_CIRCLEQ_INSERT_TAIL(&wq->entries, container, entry);
+#endif
+	taskqueue_enqueue_fast(wq->taskq, &container->task);
+}
+
+int DWC_WORKQ_PENDING(dwc_workq_t *wq)
+{
+	return wq->pending;
+}
diff --git a/drivers/usb/dwc_otg/dwc_common_linux.c b/drivers/usb/dwc_otg/dwc_common_linux.c
new file mode 100644
index 0000000..b55d860
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_common_linux.c
@@ -0,0 +1,1442 @@
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kthread.h>
+
+#ifdef DWC_CCLIB
+# include "dwc_cc.h"
+#endif
+
+#ifdef DWC_CRYPTOLIB
+# include "dwc_modpow.h"
+# include "dwc_dh.h"
+# include "dwc_crypto.h"
+#endif
+
+#ifdef DWC_NOTIFYLIB
+# include "dwc_notifier.h"
+#endif
+
+/* OS-Level Implementations */
+
+/* This is the Linux kernel implementation of the DWC platform library. */
+#include <linux/moduleparam.h>
+#include <linux/ctype.h>
+#include <linux/crypto.h>
+#include <linux/delay.h>
+#include <linux/device.h>
+#include <linux/dma-mapping.h>
+#include <linux/cdev.h>
+#include <linux/errno.h>
+#include <linux/interrupt.h>
+#include <linux/jiffies.h>
+#include <linux/list.h>
+#include <linux/pci.h>
+#include <linux/random.h>
+#include <linux/scatterlist.h>
+#include <linux/slab.h>
+#include <linux/stat.h>
+#include <linux/string.h>
+#include <linux/timer.h>
+#include <linux/usb.h>
+
+#include <linux/version.h>
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,24)
+# include <linux/usb/gadget.h>
+#else
+# include <linux/usb_gadget.h>
+#endif
+
+#include <asm/io.h>
+#include <asm/page.h>
+#include <asm/uaccess.h>
+#include <asm/unaligned.h>
+
+#include "dwc_os.h"
+#include "dwc_list.h"
+
+
+/* MISC */
+
+void *DWC_MEMSET(void *dest, uint8_t byte, uint32_t size)
+{
+	return memset(dest, byte, size);
+}
+
+void *DWC_MEMCPY(void *dest, void const *src, uint32_t size)
+{
+	return memcpy(dest, src, size);
+}
+
+void *DWC_MEMMOVE(void *dest, void *src, uint32_t size)
+{
+	return memmove(dest, src, size);
+}
+
+int DWC_MEMCMP(void *m1, void *m2, uint32_t size)
+{
+	return memcmp(m1, m2, size);
+}
+
+int DWC_STRNCMP(void *s1, void *s2, uint32_t size)
+{
+	return strncmp(s1, s2, size);
+}
+
+int DWC_STRCMP(void *s1, void *s2)
+{
+	return strcmp(s1, s2);
+}
+
+int DWC_STRLEN(char const *str)
+{
+	return strlen(str);
+}
+
+char *DWC_STRCPY(char *to, char const *from)
+{
+	return strcpy(to, from);
+}
+
+char *DWC_STRDUP(char const *str)
+{
+	int len = DWC_STRLEN(str) + 1;
+	char *new = DWC_ALLOC_ATOMIC(len);
+
+	if (!new) {
+		return NULL;
+	}
+
+	DWC_MEMCPY(new, str, len);
+	return new;
+}
+
+int DWC_ATOI(const char *str, int32_t *value)
+{
+	char *end = NULL;
+
+	*value = simple_strtol(str, &end, 0);
+	if (*end == '\0') {
+		return 0;
+	}
+
+	return -1;
+}
+
+int DWC_ATOUI(const char *str, uint32_t *value)
+{
+	char *end = NULL;
+
+	*value = simple_strtoul(str, &end, 0);
+	if (*end == '\0') {
+		return 0;
+	}
+
+	return -1;
+}
+
+
+#ifdef DWC_UTFLIB
+/* From usbstring.c */
+
+int DWC_UTF8_TO_UTF16LE(uint8_t const *s, uint16_t *cp, unsigned len)
+{
+	int	count = 0;
+	u8	c;
+	u16	uchar;
+
+	/* this insists on correct encodings, though not minimal ones.
+	 * BUT it currently rejects legit 4-byte UTF-8 code points,
+	 * which need surrogate pairs.  (Unicode 3.1 can use them.)
+	 */
+	while (len != 0 && (c = (u8) *s++) != 0) {
+		if (unlikely(c & 0x80)) {
+			// 2-byte sequence:
+			// 00000yyyyyxxxxxx = 110yyyyy 10xxxxxx
+			if ((c & 0xe0) == 0xc0) {
+				uchar = (c & 0x1f) << 6;
+
+				c = (u8) *s++;
+				if ((c & 0xc0) != 0xc0)
+					goto fail;
+				c &= 0x3f;
+				uchar |= c;
+
+			// 3-byte sequence (most CJKV characters):
+			// zzzzyyyyyyxxxxxx = 1110zzzz 10yyyyyy 10xxxxxx
+			} else if ((c & 0xf0) == 0xe0) {
+				uchar = (c & 0x0f) << 12;
+
+				c = (u8) *s++;
+				if ((c & 0xc0) != 0xc0)
+					goto fail;
+				c &= 0x3f;
+				uchar |= c << 6;
+
+				c = (u8) *s++;
+				if ((c & 0xc0) != 0xc0)
+					goto fail;
+				c &= 0x3f;
+				uchar |= c;
+
+				/* no bogus surrogates */
+				if (0xd800 <= uchar && uchar <= 0xdfff)
+					goto fail;
+
+			// 4-byte sequence (surrogate pairs, currently rare):
+			// 11101110wwwwzzzzyy + 110111yyyyxxxxxx
+			//     = 11110uuu 10uuzzzz 10yyyyyy 10xxxxxx
+			// (uuuuu = wwww + 1)
+			// FIXME accept the surrogate code points (only)
+			} else
+				goto fail;
+		} else
+			uchar = c;
+		put_unaligned (cpu_to_le16 (uchar), cp++);
+		count++;
+		len--;
+	}
+	return count;
+fail:
+	return -1;
+}
+#endif	/* DWC_UTFLIB */
+
+
+/* dwc_debug.h */
+
+dwc_bool_t DWC_IN_IRQ(void)
+{
+	return in_irq();
+}
+
+dwc_bool_t DWC_IN_BH(void)
+{
+	return in_softirq();
+}
+
+void DWC_VPRINTF(char *format, va_list args)
+{
+	vprintk(format, args);
+}
+
+int DWC_VSNPRINTF(char *str, int size, char *format, va_list args)
+{
+	return vsnprintf(str, size, format, args);
+}
+
+void DWC_PRINTF(char *format, ...)
+{
+	va_list args;
+
+	va_start(args, format);
+	DWC_VPRINTF(format, args);
+	va_end(args);
+}
+
+int DWC_SPRINTF(char *buffer, char *format, ...)
+{
+	int retval;
+	va_list args;
+
+	va_start(args, format);
+	retval = vsprintf(buffer, format, args);
+	va_end(args);
+	return retval;
+}
+
+int DWC_SNPRINTF(char *buffer, int size, char *format, ...)
+{
+	int retval;
+	va_list args;
+
+	va_start(args, format);
+	retval = vsnprintf(buffer, size, format, args);
+	va_end(args);
+	return retval;
+}
+
+void __DWC_WARN(char *format, ...)
+{
+	va_list args;
+
+	va_start(args, format);
+	DWC_PRINTF(KERN_WARNING);
+	DWC_VPRINTF(format, args);
+	va_end(args);
+}
+
+void __DWC_ERROR(char *format, ...)
+{
+	va_list args;
+
+	va_start(args, format);
+	DWC_PRINTF(KERN_ERR);
+	DWC_VPRINTF(format, args);
+	va_end(args);
+}
+
+void DWC_EXCEPTION(char *format, ...)
+{
+	va_list args;
+
+	va_start(args, format);
+	DWC_PRINTF(KERN_ERR);
+	DWC_VPRINTF(format, args);
+	va_end(args);
+	BUG_ON(1);
+}
+
+#ifdef DEBUG
+void __DWC_DEBUG(char *format, ...)
+{
+	va_list args;
+
+	va_start(args, format);
+	DWC_PRINTF(KERN_DEBUG);
+	DWC_VPRINTF(format, args);
+	va_end(args);
+}
+#endif
+
+
+/* dwc_mem.h */
+
+#if 0
+dwc_pool_t *DWC_DMA_POOL_CREATE(uint32_t size,
+				uint32_t align,
+				uint32_t alloc)
+{
+	struct dma_pool *pool = dma_pool_create("Pool", NULL,
+						size, align, alloc);
+	return (dwc_pool_t *)pool;
+}
+
+void DWC_DMA_POOL_DESTROY(dwc_pool_t *pool)
+{
+	dma_pool_destroy((struct dma_pool *)pool);
+}
+
+void *DWC_DMA_POOL_ALLOC(dwc_pool_t *pool, uint64_t *dma_addr)
+{
+	return dma_pool_alloc((struct dma_pool *)pool, GFP_KERNEL, dma_addr);
+}
+
+void *DWC_DMA_POOL_ZALLOC(dwc_pool_t *pool, uint64_t *dma_addr)
+{
+	void *vaddr = DWC_DMA_POOL_ALLOC(pool, dma_addr);
+	memset(..);
+}
+
+void DWC_DMA_POOL_FREE(dwc_pool_t *pool, void *vaddr, void *daddr)
+{
+	dma_pool_free(pool, vaddr, daddr);
+}
+#endif
+
+void *__DWC_DMA_ALLOC(void *dma_ctx, uint32_t size, dwc_dma_t *dma_addr)
+{
+#ifdef xxCOSIM /* Only works for 32-bit cosim */
+	void *buf = dma_alloc_coherent(dma_ctx, (size_t)size, dma_addr, GFP_KERNEL);
+#else
+	void *buf = dma_alloc_coherent(dma_ctx, (size_t)size, dma_addr, GFP_KERNEL);
+#endif
+	if (!buf) {
+		return NULL;
+	}
+
+	memset(buf, 0, (size_t)size);
+	return buf;
+}
+
+void *__DWC_DMA_ALLOC_ATOMIC(void *dma_ctx, uint32_t size, dwc_dma_t *dma_addr)
+{
+	void *buf = dma_alloc_coherent(NULL, (size_t)size, dma_addr, GFP_ATOMIC);
+	if (!buf) {
+		return NULL;
+	}
+	memset(buf, 0, (size_t)size);
+	return buf;
+}
+
+void __DWC_DMA_FREE(void *dma_ctx, uint32_t size, void *virt_addr, dwc_dma_t dma_addr)
+{
+	dma_free_coherent(dma_ctx, size, virt_addr, dma_addr);
+}
+
+void *__DWC_ALLOC(void *mem_ctx, uint32_t size)
+{
+	return kzalloc(size, GFP_KERNEL);
+}
+
+void *__DWC_ALLOC_ATOMIC(void *mem_ctx, uint32_t size)
+{
+	return kzalloc(size, GFP_ATOMIC);
+}
+
+void __DWC_FREE(void *mem_ctx, void *addr)
+{
+	kfree(addr);
+}
+
+
+#ifdef DWC_CRYPTOLIB
+/* dwc_crypto.h */
+
+void DWC_RANDOM_BYTES(uint8_t *buffer, uint32_t length)
+{
+	get_random_bytes(buffer, length);
+}
+
+int DWC_AES_CBC(uint8_t *message, uint32_t messagelen, uint8_t *key, uint32_t keylen, uint8_t iv[16], uint8_t *out)
+{
+	struct crypto_blkcipher *tfm;
+	struct blkcipher_desc desc;
+	struct scatterlist sgd;
+	struct scatterlist sgs;
+
+	tfm = crypto_alloc_blkcipher("cbc(aes)", 0, CRYPTO_ALG_ASYNC);
+	if (tfm == NULL) {
+		printk("failed to load transform for aes CBC\n");
+		return -1;
+	}
+
+	crypto_blkcipher_setkey(tfm, key, keylen);
+	crypto_blkcipher_set_iv(tfm, iv, 16);
+
+	sg_init_one(&sgd, out, messagelen);
+	sg_init_one(&sgs, message, messagelen);
+
+	desc.tfm = tfm;
+	desc.flags = 0;
+
+	if (crypto_blkcipher_encrypt(&desc, &sgd, &sgs, messagelen)) {
+		crypto_free_blkcipher(tfm);
+		DWC_ERROR("AES CBC encryption failed");
+		return -1;
+	}
+
+	crypto_free_blkcipher(tfm);
+	return 0;
+}
+
+int DWC_SHA256(uint8_t *message, uint32_t len, uint8_t *out)
+{
+	struct crypto_hash *tfm;
+	struct hash_desc desc;
+	struct scatterlist sg;
+
+	tfm = crypto_alloc_hash("sha256", 0, CRYPTO_ALG_ASYNC);
+	if (IS_ERR(tfm)) {
+		DWC_ERROR("Failed to load transform for sha256: %ld\n", PTR_ERR(tfm));
+		return 0;
+	}
+	desc.tfm = tfm;
+	desc.flags = 0;
+
+	sg_init_one(&sg, message, len);
+	crypto_hash_digest(&desc, &sg, len, out);
+	crypto_free_hash(tfm);
+
+	return 1;
+}
+
+int DWC_HMAC_SHA256(uint8_t *message, uint32_t messagelen,
+		    uint8_t *key, uint32_t keylen, uint8_t *out)
+{
+	struct crypto_hash *tfm;
+	struct hash_desc desc;
+	struct scatterlist sg;
+
+	tfm = crypto_alloc_hash("hmac(sha256)", 0, CRYPTO_ALG_ASYNC);
+	if (IS_ERR(tfm)) {
+		DWC_ERROR("Failed to load transform for hmac(sha256): %ld\n", PTR_ERR(tfm));
+		return 0;
+	}
+	desc.tfm = tfm;
+	desc.flags = 0;
+
+	sg_init_one(&sg, message, messagelen);
+	crypto_hash_setkey(tfm, key, keylen);
+	crypto_hash_digest(&desc, &sg, messagelen, out);
+	crypto_free_hash(tfm);
+
+	return 1;
+}
+#endif	/* DWC_CRYPTOLIB */
+
+
+/* Byte Ordering Conversions */
+
+uint32_t DWC_CPU_TO_LE32(uint32_t *p)
+{
+#ifdef __LITTLE_ENDIAN
+	return *p;
+#else
+	uint8_t *u_p = (uint8_t *)p;
+
+	return (u_p[3] | (u_p[2] << 8) | (u_p[1] << 16) | (u_p[0] << 24));
+#endif
+}
+
+uint32_t DWC_CPU_TO_BE32(uint32_t *p)
+{
+#ifdef __BIG_ENDIAN
+	return *p;
+#else
+	uint8_t *u_p = (uint8_t *)p;
+
+	return (u_p[3] | (u_p[2] << 8) | (u_p[1] << 16) | (u_p[0] << 24));
+#endif
+}
+
+uint32_t DWC_LE32_TO_CPU(uint32_t *p)
+{
+#ifdef __LITTLE_ENDIAN
+	return *p;
+#else
+	uint8_t *u_p = (uint8_t *)p;
+
+	return (u_p[3] | (u_p[2] << 8) | (u_p[1] << 16) | (u_p[0] << 24));
+#endif
+}
+
+uint32_t DWC_BE32_TO_CPU(uint32_t *p)
+{
+#ifdef __BIG_ENDIAN
+	return *p;
+#else
+	uint8_t *u_p = (uint8_t *)p;
+
+	return (u_p[3] | (u_p[2] << 8) | (u_p[1] << 16) | (u_p[0] << 24));
+#endif
+}
+
+uint16_t DWC_CPU_TO_LE16(uint16_t *p)
+{
+#ifdef __LITTLE_ENDIAN
+	return *p;
+#else
+	uint8_t *u_p = (uint8_t *)p;
+	return (u_p[1] | (u_p[0] << 8));
+#endif
+}
+
+uint16_t DWC_CPU_TO_BE16(uint16_t *p)
+{
+#ifdef __BIG_ENDIAN
+	return *p;
+#else
+	uint8_t *u_p = (uint8_t *)p;
+	return (u_p[1] | (u_p[0] << 8));
+#endif
+}
+
+uint16_t DWC_LE16_TO_CPU(uint16_t *p)
+{
+#ifdef __LITTLE_ENDIAN
+	return *p;
+#else
+	uint8_t *u_p = (uint8_t *)p;
+	return (u_p[1] | (u_p[0] << 8));
+#endif
+}
+
+uint16_t DWC_BE16_TO_CPU(uint16_t *p)
+{
+#ifdef __BIG_ENDIAN
+	return *p;
+#else
+	uint8_t *u_p = (uint8_t *)p;
+	return (u_p[1] | (u_p[0] << 8));
+#endif
+}
+
+
+/* Registers */
+
+uint32_t DWC_READ_REG32(uint32_t volatile *reg)
+{
+#ifndef DEBUG
+	return readl(reg);
+#else
+	uint32_t value;
+	value = readl(reg);
+#ifdef DEBUG_REGISTERS
+    //if (((uint32_t)reg) == 0xfe820000)
+	printk(KERN_INFO "%s:  Addr:0x%08X Val:0x%08X\n", __func__, (uint32_t)reg, value);
+#endif
+	return value;
+#endif
+}
+
+#if 0
+uint64_t DWC_READ_REG64(uint64_t volatile *reg)
+{
+}
+#endif
+
+void DWC_WRITE_REG32(uint32_t volatile *reg, uint32_t value)
+{
+	writel(value, reg);
+#ifdef DEBUG
+#ifdef DEBUG_REGISTERS
+    //if (((uint32_t)reg) == 0xfe820000)
+	printk(KERN_INFO "%s: Addr:0x%08X Val:0x%08X\n", __func__, (uint32_t)reg, value);
+#endif
+#endif
+}
+
+#if 0
+void DWC_WRITE_REG64(uint64_t volatile *reg, uint64_t value)
+{
+}
+#endif
+
+void DWC_MODIFY_REG32(uint32_t volatile *reg, uint32_t clear_mask, uint32_t set_mask)
+{
+#ifndef DEBUG
+	writel((readl(reg) & ~clear_mask) | set_mask, reg);
+#else
+    DWC_WRITE_REG32(reg,(((DWC_READ_REG32(reg)) & ~clear_mask) | set_mask));
+#endif
+
+}
+
+#if 0
+void DWC_MODIFY_REG64(uint64_t volatile *reg, uint64_t clear_mask, uint64_t set_mask)
+{
+}
+#endif
+
+
+/* Locking */
+
+dwc_spinlock_t *DWC_SPINLOCK_ALLOC(void)
+{
+	spinlock_t *sl = (spinlock_t *)1;
+
+#if defined(CONFIG_PREEMPT) || defined(CONFIG_SMP)
+	sl = DWC_ALLOC(sizeof(*sl));
+	if (!sl) {
+		DWC_ERROR("Cannot allocate memory for spinlock\n");
+		return NULL;
+	}
+
+	spin_lock_init(sl);
+#endif
+	return (dwc_spinlock_t *)sl;
+}
+
+void DWC_SPINLOCK_FREE(dwc_spinlock_t *lock)
+{
+#if defined(CONFIG_PREEMPT) || defined(CONFIG_SMP)
+	DWC_FREE(lock);
+#endif
+}
+
+void DWC_SPINLOCK(dwc_spinlock_t *lock)
+{
+#if defined(CONFIG_PREEMPT) || defined(CONFIG_SMP)
+	spin_lock((spinlock_t *)lock);
+#endif
+}
+
+void DWC_SPINUNLOCK(dwc_spinlock_t *lock)
+{
+#if defined(CONFIG_PREEMPT) || defined(CONFIG_SMP)
+	spin_unlock((spinlock_t *)lock);
+#endif
+}
+
+void DWC_SPINLOCK_IRQSAVE(dwc_spinlock_t *lock, dwc_irqflags_t *flags)
+{
+	dwc_irqflags_t f;
+
+#if defined(CONFIG_PREEMPT) || defined(CONFIG_SMP)
+	spin_lock_irqsave((spinlock_t *)lock, f);
+#else
+	local_irq_save(f);
+#endif
+	*flags = f;
+}
+
+void DWC_SPINUNLOCK_IRQRESTORE(dwc_spinlock_t *lock, dwc_irqflags_t flags)
+{
+#if defined(CONFIG_PREEMPT) || defined(CONFIG_SMP)
+	spin_unlock_irqrestore((spinlock_t *)lock, flags);
+#else
+	local_irq_restore(flags);
+#endif
+}
+
+dwc_mutex_t *DWC_MUTEX_ALLOC(void)
+{
+	struct mutex *m;
+	dwc_mutex_t *mutex = (dwc_mutex_t *)DWC_ALLOC(sizeof(struct mutex));
+
+	if (!mutex) {
+		DWC_ERROR("Cannot allocate memory for mutex\n");
+		return NULL;
+	}
+
+	m = (struct mutex *)mutex;
+	mutex_init(m);
+	return mutex;
+}
+
+#if (defined(DWC_LINUX) && defined(CONFIG_DEBUG_MUTEXES))
+#else
+void DWC_MUTEX_FREE(dwc_mutex_t *mutex)
+{
+	mutex_destroy((struct mutex *)mutex);
+	DWC_FREE(mutex);
+}
+#endif
+
+void DWC_MUTEX_LOCK(dwc_mutex_t *mutex)
+{
+	struct mutex *m = (struct mutex *)mutex;
+	mutex_lock(m);
+}
+
+int DWC_MUTEX_TRYLOCK(dwc_mutex_t *mutex)
+{
+	struct mutex *m = (struct mutex *)mutex;
+	return mutex_trylock(m);
+}
+
+void DWC_MUTEX_UNLOCK(dwc_mutex_t *mutex)
+{
+	struct mutex *m = (struct mutex *)mutex;
+	mutex_unlock(m);
+}
+
+
+/* Timing */
+
+void DWC_UDELAY(uint32_t usecs)
+{
+	udelay(usecs);
+}
+
+void DWC_MDELAY(uint32_t msecs)
+{
+	mdelay(msecs);
+}
+
+void DWC_MSLEEP(uint32_t msecs)
+{
+	msleep(msecs);
+}
+
+uint32_t DWC_TIME(void)
+{
+	return jiffies_to_msecs(jiffies);
+}
+
+
+/* Timers */
+
+struct dwc_timer {
+	struct timer_list *t;
+	char *name;
+	dwc_timer_callback_t cb;
+	void *data;
+	uint8_t scheduled;
+	dwc_spinlock_t *lock;
+};
+
+static void timer_callback(unsigned long data)
+{
+	dwc_timer_t *timer = (dwc_timer_t *)data;
+	dwc_irqflags_t flags;
+
+	DWC_SPINLOCK_IRQSAVE(timer->lock, &flags);
+	timer->scheduled = 0;
+	DWC_SPINUNLOCK_IRQRESTORE(timer->lock, flags);
+	DWC_DEBUG("Timer %s callback", timer->name);
+	timer->cb(timer->data);
+}
+
+dwc_timer_t *DWC_TIMER_ALLOC(char *name, dwc_timer_callback_t cb, void *data)
+{
+	dwc_timer_t *t = DWC_ALLOC(sizeof(*t));
+
+	if (!t) {
+		DWC_ERROR("Cannot allocate memory for timer");
+		return NULL;
+	}
+
+	t->t = DWC_ALLOC(sizeof(*t->t));
+	if (!t->t) {
+		DWC_ERROR("Cannot allocate memory for timer->t");
+		goto no_timer;
+	}
+
+	t->name = DWC_STRDUP(name);
+	if (!t->name) {
+		DWC_ERROR("Cannot allocate memory for timer->name");
+		goto no_name;
+	}
+
+	t->lock = DWC_SPINLOCK_ALLOC();
+	if (!t->lock) {
+		DWC_ERROR("Cannot allocate memory for lock");
+		goto no_lock;
+	}
+
+	t->scheduled = 0;
+	t->t->base = &boot_tvec_bases;
+	t->t->expires = jiffies;
+	setup_timer(t->t, timer_callback, (unsigned long)t);
+
+	t->cb = cb;
+	t->data = data;
+
+	return t;
+
+ no_lock:
+	DWC_FREE(t->name);
+ no_name:
+	DWC_FREE(t->t);
+ no_timer:
+	DWC_FREE(t);
+	return NULL;
+}
+
+void DWC_TIMER_FREE(dwc_timer_t *timer)
+{
+	dwc_irqflags_t flags;
+
+	DWC_SPINLOCK_IRQSAVE(timer->lock, &flags);
+
+	if (timer->scheduled) {
+		del_timer(timer->t);
+		timer->scheduled = 0;
+	}
+
+	DWC_SPINUNLOCK_IRQRESTORE(timer->lock, flags);
+	DWC_SPINLOCK_FREE(timer->lock);
+	DWC_FREE(timer->t);
+	DWC_FREE(timer->name);
+	DWC_FREE(timer);
+}
+
+void DWC_TIMER_SCHEDULE(dwc_timer_t *timer, uint32_t time)
+{
+	dwc_irqflags_t flags;
+
+	DWC_SPINLOCK_IRQSAVE(timer->lock, &flags);
+
+	if (!timer->scheduled) {
+		timer->scheduled = 1;
+		DWC_DEBUG("Scheduling timer %s to expire in +%d msec", timer->name, time);
+		timer->t->expires = jiffies + msecs_to_jiffies(time);
+		add_timer(timer->t);
+	} else {
+		DWC_DEBUG("Modifying timer %s to expire in +%d msec", timer->name, time);
+		mod_timer(timer->t, jiffies + msecs_to_jiffies(time));
+	}
+
+	DWC_SPINUNLOCK_IRQRESTORE(timer->lock, flags);
+}
+
+void DWC_TIMER_CANCEL(dwc_timer_t *timer)
+{
+	del_timer(timer->t);
+}
+
+
+/* Wait Queues */
+
+struct dwc_waitq {
+	wait_queue_head_t queue;
+	int abort;
+};
+
+dwc_waitq_t *DWC_WAITQ_ALLOC(void)
+{
+	dwc_waitq_t *wq = DWC_ALLOC(sizeof(*wq));
+
+	if (!wq) {
+		DWC_ERROR("Cannot allocate memory for waitqueue\n");
+		return NULL;
+	}
+
+	init_waitqueue_head(&wq->queue);
+	wq->abort = 0;
+	return wq;
+}
+
+void DWC_WAITQ_FREE(dwc_waitq_t *wq)
+{
+	DWC_FREE(wq);
+}
+
+int32_t DWC_WAITQ_WAIT(dwc_waitq_t *wq, dwc_waitq_condition_t cond, void *data)
+{
+	int result = wait_event_interruptible(wq->queue,
+					      cond(data) || wq->abort);
+	if (result == -ERESTARTSYS) {
+		wq->abort = 0;
+		return -DWC_E_RESTART;
+	}
+
+	if (wq->abort == 1) {
+		wq->abort = 0;
+		return -DWC_E_ABORT;
+	}
+
+	wq->abort = 0;
+
+	if (result == 0) {
+		return 0;
+	}
+
+	return -DWC_E_UNKNOWN;
+}
+
+int32_t DWC_WAITQ_WAIT_TIMEOUT(dwc_waitq_t *wq, dwc_waitq_condition_t cond,
+			       void *data, int32_t msecs)
+{
+	int32_t tmsecs;
+	int result = wait_event_interruptible_timeout(wq->queue,
+						      cond(data) || wq->abort,
+						      msecs_to_jiffies(msecs));
+	if (result == -ERESTARTSYS) {
+		wq->abort = 0;
+		return -DWC_E_RESTART;
+	}
+
+	if (wq->abort == 1) {
+		wq->abort = 0;
+		return -DWC_E_ABORT;
+	}
+
+	wq->abort = 0;
+
+	if (result > 0) {
+		tmsecs = jiffies_to_msecs(result);
+		if (!tmsecs) {
+			return 1;
+		}
+
+		return tmsecs;
+	}
+
+	if (result == 0) {
+		return -DWC_E_TIMEOUT;
+	}
+
+	return -DWC_E_UNKNOWN;
+}
+
+void DWC_WAITQ_TRIGGER(dwc_waitq_t *wq)
+{
+	wq->abort = 0;
+	wake_up_interruptible(&wq->queue);
+}
+
+void DWC_WAITQ_ABORT(dwc_waitq_t *wq)
+{
+	wq->abort = 1;
+	wake_up_interruptible(&wq->queue);
+}
+
+
+/* Threading */
+
+dwc_thread_t *DWC_THREAD_RUN(dwc_thread_function_t func, char *name, void *data)
+{
+	struct task_struct *thread = kthread_run(func, data, name);
+
+	if (thread == ERR_PTR(-ENOMEM)) {
+		return NULL;
+	}
+
+	return (dwc_thread_t *)thread;
+}
+
+int DWC_THREAD_STOP(dwc_thread_t *thread)
+{
+	return kthread_stop((struct task_struct *)thread);
+}
+
+dwc_bool_t DWC_THREAD_SHOULD_STOP(void)
+{
+	return kthread_should_stop();
+}
+
+
+/* tasklets
+ - run in interrupt context (cannot sleep)
+ - each tasklet runs on a single CPU
+ - different tasklets can be running simultaneously on different CPUs
+ */
+struct dwc_tasklet {
+	struct tasklet_struct t;
+	dwc_tasklet_callback_t cb;
+	void *data;
+};
+
+static void tasklet_callback(unsigned long data)
+{
+	dwc_tasklet_t *t = (dwc_tasklet_t *)data;
+	t->cb(t->data);
+}
+
+dwc_tasklet_t *DWC_TASK_ALLOC(char *name, dwc_tasklet_callback_t cb, void *data)
+{
+	dwc_tasklet_t *t = DWC_ALLOC(sizeof(*t));
+
+	if (t) {
+		t->cb = cb;
+		t->data = data;
+		tasklet_init(&t->t, tasklet_callback, (unsigned long)t);
+	} else {
+		DWC_ERROR("Cannot allocate memory for tasklet\n");
+	}
+
+	return t;
+}
+
+void DWC_TASK_FREE(dwc_tasklet_t *task)
+{
+	DWC_FREE(task);
+}
+
+void DWC_TASK_SCHEDULE(dwc_tasklet_t *task)
+{
+	tasklet_schedule(&task->t);
+}
+
+
+/* workqueues
+ - run in process context (can sleep)
+ */
+typedef struct work_container {
+	dwc_work_callback_t cb;
+	void *data;
+	dwc_workq_t *wq;
+	char *name;
+
+#ifdef DEBUG
+	DWC_CIRCLEQ_ENTRY(work_container) entry;
+#endif
+	struct delayed_work work;
+} work_container_t;
+
+#ifdef DEBUG
+DWC_CIRCLEQ_HEAD(work_container_queue, work_container);
+#endif
+
+struct dwc_workq {
+	struct workqueue_struct *wq;
+	dwc_spinlock_t *lock;
+	dwc_waitq_t *waitq;
+	int pending;
+
+#ifdef DEBUG
+	struct work_container_queue entries;
+#endif
+};
+
+static void do_work(struct work_struct *work)
+{
+	dwc_irqflags_t flags;
+	struct delayed_work *dw = container_of(work, struct delayed_work, work);
+	work_container_t *container = container_of(dw, struct work_container, work);
+	dwc_workq_t *wq = container->wq;
+
+	container->cb(container->data);
+
+#ifdef DEBUG
+	DWC_CIRCLEQ_REMOVE(&wq->entries, container, entry);
+#endif
+	DWC_DEBUG("Work done: %s, container=%p", container->name, container);
+	if (container->name) {
+		DWC_FREE(container->name);
+	}
+	DWC_FREE(container);
+
+	DWC_SPINLOCK_IRQSAVE(wq->lock, &flags);
+	wq->pending--;
+	DWC_SPINUNLOCK_IRQRESTORE(wq->lock, flags);
+	DWC_WAITQ_TRIGGER(wq->waitq);
+}
+
+static int work_done(void *data)
+{
+	dwc_workq_t *workq = (dwc_workq_t *)data;
+	return workq->pending == 0;
+}
+
+int DWC_WORKQ_WAIT_WORK_DONE(dwc_workq_t *workq, int timeout)
+{
+	return DWC_WAITQ_WAIT_TIMEOUT(workq->waitq, work_done, workq, timeout);
+}
+
+dwc_workq_t *DWC_WORKQ_ALLOC(char *name)
+{
+	dwc_workq_t *wq = DWC_ALLOC(sizeof(*wq));
+
+	if (!wq) {
+		return NULL;
+	}
+
+	wq->wq = create_singlethread_workqueue(name);
+	if (!wq->wq) {
+		goto no_wq;
+	}
+
+	wq->pending = 0;
+
+	wq->lock = DWC_SPINLOCK_ALLOC();
+	if (!wq->lock) {
+		goto no_lock;
+	}
+
+	wq->waitq = DWC_WAITQ_ALLOC();
+	if (!wq->waitq) {
+		goto no_waitq;
+	}
+
+#ifdef DEBUG
+	DWC_CIRCLEQ_INIT(&wq->entries);
+#endif
+	return wq;
+
+ no_waitq:
+	DWC_SPINLOCK_FREE(wq->lock);
+ no_lock:
+	destroy_workqueue(wq->wq);
+ no_wq:
+	DWC_FREE(wq);
+
+	return NULL;
+}
+
+void DWC_WORKQ_FREE(dwc_workq_t *wq)
+{
+#ifdef DEBUG
+	if (wq->pending != 0) {
+		struct work_container *wc;
+		DWC_ERROR("Destroying work queue with pending work");
+		DWC_CIRCLEQ_FOREACH(wc, &wq->entries, entry) {
+			DWC_ERROR("Work %s still pending", wc->name);
+		}
+	}
+#endif
+	destroy_workqueue(wq->wq);
+	DWC_SPINLOCK_FREE(wq->lock);
+	DWC_WAITQ_FREE(wq->waitq);
+	DWC_FREE(wq);
+}
+
+void DWC_WORKQ_SCHEDULE(dwc_workq_t *wq, dwc_work_callback_t cb, void *data,
+			char *format, ...)
+{
+	dwc_irqflags_t flags;
+	work_container_t *container;
+	static char name[128];
+	va_list args;
+
+	va_start(args, format);
+	DWC_VSNPRINTF(name, 128, format, args);
+	va_end(args);
+
+	DWC_SPINLOCK_IRQSAVE(wq->lock, &flags);
+	wq->pending++;
+	DWC_SPINUNLOCK_IRQRESTORE(wq->lock, flags);
+	DWC_WAITQ_TRIGGER(wq->waitq);
+
+	container = DWC_ALLOC_ATOMIC(sizeof(*container));
+	if (!container) {
+		DWC_ERROR("Cannot allocate memory for container\n");
+		return;
+	}
+
+	container->name = DWC_STRDUP(name);
+	if (!container->name) {
+		DWC_ERROR("Cannot allocate memory for container->name\n");
+		DWC_FREE(container);
+		return;
+	}
+
+	container->cb = cb;
+	container->data = data;
+	container->wq = wq;
+	DWC_DEBUG("Queueing work: %s, container=%p", container->name, container);
+	INIT_WORK(&container->work.work, do_work);
+
+#ifdef DEBUG
+	DWC_CIRCLEQ_INSERT_TAIL(&wq->entries, container, entry);
+#endif
+	queue_work(wq->wq, &container->work.work);
+}
+
+void DWC_WORKQ_SCHEDULE_DELAYED(dwc_workq_t *wq, dwc_work_callback_t cb,
+				void *data, uint32_t time, char *format, ...)
+{
+	dwc_irqflags_t flags;
+	work_container_t *container;
+	static char name[128];
+	va_list args;
+
+	va_start(args, format);
+	DWC_VSNPRINTF(name, 128, format, args);
+	va_end(args);
+
+	DWC_SPINLOCK_IRQSAVE(wq->lock, &flags);
+	wq->pending++;
+	DWC_SPINUNLOCK_IRQRESTORE(wq->lock, flags);
+	DWC_WAITQ_TRIGGER(wq->waitq);
+
+	container = DWC_ALLOC_ATOMIC(sizeof(*container));
+	if (!container) {
+		DWC_ERROR("Cannot allocate memory for container\n");
+		return;
+	}
+
+	container->name = DWC_STRDUP(name);
+	if (!container->name) {
+		DWC_ERROR("Cannot allocate memory for container->name\n");
+		DWC_FREE(container);
+		return;
+	}
+
+	container->cb = cb;
+	container->data = data;
+	container->wq = wq;
+	DWC_DEBUG("Queueing work: %s, container=%p", container->name, container);
+	INIT_DELAYED_WORK(&container->work, do_work);
+
+#ifdef DEBUG
+	DWC_CIRCLEQ_INSERT_TAIL(&wq->entries, container, entry);
+#endif
+	queue_delayed_work(wq->wq, &container->work, msecs_to_jiffies(time));
+}
+
+int DWC_WORKQ_PENDING(dwc_workq_t *wq)
+{
+	return wq->pending;
+}
+
+
+#ifdef DWC_LIBMODULE
+
+#ifdef DWC_CCLIB
+/* CC */
+EXPORT_SYMBOL(dwc_cc_if_alloc);
+EXPORT_SYMBOL(dwc_cc_if_free);
+EXPORT_SYMBOL(dwc_cc_clear);
+EXPORT_SYMBOL(dwc_cc_add);
+EXPORT_SYMBOL(dwc_cc_remove);
+EXPORT_SYMBOL(dwc_cc_change);
+EXPORT_SYMBOL(dwc_cc_data_for_save);
+EXPORT_SYMBOL(dwc_cc_restore_from_data);
+EXPORT_SYMBOL(dwc_cc_match_chid);
+EXPORT_SYMBOL(dwc_cc_match_cdid);
+EXPORT_SYMBOL(dwc_cc_ck);
+EXPORT_SYMBOL(dwc_cc_chid);
+EXPORT_SYMBOL(dwc_cc_cdid);
+EXPORT_SYMBOL(dwc_cc_name);
+#endif	/* DWC_CCLIB */
+
+#ifdef DWC_CRYPTOLIB
+# ifndef CONFIG_MACH_IPMATE
+/* Modpow */
+EXPORT_SYMBOL(dwc_modpow);
+
+/* DH */
+EXPORT_SYMBOL(dwc_dh_modpow);
+EXPORT_SYMBOL(dwc_dh_derive_keys);
+EXPORT_SYMBOL(dwc_dh_pk);
+# endif	/* CONFIG_MACH_IPMATE */
+
+/* Crypto */
+EXPORT_SYMBOL(dwc_wusb_aes_encrypt);
+EXPORT_SYMBOL(dwc_wusb_cmf);
+EXPORT_SYMBOL(dwc_wusb_prf);
+EXPORT_SYMBOL(dwc_wusb_fill_ccm_nonce);
+EXPORT_SYMBOL(dwc_wusb_gen_nonce);
+EXPORT_SYMBOL(dwc_wusb_gen_key);
+EXPORT_SYMBOL(dwc_wusb_gen_mic);
+#endif	/* DWC_CRYPTOLIB */
+
+/* Notification */
+#ifdef DWC_NOTIFYLIB
+EXPORT_SYMBOL(dwc_alloc_notification_manager);
+EXPORT_SYMBOL(dwc_free_notification_manager);
+EXPORT_SYMBOL(dwc_register_notifier);
+EXPORT_SYMBOL(dwc_unregister_notifier);
+EXPORT_SYMBOL(dwc_add_observer);
+EXPORT_SYMBOL(dwc_remove_observer);
+EXPORT_SYMBOL(dwc_notify);
+#endif
+
+/* Memory Debugging Routines */
+#ifdef DWC_DEBUG_MEMORY
+EXPORT_SYMBOL(dwc_alloc_debug);
+EXPORT_SYMBOL(dwc_alloc_atomic_debug);
+EXPORT_SYMBOL(dwc_free_debug);
+EXPORT_SYMBOL(dwc_dma_alloc_debug);
+EXPORT_SYMBOL(dwc_dma_free_debug);
+#endif
+
+EXPORT_SYMBOL(DWC_MEMSET);
+EXPORT_SYMBOL(DWC_MEMCPY);
+EXPORT_SYMBOL(DWC_MEMMOVE);
+EXPORT_SYMBOL(DWC_MEMCMP);
+EXPORT_SYMBOL(DWC_STRNCMP);
+EXPORT_SYMBOL(DWC_STRCMP);
+EXPORT_SYMBOL(DWC_STRLEN);
+EXPORT_SYMBOL(DWC_STRCPY);
+EXPORT_SYMBOL(DWC_STRDUP);
+EXPORT_SYMBOL(DWC_ATOI);
+EXPORT_SYMBOL(DWC_ATOUI);
+
+#ifdef DWC_UTFLIB
+EXPORT_SYMBOL(DWC_UTF8_TO_UTF16LE);
+#endif	/* DWC_UTFLIB */
+
+EXPORT_SYMBOL(DWC_IN_IRQ);
+EXPORT_SYMBOL(DWC_IN_BH);
+EXPORT_SYMBOL(DWC_VPRINTF);
+EXPORT_SYMBOL(DWC_VSNPRINTF);
+EXPORT_SYMBOL(DWC_PRINTF);
+EXPORT_SYMBOL(DWC_SPRINTF);
+EXPORT_SYMBOL(DWC_SNPRINTF);
+EXPORT_SYMBOL(__DWC_WARN);
+EXPORT_SYMBOL(__DWC_ERROR);
+EXPORT_SYMBOL(DWC_EXCEPTION);
+
+#ifdef DEBUG
+EXPORT_SYMBOL(__DWC_DEBUG);
+#endif
+
+EXPORT_SYMBOL(__DWC_DMA_ALLOC);
+EXPORT_SYMBOL(__DWC_DMA_ALLOC_ATOMIC);
+EXPORT_SYMBOL(__DWC_DMA_FREE);
+EXPORT_SYMBOL(__DWC_ALLOC);
+EXPORT_SYMBOL(__DWC_ALLOC_ATOMIC);
+EXPORT_SYMBOL(__DWC_FREE);
+
+#ifdef DWC_CRYPTOLIB
+EXPORT_SYMBOL(DWC_RANDOM_BYTES);
+EXPORT_SYMBOL(DWC_AES_CBC);
+EXPORT_SYMBOL(DWC_SHA256);
+EXPORT_SYMBOL(DWC_HMAC_SHA256);
+#endif
+
+EXPORT_SYMBOL(DWC_CPU_TO_LE32);
+EXPORT_SYMBOL(DWC_CPU_TO_BE32);
+EXPORT_SYMBOL(DWC_LE32_TO_CPU);
+EXPORT_SYMBOL(DWC_BE32_TO_CPU);
+EXPORT_SYMBOL(DWC_CPU_TO_LE16);
+EXPORT_SYMBOL(DWC_CPU_TO_BE16);
+EXPORT_SYMBOL(DWC_LE16_TO_CPU);
+EXPORT_SYMBOL(DWC_BE16_TO_CPU);
+EXPORT_SYMBOL(DWC_READ_REG32);
+EXPORT_SYMBOL(DWC_WRITE_REG32);
+EXPORT_SYMBOL(DWC_MODIFY_REG32);
+
+#if 0
+EXPORT_SYMBOL(DWC_READ_REG64);
+EXPORT_SYMBOL(DWC_WRITE_REG64);
+EXPORT_SYMBOL(DWC_MODIFY_REG64);
+#endif
+
+EXPORT_SYMBOL(DWC_SPINLOCK_ALLOC);
+EXPORT_SYMBOL(DWC_SPINLOCK_FREE);
+EXPORT_SYMBOL(DWC_SPINLOCK);
+EXPORT_SYMBOL(DWC_SPINUNLOCK);
+EXPORT_SYMBOL(DWC_SPINLOCK_IRQSAVE);
+EXPORT_SYMBOL(DWC_SPINUNLOCK_IRQRESTORE);
+EXPORT_SYMBOL(DWC_MUTEX_ALLOC);
+
+#if (!defined(DWC_LINUX) || !defined(CONFIG_DEBUG_MUTEXES))
+EXPORT_SYMBOL(DWC_MUTEX_FREE);
+#endif
+
+EXPORT_SYMBOL(DWC_MUTEX_LOCK);
+EXPORT_SYMBOL(DWC_MUTEX_TRYLOCK);
+EXPORT_SYMBOL(DWC_MUTEX_UNLOCK);
+EXPORT_SYMBOL(DWC_UDELAY);
+EXPORT_SYMBOL(DWC_MDELAY);
+EXPORT_SYMBOL(DWC_MSLEEP);
+EXPORT_SYMBOL(DWC_TIME);
+EXPORT_SYMBOL(DWC_TIMER_ALLOC);
+EXPORT_SYMBOL(DWC_TIMER_FREE);
+EXPORT_SYMBOL(DWC_TIMER_SCHEDULE);
+EXPORT_SYMBOL(DWC_TIMER_CANCEL);
+EXPORT_SYMBOL(DWC_WAITQ_ALLOC);
+EXPORT_SYMBOL(DWC_WAITQ_FREE);
+EXPORT_SYMBOL(DWC_WAITQ_WAIT);
+EXPORT_SYMBOL(DWC_WAITQ_WAIT_TIMEOUT);
+EXPORT_SYMBOL(DWC_WAITQ_TRIGGER);
+EXPORT_SYMBOL(DWC_WAITQ_ABORT);
+EXPORT_SYMBOL(DWC_THREAD_RUN);
+EXPORT_SYMBOL(DWC_THREAD_STOP);
+EXPORT_SYMBOL(DWC_THREAD_SHOULD_STOP);
+EXPORT_SYMBOL(DWC_TASK_ALLOC);
+EXPORT_SYMBOL(DWC_TASK_FREE);
+EXPORT_SYMBOL(DWC_TASK_SCHEDULE);
+EXPORT_SYMBOL(DWC_WORKQ_WAIT_WORK_DONE);
+EXPORT_SYMBOL(DWC_WORKQ_ALLOC);
+EXPORT_SYMBOL(DWC_WORKQ_FREE);
+EXPORT_SYMBOL(DWC_WORKQ_SCHEDULE);
+EXPORT_SYMBOL(DWC_WORKQ_SCHEDULE_DELAYED);
+EXPORT_SYMBOL(DWC_WORKQ_PENDING);
+
+static int dwc_common_port_init_module(void)
+{
+	int result = 0;
+
+	printk(KERN_DEBUG "Module dwc_common_port init\n" );
+
+#ifdef DWC_DEBUG_MEMORY
+	result = dwc_memory_debug_start(NULL);
+	if (result) {
+		printk(KERN_ERR
+		       "dwc_memory_debug_start() failed with error %d\n",
+		       result);
+		return result;
+	}
+#endif
+
+#ifdef DWC_NOTIFYLIB
+	result = dwc_alloc_notification_manager(NULL, NULL);
+	if (result) {
+		printk(KERN_ERR
+		       "dwc_alloc_notification_manager() failed with error %d\n",
+		       result);
+		return result;
+	}
+#endif
+	return result;
+}
+
+static void dwc_common_port_exit_module(void)
+{
+	printk(KERN_DEBUG "Module dwc_common_port exit\n" );
+
+#ifdef DWC_NOTIFYLIB
+	dwc_free_notification_manager();
+#endif
+
+#ifdef DWC_DEBUG_MEMORY
+	dwc_memory_debug_stop();
+#endif
+}
+
+module_init(dwc_common_port_init_module);
+module_exit(dwc_common_port_exit_module);
+
+MODULE_DESCRIPTION("DWC Common Library - Portable version");
+MODULE_AUTHOR("Synopsys Inc.");
+MODULE_LICENSE ("GPL");
+
+#endif	/* DWC_LIBMODULE */
diff --git a/drivers/usb/dwc_otg/dwc_common_nbsd.c b/drivers/usb/dwc_otg/dwc_common_nbsd.c
new file mode 100644
index 0000000..49b07e1
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_common_nbsd.c
@@ -0,0 +1,1275 @@
+#include "dwc_os.h"
+#include "dwc_list.h"
+
+#ifdef DWC_CCLIB
+# include "dwc_cc.h"
+#endif
+
+#ifdef DWC_CRYPTOLIB
+# include "dwc_modpow.h"
+# include "dwc_dh.h"
+# include "dwc_crypto.h"
+#endif
+
+#ifdef DWC_NOTIFYLIB
+# include "dwc_notifier.h"
+#endif
+
+/* OS-Level Implementations */
+
+/* This is the NetBSD 4.0.1 kernel implementation of the DWC platform library. */
+
+
+/* MISC */
+
+void *DWC_MEMSET(void *dest, uint8_t byte, uint32_t size)
+{
+	return memset(dest, byte, size);
+}
+
+void *DWC_MEMCPY(void *dest, void const *src, uint32_t size)
+{
+	return memcpy(dest, src, size);
+}
+
+void *DWC_MEMMOVE(void *dest, void *src, uint32_t size)
+{
+	bcopy(src, dest, size);
+	return dest;
+}
+
+int DWC_MEMCMP(void *m1, void *m2, uint32_t size)
+{
+	return memcmp(m1, m2, size);
+}
+
+int DWC_STRNCMP(void *s1, void *s2, uint32_t size)
+{
+	return strncmp(s1, s2, size);
+}
+
+int DWC_STRCMP(void *s1, void *s2)
+{
+	return strcmp(s1, s2);
+}
+
+int DWC_STRLEN(char const *str)
+{
+	return strlen(str);
+}
+
+char *DWC_STRCPY(char *to, char const *from)
+{
+	return strcpy(to, from);
+}
+
+char *DWC_STRDUP(char const *str)
+{
+	int len = DWC_STRLEN(str) + 1;
+	char *new = DWC_ALLOC_ATOMIC(len);
+
+	if (!new) {
+		return NULL;
+	}
+
+	DWC_MEMCPY(new, str, len);
+	return new;
+}
+
+int DWC_ATOI(char *str, int32_t *value)
+{
+	char *end = NULL;
+
+	/* NetBSD doesn't have 'strtol' in the kernel, but 'strtoul'
+	 * should be equivalent on 2's complement machines
+	 */
+	*value = strtoul(str, &end, 0);
+	if (*end == '\0') {
+		return 0;
+	}
+
+	return -1;
+}
+
+int DWC_ATOUI(char *str, uint32_t *value)
+{
+	char *end = NULL;
+
+	*value = strtoul(str, &end, 0);
+	if (*end == '\0') {
+		return 0;
+	}
+
+	return -1;
+}
+
+
+#ifdef DWC_UTFLIB
+/* From usbstring.c */
+
+int DWC_UTF8_TO_UTF16LE(uint8_t const *s, uint16_t *cp, unsigned len)
+{
+	int	count = 0;
+	u8	c;
+	u16	uchar;
+
+	/* this insists on correct encodings, though not minimal ones.
+	 * BUT it currently rejects legit 4-byte UTF-8 code points,
+	 * which need surrogate pairs.  (Unicode 3.1 can use them.)
+	 */
+	while (len != 0 && (c = (u8) *s++) != 0) {
+		if (unlikely(c & 0x80)) {
+			// 2-byte sequence:
+			// 00000yyyyyxxxxxx = 110yyyyy 10xxxxxx
+			if ((c & 0xe0) == 0xc0) {
+				uchar = (c & 0x1f) << 6;
+
+				c = (u8) *s++;
+				if ((c & 0xc0) != 0xc0)
+					goto fail;
+				c &= 0x3f;
+				uchar |= c;
+
+			// 3-byte sequence (most CJKV characters):
+			// zzzzyyyyyyxxxxxx = 1110zzzz 10yyyyyy 10xxxxxx
+			} else if ((c & 0xf0) == 0xe0) {
+				uchar = (c & 0x0f) << 12;
+
+				c = (u8) *s++;
+				if ((c & 0xc0) != 0xc0)
+					goto fail;
+				c &= 0x3f;
+				uchar |= c << 6;
+
+				c = (u8) *s++;
+				if ((c & 0xc0) != 0xc0)
+					goto fail;
+				c &= 0x3f;
+				uchar |= c;
+
+				/* no bogus surrogates */
+				if (0xd800 <= uchar && uchar <= 0xdfff)
+					goto fail;
+
+			// 4-byte sequence (surrogate pairs, currently rare):
+			// 11101110wwwwzzzzyy + 110111yyyyxxxxxx
+			//     = 11110uuu 10uuzzzz 10yyyyyy 10xxxxxx
+			// (uuuuu = wwww + 1)
+			// FIXME accept the surrogate code points (only)
+			} else
+				goto fail;
+		} else
+			uchar = c;
+		put_unaligned (cpu_to_le16 (uchar), cp++);
+		count++;
+		len--;
+	}
+	return count;
+fail:
+	return -1;
+}
+
+#endif	/* DWC_UTFLIB */
+
+
+/* dwc_debug.h */
+
+dwc_bool_t DWC_IN_IRQ(void)
+{
+//	return in_irq();
+	return 0;
+}
+
+dwc_bool_t DWC_IN_BH(void)
+{
+//	return in_softirq();
+	return 0;
+}
+
+void DWC_VPRINTF(char *format, va_list args)
+{
+	vprintf(format, args);
+}
+
+int DWC_VSNPRINTF(char *str, int size, char *format, va_list args)
+{
+	return vsnprintf(str, size, format, args);
+}
+
+void DWC_PRINTF(char *format, ...)
+{
+	va_list args;
+
+	va_start(args, format);
+	DWC_VPRINTF(format, args);
+	va_end(args);
+}
+
+int DWC_SPRINTF(char *buffer, char *format, ...)
+{
+	int retval;
+	va_list args;
+
+	va_start(args, format);
+	retval = vsprintf(buffer, format, args);
+	va_end(args);
+	return retval;
+}
+
+int DWC_SNPRINTF(char *buffer, int size, char *format, ...)
+{
+	int retval;
+	va_list args;
+
+	va_start(args, format);
+	retval = vsnprintf(buffer, size, format, args);
+	va_end(args);
+	return retval;
+}
+
+void __DWC_WARN(char *format, ...)
+{
+	va_list args;
+
+	va_start(args, format);
+	DWC_VPRINTF(format, args);
+	va_end(args);
+}
+
+void __DWC_ERROR(char *format, ...)
+{
+	va_list args;
+
+	va_start(args, format);
+	DWC_VPRINTF(format, args);
+	va_end(args);
+}
+
+void DWC_EXCEPTION(char *format, ...)
+{
+	va_list args;
+
+	va_start(args, format);
+	DWC_VPRINTF(format, args);
+	va_end(args);
+//	BUG_ON(1);	???
+}
+
+#ifdef DEBUG
+void __DWC_DEBUG(char *format, ...)
+{
+	va_list args;
+
+	va_start(args, format);
+	DWC_VPRINTF(format, args);
+	va_end(args);
+}
+#endif
+
+
+/* dwc_mem.h */
+
+#if 0
+dwc_pool_t *DWC_DMA_POOL_CREATE(uint32_t size,
+				uint32_t align,
+				uint32_t alloc)
+{
+	struct dma_pool *pool = dma_pool_create("Pool", NULL,
+						size, align, alloc);
+	return (dwc_pool_t *)pool;
+}
+
+void DWC_DMA_POOL_DESTROY(dwc_pool_t *pool)
+{
+	dma_pool_destroy((struct dma_pool *)pool);
+}
+
+void *DWC_DMA_POOL_ALLOC(dwc_pool_t *pool, uint64_t *dma_addr)
+{
+//	return dma_pool_alloc((struct dma_pool *)pool, GFP_KERNEL, dma_addr);
+	return dma_pool_alloc((struct dma_pool *)pool, M_WAITOK, dma_addr);
+}
+
+void *DWC_DMA_POOL_ZALLOC(dwc_pool_t *pool, uint64_t *dma_addr)
+{
+	void *vaddr = DWC_DMA_POOL_ALLOC(pool, dma_addr);
+	memset(..);
+}
+
+void DWC_DMA_POOL_FREE(dwc_pool_t *pool, void *vaddr, void *daddr)
+{
+	dma_pool_free(pool, vaddr, daddr);
+}
+#endif
+
+void *__DWC_DMA_ALLOC(void *dma_ctx, uint32_t size, dwc_dma_t *dma_addr)
+{
+	dwc_dmactx_t *dma = (dwc_dmactx_t *)dma_ctx;
+	int error;
+
+	error = bus_dmamem_alloc(dma->dma_tag, size, 1, size, dma->segs,
+				 sizeof(dma->segs) / sizeof(dma->segs[0]),
+				 &dma->nsegs, BUS_DMA_NOWAIT);
+	if (error) {
+		printf("%s: bus_dmamem_alloc(%ju) failed: %d\n", __func__,
+		       (uintmax_t)size, error);
+		goto fail_0;
+	}
+
+	error = bus_dmamem_map(dma->dma_tag, dma->segs, dma->nsegs, size,
+			       (caddr_t *)&dma->dma_vaddr,
+			       BUS_DMA_NOWAIT | BUS_DMA_COHERENT);
+	if (error) {
+		printf("%s: bus_dmamem_map failed: %d\n", __func__, error);
+		goto fail_1;
+	}
+
+	error = bus_dmamap_create(dma->dma_tag, size, 1, size, 0,
+				  BUS_DMA_NOWAIT, &dma->dma_map);
+	if (error) {
+		printf("%s: bus_dmamap_create failed: %d\n", __func__, error);
+		goto fail_2;
+	}
+
+	error = bus_dmamap_load(dma->dma_tag, dma->dma_map, dma->dma_vaddr,
+				size, NULL, BUS_DMA_NOWAIT);
+	if (error) {
+		printf("%s: bus_dmamap_load failed: %d\n", __func__, error);
+		goto fail_3;
+	}
+
+	dma->dma_paddr = (bus_addr_t)dma->segs[0].ds_addr;
+	*dma_addr = dma->dma_paddr;
+	return dma->dma_vaddr;
+
+fail_3:
+	bus_dmamap_destroy(dma->dma_tag, dma->dma_map);
+fail_2:
+	bus_dmamem_unmap(dma->dma_tag, dma->dma_vaddr, size);
+fail_1:
+	bus_dmamem_free(dma->dma_tag, dma->segs, dma->nsegs);
+fail_0:
+	dma->dma_map = NULL;
+	dma->dma_vaddr = NULL;
+	dma->nsegs = 0;
+
+	return NULL;
+}
+
+void __DWC_DMA_FREE(void *dma_ctx, uint32_t size, void *virt_addr, dwc_dma_t dma_addr)
+{
+	dwc_dmactx_t *dma = (dwc_dmactx_t *)dma_ctx;
+
+	if (dma->dma_map != NULL) {
+		bus_dmamap_sync(dma->dma_tag, dma->dma_map, 0, size,
+				BUS_DMASYNC_POSTREAD | BUS_DMASYNC_POSTWRITE);
+		bus_dmamap_unload(dma->dma_tag, dma->dma_map);
+		bus_dmamap_destroy(dma->dma_tag, dma->dma_map);
+		bus_dmamem_unmap(dma->dma_tag, dma->dma_vaddr, size);
+		bus_dmamem_free(dma->dma_tag, dma->segs, dma->nsegs);
+		dma->dma_paddr = 0;
+		dma->dma_map = NULL;
+		dma->dma_vaddr = NULL;
+		dma->nsegs = 0;
+	}
+}
+
+void *__DWC_ALLOC(void *mem_ctx, uint32_t size)
+{
+	return malloc(size, M_DEVBUF, M_WAITOK | M_ZERO);
+}
+
+void *__DWC_ALLOC_ATOMIC(void *mem_ctx, uint32_t size)
+{
+	return malloc(size, M_DEVBUF, M_NOWAIT | M_ZERO);
+}
+
+void __DWC_FREE(void *mem_ctx, void *addr)
+{
+	free(addr, M_DEVBUF);
+}
+
+
+#ifdef DWC_CRYPTOLIB
+/* dwc_crypto.h */
+
+void DWC_RANDOM_BYTES(uint8_t *buffer, uint32_t length)
+{
+	get_random_bytes(buffer, length);
+}
+
+int DWC_AES_CBC(uint8_t *message, uint32_t messagelen, uint8_t *key, uint32_t keylen, uint8_t iv[16], uint8_t *out)
+{
+	struct crypto_blkcipher *tfm;
+	struct blkcipher_desc desc;
+	struct scatterlist sgd;
+	struct scatterlist sgs;
+
+	tfm = crypto_alloc_blkcipher("cbc(aes)", 0, CRYPTO_ALG_ASYNC);
+	if (tfm == NULL) {
+		printk("failed to load transform for aes CBC\n");
+		return -1;
+	}
+
+	crypto_blkcipher_setkey(tfm, key, keylen);
+	crypto_blkcipher_set_iv(tfm, iv, 16);
+
+	sg_init_one(&sgd, out, messagelen);
+	sg_init_one(&sgs, message, messagelen);
+
+	desc.tfm = tfm;
+	desc.flags = 0;
+
+	if (crypto_blkcipher_encrypt(&desc, &sgd, &sgs, messagelen)) {
+		crypto_free_blkcipher(tfm);
+		DWC_ERROR("AES CBC encryption failed");
+		return -1;
+	}
+
+	crypto_free_blkcipher(tfm);
+	return 0;
+}
+
+int DWC_SHA256(uint8_t *message, uint32_t len, uint8_t *out)
+{
+	struct crypto_hash *tfm;
+	struct hash_desc desc;
+	struct scatterlist sg;
+
+	tfm = crypto_alloc_hash("sha256", 0, CRYPTO_ALG_ASYNC);
+	if (IS_ERR(tfm)) {
+		DWC_ERROR("Failed to load transform for sha256: %ld", PTR_ERR(tfm));
+		return 0;
+	}
+	desc.tfm = tfm;
+	desc.flags = 0;
+
+	sg_init_one(&sg, message, len);
+	crypto_hash_digest(&desc, &sg, len, out);
+	crypto_free_hash(tfm);
+
+	return 1;
+}
+
+int DWC_HMAC_SHA256(uint8_t *message, uint32_t messagelen,
+		    uint8_t *key, uint32_t keylen, uint8_t *out)
+{
+	struct crypto_hash *tfm;
+	struct hash_desc desc;
+	struct scatterlist sg;
+
+	tfm = crypto_alloc_hash("hmac(sha256)", 0, CRYPTO_ALG_ASYNC);
+	if (IS_ERR(tfm)) {
+		DWC_ERROR("Failed to load transform for hmac(sha256): %ld", PTR_ERR(tfm));
+		return 0;
+	}
+	desc.tfm = tfm;
+	desc.flags = 0;
+
+	sg_init_one(&sg, message, messagelen);
+	crypto_hash_setkey(tfm, key, keylen);
+	crypto_hash_digest(&desc, &sg, messagelen, out);
+	crypto_free_hash(tfm);
+
+	return 1;
+}
+
+#endif	/* DWC_CRYPTOLIB */
+
+
+/* Byte Ordering Conversions */
+
+uint32_t DWC_CPU_TO_LE32(uint32_t *p)
+{
+#ifdef __LITTLE_ENDIAN
+	return *p;
+#else
+	uint8_t *u_p = (uint8_t *)p;
+
+	return (u_p[3] | (u_p[2] << 8) | (u_p[1] << 16) | (u_p[0] << 24));
+#endif
+}
+
+uint32_t DWC_CPU_TO_BE32(uint32_t *p)
+{
+#ifdef __BIG_ENDIAN
+	return *p;
+#else
+	uint8_t *u_p = (uint8_t *)p;
+
+	return (u_p[3] | (u_p[2] << 8) | (u_p[1] << 16) | (u_p[0] << 24));
+#endif
+}
+
+uint32_t DWC_LE32_TO_CPU(uint32_t *p)
+{
+#ifdef __LITTLE_ENDIAN
+	return *p;
+#else
+	uint8_t *u_p = (uint8_t *)p;
+
+	return (u_p[3] | (u_p[2] << 8) | (u_p[1] << 16) | (u_p[0] << 24));
+#endif
+}
+
+uint32_t DWC_BE32_TO_CPU(uint32_t *p)
+{
+#ifdef __BIG_ENDIAN
+	return *p;
+#else
+	uint8_t *u_p = (uint8_t *)p;
+
+	return (u_p[3] | (u_p[2] << 8) | (u_p[1] << 16) | (u_p[0] << 24));
+#endif
+}
+
+uint16_t DWC_CPU_TO_LE16(uint16_t *p)
+{
+#ifdef __LITTLE_ENDIAN
+	return *p;
+#else
+	uint8_t *u_p = (uint8_t *)p;
+	return (u_p[1] | (u_p[0] << 8));
+#endif
+}
+
+uint16_t DWC_CPU_TO_BE16(uint16_t *p)
+{
+#ifdef __BIG_ENDIAN
+	return *p;
+#else
+	uint8_t *u_p = (uint8_t *)p;
+	return (u_p[1] | (u_p[0] << 8));
+#endif
+}
+
+uint16_t DWC_LE16_TO_CPU(uint16_t *p)
+{
+#ifdef __LITTLE_ENDIAN
+	return *p;
+#else
+	uint8_t *u_p = (uint8_t *)p;
+	return (u_p[1] | (u_p[0] << 8));
+#endif
+}
+
+uint16_t DWC_BE16_TO_CPU(uint16_t *p)
+{
+#ifdef __BIG_ENDIAN
+	return *p;
+#else
+	uint8_t *u_p = (uint8_t *)p;
+	return (u_p[1] | (u_p[0] << 8));
+#endif
+}
+
+
+/* Registers */
+
+uint32_t DWC_READ_REG32(void *io_ctx, uint32_t volatile *reg)
+{
+	dwc_ioctx_t *io = (dwc_ioctx_t *)io_ctx;
+	bus_size_t ior = (bus_size_t)reg;
+
+	return bus_space_read_4(io->iot, io->ioh, ior);
+}
+
+#if 0
+uint64_t DWC_READ_REG64(void *io_ctx, uint64_t volatile *reg)
+{
+	dwc_ioctx_t *io = (dwc_ioctx_t *)io_ctx;
+	bus_size_t ior = (bus_size_t)reg;
+
+	return bus_space_read_8(io->iot, io->ioh, ior);
+}
+#endif
+
+void DWC_WRITE_REG32(void *io_ctx, uint32_t volatile *reg, uint32_t value)
+{
+	dwc_ioctx_t *io = (dwc_ioctx_t *)io_ctx;
+	bus_size_t ior = (bus_size_t)reg;
+
+	bus_space_write_4(io->iot, io->ioh, ior, value);
+}
+
+#if 0
+void DWC_WRITE_REG64(void *io_ctx, uint64_t volatile *reg, uint64_t value)
+{
+	dwc_ioctx_t *io = (dwc_ioctx_t *)io_ctx;
+	bus_size_t ior = (bus_size_t)reg;
+
+	bus_space_write_8(io->iot, io->ioh, ior, value);
+}
+#endif
+
+void DWC_MODIFY_REG32(void *io_ctx, uint32_t volatile *reg, uint32_t clear_mask,
+		      uint32_t set_mask)
+{
+	dwc_ioctx_t *io = (dwc_ioctx_t *)io_ctx;
+	bus_size_t ior = (bus_size_t)reg;
+
+	bus_space_write_4(io->iot, io->ioh, ior,
+			  (bus_space_read_4(io->iot, io->ioh, ior) &
+			   ~clear_mask) | set_mask);
+}
+
+#if 0
+void DWC_MODIFY_REG64(void *io_ctx, uint64_t volatile *reg, uint64_t clear_mask,
+		      uint64_t set_mask)
+{
+	dwc_ioctx_t *io = (dwc_ioctx_t *)io_ctx;
+	bus_size_t ior = (bus_size_t)reg;
+
+	bus_space_write_8(io->iot, io->ioh, ior,
+			  (bus_space_read_8(io->iot, io->ioh, ior) &
+			   ~clear_mask) | set_mask);
+}
+#endif
+
+
+/* Locking */
+
+dwc_spinlock_t *DWC_SPINLOCK_ALLOC(void)
+{
+	struct simplelock *sl = DWC_ALLOC(sizeof(*sl));
+
+	if (!sl) {
+		DWC_ERROR("Cannot allocate memory for spinlock");
+		return NULL;
+	}
+
+	simple_lock_init(sl);
+	return (dwc_spinlock_t *)sl;
+}
+
+void DWC_SPINLOCK_FREE(dwc_spinlock_t *lock)
+{
+	struct simplelock *sl = (struct simplelock *)lock;
+
+	DWC_FREE(sl);
+}
+
+void DWC_SPINLOCK(dwc_spinlock_t *lock)
+{
+	simple_lock((struct simplelock *)lock);
+}
+
+void DWC_SPINUNLOCK(dwc_spinlock_t *lock)
+{
+	simple_unlock((struct simplelock *)lock);
+}
+
+void DWC_SPINLOCK_IRQSAVE(dwc_spinlock_t *lock, dwc_irqflags_t *flags)
+{
+	simple_lock((struct simplelock *)lock);
+	*flags = splbio();
+}
+
+void DWC_SPINUNLOCK_IRQRESTORE(dwc_spinlock_t *lock, dwc_irqflags_t flags)
+{
+	splx(flags);
+	simple_unlock((struct simplelock *)lock);
+}
+
+dwc_mutex_t *DWC_MUTEX_ALLOC(void)
+{
+	dwc_mutex_t *mutex = DWC_ALLOC(sizeof(struct lock));
+
+	if (!mutex) {
+		DWC_ERROR("Cannot allocate memory for mutex");
+		return NULL;
+	}
+
+	lockinit((struct lock *)mutex, 0, "dw3mtx", 0, 0);
+	return mutex;
+}
+
+#if (defined(DWC_LINUX) && defined(CONFIG_DEBUG_MUTEXES))
+#else
+void DWC_MUTEX_FREE(dwc_mutex_t *mutex)
+{
+	DWC_FREE(mutex);
+}
+#endif
+
+void DWC_MUTEX_LOCK(dwc_mutex_t *mutex)
+{
+	lockmgr((struct lock *)mutex, LK_EXCLUSIVE, NULL);
+}
+
+int DWC_MUTEX_TRYLOCK(dwc_mutex_t *mutex)
+{
+	int status;
+
+	status = lockmgr((struct lock *)mutex, LK_EXCLUSIVE | LK_NOWAIT, NULL);
+	return status == 0;
+}
+
+void DWC_MUTEX_UNLOCK(dwc_mutex_t *mutex)
+{
+	lockmgr((struct lock *)mutex, LK_RELEASE, NULL);
+}
+
+
+/* Timing */
+
+void DWC_UDELAY(uint32_t usecs)
+{
+	DELAY(usecs);
+}
+
+void DWC_MDELAY(uint32_t msecs)
+{
+	do {
+		DELAY(1000);
+	} while (--msecs);
+}
+
+void DWC_MSLEEP(uint32_t msecs)
+{
+	struct timeval tv;
+
+	tv.tv_sec = msecs / 1000;
+	tv.tv_usec = (msecs - tv.tv_sec * 1000) * 1000;
+	tsleep(&tv, 0, "dw3slp", tvtohz(&tv));
+}
+
+uint32_t DWC_TIME(void)
+{
+	struct timeval tv;
+
+	microuptime(&tv);	// or getmicrouptime? (less precise, but faster)
+	return tv.tv_sec * 1000 + tv.tv_usec / 1000;
+}
+
+
+/* Timers */
+
+struct dwc_timer {
+	struct callout t;
+	char *name;
+	dwc_spinlock_t *lock;
+	dwc_timer_callback_t cb;
+	void *data;
+};
+
+dwc_timer_t *DWC_TIMER_ALLOC(char *name, dwc_timer_callback_t cb, void *data)
+{
+	dwc_timer_t *t = DWC_ALLOC(sizeof(*t));
+
+	if (!t) {
+		DWC_ERROR("Cannot allocate memory for timer");
+		return NULL;
+	}
+
+	callout_init(&t->t);
+
+	t->name = DWC_STRDUP(name);
+	if (!t->name) {
+		DWC_ERROR("Cannot allocate memory for timer->name");
+		goto no_name;
+	}
+
+	t->lock = DWC_SPINLOCK_ALLOC();
+	if (!t->lock) {
+		DWC_ERROR("Cannot allocate memory for timer->lock");
+		goto no_lock;
+	}
+
+	t->cb = cb;
+	t->data = data;
+
+	return t;
+
+ no_lock:
+	DWC_FREE(t->name);
+ no_name:
+	DWC_FREE(t);
+
+	return NULL;
+}
+
+void DWC_TIMER_FREE(dwc_timer_t *timer)
+{
+	callout_stop(&timer->t);
+	DWC_SPINLOCK_FREE(timer->lock);
+	DWC_FREE(timer->name);
+	DWC_FREE(timer);
+}
+
+void DWC_TIMER_SCHEDULE(dwc_timer_t *timer, uint32_t time)
+{
+	struct timeval tv;
+
+	tv.tv_sec = time / 1000;
+	tv.tv_usec = (time - tv.tv_sec * 1000) * 1000;
+	callout_reset(&timer->t, tvtohz(&tv), timer->cb, timer->data);
+}
+
+void DWC_TIMER_CANCEL(dwc_timer_t *timer)
+{
+	callout_stop(&timer->t);
+}
+
+
+/* Wait Queues */
+
+struct dwc_waitq {
+	struct simplelock lock;
+	int abort;
+};
+
+dwc_waitq_t *DWC_WAITQ_ALLOC(void)
+{
+	dwc_waitq_t *wq = DWC_ALLOC(sizeof(*wq));
+
+	if (!wq) {
+		DWC_ERROR("Cannot allocate memory for waitqueue");
+		return NULL;
+	}
+
+	simple_lock_init(&wq->lock);
+	wq->abort = 0;
+
+	return wq;
+}
+
+void DWC_WAITQ_FREE(dwc_waitq_t *wq)
+{
+	DWC_FREE(wq);
+}
+
+int32_t DWC_WAITQ_WAIT(dwc_waitq_t *wq, dwc_waitq_condition_t cond, void *data)
+{
+	int ipl;
+	int result = 0;
+
+	simple_lock(&wq->lock);
+	ipl = splbio();
+
+	/* Skip the sleep if already aborted or triggered */
+	if (!wq->abort && !cond(data)) {
+		splx(ipl);
+		result = ltsleep(wq, PCATCH, "dw3wat", 0, &wq->lock); // infinite timeout
+		ipl = splbio();
+	}
+
+	if (result == 0) {			// awoken
+		if (wq->abort) {
+			wq->abort = 0;
+			result = -DWC_E_ABORT;
+		} else {
+			result = 0;
+		}
+
+		splx(ipl);
+		simple_unlock(&wq->lock);
+	} else {
+		wq->abort = 0;
+		splx(ipl);
+		simple_unlock(&wq->lock);
+
+		if (result == ERESTART) {	// signaled - restart
+			result = -DWC_E_RESTART;
+		} else {			// signaled - must be EINTR
+			result = -DWC_E_ABORT;
+		}
+	}
+
+	return result;
+}
+
+int32_t DWC_WAITQ_WAIT_TIMEOUT(dwc_waitq_t *wq, dwc_waitq_condition_t cond,
+			       void *data, int32_t msecs)
+{
+	struct timeval tv, tv1, tv2;
+	int ipl;
+	int result = 0;
+
+	tv.tv_sec = msecs / 1000;
+	tv.tv_usec = (msecs - tv.tv_sec * 1000) * 1000;
+
+	simple_lock(&wq->lock);
+	ipl = splbio();
+
+	/* Skip the sleep if already aborted or triggered */
+	if (!wq->abort && !cond(data)) {
+		splx(ipl);
+		getmicrouptime(&tv1);
+		result = ltsleep(wq, PCATCH, "dw3wto", tvtohz(&tv), &wq->lock);
+		getmicrouptime(&tv2);
+		ipl = splbio();
+	}
+
+	if (result == 0) {			// awoken
+		if (wq->abort) {
+			wq->abort = 0;
+			splx(ipl);
+			simple_unlock(&wq->lock);
+			result = -DWC_E_ABORT;
+		} else {
+			splx(ipl);
+			simple_unlock(&wq->lock);
+
+			tv2.tv_usec -= tv1.tv_usec;
+			if (tv2.tv_usec < 0) {
+				tv2.tv_usec += 1000000;
+				tv2.tv_sec--;
+			}
+
+			tv2.tv_sec -= tv1.tv_sec;
+			result = tv2.tv_sec * 1000 + tv2.tv_usec / 1000;
+			result = msecs - result;
+			if (result <= 0)
+				result = 1;
+		}
+	} else {
+		wq->abort = 0;
+		splx(ipl);
+		simple_unlock(&wq->lock);
+
+		if (result == ERESTART) {	// signaled - restart
+			result = -DWC_E_RESTART;
+
+		} else if (result == EINTR) {		// signaled - interrupt
+			result = -DWC_E_ABORT;
+
+		} else {				// timed out
+			result = -DWC_E_TIMEOUT;
+		}
+	}
+
+	return result;
+}
+
+void DWC_WAITQ_TRIGGER(dwc_waitq_t *wq)
+{
+	wakeup(wq);
+}
+
+void DWC_WAITQ_ABORT(dwc_waitq_t *wq)
+{
+	int ipl;
+
+	simple_lock(&wq->lock);
+	ipl = splbio();
+	wq->abort = 1;
+	wakeup(wq);
+	splx(ipl);
+	simple_unlock(&wq->lock);
+}
+
+
+/* Threading */
+
+struct dwc_thread {
+	struct proc *proc;
+	int abort;
+};
+
+dwc_thread_t *DWC_THREAD_RUN(dwc_thread_function_t func, char *name, void *data)
+{
+	int retval;
+	dwc_thread_t *thread = DWC_ALLOC(sizeof(*thread));
+
+	if (!thread) {
+		return NULL;
+	}
+
+	thread->abort = 0;
+	retval = kthread_create1((void (*)(void *))func, data, &thread->proc,
+				 "%s", name);
+	if (retval) {
+		DWC_FREE(thread);
+		return NULL;
+	}
+
+	return thread;
+}
+
+int DWC_THREAD_STOP(dwc_thread_t *thread)
+{
+	int retval;
+
+	thread->abort = 1;
+	retval = tsleep(&thread->abort, 0, "dw3stp", 60 * hz);
+
+	if (retval == 0) {
+		/* DWC_THREAD_EXIT() will free the thread struct */
+		return 0;
+	}
+
+	/* NOTE: We leak the thread struct if thread doesn't die */
+
+	if (retval == EWOULDBLOCK) {
+		return -DWC_E_TIMEOUT;
+	}
+
+	return -DWC_E_UNKNOWN;
+}
+
+dwc_bool_t DWC_THREAD_SHOULD_STOP(dwc_thread_t *thread)
+{
+	return thread->abort;
+}
+
+void DWC_THREAD_EXIT(dwc_thread_t *thread)
+{
+	wakeup(&thread->abort);
+	DWC_FREE(thread);
+	kthread_exit(0);
+}
+
+/* tasklets
+ - Runs in interrupt context (cannot sleep)
+ - Each tasklet runs on a single CPU
+ - Different tasklets can be running simultaneously on different CPUs
+ [ On NetBSD there is no corresponding mechanism, drivers don't have bottom-
+   halves. So we just call the callback directly from DWC_TASK_SCHEDULE() ]
+ */
+struct dwc_tasklet {
+	dwc_tasklet_callback_t cb;
+	void *data;
+};
+
+static void tasklet_callback(void *data)
+{
+	dwc_tasklet_t *task = (dwc_tasklet_t *)data;
+
+	task->cb(task->data);
+}
+
+dwc_tasklet_t *DWC_TASK_ALLOC(char *name, dwc_tasklet_callback_t cb, void *data)
+{
+	dwc_tasklet_t *task = DWC_ALLOC(sizeof(*task));
+
+	if (task) {
+		task->cb = cb;
+		task->data = data;
+	} else {
+		DWC_ERROR("Cannot allocate memory for tasklet");
+	}
+
+	return task;
+}
+
+void DWC_TASK_FREE(dwc_tasklet_t *task)
+{
+	DWC_FREE(task);
+}
+
+void DWC_TASK_SCHEDULE(dwc_tasklet_t *task)
+{
+	tasklet_callback(task);
+}
+
+
+/* workqueues
+ - Runs in process context (can sleep)
+ */
+typedef struct work_container {
+	dwc_work_callback_t cb;
+	void *data;
+	dwc_workq_t *wq;
+	char *name;
+	int hz;
+	struct work task;
+} work_container_t;
+
+struct dwc_workq {
+	struct workqueue *taskq;
+	dwc_spinlock_t *lock;
+	dwc_waitq_t *waitq;
+	int pending;
+	struct work_container *container;
+};
+
+static void do_work(struct work *task, void *data)
+{
+	dwc_workq_t *wq = (dwc_workq_t *)data;
+	work_container_t *container = wq->container;
+	dwc_irqflags_t flags;
+
+	if (container->hz) {
+		tsleep(container, 0, "dw3wrk", container->hz);
+	}
+
+	container->cb(container->data);
+	DWC_DEBUG("Work done: %s, container=%p", container->name, container);
+
+	DWC_SPINLOCK_IRQSAVE(wq->lock, &flags);
+	if (container->name)
+		DWC_FREE(container->name);
+	DWC_FREE(container);
+	wq->pending--;
+	DWC_SPINUNLOCK_IRQRESTORE(wq->lock, flags);
+	DWC_WAITQ_TRIGGER(wq->waitq);
+}
+
+static int work_done(void *data)
+{
+	dwc_workq_t *workq = (dwc_workq_t *)data;
+
+	return workq->pending == 0;
+}
+
+int DWC_WORKQ_WAIT_WORK_DONE(dwc_workq_t *workq, int timeout)
+{
+	return DWC_WAITQ_WAIT_TIMEOUT(workq->waitq, work_done, workq, timeout);
+}
+
+dwc_workq_t *DWC_WORKQ_ALLOC(char *name)
+{
+	int result;
+	dwc_workq_t *wq = DWC_ALLOC(sizeof(*wq));
+
+	if (!wq) {
+		DWC_ERROR("Cannot allocate memory for workqueue");
+		return NULL;
+	}
+
+	result = workqueue_create(&wq->taskq, name, do_work, wq, 0 /*PWAIT*/,
+				  IPL_BIO, 0);
+	if (result) {
+		DWC_ERROR("Cannot create workqueue");
+		goto no_taskq;
+	}
+
+	wq->pending = 0;
+
+	wq->lock = DWC_SPINLOCK_ALLOC();
+	if (!wq->lock) {
+		DWC_ERROR("Cannot allocate memory for spinlock");
+		goto no_lock;
+	}
+
+	wq->waitq = DWC_WAITQ_ALLOC();
+	if (!wq->waitq) {
+		DWC_ERROR("Cannot allocate memory for waitqueue");
+		goto no_waitq;
+	}
+
+	return wq;
+
+ no_waitq:
+	DWC_SPINLOCK_FREE(wq->lock);
+ no_lock:
+	workqueue_destroy(wq->taskq);
+ no_taskq:
+	DWC_FREE(wq);
+
+	return NULL;
+}
+
+void DWC_WORKQ_FREE(dwc_workq_t *wq)
+{
+#ifdef DEBUG
+	dwc_irqflags_t flags;
+
+	DWC_SPINLOCK_IRQSAVE(wq->lock, &flags);
+
+	if (wq->pending != 0) {
+		struct work_container *container = wq->container;
+
+		DWC_ERROR("Destroying work queue with pending work");
+
+		if (container && container->name) {
+			DWC_ERROR("Work %s still pending", container->name);
+		}
+	}
+
+	DWC_SPINUNLOCK_IRQRESTORE(wq->lock, flags);
+#endif
+	DWC_WAITQ_FREE(wq->waitq);
+	DWC_SPINLOCK_FREE(wq->lock);
+	workqueue_destroy(wq->taskq);
+	DWC_FREE(wq);
+}
+
+void DWC_WORKQ_SCHEDULE(dwc_workq_t *wq, dwc_work_callback_t cb, void *data,
+			char *format, ...)
+{
+	dwc_irqflags_t flags;
+	work_container_t *container;
+	static char name[128];
+	va_list args;
+
+	va_start(args, format);
+	DWC_VSNPRINTF(name, 128, format, args);
+	va_end(args);
+
+	DWC_SPINLOCK_IRQSAVE(wq->lock, &flags);
+	wq->pending++;
+	DWC_SPINUNLOCK_IRQRESTORE(wq->lock, flags);
+	DWC_WAITQ_TRIGGER(wq->waitq);
+
+	container = DWC_ALLOC_ATOMIC(sizeof(*container));
+	if (!container) {
+		DWC_ERROR("Cannot allocate memory for container");
+		return;
+	}
+
+	container->name = DWC_STRDUP(name);
+	if (!container->name) {
+		DWC_ERROR("Cannot allocate memory for container->name");
+		DWC_FREE(container);
+		return;
+	}
+
+	container->cb = cb;
+	container->data = data;
+	container->wq = wq;
+	container->hz = 0;
+	wq->container = container;
+
+	DWC_DEBUG("Queueing work: %s, container=%p", container->name, container);
+	workqueue_enqueue(wq->taskq, &container->task);
+}
+
+void DWC_WORKQ_SCHEDULE_DELAYED(dwc_workq_t *wq, dwc_work_callback_t cb,
+				void *data, uint32_t time, char *format, ...)
+{
+	dwc_irqflags_t flags;
+	work_container_t *container;
+	static char name[128];
+	struct timeval tv;
+	va_list args;
+
+	va_start(args, format);
+	DWC_VSNPRINTF(name, 128, format, args);
+	va_end(args);
+
+	DWC_SPINLOCK_IRQSAVE(wq->lock, &flags);
+	wq->pending++;
+	DWC_SPINUNLOCK_IRQRESTORE(wq->lock, flags);
+	DWC_WAITQ_TRIGGER(wq->waitq);
+
+	container = DWC_ALLOC_ATOMIC(sizeof(*container));
+	if (!container) {
+		DWC_ERROR("Cannot allocate memory for container");
+		return;
+	}
+
+	container->name = DWC_STRDUP(name);
+	if (!container->name) {
+		DWC_ERROR("Cannot allocate memory for container->name");
+		DWC_FREE(container);
+		return;
+	}
+
+	container->cb = cb;
+	container->data = data;
+	container->wq = wq;
+	tv.tv_sec = time / 1000;
+	tv.tv_usec = (time - tv.tv_sec * 1000) * 1000;
+	container->hz = tvtohz(&tv);
+	wq->container = container;
+
+	DWC_DEBUG("Queueing work: %s, container=%p", container->name, container);
+	workqueue_enqueue(wq->taskq, &container->task);
+}
+
+int DWC_WORKQ_PENDING(dwc_workq_t *wq)
+{
+	return wq->pending;
+}
diff --git a/drivers/usb/dwc_otg/dwc_crypto.c b/drivers/usb/dwc_otg/dwc_crypto.c
new file mode 100644
index 0000000..3b03532
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_crypto.c
@@ -0,0 +1,308 @@
+/* =========================================================================
+ * $File: //dwh/usb_iip/dev/software/dwc_common_port_2/dwc_crypto.c $
+ * $Revision: #5 $
+ * $Date: 2010/09/28 $
+ * $Change: 1596182 $
+ *
+ * Synopsys Portability Library Software and documentation
+ * (hereinafter, "Software") is an Unsupported proprietary work of
+ * Synopsys, Inc. unless otherwise expressly agreed to in writing
+ * between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product
+ * under any End User Software License Agreement or Agreement for
+ * Licensed Product with Synopsys or any supplement thereto. You are
+ * permitted to use and redistribute this Software in source and binary
+ * forms, with or without modification, provided that redistributions
+ * of source code must retain this notice. You may not view, use,
+ * disclose, copy or distribute this file or any information contained
+ * herein except pursuant to this license grant from Synopsys. If you
+ * do not agree with this notice, including the disclaimer below, then
+ * you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS"
+ * BASIS AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+ * FOR A PARTICULAR PURPOSE ARE HEREBY DISCLAIMED. IN NO EVENT SHALL
+ * SYNOPSYS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
+ * USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================= */
+
+/** @file
+ * This file contains the WUSB cryptographic routines.
+ */
+
+#ifdef DWC_CRYPTOLIB
+
+#include "dwc_crypto.h"
+#include "usb.h"
+
+#ifdef DEBUG
+static inline void dump_bytes(char *name, uint8_t *bytes, int len)
+{
+	int i;
+	DWC_PRINTF("%s: ", name);
+	for (i=0; i<len; i++) {
+		DWC_PRINTF("%02x ", bytes[i]);
+	}
+	DWC_PRINTF("\n");
+}
+#else
+#define dump_bytes(x...)
+#endif
+
+/* Display a block */
+void show_block(const u8 *blk, const char *prefix, const char *suffix, int a)
+{
+#ifdef DWC_DEBUG_CRYPTO
+	int i, blksize = 16;
+
+	DWC_DEBUG("%s", prefix);
+
+	if (suffix == NULL) {
+		suffix = "\n";
+		blksize = a;
+	}
+
+	for (i = 0; i < blksize; i++)
+		DWC_PRINT("%02x%s", *blk++, ((i & 3) == 3) ? "  " : " ");
+	DWC_PRINT(suffix);
+#endif
+}
+
+/**
+ * Encrypts an array of bytes using the AES encryption engine.
+ * If <code>dst</code> == <code>src</code>, then the bytes will be encrypted
+ * in-place.
+ *
+ * @return  0 on success, negative error code on error.
+ */
+int dwc_wusb_aes_encrypt(u8 *src, u8 *key, u8 *dst)
+{
+	u8 block_t[16];
+	DWC_MEMSET(block_t, 0, 16);
+
+	return DWC_AES_CBC(src, 16, key, 16, block_t, dst);
+}
+
+/**
+ * The CCM-MAC-FUNCTION described in section 6.5 of the WUSB spec.
+ * This function takes a data string and returns the encrypted CBC
+ * Counter-mode MIC.
+ *
+ * @param key     The 128-bit symmetric key.
+ * @param nonce   The CCM nonce.
+ * @param label   The unique 14-byte ASCII text label.
+ * @param bytes   The byte array to be encrypted.
+ * @param len     Length of the byte array.
+ * @param result  Byte array to receive the 8-byte encrypted MIC.
+ */
+void dwc_wusb_cmf(u8 *key, u8 *nonce,
+		  char *label, u8 *bytes, int len, u8 *result)
+{
+	u8 block_m[16];
+	u8 block_x[16];
+	u8 block_t[8];
+	int idx, blkNum;
+	u16 la = (u16)(len + 14);
+
+	/* Set the AES-128 key */
+	//dwc_aes_setkey(tfm, key, 16);
+
+	/* Fill block B0 from flags = 0x59, N, and l(m) = 0 */
+	block_m[0] = 0x59;
+	for (idx = 0; idx < 13; idx++)
+		block_m[idx + 1] = nonce[idx];
+	block_m[14] = 0;
+	block_m[15] = 0;
+
+	/* Produce the CBC IV */
+	dwc_wusb_aes_encrypt(block_m, key, block_x);
+	show_block(block_m, "CBC IV in: ", "\n", 0);
+	show_block(block_x, "CBC IV out:", "\n", 0);
+
+	/* Fill block B1 from l(a) = Blen + 14, and A */
+	block_x[0] ^= (u8)(la >> 8);
+	block_x[1] ^= (u8)la;
+	for (idx = 0; idx < 14; idx++)
+		block_x[idx + 2] ^= label[idx];
+	show_block(block_x, "After xor: ", "b1\n", 16);
+
+	dwc_wusb_aes_encrypt(block_x, key, block_x);
+	show_block(block_x, "After AES: ", "b1\n", 16);
+
+	idx = 0;
+	blkNum = 0;
+
+	/* Fill remaining blocks with B */
+	while (len-- > 0) {
+		block_x[idx] ^= *bytes++;
+		if (++idx >= 16) {
+			idx = 0;
+			show_block(block_x, "After xor: ", "\n", blkNum);
+			dwc_wusb_aes_encrypt(block_x, key, block_x);
+			show_block(block_x, "After AES: ", "\n", blkNum);
+			blkNum++;
+		}
+	}
+
+	/* Handle partial last block */
+	if (idx > 0) {
+		show_block(block_x, "After xor: ", "\n", blkNum);
+		dwc_wusb_aes_encrypt(block_x, key, block_x);
+		show_block(block_x, "After AES: ", "\n", blkNum);
+	}
+
+	/* Save the MIC tag */
+	DWC_MEMCPY(block_t, block_x, 8);
+	show_block(block_t, "MIC tag  : ", NULL, 8);
+
+	/* Fill block A0 from flags = 0x01, N, and counter = 0 */
+	block_m[0] = 0x01;
+	block_m[14] = 0;
+	block_m[15] = 0;
+
+	/* Encrypt the counter */
+	dwc_wusb_aes_encrypt(block_m, key, block_x);
+	show_block(block_x, "CTR[MIC] : ", NULL, 8);
+
+	/* XOR with MIC tag */
+	for (idx = 0; idx < 8; idx++) {
+		block_t[idx] ^= block_x[idx];
+	}
+
+	/* Return result to caller */
+	DWC_MEMCPY(result, block_t, 8);
+	show_block(result, "CCM-MIC  : ", NULL, 8);
+
+}
+
+/**
+ * The PRF function described in section 6.5 of the WUSB spec. This function
+ * concatenates MIC values returned from dwc_cmf() to create a value of
+ * the requested length.
+ *
+ * @param prf_len  Length of the PRF function in bits (64, 128, or 256).
+ * @param key, nonce, label, bytes, len  Same as for dwc_cmf().
+ * @param result   Byte array to receive the result.
+ */
+void dwc_wusb_prf(int prf_len, u8 *key,
+		  u8 *nonce, char *label, u8 *bytes, int len, u8 *result)
+{
+	int i;
+
+	nonce[0] = 0;
+	for (i = 0; i < prf_len >> 6; i++, nonce[0]++) {
+		dwc_wusb_cmf(key, nonce, label, bytes, len, result);
+		result += 8;
+	}
+}
+
+/**
+ * Fills in CCM Nonce per the WUSB spec.
+ *
+ * @param[in] haddr Host address.
+ * @param[in] daddr Device address.
+ * @param[in] tkid Session Key(PTK) identifier.
+ * @param[out] nonce Pointer to where the CCM Nonce output is to be written.
+ */
+void dwc_wusb_fill_ccm_nonce(uint16_t haddr, uint16_t daddr, uint8_t *tkid,
+			     uint8_t *nonce)
+{
+
+	DWC_DEBUG("%s %x %x\n", __func__, daddr, haddr);
+
+	DWC_MEMSET(&nonce[0], 0, 16);
+
+	DWC_MEMCPY(&nonce[6], tkid, 3);
+	nonce[9] = daddr & 0xFF;
+	nonce[10] = (daddr >> 8) & 0xFF;
+	nonce[11] = haddr & 0xFF;
+	nonce[12] = (haddr >> 8) & 0xFF;
+
+	dump_bytes("CCM nonce", nonce, 16);
+}
+
+/**
+ * Generates a 16-byte cryptographic-grade random number for the Host/Device
+ * Nonce.
+ */
+void dwc_wusb_gen_nonce(uint16_t addr, uint8_t *nonce)
+{
+	uint8_t inonce[16];
+	uint32_t temp[4];
+
+	/* Fill in the Nonce */
+	DWC_MEMSET(&inonce[0], 0, sizeof(inonce));
+	inonce[9] = addr & 0xFF;
+	inonce[10] = (addr >> 8) & 0xFF;
+	inonce[11] = inonce[9];
+	inonce[12] = inonce[10];
+
+	/* Collect "randomness samples" */
+	DWC_RANDOM_BYTES((uint8_t *)temp, 16);
+
+	dwc_wusb_prf_128((uint8_t *)temp, nonce,
+			 "Random Numbers", (uint8_t *)temp, sizeof(temp),
+			 nonce);
+}
+
+/**
+ * Generates the Session Key (PTK) and Key Confirmation Key (KCK) per the
+ * WUSB spec.
+ *
+ * @param[in] ccm_nonce Pointer to CCM Nonce.
+ * @param[in] mk Master Key to derive the session from
+ * @param[in] hnonce Pointer to Host Nonce.
+ * @param[in] dnonce Pointer to Device Nonce.
+ * @param[out] kck Pointer to where the KCK output is to be written.
+ * @param[out] ptk Pointer to where the PTK output is to be written.
+ */
+void dwc_wusb_gen_key(uint8_t *ccm_nonce, uint8_t *mk, uint8_t *hnonce,
+		      uint8_t *dnonce, uint8_t *kck, uint8_t *ptk)
+{
+	uint8_t idata[32];
+	uint8_t odata[32];
+
+	dump_bytes("ck", mk, 16);
+	dump_bytes("hnonce", hnonce, 16);
+	dump_bytes("dnonce", dnonce, 16);
+
+	/* The data is the HNonce and DNonce concatenated */
+	DWC_MEMCPY(&idata[0], hnonce, 16);
+	DWC_MEMCPY(&idata[16], dnonce, 16);
+
+	dwc_wusb_prf_256(mk, ccm_nonce, "Pair-wise keys", idata, 32, odata);
+
+	/* Low 16 bytes of the result is the KCK, high 16 is the PTK */
+	DWC_MEMCPY(kck, &odata[0], 16);
+	DWC_MEMCPY(ptk, &odata[16], 16);
+
+	dump_bytes("kck", kck, 16);
+	dump_bytes("ptk", ptk, 16);
+}
+
+/**
+ * Generates the Message Integrity Code over the Handshake data per the
+ * WUSB spec.
+ *
+ * @param ccm_nonce Pointer to CCM Nonce.
+ * @param kck   Pointer to Key Confirmation Key.
+ * @param data  Pointer to Handshake data to be checked.
+ * @param mic   Pointer to where the MIC output is to be written.
+ */
+void dwc_wusb_gen_mic(uint8_t *ccm_nonce, uint8_t *kck,
+		      uint8_t *data, uint8_t *mic)
+{
+
+	dwc_wusb_prf_64(kck, ccm_nonce, "out-of-bandMIC",
+			data, WUSB_HANDSHAKE_LEN_FOR_MIC, mic);
+}
+
+#endif	/* DWC_CRYPTOLIB */
diff --git a/drivers/usb/dwc_otg/dwc_crypto.h b/drivers/usb/dwc_otg/dwc_crypto.h
new file mode 100644
index 0000000..26fcddc
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_crypto.h
@@ -0,0 +1,111 @@
+/* =========================================================================
+ * $File: //dwh/usb_iip/dev/software/dwc_common_port_2/dwc_crypto.h $
+ * $Revision: #3 $
+ * $Date: 2010/09/28 $
+ * $Change: 1596182 $
+ *
+ * Synopsys Portability Library Software and documentation
+ * (hereinafter, "Software") is an Unsupported proprietary work of
+ * Synopsys, Inc. unless otherwise expressly agreed to in writing
+ * between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product
+ * under any End User Software License Agreement or Agreement for
+ * Licensed Product with Synopsys or any supplement thereto. You are
+ * permitted to use and redistribute this Software in source and binary
+ * forms, with or without modification, provided that redistributions
+ * of source code must retain this notice. You may not view, use,
+ * disclose, copy or distribute this file or any information contained
+ * herein except pursuant to this license grant from Synopsys. If you
+ * do not agree with this notice, including the disclaimer below, then
+ * you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS"
+ * BASIS AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+ * FOR A PARTICULAR PURPOSE ARE HEREBY DISCLAIMED. IN NO EVENT SHALL
+ * SYNOPSYS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
+ * USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================= */
+
+#ifndef _DWC_CRYPTO_H_
+#define _DWC_CRYPTO_H_
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/** @file
+ *
+ * This file contains declarations for the WUSB Cryptographic routines as
+ * defined in the WUSB spec.  They are only to be used internally by the DWC UWB
+ * modules.
+ */
+
+#include "dwc_os.h"
+
+int dwc_wusb_aes_encrypt(u8 *src, u8 *key, u8 *dst);
+
+void dwc_wusb_cmf(u8 *key, u8 *nonce,
+		  char *label, u8 *bytes, int len, u8 *result);
+void dwc_wusb_prf(int prf_len, u8 *key,
+		  u8 *nonce, char *label, u8 *bytes, int len, u8 *result);
+
+/**
+ * The PRF-64 function described in section 6.5 of the WUSB spec.
+ *
+ * @param key, nonce, label, bytes, len, result  Same as for dwc_prf().
+ */
+static inline void dwc_wusb_prf_64(u8 *key, u8 *nonce,
+				   char *label, u8 *bytes, int len, u8 *result)
+{
+	dwc_wusb_prf(64, key, nonce, label, bytes, len, result);
+}
+
+/**
+ * The PRF-128 function described in section 6.5 of the WUSB spec.
+ *
+ * @param key, nonce, label, bytes, len, result  Same as for dwc_prf().
+ */
+static inline void dwc_wusb_prf_128(u8 *key, u8 *nonce,
+				    char *label, u8 *bytes, int len, u8 *result)
+{
+	dwc_wusb_prf(128, key, nonce, label, bytes, len, result);
+}
+
+/**
+ * The PRF-256 function described in section 6.5 of the WUSB spec.
+ *
+ * @param key, nonce, label, bytes, len, result  Same as for dwc_prf().
+ */
+static inline void dwc_wusb_prf_256(u8 *key, u8 *nonce,
+				    char *label, u8 *bytes, int len, u8 *result)
+{
+	dwc_wusb_prf(256, key, nonce, label, bytes, len, result);
+}
+
+
+void dwc_wusb_fill_ccm_nonce(uint16_t haddr, uint16_t daddr, uint8_t *tkid,
+			       uint8_t *nonce);
+void dwc_wusb_gen_nonce(uint16_t addr,
+			  uint8_t *nonce);
+
+void dwc_wusb_gen_key(uint8_t *ccm_nonce, uint8_t *mk,
+			uint8_t *hnonce, uint8_t *dnonce,
+			uint8_t *kck, uint8_t *ptk);
+
+
+void dwc_wusb_gen_mic(uint8_t *ccm_nonce, uint8_t
+			*kck, uint8_t *data, uint8_t *mic);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* _DWC_CRYPTO_H_ */
diff --git a/drivers/usb/dwc_otg/dwc_dh.c b/drivers/usb/dwc_otg/dwc_dh.c
new file mode 100644
index 0000000..997f753
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_dh.c
@@ -0,0 +1,291 @@
+/* =========================================================================
+ * $File: //dwh/usb_iip/dev/software/dwc_common_port_2/dwc_dh.c $
+ * $Revision: #3 $
+ * $Date: 2010/09/28 $
+ * $Change: 1596182 $
+ *
+ * Synopsys Portability Library Software and documentation
+ * (hereinafter, "Software") is an Unsupported proprietary work of
+ * Synopsys, Inc. unless otherwise expressly agreed to in writing
+ * between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product
+ * under any End User Software License Agreement or Agreement for
+ * Licensed Product with Synopsys or any supplement thereto. You are
+ * permitted to use and redistribute this Software in source and binary
+ * forms, with or without modification, provided that redistributions
+ * of source code must retain this notice. You may not view, use,
+ * disclose, copy or distribute this file or any information contained
+ * herein except pursuant to this license grant from Synopsys. If you
+ * do not agree with this notice, including the disclaimer below, then
+ * you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS"
+ * BASIS AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+ * FOR A PARTICULAR PURPOSE ARE HEREBY DISCLAIMED. IN NO EVENT SHALL
+ * SYNOPSYS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
+ * USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================= */
+#ifdef DWC_CRYPTOLIB
+
+#ifndef CONFIG_MACH_IPMATE
+
+#include "dwc_dh.h"
+#include "dwc_modpow.h"
+
+#ifdef DEBUG
+/* This function prints out a buffer in the format described in the Association
+ * Model specification. */
+static void dh_dump(char *str, void *_num, int len)
+{
+	uint8_t *num = _num;
+	int i;
+	DWC_PRINTF("%s\n", str);
+	for (i = 0; i < len; i ++) {
+		DWC_PRINTF("%02x", num[i]);
+		if (((i + 1) % 2) == 0) DWC_PRINTF(" ");
+		if (((i + 1) % 26) == 0) DWC_PRINTF("\n");
+	}
+
+	DWC_PRINTF("\n");
+}
+#else
+#define dh_dump(_x...) do {; } while(0)
+#endif
+
+/* Constant g value */
+static __u32 dh_g[] = {
+	0x02000000,
+};
+
+/* Constant p value */
+static __u32 dh_p[] = {
+	0xFFFFFFFF, 0xFFFFFFFF, 0xA2DA0FC9, 0x34C26821, 0x8B62C6C4, 0xD11CDC80, 0x084E0229, 0x74CC678A,
+	0xA6BE0B02, 0x229B133B, 0x79084A51, 0xDD04348E, 0xB31995EF, 0x1B433ACD, 0x6D0A2B30, 0x37145FF2,
+	0x6D35E14F, 0x45C2516D, 0x76B585E4, 0xC67E5E62, 0xE9424CF4, 0x6BED37A6, 0xB65CFF0B, 0xEDB706F4,
+	0xFB6B38EE, 0xA59F895A, 0x11249FAE, 0xE61F4B7C, 0x51662849, 0x3D5BE4EC, 0xB87C00C2, 0x05BF63A1,
+	0x3648DA98, 0x9AD3551C, 0xA83F1669, 0x5FCF24FD, 0x235D6583, 0x96ADA3DC, 0x56F3621C, 0xBB528520,
+	0x0729D59E, 0x6D969670, 0x4E350C67, 0x0498BC4A, 0x086C74F1, 0x7C2118CA, 0x465E9032, 0x3BCE362E,
+	0x2C779EE3, 0x03860E18, 0xA283279B, 0x8FA207EC, 0xF05DC5B5, 0xC9524C6F, 0xF6CB2BDE, 0x18175895,
+	0x7C499539, 0xE56A95EA, 0x1826D215, 0x1005FA98, 0x5A8E7215, 0x2DC4AA8A, 0x0D1733AD, 0x337A5004,
+	0xAB2155A8, 0x64BA1CDF, 0x0485FBEC, 0x0AEFDB58, 0x5771EA8A, 0x7D0C065D, 0x850F97B3, 0xC7E4E1A6,
+	0x8CAEF5AB, 0xD73309DB, 0xE0948C1E, 0x9D61254A, 0x26D2E3CE, 0x6BEED21A, 0x06FA2FF1, 0x64088AD9,
+	0x730276D8, 0x646AC83E, 0x182B1F52, 0x0C207B17, 0x5717E1BB, 0x6C5D617A, 0xC0880977, 0xE246D9BA,
+	0xA04FE208, 0x31ABE574, 0xFC5BDB43, 0x8E10FDE0, 0x20D1824B, 0xCAD23AA9, 0xFFFFFFFF, 0xFFFFFFFF,
+};
+
+static void dh_swap_bytes(void *_in, void *_out, uint32_t len)
+{
+	uint8_t *in = _in;
+	uint8_t *out = _out;
+	int i;
+	for (i=0; i<len; i++) {
+		out[i] = in[len-1-i];
+	}
+}
+
+/* Computes the modular exponentiation (num^exp % mod).  num, exp, and mod are
+ * big endian numbers of size len, in bytes.  Each len value must be a multiple
+ * of 4. */
+int dwc_dh_modpow(void *mem_ctx, void *num, uint32_t num_len,
+		  void *exp, uint32_t exp_len,
+		  void *mod, uint32_t mod_len,
+		  void *out)
+{
+	/* modpow() takes little endian numbers.  AM uses big-endian.  This
+	 * function swaps bytes of numbers before passing onto modpow. */
+
+	int retval = 0;
+	uint32_t *result;
+
+	uint32_t *bignum_num = dwc_alloc(mem_ctx, num_len + 4);
+	uint32_t *bignum_exp = dwc_alloc(mem_ctx, exp_len + 4);
+	uint32_t *bignum_mod = dwc_alloc(mem_ctx, mod_len + 4);
+
+	dh_swap_bytes(num, &bignum_num[1], num_len);
+	bignum_num[0] = num_len / 4;
+
+	dh_swap_bytes(exp, &bignum_exp[1], exp_len);
+	bignum_exp[0] = exp_len / 4;
+
+	dh_swap_bytes(mod, &bignum_mod[1], mod_len);
+	bignum_mod[0] = mod_len / 4;
+
+	result = dwc_modpow(mem_ctx, bignum_num, bignum_exp, bignum_mod);
+	if (!result) {
+		retval = -1;
+		goto dh_modpow_nomem;
+	}
+
+	dh_swap_bytes(&result[1], out, result[0] * 4);
+	dwc_free(mem_ctx, result);
+
+ dh_modpow_nomem:
+	dwc_free(mem_ctx, bignum_num);
+	dwc_free(mem_ctx, bignum_exp);
+	dwc_free(mem_ctx, bignum_mod);
+	return retval;
+}
+
+
+int dwc_dh_pk(void *mem_ctx, uint8_t nd, uint8_t *exp, uint8_t *pk, uint8_t *hash)
+{
+	int retval;
+	uint8_t m3[385];
+
+#ifndef DH_TEST_VECTORS
+	DWC_RANDOM_BYTES(exp, 32);
+#endif
+
+	/* Compute the pkd */
+	if ((retval = dwc_dh_modpow(mem_ctx, dh_g, 4,
+				    exp, 32,
+				    dh_p, 384, pk))) {
+		return retval;
+	}
+
+	m3[384] = nd;
+	DWC_MEMCPY(&m3[0], pk, 384);
+	DWC_SHA256(m3, 385, hash);
+
+ 	dh_dump("PK", pk, 384);
+ 	dh_dump("SHA-256(M3)", hash, 32);
+	return 0;
+}
+
+int dwc_dh_derive_keys(void *mem_ctx, uint8_t nd, uint8_t *pkh, uint8_t *pkd,
+		       uint8_t *exp, int is_host,
+		       char *dd, uint8_t *ck, uint8_t *kdk)
+{
+	int retval;
+	uint8_t mv[784];
+	uint8_t sha_result[32];
+	uint8_t dhkey[384];
+	uint8_t shared_secret[384];
+	char *message;
+	uint32_t vd;
+
+	uint8_t *pk;
+
+	if (is_host) {
+		pk = pkd;
+	}
+	else {
+		pk = pkh;
+	}
+
+	if ((retval = dwc_dh_modpow(mem_ctx, pk, 384,
+				    exp, 32,
+				    dh_p, 384, shared_secret))) {
+		return retval;
+	}
+	dh_dump("Shared Secret", shared_secret, 384);
+
+	DWC_SHA256(shared_secret, 384, dhkey);
+	dh_dump("DHKEY", dhkey, 384);
+
+	DWC_MEMCPY(&mv[0], pkd, 384);
+	DWC_MEMCPY(&mv[384], pkh, 384);
+	DWC_MEMCPY(&mv[768], "displayed digest", 16);
+	dh_dump("MV", mv, 784);
+
+	DWC_SHA256(mv, 784, sha_result);
+	dh_dump("SHA-256(MV)", sha_result, 32);
+	dh_dump("First 32-bits of SHA-256(MV)", sha_result, 4);
+
+	dh_swap_bytes(sha_result, &vd, 4);
+#ifdef DEBUG
+	DWC_PRINTF("Vd (decimal) = %d\n", vd);
+#endif
+
+	switch (nd) {
+	case 2:
+		vd = vd % 100;
+		DWC_SPRINTF(dd, "%02d", vd);
+		break;
+	case 3:
+		vd = vd % 1000;
+		DWC_SPRINTF(dd, "%03d", vd);
+		break;
+	case 4:
+		vd = vd % 10000;
+		DWC_SPRINTF(dd, "%04d", vd);
+		break;
+	}
+#ifdef DEBUG
+	DWC_PRINTF("Display Digits: %s\n", dd);
+#endif
+
+	message = "connection key";
+	DWC_HMAC_SHA256(message, DWC_STRLEN(message), dhkey, 32, sha_result);
+ 	dh_dump("HMAC(SHA-256, DHKey, connection key)", sha_result, 32);
+	DWC_MEMCPY(ck, sha_result, 16);
+
+	message = "key derivation key";
+	DWC_HMAC_SHA256(message, DWC_STRLEN(message), dhkey, 32, sha_result);
+ 	dh_dump("HMAC(SHA-256, DHKey, key derivation key)", sha_result, 32);
+	DWC_MEMCPY(kdk, sha_result, 32);
+
+	return 0;
+}
+
+
+#ifdef DH_TEST_VECTORS
+
+static __u8 dh_a[] = {
+	0x44, 0x00, 0x51, 0xd6,
+	0xf0, 0xb5, 0x5e, 0xa9,
+	0x67, 0xab, 0x31, 0xc6,
+	0x8a, 0x8b, 0x5e, 0x37,
+	0xd9, 0x10, 0xda, 0xe0,
+	0xe2, 0xd4, 0x59, 0xa4,
+	0x86, 0x45, 0x9c, 0xaa,
+	0xdf, 0x36, 0x75, 0x16,
+};
+
+static __u8 dh_b[] = {
+	0x5d, 0xae, 0xc7, 0x86,
+	0x79, 0x80, 0xa3, 0x24,
+	0x8c, 0xe3, 0x57, 0x8f,
+	0xc7, 0x5f, 0x1b, 0x0f,
+	0x2d, 0xf8, 0x9d, 0x30,
+	0x6f, 0xa4, 0x52, 0xcd,
+	0xe0, 0x7a, 0x04, 0x8a,
+	0xde, 0xd9, 0x26, 0x56,
+};
+
+void dwc_run_dh_test_vectors(void *mem_ctx)
+{
+	uint8_t pkd[384];
+	uint8_t pkh[384];
+	uint8_t hashd[32];
+	uint8_t hashh[32];
+	uint8_t ck[16];
+	uint8_t kdk[32];
+	char dd[5];
+
+	DWC_PRINTF("\n\n\nDH_TEST_VECTORS\n\n");
+
+	/* compute the PKd and SHA-256(PKd || Nd) */
+	DWC_PRINTF("Computing PKd\n");
+	dwc_dh_pk(mem_ctx, 2, dh_a, pkd, hashd);
+
+	/* compute the PKd and SHA-256(PKh || Nd) */
+	DWC_PRINTF("Computing PKh\n");
+	dwc_dh_pk(mem_ctx, 2, dh_b, pkh, hashh);
+
+	/* compute the dhkey */
+	dwc_dh_derive_keys(mem_ctx, 2, pkh, pkd, dh_a, 0, dd, ck, kdk);
+}
+#endif /* DH_TEST_VECTORS */
+
+#endif /* !CONFIG_MACH_IPMATE */
+
+#endif /* DWC_CRYPTOLIB */
diff --git a/drivers/usb/dwc_otg/dwc_dh.h b/drivers/usb/dwc_otg/dwc_dh.h
new file mode 100644
index 0000000..25c1cc0
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_dh.h
@@ -0,0 +1,106 @@
+/* =========================================================================
+ * $File: //dwh/usb_iip/dev/software/dwc_common_port_2/dwc_dh.h $
+ * $Revision: #4 $
+ * $Date: 2010/09/28 $
+ * $Change: 1596182 $
+ *
+ * Synopsys Portability Library Software and documentation
+ * (hereinafter, "Software") is an Unsupported proprietary work of
+ * Synopsys, Inc. unless otherwise expressly agreed to in writing
+ * between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product
+ * under any End User Software License Agreement or Agreement for
+ * Licensed Product with Synopsys or any supplement thereto. You are
+ * permitted to use and redistribute this Software in source and binary
+ * forms, with or without modification, provided that redistributions
+ * of source code must retain this notice. You may not view, use,
+ * disclose, copy or distribute this file or any information contained
+ * herein except pursuant to this license grant from Synopsys. If you
+ * do not agree with this notice, including the disclaimer below, then
+ * you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS"
+ * BASIS AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+ * FOR A PARTICULAR PURPOSE ARE HEREBY DISCLAIMED. IN NO EVENT SHALL
+ * SYNOPSYS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
+ * USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================= */
+#ifndef _DWC_DH_H_
+#define _DWC_DH_H_
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#include "dwc_os.h"
+
+/** @file
+ *
+ * This file defines the common functions on device and host for performing
+ * numeric association as defined in the WUSB spec.  They are only to be
+ * used internally by the DWC UWB modules. */
+
+extern int dwc_dh_sha256(uint8_t *message, uint32_t len, uint8_t *out);
+extern int dwc_dh_hmac_sha256(uint8_t *message, uint32_t messagelen,
+			      uint8_t *key, uint32_t keylen,
+			      uint8_t *out);
+extern int dwc_dh_modpow(void *mem_ctx, void *num, uint32_t num_len,
+			 void *exp, uint32_t exp_len,
+			 void *mod, uint32_t mod_len,
+			 void *out);
+
+/** Computes PKD or PKH, and SHA-256(PKd || Nd)
+ *
+ * PK = g^exp mod p.
+ *
+ * Input:
+ * Nd = Number of digits on the device.
+ *
+ * Output:
+ * exp = A 32-byte buffer to be filled with a randomly generated number.
+ *       used as either A or B.
+ * pk = A 384-byte buffer to be filled with the PKH or PKD.
+ * hash = A 32-byte buffer to be filled with SHA-256(PK || ND).
+ */
+extern int dwc_dh_pk(void *mem_ctx, uint8_t nd, uint8_t *exp, uint8_t *pkd, uint8_t *hash);
+
+/** Computes the DHKEY, and VD.
+ *
+ * If called from host, then it will comput DHKEY=PKD^exp % p.
+ * If called from device, then it will comput DHKEY=PKH^exp % p.
+ *
+ * Input:
+ * pkd = The PKD value.
+ * pkh = The PKH value.
+ * exp = The A value (if device) or B value (if host) generated in dwc_wudev_dh_pk.
+ * is_host = Set to non zero if a WUSB host is calling this function.
+ *
+ * Output:
+
+ * dd = A pointer to an buffer to be set to the displayed digits string to be shown
+ *      to the user.  This buffer should be at 5 bytes long to hold 4 digits plus a
+ *      null termination character.  This buffer can be used directly for display.
+ * ck = A 16-byte buffer to be filled with the CK.
+ * kdk = A 32-byte buffer to be filled with the KDK.
+ */
+extern int dwc_dh_derive_keys(void *mem_ctx, uint8_t nd, uint8_t *pkh, uint8_t *pkd,
+			      uint8_t *exp, int is_host,
+			      char *dd, uint8_t *ck, uint8_t *kdk);
+
+#ifdef DH_TEST_VECTORS
+extern void dwc_run_dh_test_vectors(void);
+#endif
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* _DWC_DH_H_ */
diff --git a/drivers/usb/dwc_otg/dwc_list.h b/drivers/usb/dwc_otg/dwc_list.h
new file mode 100644
index 0000000..89cc325
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_list.h
@@ -0,0 +1,594 @@
+/*	$OpenBSD: queue.h,v 1.26 2004/05/04 16:59:32 grange Exp $	*/
+/*	$NetBSD: queue.h,v 1.11 1996/05/16 05:17:14 mycroft Exp $	*/
+
+/*
+ * Copyright (c) 1991, 1993
+ *	The Regents of the University of California.  All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the name of the University nor the names of its contributors
+ *    may be used to endorse or promote products derived from this software
+ *    without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ *	@(#)queue.h	8.5 (Berkeley) 8/20/94
+ */
+
+#ifndef _DWC_LIST_H_
+#define _DWC_LIST_H_
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/** @file
+ *
+ * This file defines linked list operations.  It is derived from BSD with
+ * only the MACRO names being prefixed with DWC_.  This is because a few of
+ * these names conflict with those on Linux.  For documentation on use, see the
+ * inline comments in the source code.  The original license for this source
+ * code applies and is preserved in the dwc_list.h source file.
+ */
+
+/*
+ * This file defines five types of data structures: singly-linked lists,
+ * lists, simple queues, tail queues, and circular queues.
+ *
+ *
+ * A singly-linked list is headed by a single forward pointer. The elements
+ * are singly linked for minimum space and pointer manipulation overhead at
+ * the expense of O(n) removal for arbitrary elements. New elements can be
+ * added to the list after an existing element or at the head of the list.
+ * Elements being removed from the head of the list should use the explicit
+ * macro for this purpose for optimum efficiency. A singly-linked list may
+ * only be traversed in the forward direction.  Singly-linked lists are ideal
+ * for applications with large datasets and few or no removals or for
+ * implementing a LIFO queue.
+ *
+ * A list is headed by a single forward pointer (or an array of forward
+ * pointers for a hash table header). The elements are doubly linked
+ * so that an arbitrary element can be removed without a need to
+ * traverse the list. New elements can be added to the list before
+ * or after an existing element or at the head of the list. A list
+ * may only be traversed in the forward direction.
+ *
+ * A simple queue is headed by a pair of pointers, one the head of the
+ * list and the other to the tail of the list. The elements are singly
+ * linked to save space, so elements can only be removed from the
+ * head of the list. New elements can be added to the list before or after
+ * an existing element, at the head of the list, or at the end of the
+ * list. A simple queue may only be traversed in the forward direction.
+ *
+ * A tail queue is headed by a pair of pointers, one to the head of the
+ * list and the other to the tail of the list. The elements are doubly
+ * linked so that an arbitrary element can be removed without a need to
+ * traverse the list. New elements can be added to the list before or
+ * after an existing element, at the head of the list, or at the end of
+ * the list. A tail queue may be traversed in either direction.
+ *
+ * A circle queue is headed by a pair of pointers, one to the head of the
+ * list and the other to the tail of the list. The elements are doubly
+ * linked so that an arbitrary element can be removed without a need to
+ * traverse the list. New elements can be added to the list before or after
+ * an existing element, at the head of the list, or at the end of the list.
+ * A circle queue may be traversed in either direction, but has a more
+ * complex end of list detection.
+ *
+ * For details on the use of these macros, see the queue(3) manual page.
+ */
+
+/*
+ * Double-linked List.
+ */
+
+typedef struct dwc_list_link {
+	struct dwc_list_link *next;
+	struct dwc_list_link *prev;
+} dwc_list_link_t;
+
+#define DWC_LIST_INIT(link) do {	\
+	(link)->next = (link);		\
+	(link)->prev = (link);		\
+} while (0)
+
+#define DWC_LIST_FIRST(link)	((link)->next)
+#define DWC_LIST_LAST(link)	((link)->prev)
+#define DWC_LIST_END(link)	(link)
+#define DWC_LIST_NEXT(link)	((link)->next)
+#define DWC_LIST_PREV(link)	((link)->prev)
+#define DWC_LIST_EMPTY(link)	\
+	(DWC_LIST_FIRST(link) == DWC_LIST_END(link))
+#define DWC_LIST_ENTRY(link, type, field)			\
+	(type *)((uint8_t *)(link) - (size_t)(&((type *)0)->field))
+
+#if 0
+#define DWC_LIST_INSERT_HEAD(list, link) do {			\
+	(link)->next = (list)->next;				\
+	(link)->prev = (list);					\
+	(list)->next->prev = (link);				\
+	(list)->next = (link);					\
+} while (0)
+
+#define DWC_LIST_INSERT_TAIL(list, link) do {			\
+	(link)->next = (list);					\
+	(link)->prev = (list)->prev;				\
+	(list)->prev->next = (link);				\
+	(list)->prev = (link);					\
+} while (0)
+#else
+#define DWC_LIST_INSERT_HEAD(list, link) do {			\
+	dwc_list_link_t *__next__ = (list)->next;		\
+	__next__->prev = (link);				\
+	(link)->next = __next__;				\
+	(link)->prev = (list);					\
+	(list)->next = (link);					\
+} while (0)
+
+#define DWC_LIST_INSERT_TAIL(list, link) do {			\
+	dwc_list_link_t *__prev__ = (list)->prev;		\
+	(list)->prev = (link);					\
+	(link)->next = (list);					\
+	(link)->prev = __prev__;				\
+	__prev__->next = (link);				\
+} while (0)
+#endif
+
+#if 0
+static inline void __list_add(struct list_head *new,
+                              struct list_head *prev,
+                              struct list_head *next)
+{
+        next->prev = new;
+        new->next = next;
+        new->prev = prev;
+        prev->next = new;
+}
+
+static inline void list_add(struct list_head *new, struct list_head *head)
+{
+        __list_add(new, head, head->next);
+}
+
+static inline void list_add_tail(struct list_head *new, struct list_head *head)
+{
+        __list_add(new, head->prev, head);
+}
+
+static inline void __list_del(struct list_head * prev, struct list_head * next)
+{
+        next->prev = prev;
+        prev->next = next;
+}
+
+static inline void list_del(struct list_head *entry)
+{
+        __list_del(entry->prev, entry->next);
+        entry->next = LIST_POISON1;
+        entry->prev = LIST_POISON2;
+}
+#endif
+
+#define DWC_LIST_REMOVE(link) do {				\
+	(link)->next->prev = (link)->prev;			\
+	(link)->prev->next = (link)->next;			\
+} while (0)
+
+#define DWC_LIST_REMOVE_INIT(link) do {				\
+	DWC_LIST_REMOVE(link);					\
+	DWC_LIST_INIT(link);					\
+} while (0)
+
+#define DWC_LIST_MOVE_HEAD(list, link) do {			\
+	DWC_LIST_REMOVE(link);					\
+	DWC_LIST_INSERT_HEAD(list, link);			\
+} while (0)
+
+#define DWC_LIST_MOVE_TAIL(list, link) do {			\
+	DWC_LIST_REMOVE(link);					\
+	DWC_LIST_INSERT_TAIL(list, link);			\
+} while (0)
+
+#define DWC_LIST_FOREACH(var, list)				\
+	for((var) = DWC_LIST_FIRST(list);			\
+	    (var) != DWC_LIST_END(list);			\
+	    (var) = DWC_LIST_NEXT(var))
+
+#define DWC_LIST_FOREACH_SAFE(var, var2, list)			\
+	for((var) = DWC_LIST_FIRST(list), (var2) = DWC_LIST_NEXT(var);	\
+	    (var) != DWC_LIST_END(list);			\
+	    (var) = (var2), (var2) = DWC_LIST_NEXT(var2))
+
+#define DWC_LIST_FOREACH_REVERSE(var, list)			\
+	for((var) = DWC_LIST_LAST(list);			\
+	    (var) != DWC_LIST_END(list);			\
+	    (var) = DWC_LIST_PREV(var))
+
+/*
+ * Singly-linked List definitions.
+ */
+#define DWC_SLIST_HEAD(name, type)					\
+struct name {								\
+	struct type *slh_first;	/* first element */			\
+}
+
+#define DWC_SLIST_HEAD_INITIALIZER(head)				\
+	{ NULL }
+
+#define DWC_SLIST_ENTRY(type)						\
+struct {								\
+	struct type *sle_next;	/* next element */			\
+}
+
+/*
+ * Singly-linked List access methods.
+ */
+#define DWC_SLIST_FIRST(head)	((head)->slh_first)
+#define DWC_SLIST_END(head)		NULL
+#define DWC_SLIST_EMPTY(head)	(SLIST_FIRST(head) == SLIST_END(head))
+#define DWC_SLIST_NEXT(elm, field)	((elm)->field.sle_next)
+
+#define DWC_SLIST_FOREACH(var, head, field)				\
+	for((var) = SLIST_FIRST(head);					\
+	    (var) != SLIST_END(head);					\
+	    (var) = SLIST_NEXT(var, field))
+
+#define DWC_SLIST_FOREACH_PREVPTR(var, varp, head, field)		\
+	for((varp) = &SLIST_FIRST((head));				\
+	    ((var) = *(varp)) != SLIST_END(head);			\
+	    (varp) = &SLIST_NEXT((var), field))
+
+/*
+ * Singly-linked List functions.
+ */
+#define DWC_SLIST_INIT(head) {						\
+	SLIST_FIRST(head) = SLIST_END(head);				\
+}
+
+#define DWC_SLIST_INSERT_AFTER(slistelm, elm, field) do {		\
+	(elm)->field.sle_next = (slistelm)->field.sle_next;		\
+	(slistelm)->field.sle_next = (elm);				\
+} while (0)
+
+#define DWC_SLIST_INSERT_HEAD(head, elm, field) do {			\
+	(elm)->field.sle_next = (head)->slh_first;			\
+	(head)->slh_first = (elm);					\
+} while (0)
+
+#define DWC_SLIST_REMOVE_NEXT(head, elm, field) do {			\
+	(elm)->field.sle_next = (elm)->field.sle_next->field.sle_next;	\
+} while (0)
+
+#define DWC_SLIST_REMOVE_HEAD(head, field) do {				\
+	(head)->slh_first = (head)->slh_first->field.sle_next;		\
+} while (0)
+
+#define DWC_SLIST_REMOVE(head, elm, type, field) do {			\
+	if ((head)->slh_first == (elm)) {				\
+		SLIST_REMOVE_HEAD((head), field);			\
+	}								\
+	else {								\
+		struct type *curelm = (head)->slh_first;		\
+		while( curelm->field.sle_next != (elm) )		\
+			curelm = curelm->field.sle_next;		\
+		curelm->field.sle_next =				\
+		    curelm->field.sle_next->field.sle_next;		\
+	}								\
+} while (0)
+
+/*
+ * Simple queue definitions.
+ */
+#define DWC_SIMPLEQ_HEAD(name, type)					\
+struct name {								\
+	struct type *sqh_first;	/* first element */			\
+	struct type **sqh_last;	/* addr of last next element */		\
+}
+
+#define DWC_SIMPLEQ_HEAD_INITIALIZER(head)				\
+	{ NULL, &(head).sqh_first }
+
+#define DWC_SIMPLEQ_ENTRY(type)						\
+struct {								\
+	struct type *sqe_next;	/* next element */			\
+}
+
+/*
+ * Simple queue access methods.
+ */
+#define DWC_SIMPLEQ_FIRST(head)	    ((head)->sqh_first)
+#define DWC_SIMPLEQ_END(head)	    NULL
+#define DWC_SIMPLEQ_EMPTY(head)	    (SIMPLEQ_FIRST(head) == SIMPLEQ_END(head))
+#define DWC_SIMPLEQ_NEXT(elm, field)    ((elm)->field.sqe_next)
+
+#define DWC_SIMPLEQ_FOREACH(var, head, field)				\
+	for((var) = SIMPLEQ_FIRST(head);				\
+	    (var) != SIMPLEQ_END(head);					\
+	    (var) = SIMPLEQ_NEXT(var, field))
+
+/*
+ * Simple queue functions.
+ */
+#define DWC_SIMPLEQ_INIT(head) do {					\
+	(head)->sqh_first = NULL;					\
+	(head)->sqh_last = &(head)->sqh_first;				\
+} while (0)
+
+#define DWC_SIMPLEQ_INSERT_HEAD(head, elm, field) do {			\
+	if (((elm)->field.sqe_next = (head)->sqh_first) == NULL)	\
+		(head)->sqh_last = &(elm)->field.sqe_next;		\
+	(head)->sqh_first = (elm);					\
+} while (0)
+
+#define DWC_SIMPLEQ_INSERT_TAIL(head, elm, field) do {			\
+	(elm)->field.sqe_next = NULL;					\
+	*(head)->sqh_last = (elm);					\
+	(head)->sqh_last = &(elm)->field.sqe_next;			\
+} while (0)
+
+#define DWC_SIMPLEQ_INSERT_AFTER(head, listelm, elm, field) do {	\
+	if (((elm)->field.sqe_next = (listelm)->field.sqe_next) == NULL)\
+		(head)->sqh_last = &(elm)->field.sqe_next;		\
+	(listelm)->field.sqe_next = (elm);				\
+} while (0)
+
+#define DWC_SIMPLEQ_REMOVE_HEAD(head, field) do {			\
+	if (((head)->sqh_first = (head)->sqh_first->field.sqe_next) == NULL) \
+		(head)->sqh_last = &(head)->sqh_first;			\
+} while (0)
+
+/*
+ * Tail queue definitions.
+ */
+#define DWC_TAILQ_HEAD(name, type)					\
+struct name {								\
+	struct type *tqh_first;	/* first element */			\
+	struct type **tqh_last;	/* addr of last next element */		\
+}
+
+#define DWC_TAILQ_HEAD_INITIALIZER(head)				\
+	{ NULL, &(head).tqh_first }
+
+#define DWC_TAILQ_ENTRY(type)						\
+struct {								\
+	struct type *tqe_next;	/* next element */			\
+	struct type **tqe_prev;	/* address of previous next element */	\
+}
+
+/*
+ * tail queue access methods
+ */
+#define DWC_TAILQ_FIRST(head)		((head)->tqh_first)
+#define DWC_TAILQ_END(head)		NULL
+#define DWC_TAILQ_NEXT(elm, field)	((elm)->field.tqe_next)
+#define DWC_TAILQ_LAST(head, headname)					\
+	(*(((struct headname *)((head)->tqh_last))->tqh_last))
+/* XXX */
+#define DWC_TAILQ_PREV(elm, headname, field)				\
+	(*(((struct headname *)((elm)->field.tqe_prev))->tqh_last))
+#define DWC_TAILQ_EMPTY(head)						\
+	(TAILQ_FIRST(head) == TAILQ_END(head))
+
+#define DWC_TAILQ_FOREACH(var, head, field)				\
+	for((var) = TAILQ_FIRST(head);					\
+	    (var) != TAILQ_END(head);					\
+	    (var) = TAILQ_NEXT(var, field))
+
+#define DWC_TAILQ_FOREACH_REVERSE(var, head, headname, field)		\
+	for((var) = TAILQ_LAST(head, headname);				\
+	    (var) != TAILQ_END(head);					\
+	    (var) = TAILQ_PREV(var, headname, field))
+
+/*
+ * Tail queue functions.
+ */
+#define DWC_TAILQ_INIT(head) do {					\
+	(head)->tqh_first = NULL;					\
+	(head)->tqh_last = &(head)->tqh_first;				\
+} while (0)
+
+#define DWC_TAILQ_INSERT_HEAD(head, elm, field) do {			\
+	if (((elm)->field.tqe_next = (head)->tqh_first) != NULL)	\
+		(head)->tqh_first->field.tqe_prev =			\
+		    &(elm)->field.tqe_next;				\
+	else								\
+		(head)->tqh_last = &(elm)->field.tqe_next;		\
+	(head)->tqh_first = (elm);					\
+	(elm)->field.tqe_prev = &(head)->tqh_first;			\
+} while (0)
+
+#define DWC_TAILQ_INSERT_TAIL(head, elm, field) do {			\
+	(elm)->field.tqe_next = NULL;					\
+	(elm)->field.tqe_prev = (head)->tqh_last;			\
+	*(head)->tqh_last = (elm);					\
+	(head)->tqh_last = &(elm)->field.tqe_next;			\
+} while (0)
+
+#define DWC_TAILQ_INSERT_AFTER(head, listelm, elm, field) do {		\
+	if (((elm)->field.tqe_next = (listelm)->field.tqe_next) != NULL)\
+		(elm)->field.tqe_next->field.tqe_prev =			\
+		    &(elm)->field.tqe_next;				\
+	else								\
+		(head)->tqh_last = &(elm)->field.tqe_next;		\
+	(listelm)->field.tqe_next = (elm);				\
+	(elm)->field.tqe_prev = &(listelm)->field.tqe_next;		\
+} while (0)
+
+#define DWC_TAILQ_INSERT_BEFORE(listelm, elm, field) do {		\
+	(elm)->field.tqe_prev = (listelm)->field.tqe_prev;		\
+	(elm)->field.tqe_next = (listelm);				\
+	*(listelm)->field.tqe_prev = (elm);				\
+	(listelm)->field.tqe_prev = &(elm)->field.tqe_next;		\
+} while (0)
+
+#define DWC_TAILQ_REMOVE(head, elm, field) do {				\
+	if (((elm)->field.tqe_next) != NULL)				\
+		(elm)->field.tqe_next->field.tqe_prev =			\
+		    (elm)->field.tqe_prev;				\
+	else								\
+		(head)->tqh_last = (elm)->field.tqe_prev;		\
+	*(elm)->field.tqe_prev = (elm)->field.tqe_next;			\
+} while (0)
+
+#define DWC_TAILQ_REPLACE(head, elm, elm2, field) do {			\
+	if (((elm2)->field.tqe_next = (elm)->field.tqe_next) != NULL)	\
+		(elm2)->field.tqe_next->field.tqe_prev =		\
+		    &(elm2)->field.tqe_next;				\
+	else								\
+		(head)->tqh_last = &(elm2)->field.tqe_next;		\
+	(elm2)->field.tqe_prev = (elm)->field.tqe_prev;			\
+	*(elm2)->field.tqe_prev = (elm2);				\
+} while (0)
+
+/*
+ * Circular queue definitions.
+ */
+#define DWC_CIRCLEQ_HEAD(name, type)					\
+struct name {								\
+	struct type *cqh_first;		/* first element */		\
+	struct type *cqh_last;		/* last element */		\
+}
+
+#define DWC_CIRCLEQ_HEAD_INITIALIZER(head)				\
+	{ DWC_CIRCLEQ_END(&head), DWC_CIRCLEQ_END(&head) }
+
+#define DWC_CIRCLEQ_ENTRY(type)						\
+struct {								\
+	struct type *cqe_next;		/* next element */		\
+	struct type *cqe_prev;		/* previous element */		\
+}
+
+/*
+ * Circular queue access methods
+ */
+#define DWC_CIRCLEQ_FIRST(head)		((head)->cqh_first)
+#define DWC_CIRCLEQ_LAST(head)		((head)->cqh_last)
+#define DWC_CIRCLEQ_END(head)		((void *)(head))
+#define DWC_CIRCLEQ_NEXT(elm, field)	((elm)->field.cqe_next)
+#define DWC_CIRCLEQ_PREV(elm, field)	((elm)->field.cqe_prev)
+#define DWC_CIRCLEQ_EMPTY(head)						\
+	(DWC_CIRCLEQ_FIRST(head) == DWC_CIRCLEQ_END(head))
+
+#define DWC_CIRCLEQ_EMPTY_ENTRY(elm, field) (((elm)->field.cqe_next == NULL) && ((elm)->field.cqe_prev == NULL))
+
+#define DWC_CIRCLEQ_FOREACH(var, head, field)				\
+	for((var) = DWC_CIRCLEQ_FIRST(head);				\
+	    (var) != DWC_CIRCLEQ_END(head);				\
+	    (var) = DWC_CIRCLEQ_NEXT(var, field))
+
+#define DWC_CIRCLEQ_FOREACH_SAFE(var, var2, head, field)			\
+	for((var) = DWC_CIRCLEQ_FIRST(head), var2 = DWC_CIRCLEQ_NEXT(var, field); \
+	    (var) != DWC_CIRCLEQ_END(head);					\
+	    (var) = var2, var2 = DWC_CIRCLEQ_NEXT(var, field))
+
+#define DWC_CIRCLEQ_FOREACH_REVERSE(var, head, field)			\
+	for((var) = DWC_CIRCLEQ_LAST(head);				\
+	    (var) != DWC_CIRCLEQ_END(head);				\
+	    (var) = DWC_CIRCLEQ_PREV(var, field))
+
+/*
+ * Circular queue functions.
+ */
+#define DWC_CIRCLEQ_INIT(head) do {					\
+	(head)->cqh_first = DWC_CIRCLEQ_END(head);			\
+	(head)->cqh_last = DWC_CIRCLEQ_END(head);			\
+} while (0)
+
+#define DWC_CIRCLEQ_INIT_ENTRY(elm, field) do {				\
+	(elm)->field.cqe_next = NULL;					\
+	(elm)->field.cqe_prev = NULL;					\
+} while (0)
+
+#define DWC_CIRCLEQ_INSERT_AFTER(head, listelm, elm, field) do {	\
+	(elm)->field.cqe_next = (listelm)->field.cqe_next;		\
+	(elm)->field.cqe_prev = (listelm);				\
+	if ((listelm)->field.cqe_next == DWC_CIRCLEQ_END(head))		\
+		(head)->cqh_last = (elm);				\
+	else								\
+		(listelm)->field.cqe_next->field.cqe_prev = (elm);	\
+	(listelm)->field.cqe_next = (elm);				\
+} while (0)
+
+#define DWC_CIRCLEQ_INSERT_BEFORE(head, listelm, elm, field) do {	\
+	(elm)->field.cqe_next = (listelm);				\
+	(elm)->field.cqe_prev = (listelm)->field.cqe_prev;		\
+	if ((listelm)->field.cqe_prev == DWC_CIRCLEQ_END(head))		\
+		(head)->cqh_first = (elm);				\
+	else								\
+		(listelm)->field.cqe_prev->field.cqe_next = (elm);	\
+	(listelm)->field.cqe_prev = (elm);				\
+} while (0)
+
+#define DWC_CIRCLEQ_INSERT_HEAD(head, elm, field) do {			\
+	(elm)->field.cqe_next = (head)->cqh_first;			\
+	(elm)->field.cqe_prev = DWC_CIRCLEQ_END(head);			\
+	if ((head)->cqh_last == DWC_CIRCLEQ_END(head))			\
+		(head)->cqh_last = (elm);				\
+	else								\
+		(head)->cqh_first->field.cqe_prev = (elm);		\
+	(head)->cqh_first = (elm);					\
+} while (0)
+
+#define DWC_CIRCLEQ_INSERT_TAIL(head, elm, field) do {			\
+	(elm)->field.cqe_next = DWC_CIRCLEQ_END(head);			\
+	(elm)->field.cqe_prev = (head)->cqh_last;			\
+	if ((head)->cqh_first == DWC_CIRCLEQ_END(head))			\
+		(head)->cqh_first = (elm);				\
+	else								\
+		(head)->cqh_last->field.cqe_next = (elm);		\
+	(head)->cqh_last = (elm);					\
+} while (0)
+
+#define DWC_CIRCLEQ_REMOVE(head, elm, field) do {			\
+	if ((elm)->field.cqe_next == DWC_CIRCLEQ_END(head))		\
+		(head)->cqh_last = (elm)->field.cqe_prev;		\
+	else								\
+		(elm)->field.cqe_next->field.cqe_prev =			\
+		    (elm)->field.cqe_prev;				\
+	if ((elm)->field.cqe_prev == DWC_CIRCLEQ_END(head))		\
+		(head)->cqh_first = (elm)->field.cqe_next;		\
+	else								\
+		(elm)->field.cqe_prev->field.cqe_next =			\
+		    (elm)->field.cqe_next;				\
+} while (0)
+
+#define DWC_CIRCLEQ_REMOVE_INIT(head, elm, field) do {			\
+	DWC_CIRCLEQ_REMOVE(head, elm, field);				\
+	DWC_CIRCLEQ_INIT_ENTRY(elm, field);				\
+} while (0)
+
+#define DWC_CIRCLEQ_REPLACE(head, elm, elm2, field) do {		\
+	if (((elm2)->field.cqe_next = (elm)->field.cqe_next) ==		\
+	    DWC_CIRCLEQ_END(head))					\
+		(head).cqh_last = (elm2);				\
+	else								\
+		(elm2)->field.cqe_next->field.cqe_prev = (elm2);	\
+	if (((elm2)->field.cqe_prev = (elm)->field.cqe_prev) ==		\
+	    DWC_CIRCLEQ_END(head))					\
+		(head).cqh_first = (elm2);				\
+	else								\
+		(elm2)->field.cqe_prev->field.cqe_next = (elm2);	\
+} while (0)
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* _DWC_LIST_H_ */
diff --git a/drivers/usb/dwc_otg/dwc_mem.c b/drivers/usb/dwc_otg/dwc_mem.c
new file mode 100644
index 0000000..ad645ff
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_mem.c
@@ -0,0 +1,245 @@
+/* Memory Debugging */
+#ifdef DWC_DEBUG_MEMORY
+
+#include "dwc_os.h"
+#include "dwc_list.h"
+
+struct allocation {
+	void *addr;
+	void *ctx;
+	char *func;
+	int line;
+	uint32_t size;
+	int dma;
+	DWC_CIRCLEQ_ENTRY(allocation) entry;
+};
+
+DWC_CIRCLEQ_HEAD(allocation_queue, allocation);
+
+struct allocation_manager {
+	void *mem_ctx;
+	struct allocation_queue allocations;
+
+	/* statistics */
+	int num;
+	int num_freed;
+	int num_active;
+	uint32_t total;
+	uint32_t cur;
+	uint32_t max;
+};
+
+static struct allocation_manager *manager = NULL;
+
+static int add_allocation(void *ctx, uint32_t size, char const *func, int line, void *addr,
+			  int dma)
+{
+	struct allocation *a;
+
+	DWC_ASSERT(manager != NULL, "manager not allocated");
+
+	a = __DWC_ALLOC_ATOMIC(manager->mem_ctx, sizeof(*a));
+	if (!a) {
+		return -DWC_E_NO_MEMORY;
+	}
+
+	a->func = __DWC_ALLOC_ATOMIC(manager->mem_ctx, DWC_STRLEN(func) + 1);
+	if (!a->func) {
+		__DWC_FREE(manager->mem_ctx, a);
+		return -DWC_E_NO_MEMORY;
+	}
+
+	DWC_MEMCPY(a->func, func, DWC_STRLEN(func) + 1);
+	a->addr = addr;
+	a->ctx = ctx;
+	a->line = line;
+	a->size = size;
+	a->dma = dma;
+	DWC_CIRCLEQ_INSERT_TAIL(&manager->allocations, a, entry);
+
+	/* Update stats */
+	manager->num++;
+	manager->num_active++;
+	manager->total += size;
+	manager->cur += size;
+
+	if (manager->max < manager->cur) {
+		manager->max = manager->cur;
+	}
+
+	return 0;
+}
+
+static struct allocation *find_allocation(void *ctx, void *addr)
+{
+	struct allocation *a;
+
+	DWC_CIRCLEQ_FOREACH(a, &manager->allocations, entry) {
+		if (a->ctx == ctx && a->addr == addr) {
+			return a;
+		}
+	}
+
+	return NULL;
+}
+
+static void free_allocation(void *ctx, void *addr, char const *func, int line)
+{
+	struct allocation *a = find_allocation(ctx, addr);
+
+	if (!a) {
+		DWC_ASSERT(0,
+			   "Free of address %p that was never allocated or already freed %s:%d",
+			   addr, func, line);
+		return;
+	}
+
+	DWC_CIRCLEQ_REMOVE(&manager->allocations, a, entry);
+
+	manager->num_active--;
+	manager->num_freed++;
+	manager->cur -= a->size;
+	__DWC_FREE(manager->mem_ctx, a->func);
+	__DWC_FREE(manager->mem_ctx, a);
+}
+
+int dwc_memory_debug_start(void *mem_ctx)
+{
+	DWC_ASSERT(manager == NULL, "Memory debugging has already started\n");
+
+	if (manager) {
+		return -DWC_E_BUSY;
+	}
+
+	manager = __DWC_ALLOC(mem_ctx, sizeof(*manager));
+	if (!manager) {
+		return -DWC_E_NO_MEMORY;
+	}
+
+	DWC_CIRCLEQ_INIT(&manager->allocations);
+	manager->mem_ctx = mem_ctx;
+	manager->num = 0;
+	manager->num_freed = 0;
+	manager->num_active = 0;
+	manager->total = 0;
+	manager->cur = 0;
+	manager->max = 0;
+
+	return 0;
+}
+
+void dwc_memory_debug_stop(void)
+{
+	struct allocation *a;
+
+	dwc_memory_debug_report();
+
+	DWC_CIRCLEQ_FOREACH(a, &manager->allocations, entry) {
+		DWC_ERROR("Memory leaked from %s:%d\n", a->func, a->line);
+		free_allocation(a->ctx, a->addr, NULL, -1);
+	}
+
+	__DWC_FREE(manager->mem_ctx, manager);
+}
+
+void dwc_memory_debug_report(void)
+{
+	struct allocation *a;
+
+	DWC_PRINTF("\n\n\n----------------- Memory Debugging Report -----------------\n\n");
+	DWC_PRINTF("Num Allocations = %d\n", manager->num);
+	DWC_PRINTF("Freed = %d\n", manager->num_freed);
+	DWC_PRINTF("Active = %d\n", manager->num_active);
+	DWC_PRINTF("Current Memory Used = %d\n", manager->cur);
+	DWC_PRINTF("Total Memory Used = %d\n", manager->total);
+	DWC_PRINTF("Maximum Memory Used at Once = %d\n", manager->max);
+	DWC_PRINTF("Unfreed allocations:\n");
+
+	DWC_CIRCLEQ_FOREACH(a, &manager->allocations, entry) {
+		DWC_PRINTF("    addr=%p, size=%d from %s:%d, DMA=%d\n",
+			   a->addr, a->size, a->func, a->line, a->dma);
+	}
+}
+
+/* The replacement functions */
+void *dwc_alloc_debug(void *mem_ctx, uint32_t size, char const *func, int line)
+{
+	void *addr = __DWC_ALLOC(mem_ctx, size);
+
+	if (!addr) {
+		return NULL;
+	}
+
+	if (add_allocation(mem_ctx, size, func, line, addr, 0)) {
+		__DWC_FREE(mem_ctx, addr);
+		return NULL;
+	}
+
+	return addr;
+}
+
+void *dwc_alloc_atomic_debug(void *mem_ctx, uint32_t size, char const *func,
+			     int line)
+{
+	void *addr = __DWC_ALLOC_ATOMIC(mem_ctx, size);
+
+	if (!addr) {
+		return NULL;
+	}
+
+	if (add_allocation(mem_ctx, size, func, line, addr, 0)) {
+		__DWC_FREE(mem_ctx, addr);
+		return NULL;
+	}
+
+	return addr;
+}
+
+void dwc_free_debug(void *mem_ctx, void *addr, char const *func, int line)
+{
+	free_allocation(mem_ctx, addr, func, line);
+	__DWC_FREE(mem_ctx, addr);
+}
+
+void *dwc_dma_alloc_debug(void *dma_ctx, uint32_t size, dwc_dma_t *dma_addr,
+			  char const *func, int line)
+{
+	void *addr = __DWC_DMA_ALLOC(dma_ctx, size, dma_addr);
+
+	if (!addr) {
+		return NULL;
+	}
+
+	if (add_allocation(dma_ctx, size, func, line, addr, 1)) {
+		__DWC_DMA_FREE(dma_ctx, size, addr, *dma_addr);
+		return NULL;
+	}
+
+	return addr;
+}
+
+void *dwc_dma_alloc_atomic_debug(void *dma_ctx, uint32_t size,
+				 dwc_dma_t *dma_addr, char const *func, int line)
+{
+	void *addr = __DWC_DMA_ALLOC_ATOMIC(dma_ctx, size, dma_addr);
+
+	if (!addr) {
+		return NULL;
+	}
+
+	if (add_allocation(dma_ctx, size, func, line, addr, 1)) {
+		__DWC_DMA_FREE(dma_ctx, size, addr, *dma_addr);
+		return NULL;
+	}
+
+	return addr;
+}
+
+void dwc_dma_free_debug(void *dma_ctx, uint32_t size, void *virt_addr,
+			dwc_dma_t dma_addr, char const *func, int line)
+{
+	free_allocation(dma_ctx, virt_addr, func, line);
+	__DWC_DMA_FREE(dma_ctx, size, virt_addr, dma_addr);
+}
+
+#endif /* DWC_DEBUG_MEMORY */
diff --git a/drivers/usb/dwc_otg/dwc_modpow.c b/drivers/usb/dwc_otg/dwc_modpow.c
new file mode 100644
index 0000000..f410f6e
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_modpow.c
@@ -0,0 +1,633 @@
+/* Bignum routines adapted from PUTTY sources.  PuTTY copyright notice follows.
+ *
+ * PuTTY is copyright 1997-2007 Simon Tatham.
+ *
+ * Portions copyright Robert de Bath, Joris van Rantwijk, Delian
+ * Delchev, Andreas Schultz, Jeroen Massar, Wez Furlong, Nicolas Barry,
+ * Justin Bradford, Ben Harris, Malcolm Smith, Ahmad Khalifa, Markus
+ * Kuhn, and CORE SDI S.A.
+ *
+ * Permission is hereby granted, free of charge, to any person
+ * obtaining a copy of this software and associated documentation files
+ * (the "Software"), to deal in the Software without restriction,
+ * including without limitation the rights to use, copy, modify, merge,
+ * publish, distribute, sublicense, and/or sell copies of the Software,
+ * and to permit persons to whom the Software is furnished to do so,
+ * subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be
+ * included in all copies or substantial portions of the Software.
+
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT.  IN NO EVENT SHALL THE COPYRIGHT HOLDERS BE LIABLE
+ * FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF
+ * CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+ * WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+ *
+ */
+#ifdef DWC_CRYPTOLIB
+
+#ifndef CONFIG_MACH_IPMATE
+
+#include "dwc_modpow.h"
+
+#define BIGNUM_INT_MASK  0xFFFFFFFFUL
+#define BIGNUM_TOP_BIT   0x80000000UL
+#define BIGNUM_INT_BITS  32
+
+
+static void *snmalloc(void *mem_ctx, size_t n, size_t size)
+{
+    void *p;
+    size *= n;
+    if (size == 0) size = 1;
+    p = dwc_alloc(mem_ctx, size);
+    return p;
+}
+
+#define snewn(ctx, n, type) ((type *)snmalloc((ctx), (n), sizeof(type)))
+#define sfree dwc_free
+
+/*
+ * Usage notes:
+ *  * Do not call the DIVMOD_WORD macro with expressions such as array
+ *    subscripts, as some implementations object to this (see below).
+ *  * Note that none of the division methods below will cope if the
+ *    quotient won't fit into BIGNUM_INT_BITS. Callers should be careful
+ *    to avoid this case.
+ *    If this condition occurs, in the case of the x86 DIV instruction,
+ *    an overflow exception will occur, which (according to a correspondent)
+ *    will manifest on Windows as something like
+ *      0xC0000095: Integer overflow
+ *    The C variant won't give the right answer, either.
+ */
+
+#define MUL_WORD(w1, w2) ((BignumDblInt)w1 * w2)
+
+#if defined __GNUC__ && defined __i386__
+#define DIVMOD_WORD(q, r, hi, lo, w) \
+    __asm__("div %2" : \
+	    "=d" (r), "=a" (q) : \
+	    "r" (w), "d" (hi), "a" (lo))
+#else
+#define DIVMOD_WORD(q, r, hi, lo, w) do { \
+    BignumDblInt n = (((BignumDblInt)hi) << BIGNUM_INT_BITS) | lo; \
+    q = n / w; \
+    r = n % w; \
+} while (0)
+#endif
+
+#define BIGNUM_INT_BYTES (BIGNUM_INT_BITS / 8)
+
+#define BIGNUM_INTERNAL
+
+static Bignum newbn(void *mem_ctx, int length)
+{
+    Bignum b = snewn(mem_ctx, length + 1, BignumInt);
+    //if (!b)
+    //abort();		       /* FIXME */
+    DWC_MEMSET(b, 0, (length + 1) * sizeof(*b));
+    b[0] = length;
+    return b;
+}
+
+void freebn(void *mem_ctx, Bignum b)
+{
+    /*
+     * Burn the evidence, just in case.
+     */
+    DWC_MEMSET(b, 0, sizeof(b[0]) * (b[0] + 1));
+    sfree(mem_ctx, b);
+}
+
+/*
+ * Compute c = a * b.
+ * Input is in the first len words of a and b.
+ * Result is returned in the first 2*len words of c.
+ */
+static void internal_mul(BignumInt *a, BignumInt *b,
+			 BignumInt *c, int len)
+{
+    int i, j;
+    BignumDblInt t;
+
+    for (j = 0; j < 2 * len; j++)
+	c[j] = 0;
+
+    for (i = len - 1; i >= 0; i--) {
+	t = 0;
+	for (j = len - 1; j >= 0; j--) {
+	    t += MUL_WORD(a[i], (BignumDblInt) b[j]);
+	    t += (BignumDblInt) c[i + j + 1];
+	    c[i + j + 1] = (BignumInt) t;
+	    t = t >> BIGNUM_INT_BITS;
+	}
+	c[i] = (BignumInt) t;
+    }
+}
+
+static void internal_add_shifted(BignumInt *number,
+				 unsigned n, int shift)
+{
+    int word = 1 + (shift / BIGNUM_INT_BITS);
+    int bshift = shift % BIGNUM_INT_BITS;
+    BignumDblInt addend;
+
+    addend = (BignumDblInt)n << bshift;
+
+    while (addend) {
+	addend += number[word];
+	number[word] = (BignumInt) addend & BIGNUM_INT_MASK;
+	addend >>= BIGNUM_INT_BITS;
+	word++;
+    }
+}
+
+/*
+ * Compute a = a % m.
+ * Input in first alen words of a and first mlen words of m.
+ * Output in first alen words of a
+ * (of which first alen-mlen words will be zero).
+ * The MSW of m MUST have its high bit set.
+ * Quotient is accumulated in the `quotient' array, which is a Bignum
+ * rather than the internal bigendian format. Quotient parts are shifted
+ * left by `qshift' before adding into quot.
+ */
+static void internal_mod(BignumInt *a, int alen,
+			 BignumInt *m, int mlen,
+			 BignumInt *quot, int qshift)
+{
+    BignumInt m0, m1;
+    unsigned int h;
+    int i, k;
+
+    m0 = m[0];
+    if (mlen > 1)
+	m1 = m[1];
+    else
+	m1 = 0;
+
+    for (i = 0; i <= alen - mlen; i++) {
+	BignumDblInt t;
+	unsigned int q, r, c, ai1;
+
+	if (i == 0) {
+	    h = 0;
+	} else {
+	    h = a[i - 1];
+	    a[i - 1] = 0;
+	}
+
+	if (i == alen - 1)
+	    ai1 = 0;
+	else
+	    ai1 = a[i + 1];
+
+	/* Find q = h:a[i] / m0 */
+	if (h >= m0) {
+	    /*
+	     * Special case.
+	     * 
+	     * To illustrate it, suppose a BignumInt is 8 bits, and
+	     * we are dividing (say) A1:23:45:67 by A1:B2:C3. Then
+	     * our initial division will be 0xA123 / 0xA1, which
+	     * will give a quotient of 0x100 and a divide overflow.
+	     * However, the invariants in this division algorithm
+	     * are not violated, since the full number A1:23:... is
+	     * _less_ than the quotient prefix A1:B2:... and so the
+	     * following correction loop would have sorted it out.
+	     * 
+	     * In this situation we set q to be the largest
+	     * quotient we _can_ stomach (0xFF, of course).
+	     */
+	    q = BIGNUM_INT_MASK;
+	} else {
+	    /* Macro doesn't want an array subscript expression passed
+	     * into it (see definition), so use a temporary. */
+	    BignumInt tmplo = a[i];
+	    DIVMOD_WORD(q, r, h, tmplo, m0);
+
+	    /* Refine our estimate of q by looking at
+	     h:a[i]:a[i+1] / m0:m1 */
+	    t = MUL_WORD(m1, q);
+	    if (t > ((BignumDblInt) r << BIGNUM_INT_BITS) + ai1) {
+		q--;
+		t -= m1;
+		r = (r + m0) & BIGNUM_INT_MASK;     /* overflow? */
+		if (r >= (BignumDblInt) m0 &&
+		    t > ((BignumDblInt) r << BIGNUM_INT_BITS) + ai1) q--;
+	    }
+	}
+
+	/* Subtract q * m from a[i...] */
+	c = 0;
+	for (k = mlen - 1; k >= 0; k--) {
+	    t = MUL_WORD(q, m[k]);
+	    t += c;
+	    c = (unsigned)(t >> BIGNUM_INT_BITS);
+	    if ((BignumInt) t > a[i + k])
+		c++;
+	    a[i + k] -= (BignumInt) t;
+	}
+
+	/* Add back m in case of borrow */
+	if (c != h) {
+	    t = 0;
+	    for (k = mlen - 1; k >= 0; k--) {
+		t += m[k];
+		t += a[i + k];
+		a[i + k] = (BignumInt) t;
+		t = t >> BIGNUM_INT_BITS;
+	    }
+	    q--;
+	}
+	if (quot)
+	    internal_add_shifted(quot, q, qshift + BIGNUM_INT_BITS * (alen - mlen - i));
+    }
+}
+
+/*
+ * Compute p % mod.
+ * The most significant word of mod MUST be non-zero.
+ * We assume that the result array is the same size as the mod array.
+ * We optionally write out a quotient if `quotient' is non-NULL.
+ * We can avoid writing out the result if `result' is NULL.
+ */
+void bigdivmod(void *mem_ctx, Bignum p, Bignum mod, Bignum result, Bignum quotient)
+{
+    BignumInt *n, *m;
+    int mshift;
+    int plen, mlen, i, j;
+
+    /* Allocate m of size mlen, copy mod to m */
+    /* We use big endian internally */
+    mlen = mod[0];
+    m = snewn(mem_ctx, mlen, BignumInt);
+    //if (!m)
+    //abort();		       /* FIXME */
+    for (j = 0; j < mlen; j++)
+	m[j] = mod[mod[0] - j];
+
+    /* Shift m left to make msb bit set */
+    for (mshift = 0; mshift < BIGNUM_INT_BITS-1; mshift++)
+	if ((m[0] << mshift) & BIGNUM_TOP_BIT)
+	    break;
+    if (mshift) {
+	for (i = 0; i < mlen - 1; i++)
+	    m[i] = (m[i] << mshift) | (m[i + 1] >> (BIGNUM_INT_BITS - mshift));
+	m[mlen - 1] = m[mlen - 1] << mshift;
+    }
+
+    plen = p[0];
+    /* Ensure plen > mlen */
+    if (plen <= mlen)
+	plen = mlen + 1;
+
+    /* Allocate n of size plen, copy p to n */
+    n = snewn(mem_ctx, plen, BignumInt);
+    //if (!n)
+    //abort();		       /* FIXME */
+    for (j = 0; j < plen; j++)
+	n[j] = 0;
+    for (j = 1; j <= (int)p[0]; j++)
+	n[plen - j] = p[j];
+
+    /* Main computation */
+    internal_mod(n, plen, m, mlen, quotient, mshift);
+
+    /* Fixup result in case the modulus was shifted */
+    if (mshift) {
+	for (i = plen - mlen - 1; i < plen - 1; i++)
+	    n[i] = (n[i] << mshift) | (n[i + 1] >> (BIGNUM_INT_BITS - mshift));
+	n[plen - 1] = n[plen - 1] << mshift;
+	internal_mod(n, plen, m, mlen, quotient, 0);
+	for (i = plen - 1; i >= plen - mlen; i--)
+	    n[i] = (n[i] >> mshift) | (n[i - 1] << (BIGNUM_INT_BITS - mshift));
+    }
+
+    /* Copy result to buffer */
+    if (result) {
+	for (i = 1; i <= (int)result[0]; i++) {
+	    int j = plen - i;
+	    result[i] = j >= 0 ? n[j] : 0;
+	}
+    }
+
+    /* Free temporary arrays */
+    for (i = 0; i < mlen; i++)
+	m[i] = 0;
+    sfree(mem_ctx, m);
+    for (i = 0; i < plen; i++)
+	n[i] = 0;
+    sfree(mem_ctx, n);
+}
+
+/*
+ * Simple remainder.
+ */
+Bignum bigmod(void *mem_ctx, Bignum a, Bignum b)
+{
+    Bignum r = newbn(mem_ctx, b[0]);
+    bigdivmod(mem_ctx, a, b, r, NULL);
+    return r;
+}
+
+/*
+ * Compute (base ^ exp) % mod.
+ */
+Bignum dwc_modpow(void *mem_ctx, Bignum base_in, Bignum exp, Bignum mod)
+{
+    BignumInt *a, *b, *n, *m;
+    int mshift;
+    int mlen, i, j;
+    Bignum base, result;
+
+    /*
+     * The most significant word of mod needs to be non-zero. It
+     * should already be, but let's make sure.
+     */
+    //assert(mod[mod[0]] != 0);
+
+    /*
+     * Make sure the base is smaller than the modulus, by reducing
+     * it modulo the modulus if not.
+     */
+    base = bigmod(mem_ctx, base_in, mod);
+
+    /* Allocate m of size mlen, copy mod to m */
+    /* We use big endian internally */
+    mlen = mod[0];
+    m = snewn(mem_ctx, mlen, BignumInt);
+    //if (!m)
+    //abort();		       /* FIXME */
+    for (j = 0; j < mlen; j++)
+	m[j] = mod[mod[0] - j];
+
+    /* Shift m left to make msb bit set */
+    for (mshift = 0; mshift < BIGNUM_INT_BITS - 1; mshift++)
+	if ((m[0] << mshift) & BIGNUM_TOP_BIT)
+	    break;
+    if (mshift) {
+	for (i = 0; i < mlen - 1; i++)
+	    m[i] =
+		(m[i] << mshift) | (m[i + 1] >>
+				    (BIGNUM_INT_BITS - mshift));
+	m[mlen - 1] = m[mlen - 1] << mshift;
+    }
+
+    /* Allocate n of size mlen, copy base to n */
+    n = snewn(mem_ctx, mlen, BignumInt);
+    //if (!n)
+    //abort();		       /* FIXME */
+    i = mlen - base[0];
+    for (j = 0; j < i; j++)
+	n[j] = 0;
+    for (j = 0; j < base[0]; j++)
+	n[i + j] = base[base[0] - j];
+
+    /* Allocate a and b of size 2*mlen. Set a = 1 */
+    a = snewn(mem_ctx, 2 * mlen, BignumInt);
+    //if (!a)
+    //abort();		       /* FIXME */
+    b = snewn(mem_ctx, 2 * mlen, BignumInt);
+    //if (!b)
+    //abort();		       /* FIXME */
+    for (i = 0; i < 2 * mlen; i++)
+	a[i] = 0;
+    a[2 * mlen - 1] = 1;
+
+    /* Skip leading zero bits of exp. */
+    i = 0;
+    j = BIGNUM_INT_BITS - 1;
+    while (i < exp[0] && (exp[exp[0] - i] & (1 << j)) == 0) {
+	j--;
+	if (j < 0) {
+	    i++;
+	    j = BIGNUM_INT_BITS - 1;
+	}
+    }
+
+    /* Main computation */
+    while (i < exp[0]) {
+	while (j >= 0) {
+	    internal_mul(a + mlen, a + mlen, b, mlen);
+	    internal_mod(b, mlen * 2, m, mlen, NULL, 0);
+	    if ((exp[exp[0] - i] & (1 << j)) != 0) {
+		internal_mul(b + mlen, n, a, mlen);
+		internal_mod(a, mlen * 2, m, mlen, NULL, 0);
+	    } else {
+		BignumInt *t;
+		t = a;
+		a = b;
+		b = t;
+	    }
+	    j--;
+	}
+	i++;
+	j = BIGNUM_INT_BITS - 1;
+    }
+
+    /* Fixup result in case the modulus was shifted */
+    if (mshift) {
+	for (i = mlen - 1; i < 2 * mlen - 1; i++)
+	    a[i] =
+		(a[i] << mshift) | (a[i + 1] >>
+				    (BIGNUM_INT_BITS - mshift));
+	a[2 * mlen - 1] = a[2 * mlen - 1] << mshift;
+	internal_mod(a, mlen * 2, m, mlen, NULL, 0);
+	for (i = 2 * mlen - 1; i >= mlen; i--)
+	    a[i] =
+		(a[i] >> mshift) | (a[i - 1] <<
+				    (BIGNUM_INT_BITS - mshift));
+    }
+
+    /* Copy result to buffer */
+    result = newbn(mem_ctx, mod[0]);
+    for (i = 0; i < mlen; i++)
+	result[result[0] - i] = a[i + mlen];
+    while (result[0] > 1 && result[result[0]] == 0)
+	result[0]--;
+
+    /* Free temporary arrays */
+    for (i = 0; i < 2 * mlen; i++)
+	a[i] = 0;
+    sfree(mem_ctx, a);
+    for (i = 0; i < 2 * mlen; i++)
+	b[i] = 0;
+    sfree(mem_ctx, b);
+    for (i = 0; i < mlen; i++)
+	m[i] = 0;
+    sfree(mem_ctx, m);
+    for (i = 0; i < mlen; i++)
+	n[i] = 0;
+    sfree(mem_ctx, n);
+
+    freebn(mem_ctx, base);
+
+    return result;
+}
+
+
+#ifdef UNITTEST
+
+static __u32 dh_p[] = {
+	96,
+	0xFFFFFFFF,
+	0xFFFFFFFF,
+	0xA93AD2CA,
+	0x4B82D120,
+	0xE0FD108E,
+	0x43DB5BFC,
+	0x74E5AB31,
+	0x08E24FA0,
+	0xBAD946E2,
+	0x770988C0,
+	0x7A615D6C,
+	0xBBE11757,
+	0x177B200C,
+	0x521F2B18,
+	0x3EC86A64,
+	0xD8760273,
+	0xD98A0864,
+	0xF12FFA06,
+	0x1AD2EE6B,
+	0xCEE3D226,
+	0x4A25619D,
+	0x1E8C94E0,
+	0xDB0933D7,
+	0xABF5AE8C,
+	0xA6E1E4C7,
+	0xB3970F85,
+	0x5D060C7D,
+	0x8AEA7157,
+	0x58DBEF0A,
+	0xECFB8504,
+	0xDF1CBA64,
+	0xA85521AB,
+	0x04507A33,
+	0xAD33170D,
+	0x8AAAC42D,
+	0x15728E5A,
+	0x98FA0510,
+	0x15D22618,
+	0xEA956AE5,
+	0x3995497C,
+	0x95581718,
+	0xDE2BCBF6,
+	0x6F4C52C9,
+	0xB5C55DF0,
+	0xEC07A28F,
+	0x9B2783A2,
+	0x180E8603,
+	0xE39E772C,
+	0x2E36CE3B,
+	0x32905E46,
+	0xCA18217C,
+	0xF1746C08,
+	0x4ABC9804,
+	0x670C354E,
+	0x7096966D,
+	0x9ED52907,
+	0x208552BB,
+	0x1C62F356,
+	0xDCA3AD96,
+	0x83655D23,
+	0xFD24CF5F,
+	0x69163FA8,
+	0x1C55D39A,
+	0x98DA4836,
+	0xA163BF05,
+	0xC2007CB8,
+	0xECE45B3D,
+	0x49286651,
+	0x7C4B1FE6,
+	0xAE9F2411,
+	0x5A899FA5,
+	0xEE386BFB,
+	0xF406B7ED,
+	0x0BFF5CB6,
+	0xA637ED6B,
+	0xF44C42E9,
+	0x625E7EC6,
+	0xE485B576,
+	0x6D51C245,
+	0x4FE1356D,
+	0xF25F1437,
+	0x302B0A6D,
+	0xCD3A431B,
+	0xEF9519B3,
+	0x8E3404DD,
+	0x514A0879,
+	0x3B139B22,
+	0x020BBEA6,
+	0x8A67CC74,
+	0x29024E08,
+	0x80DC1CD1,
+	0xC4C6628B,
+	0x2168C234,
+	0xC90FDAA2,
+	0xFFFFFFFF,
+	0xFFFFFFFF,
+};
+
+static __u32 dh_a[] = {
+	8,
+	0xdf367516,
+	0x86459caa,
+	0xe2d459a4,
+	0xd910dae0,
+	0x8a8b5e37,
+	0x67ab31c6,
+	0xf0b55ea9,
+	0x440051d6,
+};
+
+static __u32 dh_b[] = {
+	8,
+	0xded92656,
+	0xe07a048a,
+	0x6fa452cd,
+	0x2df89d30,
+	0xc75f1b0f,
+	0x8ce3578f, 
+	0x7980a324,
+	0x5daec786,
+};
+
+static __u32 dh_g[] = {
+	1,
+	2,
+};
+
+int main(void)
+{
+	int i;
+	__u32 *k;
+	k = dwc_modpow(NULL, dh_g, dh_a, dh_p);
+
+	printf("\n\n");
+	for (i=0; i<k[0]; i++) {
+		__u32 word32 = k[k[0] - i];
+		__u16 l = word32 & 0xffff;
+		__u16 m = (word32 & 0xffff0000) >> 16;
+		printf("%04x %04x ", m, l);
+		if (!((i + 1)%13)) printf("\n");
+	}
+	printf("\n\n");
+
+	if ((k[0] == 0x60) && (k[1] == 0x28e490e5) && (k[0x60] == 0x5a0d3d4e)) {
+		printf("PASS\n\n");
+	}
+	else {
+		printf("FAIL\n\n");
+	}
+
+}
+
+#endif /* UNITTEST */
+
+#endif /* CONFIG_MACH_IPMATE */
+
+#endif /*DWC_CRYPTOLIB */
diff --git a/drivers/usb/dwc_otg/dwc_modpow.h b/drivers/usb/dwc_otg/dwc_modpow.h
new file mode 100644
index 0000000..64f00c2
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_modpow.h
@@ -0,0 +1,34 @@
+/*
+ * dwc_modpow.h
+ * See dwc_modpow.c for license and changes
+ */
+#ifndef _DWC_MODPOW_H
+#define _DWC_MODPOW_H
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#include "dwc_os.h"
+
+/** @file
+ *
+ * This file defines the module exponentiation function which is only used
+ * internally by the DWC UWB modules for calculation of PKs during numeric
+ * association.  The routine is taken from the PUTTY, an open source terminal
+ * emulator.  The PUTTY License is preserved in the dwc_modpow.c file.
+ *
+ */
+
+typedef uint32_t BignumInt;
+typedef uint64_t BignumDblInt;
+typedef BignumInt *Bignum;
+
+/* Compute modular exponentiaion */
+extern Bignum dwc_modpow(void *mem_ctx, Bignum base_in, Bignum exp, Bignum mod);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* _LINUX_BIGNUM_H */
diff --git a/drivers/usb/dwc_otg/dwc_notifier.c b/drivers/usb/dwc_otg/dwc_notifier.c
new file mode 100644
index 0000000..d3dadce
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_notifier.c
@@ -0,0 +1,319 @@
+#ifdef DWC_NOTIFYLIB
+
+#include "dwc_notifier.h"
+#include "dwc_list.h"
+
+typedef struct dwc_observer {
+	void *observer;
+	dwc_notifier_callback_t callback;
+	void *data;
+	char *notification;
+	DWC_CIRCLEQ_ENTRY(dwc_observer) list_entry;
+} observer_t;
+
+DWC_CIRCLEQ_HEAD(observer_queue, dwc_observer);
+
+typedef struct dwc_notifier {
+	void *mem_ctx;
+	void *object;
+	struct observer_queue observers;
+	DWC_CIRCLEQ_ENTRY(dwc_notifier) list_entry;
+} notifier_t;
+
+DWC_CIRCLEQ_HEAD(notifier_queue, dwc_notifier);
+
+typedef struct manager {
+	void *mem_ctx;
+	void *wkq_ctx;
+	dwc_workq_t *wq;
+//	dwc_mutex_t *mutex;
+	struct notifier_queue notifiers;
+} manager_t;
+
+static manager_t *manager = NULL;
+
+static int create_manager(void *mem_ctx, void *wkq_ctx)
+{
+	manager = dwc_alloc(mem_ctx, sizeof(manager_t));
+	if (!manager) {
+		return -DWC_E_NO_MEMORY;
+	}
+
+	DWC_CIRCLEQ_INIT(&manager->notifiers);
+
+	manager->wq = dwc_workq_alloc(wkq_ctx, "DWC Notification WorkQ");
+	if (!manager->wq) {
+		return -DWC_E_NO_MEMORY;
+	}
+
+	return 0;
+}
+
+static void free_manager(void)
+{
+	dwc_workq_free(manager->wq);
+
+	/* All notifiers must have unregistered themselves before this module
+	 * can be removed.  Hitting this assertion indicates a programmer
+	 * error. */
+	DWC_ASSERT(DWC_CIRCLEQ_EMPTY(&manager->notifiers),
+		   "Notification manager being freed before all notifiers have been removed");
+	dwc_free(manager->mem_ctx, manager);
+}
+
+#ifdef DEBUG
+static void dump_manager(void)
+{
+	notifier_t *n;
+	observer_t *o;
+
+	DWC_ASSERT(manager, "Notification manager not found");
+
+	DWC_DEBUG("List of all notifiers and observers:\n");
+	DWC_CIRCLEQ_FOREACH(n, &manager->notifiers, list_entry) {
+		DWC_DEBUG("Notifier %p has observers:\n", n->object);
+		DWC_CIRCLEQ_FOREACH(o, &n->observers, list_entry) {
+			DWC_DEBUG("    %p watching %s\n", o->observer, o->notification);
+		}
+	}
+}
+#else
+#define dump_manager(...)
+#endif
+
+static observer_t *alloc_observer(void *mem_ctx, void *observer, char *notification,
+				  dwc_notifier_callback_t callback, void *data)
+{
+	observer_t *new_observer = dwc_alloc(mem_ctx, sizeof(observer_t));
+
+	if (!new_observer) {
+		return NULL;
+	}
+
+	DWC_CIRCLEQ_INIT_ENTRY(new_observer, list_entry);
+	new_observer->observer = observer;
+	new_observer->notification = notification;
+	new_observer->callback = callback;
+	new_observer->data = data;
+	return new_observer;
+}
+
+static void free_observer(void *mem_ctx, observer_t *observer)
+{
+	dwc_free(mem_ctx, observer);
+}
+
+static notifier_t *alloc_notifier(void *mem_ctx, void *object)
+{
+	notifier_t *notifier;
+
+	if (!object) {
+		return NULL;
+	}
+
+	notifier = dwc_alloc(mem_ctx, sizeof(notifier_t));
+	if (!notifier) {
+		return NULL;
+	}
+
+	DWC_CIRCLEQ_INIT(&notifier->observers);
+	DWC_CIRCLEQ_INIT_ENTRY(notifier, list_entry);
+
+	notifier->mem_ctx = mem_ctx;
+	notifier->object = object;
+	return notifier;
+}
+
+static void free_notifier(notifier_t *notifier)
+{
+	observer_t *observer;
+
+	DWC_CIRCLEQ_FOREACH(observer, &notifier->observers, list_entry) {
+		free_observer(notifier->mem_ctx, observer);
+	}
+
+	dwc_free(notifier->mem_ctx, notifier);
+}
+
+static notifier_t *find_notifier(void *object)
+{
+	notifier_t *notifier;
+
+	DWC_ASSERT(manager, "Notification manager not found");
+
+	if (!object) {
+		return NULL;
+	}
+
+	DWC_CIRCLEQ_FOREACH(notifier, &manager->notifiers, list_entry) {
+		if (notifier->object == object) {
+			return notifier;
+		}
+	}
+
+	return NULL;
+}
+
+int dwc_alloc_notification_manager(void *mem_ctx, void *wkq_ctx)
+{
+	return create_manager(mem_ctx, wkq_ctx);
+}
+
+void dwc_free_notification_manager(void)
+{
+	free_manager();
+}
+
+dwc_notifier_t *dwc_register_notifier(void *mem_ctx, void *object)
+{
+	notifier_t *notifier;
+
+	DWC_ASSERT(manager, "Notification manager not found");
+
+	notifier = find_notifier(object);
+	if (notifier) {
+		DWC_ERROR("Notifier %p is already registered\n", object);
+		return NULL;
+	}
+
+	notifier = alloc_notifier(mem_ctx, object);
+	if (!notifier) {
+		return NULL;
+	}
+
+	DWC_CIRCLEQ_INSERT_TAIL(&manager->notifiers, notifier, list_entry);
+
+	DWC_INFO("Notifier %p registered", object);
+	dump_manager();
+
+	return notifier;
+}
+
+void dwc_unregister_notifier(dwc_notifier_t *notifier)
+{
+	DWC_ASSERT(manager, "Notification manager not found");
+
+	if (!DWC_CIRCLEQ_EMPTY(&notifier->observers)) {
+		observer_t *o;
+
+		DWC_ERROR("Notifier %p has active observers when removing\n", notifier->object);
+		DWC_CIRCLEQ_FOREACH(o, &notifier->observers, list_entry) {
+			DWC_DEBUG("    %p watching %s\n", o->observer, o->notification);
+		}
+
+		DWC_ASSERT(DWC_CIRCLEQ_EMPTY(&notifier->observers),
+			   "Notifier %p has active observers when removing", notifier);
+	}
+
+	DWC_CIRCLEQ_REMOVE_INIT(&manager->notifiers, notifier, list_entry);
+	free_notifier(notifier);
+
+	DWC_INFO("Notifier unregistered");
+	dump_manager();
+}
+
+/* Add an observer to observe the notifier for a particular state, event, or notification. */
+int dwc_add_observer(void *observer, void *object, char *notification,
+		     dwc_notifier_callback_t callback, void *data)
+{
+	notifier_t *notifier = find_notifier(object);
+	observer_t *new_observer;
+
+	if (!notifier) {
+		DWC_ERROR("Notifier %p is not found when adding observer\n", object);
+		return -DWC_E_INVALID;
+	}
+
+	new_observer = alloc_observer(notifier->mem_ctx, observer, notification, callback, data);
+	if (!new_observer) {
+		return -DWC_E_NO_MEMORY;
+	}
+
+	DWC_CIRCLEQ_INSERT_TAIL(&notifier->observers, new_observer, list_entry);
+
+	DWC_INFO("Added observer %p to notifier %p observing notification %s, callback=%p, data=%p",
+		 observer, object, notification, callback, data);
+
+	dump_manager();
+	return 0;
+}
+
+int dwc_remove_observer(void *observer)
+{
+	notifier_t *n;
+
+	DWC_ASSERT(manager, "Notification manager not found");
+
+	DWC_CIRCLEQ_FOREACH(n, &manager->notifiers, list_entry) {
+		observer_t *o;
+		observer_t *o2;
+
+		DWC_CIRCLEQ_FOREACH_SAFE(o, o2, &n->observers, list_entry) {
+			if (o->observer == observer) {
+				DWC_CIRCLEQ_REMOVE_INIT(&n->observers, o, list_entry);
+				DWC_INFO("Removing observer %p from notifier %p watching notification %s:",
+					 o->observer, n->object, o->notification);
+				free_observer(n->mem_ctx, o);
+			}
+		}
+	}
+
+	dump_manager();
+	return 0;
+}
+
+typedef struct callback_data {
+	void *mem_ctx;
+	dwc_notifier_callback_t cb;
+	void *observer;
+	void *data;
+	void *object;
+	char *notification;
+	void *notification_data;
+} cb_data_t;
+
+static void cb_task(void *data)
+{
+	cb_data_t *cb = (cb_data_t *)data;
+
+	cb->cb(cb->object, cb->notification, cb->observer, cb->notification_data, cb->data);
+	dwc_free(cb->mem_ctx, cb);
+}
+
+void dwc_notify(dwc_notifier_t *notifier, char *notification, void *notification_data)
+{
+	observer_t *o;
+
+	DWC_ASSERT(manager, "Notification manager not found");
+
+	DWC_CIRCLEQ_FOREACH(o, &notifier->observers, list_entry) {
+		int len = DWC_STRLEN(notification);
+
+		if (DWC_STRLEN(o->notification) != len) {
+			continue;
+		}
+
+		if (DWC_STRNCMP(o->notification, notification, len) == 0) {
+			cb_data_t *cb_data = dwc_alloc(notifier->mem_ctx, sizeof(cb_data_t));
+
+			if (!cb_data) {
+				DWC_ERROR("Failed to allocate callback data\n");
+				return;
+			}
+
+			cb_data->mem_ctx = notifier->mem_ctx;
+			cb_data->cb = o->callback;
+			cb_data->observer = o->observer;
+			cb_data->data = o->data;
+			cb_data->object = notifier->object;
+			cb_data->notification = notification;
+			cb_data->notification_data = notification_data;
+			DWC_DEBUG("Observer found %p for notification %s\n", o->observer, notification);
+			DWC_WORKQ_SCHEDULE(manager->wq, cb_task, cb_data,
+					   "Notify callback from %p for Notification %s, to observer %p",
+					   cb_data->object, notification, cb_data->observer);
+		}
+	}
+}
+
+#endif	/* DWC_NOTIFYLIB */
diff --git a/drivers/usb/dwc_otg/dwc_notifier.h b/drivers/usb/dwc_otg/dwc_notifier.h
new file mode 100644
index 0000000..4a8cdfe
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_notifier.h
@@ -0,0 +1,122 @@
+
+#ifndef __DWC_NOTIFIER_H__
+#define __DWC_NOTIFIER_H__
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#include "dwc_os.h"
+
+/** @file
+ *
+ * A simple implementation of the Observer pattern.  Any "module" can
+ * register as an observer or notifier.  The notion of "module" is abstract and
+ * can mean anything used to identify either an observer or notifier.  Usually
+ * it will be a pointer to a data structure which contains some state, ie an
+ * object.
+ *
+ * Before any notifiers can be added, the global notification manager must be
+ * brought up with dwc_alloc_notification_manager().
+ * dwc_free_notification_manager() will bring it down and free all resources.
+ * These would typically be called upon module load and unload.  The
+ * notification manager is a single global instance that handles all registered
+ * observable modules and observers so this should be done only once.
+ *
+ * A module can be observable by using Notifications to publicize some general
+ * information about it's state or operation.  It does not care who listens, or
+ * even if anyone listens, or what they do with the information.  The observable
+ * modules do not need to know any information about it's observers or their
+ * interface, or their state or data.
+ *
+ * Any module can register to emit Notifications.  It should publish a list of
+ * notifications that it can emit and their behavior, such as when they will get
+ * triggered, and what information will be provided to the observer.  Then it
+ * should register itself as an observable module. See dwc_register_notifier().
+ *
+ * Any module can observe any observable, registered module, provided it has a
+ * handle to the other module and knows what notifications to observe.  See
+ * dwc_add_observer().
+ *
+ * A function of type dwc_notifier_callback_t is called whenever a notification
+ * is triggered with one or more observers observing it.  This function is
+ * called in it's own process so it may sleep or block if needed.  It is
+ * guaranteed to be called sometime after the notification has occurred and will
+ * be called once per each time the notification is triggered.  It will NOT be
+ * called in the same process context used to trigger the notification.
+ *
+ * @section Limitiations
+ *
+ * Keep in mind that Notifications that can be triggered in rapid sucession may
+ * schedule too many processes too handle.  Be aware of this limitation when
+ * designing to use notifications, and only add notifications for appropriate
+ * observable information.
+ *
+ * Also Notification callbacks are not synchronous.  If you need to synchronize
+ * the behavior between module/observer you must use other means.  And perhaps
+ * that will mean Notifications are not the proper solution.
+ */
+
+struct dwc_notifier;
+typedef struct dwc_notifier dwc_notifier_t;
+
+/** The callback function must be of this type.
+ *
+ * @param object This is the object that is being observed.
+ * @param notification This is the notification that was triggered.
+ * @param observer This is the observer
+ * @param notification_data This is notification-specific data that the notifier
+ * has included in this notification.  The value of this should be published in
+ * the documentation of the observable module with the notifications.
+ * @param user_data This is any custom data that the observer provided when
+ * adding itself as an observer to the notification. */
+typedef void (*dwc_notifier_callback_t)(void *object, char *notification, void *observer,
+					void *notification_data, void *user_data);
+
+/** Brings up the notification manager. */
+extern int dwc_alloc_notification_manager(void *mem_ctx, void *wkq_ctx);
+/** Brings down the notification manager. */
+extern void dwc_free_notification_manager(void);
+
+/** This function registers an observable module.  A dwc_notifier_t object is
+ * returned to the observable module.  This is an opaque object that is used by
+ * the observable module to trigger notifications.  This object should only be
+ * accessible to functions that are authorized to trigger notifications for this
+ * module.  Observers do not need this object. */
+extern dwc_notifier_t *dwc_register_notifier(void *mem_ctx, void *object);
+
+/** This function unregisters an observable module.  All observers have to be
+ * removed prior to unregistration. */
+extern void dwc_unregister_notifier(dwc_notifier_t *notifier);
+
+/** Add a module as an observer to the observable module.  The observable module
+ * needs to have previously registered with the notification manager.
+ *
+ * @param observer The observer module
+ * @param object The module to observe
+ * @param notification The notification to observe
+ * @param callback The callback function to call
+ * @param user_data Any additional user data to pass into the callback function */
+extern int dwc_add_observer(void *observer, void *object, char *notification,
+			    dwc_notifier_callback_t callback, void *user_data);
+
+/** Removes the specified observer from all notifications that it is currently
+ * observing. */
+extern int dwc_remove_observer(void *observer);
+
+/** This function triggers a Notification.  It should be called by the
+ * observable module, or any module or library which the observable module
+ * allows to trigger notification on it's behalf.  Such as the dwc_cc_t.
+ *
+ * dwc_notify is a non-blocking function.  Callbacks are scheduled called in
+ * their own process context for each trigger.  Callbacks can be blocking.
+ * dwc_notify can be called from interrupt context if needed.
+ *
+ */
+void dwc_notify(dwc_notifier_t *notifier, char *notification, void *notification_data);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* __DWC_NOTIFIER_H__ */
diff --git a/drivers/usb/dwc_otg/dwc_os.h b/drivers/usb/dwc_otg/dwc_os.h
new file mode 100644
index 0000000..fa9d5c6
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_os.h
@@ -0,0 +1,1237 @@
+/* =========================================================================
+ * $File: //dwh/usb_iip/dev/software/dwc_common_port_2/dwc_os.h $
+ * $Revision: #14 $
+ * $Date: 2010/11/04 $
+ * $Change: 1621695 $
+ *
+ * Synopsys Portability Library Software and documentation
+ * (hereinafter, "Software") is an Unsupported proprietary work of
+ * Synopsys, Inc. unless otherwise expressly agreed to in writing
+ * between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product
+ * under any End User Software License Agreement or Agreement for
+ * Licensed Product with Synopsys or any supplement thereto. You are
+ * permitted to use and redistribute this Software in source and binary
+ * forms, with or without modification, provided that redistributions
+ * of source code must retain this notice. You may not view, use,
+ * disclose, copy or distribute this file or any information contained
+ * herein except pursuant to this license grant from Synopsys. If you
+ * do not agree with this notice, including the disclaimer below, then
+ * you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS"
+ * BASIS AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+ * FOR A PARTICULAR PURPOSE ARE HEREBY DISCLAIMED. IN NO EVENT SHALL
+ * SYNOPSYS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
+ * USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================= */
+#ifndef _DWC_OS_H_
+#define _DWC_OS_H_
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/** @file
+ *
+ * DWC portability library, low level os-wrapper functions
+ *
+ */
+
+/* These basic types need to be defined by some OS header file or custom header
+ * file for your specific target architecture.
+ *
+ * uint8_t, int8_t, uint16_t, int16_t, uint32_t, int32_t, uint64_t, int64_t
+ *
+ * Any custom or alternate header file must be added and enabled here.
+ */
+
+#ifdef DWC_LINUX
+# include <linux/types.h>
+# ifdef CONFIG_DEBUG_MUTEXES
+#  include <linux/mutex.h>
+# endif
+# include <linux/errno.h>
+# include <stdarg.h>
+#endif
+
+#if defined(DWC_FREEBSD) || defined(DWC_NETBSD)
+# include <os_dep.h>
+#endif
+
+
+/** @name Primitive Types and Values */
+
+/** We define a boolean type for consistency.  Can be either YES or NO */
+typedef uint8_t dwc_bool_t;
+#define YES  1
+#define NO   0
+
+#ifdef DWC_LINUX
+
+/** @name Error Codes */
+#define DWC_E_INVALID		EINVAL
+#define DWC_E_NO_MEMORY		ENOMEM
+#define DWC_E_NO_DEVICE		ENODEV
+#define DWC_E_NOT_SUPPORTED	EOPNOTSUPP
+#define DWC_E_TIMEOUT		ETIMEDOUT
+#define DWC_E_BUSY		EBUSY
+#define DWC_E_AGAIN		EAGAIN
+#define DWC_E_RESTART		ERESTART
+#define DWC_E_ABORT		ECONNABORTED
+#define DWC_E_SHUTDOWN		ESHUTDOWN
+#define DWC_E_NO_DATA		ENODATA
+#define DWC_E_DISCONNECT	ECONNRESET
+#define DWC_E_UNKNOWN		EINVAL
+#define DWC_E_NO_STREAM_RES	ENOSR
+#define DWC_E_COMMUNICATION	ECOMM
+#define DWC_E_OVERFLOW		EOVERFLOW
+#define DWC_E_PROTOCOL		EPROTO
+#define DWC_E_IN_PROGRESS	EINPROGRESS
+#define DWC_E_PIPE		EPIPE
+#define DWC_E_IO		EIO
+#define DWC_E_NO_SPACE		ENOSPC
+
+#else
+
+/** @name Error Codes */
+#define DWC_E_INVALID		1001
+#define DWC_E_NO_MEMORY		1002
+#define DWC_E_NO_DEVICE		1003
+#define DWC_E_NOT_SUPPORTED	1004
+#define DWC_E_TIMEOUT		1005
+#define DWC_E_BUSY		1006
+#define DWC_E_AGAIN		1007
+#define DWC_E_RESTART		1008
+#define DWC_E_ABORT		1009
+#define DWC_E_SHUTDOWN		1010
+#define DWC_E_NO_DATA		1011
+#define DWC_E_DISCONNECT	2000
+#define DWC_E_UNKNOWN		3000
+#define DWC_E_NO_STREAM_RES	4001
+#define DWC_E_COMMUNICATION	4002
+#define DWC_E_OVERFLOW		4003
+#define DWC_E_PROTOCOL		4004
+#define DWC_E_IN_PROGRESS	4005
+#define DWC_E_PIPE		4006
+#define DWC_E_IO		4007
+#define DWC_E_NO_SPACE		4008
+
+#endif
+
+
+/** @name Tracing/Logging Functions
+ *
+ * These function provide the capability to add tracing, debugging, and error
+ * messages, as well exceptions as assertions.  The WUDEV uses these
+ * extensively.  These could be logged to the main console, the serial port, an
+ * internal buffer, etc.  These functions could also be no-op if they are too
+ * expensive on your system.  By default undefining the DEBUG macro already
+ * no-ops some of these functions. */
+
+/** Returns non-zero if in interrupt context. */
+extern dwc_bool_t DWC_IN_IRQ(void);
+#define dwc_in_irq DWC_IN_IRQ
+
+/** Returns "IRQ" if DWC_IN_IRQ is true. */
+static inline char *dwc_irq(void) {
+	return DWC_IN_IRQ() ? "IRQ" : "";
+}
+
+/** Returns non-zero if in bottom-half context. */
+extern dwc_bool_t DWC_IN_BH(void);
+#define dwc_in_bh DWC_IN_BH
+
+/** Returns "BH" if DWC_IN_BH is true. */
+static inline char *dwc_bh(void) {
+	return DWC_IN_BH() ? "BH" : "";
+}
+
+/**
+ * A vprintf() clone.  Just call vprintf if you've got it.
+ */
+extern void DWC_VPRINTF(char *format, va_list args);
+#define dwc_vprintf DWC_VPRINTF
+
+/**
+ * A vsnprintf() clone.  Just call vprintf if you've got it.
+ */
+extern int DWC_VSNPRINTF(char *str, int size, char *format, va_list args);
+#define dwc_vsnprintf DWC_VSNPRINTF
+
+/**
+ * printf() clone.  Just call printf if you've go it.
+ */
+extern void DWC_PRINTF(char *format, ...)
+/* This provides compiler level static checking of the parameters if you're
+ * using GCC. */
+#ifdef __GNUC__
+	__attribute__ ((format(printf, 1, 2)));
+#else
+	;
+#endif
+#define dwc_printf DWC_PRINTF
+
+/**
+ * sprintf() clone.  Just call sprintf if you've got it.
+ */
+extern int DWC_SPRINTF(char *string, char *format, ...)
+#ifdef __GNUC__
+	__attribute__ ((format(printf, 2, 3)));
+#else
+	;
+#endif
+#define dwc_sprintf DWC_SPRINTF
+
+/**
+ * snprintf() clone.  Just call snprintf if you've got it.
+ */
+extern int DWC_SNPRINTF(char *string, int size, char *format, ...)
+#ifdef __GNUC__
+	__attribute__ ((format(printf, 3, 4)));
+#else
+	;
+#endif
+#define dwc_snprintf DWC_SNPRINTF
+
+/**
+ * Prints a WARNING message.  On systems that don't differentiate between
+ * warnings and regular log messages, just print it.  Indicates that something
+ * may be wrong with the driver.  Works like printf().
+ *
+ * Use the DWC_WARN macro to call this function.
+ */
+extern void __DWC_WARN(char *format, ...)
+#ifdef __GNUC__
+	__attribute__ ((format(printf, 1, 2)));
+#else
+	;
+#endif
+
+/**
+ * Prints an error message.  On systems that don't differentiate between errors
+ * and regular log messages, just print it.  Indicates that something went wrong
+ * with the driver.  Works like printf().
+ *
+ * Use the DWC_ERROR macro to call this function.
+ */
+extern void __DWC_ERROR(char *format, ...)
+#ifdef __GNUC__
+	__attribute__ ((format(printf, 1, 2)));
+#else
+	;
+#endif
+
+/**
+ * Prints an exception error message and takes some user-defined action such as
+ * print out a backtrace or trigger a breakpoint.  Indicates that something went
+ * abnormally wrong with the driver such as programmer error, or other
+ * exceptional condition.  It should not be ignored so even on systems without
+ * printing capability, some action should be taken to notify the developer of
+ * it.  Works like printf().
+ */
+extern void DWC_EXCEPTION(char *format, ...)
+#ifdef __GNUC__
+	__attribute__ ((format(printf, 1, 2)));
+#else
+	;
+#endif
+#define dwc_exception DWC_EXCEPTION
+
+#ifdef DEBUG
+/**
+ * Prints out a debug message.  Used for logging/trace messages.
+ *
+ * Use the DWC_DEBUG macro to call this function
+ */
+extern void __DWC_DEBUG(char *format, ...)
+#ifdef __GNUC__
+	__attribute__ ((format(printf, 1, 2)));
+#else
+	;
+#endif
+#else
+#define __DWC_DEBUG(...)
+#endif
+
+/**
+ * Prints out a Debug message.
+ */
+#define DWC_DEBUG(_format, _args...) __DWC_DEBUG("DEBUG:%s:%s: " _format "\n", \
+						 __func__, dwc_irq(), ## _args)
+#define dwc_debug DWC_DEBUG
+/**
+ * Prints out an informative message.
+ */
+#define DWC_INFO(_format, _args...) DWC_PRINTF("INFO:%s: " _format "\n", \
+					       dwc_irq(), ## _args)
+#define dwc_info DWC_INFO
+/**
+ * Prints out a warning message.
+ */
+#define DWC_WARN(_format, _args...) __DWC_WARN("WARN:%s:%s:%d: " _format "\n", \
+					dwc_irq(), __func__, __LINE__, ## _args)
+#define dwc_warn DWC_WARN
+/**
+ * Prints out an error message.
+ */
+#define DWC_ERROR(_format, _args...) __DWC_ERROR("ERROR:%s:%s:%d: " _format "\n", \
+					dwc_irq(), __func__, __LINE__, ## _args)
+#define dwc_error DWC_ERROR
+
+#define DWC_PROTO_ERROR(_format, _args...) __DWC_WARN("ERROR:%s:%s:%d: " _format "\n", \
+						dwc_irq(), __func__, __LINE__, ## _args)
+#define dwc_proto_error DWC_PROTO_ERROR
+
+#ifdef DEBUG
+/** Prints out a exception error message if the _expr expression fails.  Disabled
+ * if DEBUG is not enabled. */
+#define DWC_ASSERT(_expr, _format, _args...) do { \
+	if (!(_expr)) { DWC_EXCEPTION("%s:%s:%d: " _format "\n", dwc_irq(), \
+				      __FILE__, __LINE__, ## _args); } \
+	} while (0)
+#else
+#define DWC_ASSERT(_x...)
+#endif
+#define dwc_assert DWC_ASSERT
+
+
+/** @name Byte Ordering
+ * The following functions are for conversions between processor's byte ordering
+ * and specific ordering you want.
+ */
+
+/** Converts 32 bit data in CPU byte ordering to little endian. */
+extern uint32_t DWC_CPU_TO_LE32(uint32_t *p);
+#define dwc_cpu_to_le32 DWC_CPU_TO_LE32
+
+/** Converts 32 bit data in CPU byte orderint to big endian. */
+extern uint32_t DWC_CPU_TO_BE32(uint32_t *p);
+#define dwc_cpu_to_be32 DWC_CPU_TO_BE32
+
+/** Converts 32 bit little endian data to CPU byte ordering. */
+extern uint32_t DWC_LE32_TO_CPU(uint32_t *p);
+#define dwc_le32_to_cpu DWC_LE32_TO_CPU
+
+/** Converts 32 bit big endian data to CPU byte ordering. */
+extern uint32_t DWC_BE32_TO_CPU(uint32_t *p);
+#define dwc_be32_to_cpu DWC_BE32_TO_CPU
+
+/** Converts 16 bit data in CPU byte ordering to little endian. */
+extern uint16_t DWC_CPU_TO_LE16(uint16_t *p);
+#define dwc_cpu_to_le16 DWC_CPU_TO_LE16
+
+/** Converts 16 bit data in CPU byte orderint to big endian. */
+extern uint16_t DWC_CPU_TO_BE16(uint16_t *p);
+#define dwc_cpu_to_be16 DWC_CPU_TO_BE16
+
+/** Converts 16 bit little endian data to CPU byte ordering. */
+extern uint16_t DWC_LE16_TO_CPU(uint16_t *p);
+#define dwc_le16_to_cpu DWC_LE16_TO_CPU
+
+/** Converts 16 bit bi endian data to CPU byte ordering. */
+extern uint16_t DWC_BE16_TO_CPU(uint16_t *p);
+#define dwc_be16_to_cpu DWC_BE16_TO_CPU
+
+
+/** @name Register Read/Write
+ *
+ * The following six functions should be implemented to read/write registers of
+ * 32-bit and 64-bit sizes.  All modules use this to read/write register values.
+ * The reg value is a pointer to the register calculated from the void *base
+ * variable passed into the driver when it is started.  */
+
+#ifdef DWC_LINUX
+/* Linux doesn't need any extra parameters for register read/write, so we
+ * just throw away the IO context parameter.
+ */
+/** Reads the content of a 32-bit register. */
+extern uint32_t DWC_READ_REG32(uint32_t volatile *reg);
+#define dwc_read_reg32(_ctx_,_reg_) DWC_READ_REG32(_reg_)
+
+/** Reads the content of a 64-bit register. */
+extern uint64_t DWC_READ_REG64(uint64_t volatile *reg);
+#define dwc_read_reg64(_ctx_,_reg_) DWC_READ_REG64(_reg_)
+
+/** Writes to a 32-bit register. */
+extern void DWC_WRITE_REG32(uint32_t volatile *reg, uint32_t value);
+#define dwc_write_reg32(_ctx_,_reg_,_val_) DWC_WRITE_REG32(_reg_, _val_)
+
+/** Writes to a 64-bit register. */
+extern void DWC_WRITE_REG64(uint64_t volatile *reg, uint64_t value);
+#define dwc_write_reg64(_ctx_,_reg_,_val_) DWC_WRITE_REG64(_reg_, _val_)
+
+/**
+ * Modify bit values in a register.  Using the
+ * algorithm: (reg_contents & ~clear_mask) | set_mask.
+ */
+extern void DWC_MODIFY_REG32(uint32_t volatile *reg, uint32_t clear_mask, uint32_t set_mask);
+#define dwc_modify_reg32(_ctx_,_reg_,_cmsk_,_smsk_) DWC_MODIFY_REG32(_reg_,_cmsk_,_smsk_)
+extern void DWC_MODIFY_REG64(uint64_t volatile *reg, uint64_t clear_mask, uint64_t set_mask);
+#define dwc_modify_reg64(_ctx_,_reg_,_cmsk_,_smsk_) DWC_MODIFY_REG64(_reg_,_cmsk_,_smsk_)
+
+#endif	/* DWC_LINUX */
+
+#if defined(DWC_FREEBSD) || defined(DWC_NETBSD)
+typedef struct dwc_ioctx {
+	struct device *dev;
+	bus_space_tag_t iot;
+	bus_space_handle_t ioh;
+} dwc_ioctx_t;
+
+/** BSD needs two extra parameters for register read/write, so we pass
+ * them in using the IO context parameter.
+ */
+/** Reads the content of a 32-bit register. */
+extern uint32_t DWC_READ_REG32(void *io_ctx, uint32_t volatile *reg);
+#define dwc_read_reg32 DWC_READ_REG32
+
+/** Reads the content of a 64-bit register. */
+extern uint64_t DWC_READ_REG64(void *io_ctx, uint64_t volatile *reg);
+#define dwc_read_reg64 DWC_READ_REG64
+
+/** Writes to a 32-bit register. */
+extern void DWC_WRITE_REG32(void *io_ctx, uint32_t volatile *reg, uint32_t value);
+#define dwc_write_reg32 DWC_WRITE_REG32
+
+/** Writes to a 64-bit register. */
+extern void DWC_WRITE_REG64(void *io_ctx, uint64_t volatile *reg, uint64_t value);
+#define dwc_write_reg64 DWC_WRITE_REG64
+
+/**
+ * Modify bit values in a register.  Using the
+ * algorithm: (reg_contents & ~clear_mask) | set_mask.
+ */
+extern void DWC_MODIFY_REG32(void *io_ctx, uint32_t volatile *reg, uint32_t clear_mask, uint32_t set_mask);
+#define dwc_modify_reg32 DWC_MODIFY_REG32
+extern void DWC_MODIFY_REG64(void *io_ctx, uint64_t volatile *reg, uint64_t clear_mask, uint64_t set_mask);
+#define dwc_modify_reg64 DWC_MODIFY_REG64
+
+#endif	/* DWC_FREEBSD || DWC_NETBSD */
+
+/** @cond */
+
+/** @name Some convenience MACROS used internally.  Define DWC_DEBUG_REGS to log the
+ * register writes. */
+
+#ifdef DWC_LINUX
+
+# ifdef DWC_DEBUG_REGS
+
+#define dwc_define_read_write_reg_n(_reg,_container_type) \
+static inline uint32_t dwc_read_##_reg##_n(_container_type *container, int num) { \
+	return DWC_READ_REG32(&container->regs->_reg[num]); \
+} \
+static inline void dwc_write_##_reg##_n(_container_type *container, int num, uint32_t data) { \
+	DWC_DEBUG("WRITING %8s[%d]: %p: %08x", #_reg, num, \
+		  &(((uint32_t*)container->regs->_reg)[num]), data); \
+	DWC_WRITE_REG32(&(((uint32_t*)container->regs->_reg)[num]), data); \
+}
+
+#define dwc_define_read_write_reg(_reg,_container_type) \
+static inline uint32_t dwc_read_##_reg(_container_type *container) { \
+	return DWC_READ_REG32(&container->regs->_reg); \
+} \
+static inline void dwc_write_##_reg(_container_type *container, uint32_t data) { \
+	DWC_DEBUG("WRITING %11s: %p: %08x", #_reg, &container->regs->_reg, data); \
+	DWC_WRITE_REG32(&container->regs->_reg, data); \
+}
+
+# else	/* DWC_DEBUG_REGS */
+
+#define dwc_define_read_write_reg_n(_reg,_container_type) \
+static inline uint32_t dwc_read_##_reg##_n(_container_type *container, int num) { \
+	return DWC_READ_REG32(&container->regs->_reg[num]); \
+} \
+static inline void dwc_write_##_reg##_n(_container_type *container, int num, uint32_t data) { \
+	DWC_WRITE_REG32(&(((uint32_t*)container->regs->_reg)[num]), data); \
+}
+
+#define dwc_define_read_write_reg(_reg,_container_type) \
+static inline uint32_t dwc_read_##_reg(_container_type *container) { \
+	return DWC_READ_REG32(&container->regs->_reg); \
+} \
+static inline void dwc_write_##_reg(_container_type *container, uint32_t data) { \
+	DWC_WRITE_REG32(&container->regs->_reg, data); \
+}
+
+# endif	/* DWC_DEBUG_REGS */
+
+#endif	/* DWC_LINUX */
+
+#if defined(DWC_FREEBSD) || defined(DWC_NETBSD)
+
+# ifdef DWC_DEBUG_REGS
+
+#define dwc_define_read_write_reg_n(_reg,_container_type) \
+static inline uint32_t dwc_read_##_reg##_n(void *io_ctx, _container_type *container, int num) { \
+	return DWC_READ_REG32(io_ctx, &container->regs->_reg[num]); \
+} \
+static inline void dwc_write_##_reg##_n(void *io_ctx, _container_type *container, int num, uint32_t data) { \
+	DWC_DEBUG("WRITING %8s[%d]: %p: %08x", #_reg, num, \
+		  &(((uint32_t*)container->regs->_reg)[num]), data); \
+	DWC_WRITE_REG32(io_ctx, &(((uint32_t*)container->regs->_reg)[num]), data); \
+}
+
+#define dwc_define_read_write_reg(_reg,_container_type) \
+static inline uint32_t dwc_read_##_reg(void *io_ctx, _container_type *container) { \
+	return DWC_READ_REG32(io_ctx, &container->regs->_reg); \
+} \
+static inline void dwc_write_##_reg(void *io_ctx, _container_type *container, uint32_t data) { \
+	DWC_DEBUG("WRITING %11s: %p: %08x", #_reg, &container->regs->_reg, data); \
+	DWC_WRITE_REG32(io_ctx, &container->regs->_reg, data); \
+}
+
+# else	/* DWC_DEBUG_REGS */
+
+#define dwc_define_read_write_reg_n(_reg,_container_type) \
+static inline uint32_t dwc_read_##_reg##_n(void *io_ctx, _container_type *container, int num) { \
+	return DWC_READ_REG32(io_ctx, &container->regs->_reg[num]); \
+} \
+static inline void dwc_write_##_reg##_n(void *io_ctx, _container_type *container, int num, uint32_t data) { \
+	DWC_WRITE_REG32(io_ctx, &(((uint32_t*)container->regs->_reg)[num]), data); \
+}
+
+#define dwc_define_read_write_reg(_reg,_container_type) \
+static inline uint32_t dwc_read_##_reg(void *io_ctx, _container_type *container) { \
+	return DWC_READ_REG32(io_ctx, &container->regs->_reg); \
+} \
+static inline void dwc_write_##_reg(void *io_ctx, _container_type *container, uint32_t data) { \
+	DWC_WRITE_REG32(io_ctx, &container->regs->_reg, data); \
+}
+
+# endif	/* DWC_DEBUG_REGS */
+
+#endif	/* DWC_FREEBSD || DWC_NETBSD */
+
+/** @endcond */
+
+
+#ifdef DWC_CRYPTOLIB
+/** @name Crypto Functions
+ *
+ * These are the low-level cryptographic functions used by the driver. */
+
+/** Perform AES CBC */
+extern int DWC_AES_CBC(uint8_t *message, uint32_t messagelen, uint8_t *key, uint32_t keylen, uint8_t iv[16], uint8_t *out);
+#define dwc_aes_cbc DWC_AES_CBC
+
+/** Fill the provided buffer with random bytes.  These should be cryptographic grade random numbers. */
+extern void DWC_RANDOM_BYTES(uint8_t *buffer, uint32_t length);
+#define dwc_random_bytes DWC_RANDOM_BYTES
+
+/** Perform the SHA-256 hash function */
+extern int DWC_SHA256(uint8_t *message, uint32_t len, uint8_t *out);
+#define dwc_sha256 DWC_SHA256
+
+/** Calculated the HMAC-SHA256 */
+extern int DWC_HMAC_SHA256(uint8_t *message, uint32_t messagelen, uint8_t *key, uint32_t keylen, uint8_t *out);
+#define dwc_hmac_sha256 DWC_HMAC_SHA256
+
+#endif	/* DWC_CRYPTOLIB */
+
+
+/** @name Memory Allocation
+ *
+ * These function provide access to memory allocation.  There are only 2 DMA
+ * functions and 3 Regular memory functions that need to be implemented.  None
+ * of the memory debugging routines need to be implemented.  The allocation
+ * routines all ZERO the contents of the memory.
+ *
+ * Defining DWC_DEBUG_MEMORY turns on memory debugging and statistic gathering.
+ * This checks for memory leaks, keeping track of alloc/free pairs.  It also
+ * keeps track of how much memory the driver is using at any given time. */
+
+#define DWC_PAGE_SIZE 4096
+#define DWC_PAGE_OFFSET(addr) (((uint32_t)addr) & 0xfff)
+#define DWC_PAGE_ALIGNED(addr) ((((uint32_t)addr) & 0xfff) == 0)
+
+#define DWC_INVALID_DMA_ADDR 0x0
+
+#ifdef DWC_LINUX
+/** Type for a DMA address */
+typedef dma_addr_t dwc_dma_t;
+#endif
+
+#if defined(DWC_FREEBSD) || defined(DWC_NETBSD)
+typedef bus_addr_t dwc_dma_t;
+#endif
+
+#ifdef DWC_FREEBSD
+typedef struct dwc_dmactx {
+	struct device *dev;
+	bus_dma_tag_t dma_tag;
+	bus_dmamap_t dma_map;
+	bus_addr_t dma_paddr;
+	void *dma_vaddr;
+} dwc_dmactx_t;
+#endif
+
+#ifdef DWC_NETBSD
+typedef struct dwc_dmactx {
+	struct device *dev;
+	bus_dma_tag_t dma_tag;
+	bus_dmamap_t dma_map;
+	bus_dma_segment_t segs[1];
+	int nsegs;
+	bus_addr_t dma_paddr;
+	void *dma_vaddr;
+} dwc_dmactx_t;
+#endif
+
+/* @todo these functions will be added in the future */
+#if 0
+/**
+ * Creates a DMA pool from which you can allocate DMA buffers.  Buffers
+ * allocated from this pool will be guaranteed to meet the size, alignment, and
+ * boundary requirements specified.
+ *
+ * @param[in] size Specifies the size of the buffers that will be allocated from
+ * this pool.
+ * @param[in] align Specifies the byte alignment requirements of the buffers
+ * allocated from this pool.  Must be a power of 2.
+ * @param[in] boundary Specifies the N-byte boundary that buffers allocated from
+ * this pool must not cross.
+ *
+ * @returns A pointer to an internal opaque structure which is not to be
+ * accessed outside of these library functions.  Use this handle to specify
+ * which pools to allocate/free DMA buffers from and also to destroy the pool,
+ * when you are done with it.
+ */
+extern dwc_pool_t *DWC_DMA_POOL_CREATE(uint32_t size, uint32_t align, uint32_t boundary);
+
+/**
+ * Destroy a DMA pool.  All buffers allocated from that pool must be freed first.
+ */
+extern void DWC_DMA_POOL_DESTROY(dwc_pool_t *pool);
+
+/**
+ * Allocate a buffer from the specified DMA pool and zeros its contents.
+ */
+extern void *DWC_DMA_POOL_ALLOC(dwc_pool_t *pool, uint64_t *dma_addr);
+
+/**
+ * Free a previously allocated buffer from the DMA pool.
+ */
+extern void DWC_DMA_POOL_FREE(dwc_pool_t *pool, void *vaddr, void *daddr);
+#endif
+
+/** Allocates a DMA capable buffer and zeroes its contents. */
+extern void *__DWC_DMA_ALLOC(void *dma_ctx, uint32_t size, dwc_dma_t *dma_addr);
+
+/** Allocates a DMA capable buffer and zeroes its contents in atomic contest */
+extern void *__DWC_DMA_ALLOC_ATOMIC(void *dma_ctx, uint32_t size, dwc_dma_t *dma_addr);
+
+/** Frees a previously allocated buffer. */
+extern void __DWC_DMA_FREE(void *dma_ctx, uint32_t size, void *virt_addr, dwc_dma_t dma_addr);
+
+/** Allocates a block of memory and zeroes its contents. */
+extern void *__DWC_ALLOC(void *mem_ctx, uint32_t size);
+
+/** Allocates a block of memory and zeroes its contents, in an atomic manner
+ * which can be used inside interrupt context.  The size should be sufficiently
+ * small, a few KB at most, such that failures are not likely to occur.  Can just call
+ * __DWC_ALLOC if it is atomic. */
+extern void *__DWC_ALLOC_ATOMIC(void *mem_ctx, uint32_t size);
+
+/** Frees a previously allocated buffer. */
+extern void __DWC_FREE(void *mem_ctx, void *addr);
+
+#ifndef DWC_DEBUG_MEMORY
+
+#define DWC_ALLOC(_size_) __DWC_ALLOC(NULL, _size_)
+#define DWC_ALLOC_ATOMIC(_size_) __DWC_ALLOC_ATOMIC(NULL, _size_)
+#define DWC_FREE(_addr_) __DWC_FREE(NULL, _addr_)
+
+# ifdef DWC_LINUX
+#define DWC_DMA_ALLOC(_size_,_dma_) __DWC_DMA_ALLOC(NULL, _size_, _dma_)
+#define DWC_DMA_ALLOC_ATOMIC(_size_,_dma_) __DWC_DMA_ALLOC_ATOMIC(NULL, _size_,_dma_)
+#define DWC_DMA_FREE(_size_,_virt_,_dma_) __DWC_DMA_FREE(NULL, _size_, _virt_, _dma_)
+# endif
+
+# if defined(DWC_FREEBSD) || defined(DWC_NETBSD)
+#define DWC_DMA_ALLOC __DWC_DMA_ALLOC
+#define DWC_DMA_FREE __DWC_DMA_FREE
+# endif
+
+#else	/* DWC_DEBUG_MEMORY */
+
+extern void *dwc_alloc_debug(void *mem_ctx, uint32_t size, char const *func, int line);
+extern void *dwc_alloc_atomic_debug(void *mem_ctx, uint32_t size, char const *func, int line);
+extern void dwc_free_debug(void *mem_ctx, void *addr, char const *func, int line);
+extern void *dwc_dma_alloc_debug(void *dma_ctx, uint32_t size, dwc_dma_t *dma_addr,
+				 char const *func, int line);
+extern void *dwc_dma_alloc_atomic_debug(void *dma_ctx, uint32_t size, dwc_dma_t *dma_addr, 
+				char const *func, int line);
+extern void dwc_dma_free_debug(void *dma_ctx, uint32_t size, void *virt_addr,
+			       dwc_dma_t dma_addr, char const *func, int line);
+
+extern int dwc_memory_debug_start(void *mem_ctx);
+extern void dwc_memory_debug_stop(void);
+extern void dwc_memory_debug_report(void);
+
+#define DWC_ALLOC(_size_) dwc_alloc_debug(NULL, _size_, __func__, __LINE__)
+#define DWC_ALLOC_ATOMIC(_size_) dwc_alloc_atomic_debug(NULL, _size_, \
+							__func__, __LINE__)
+#define DWC_FREE(_addr_) dwc_free_debug(NULL, _addr_, __func__, __LINE__)
+
+# ifdef DWC_LINUX
+#define DWC_DMA_ALLOC(_size_,_dma_) dwc_dma_alloc_debug(NULL, _size_, \
+						_dma_, __func__, __LINE__)
+#define DWC_DMA_ALLOC_ATOMIC(_size_,_dma_) dwc_dma_alloc_atomic_debug(NULL, _size_, \
+						_dma_, __func__, __LINE__)
+#define DWC_DMA_FREE(_size_,_virt_,_dma_) dwc_dma_free_debug(NULL, _size_, \
+						_virt_, _dma_, __func__, __LINE__)
+# endif
+
+# if defined(DWC_FREEBSD) || defined(DWC_NETBSD)
+#define DWC_DMA_ALLOC(_ctx_,_size_,_dma_) dwc_dma_alloc_debug(_ctx_, _size_, \
+						_dma_, __func__, __LINE__)
+#define DWC_DMA_FREE(_ctx_,_size_,_virt_,_dma_) dwc_dma_free_debug(_ctx_, _size_, \
+						 _virt_, _dma_, __func__, __LINE__)
+# endif
+
+#endif /* DWC_DEBUG_MEMORY */
+
+#define dwc_alloc(_ctx_,_size_) DWC_ALLOC(_size_)
+#define dwc_alloc_atomic(_ctx_,_size_) DWC_ALLOC_ATOMIC(_size_)
+#define dwc_free(_ctx_,_addr_) DWC_FREE(_addr_)
+
+#ifdef DWC_LINUX
+/* Linux doesn't need any extra parameters for DMA buffer allocation, so we
+ * just throw away the DMA context parameter.
+ */
+#define dwc_dma_alloc(_ctx_,_size_,_dma_) DWC_DMA_ALLOC(_size_, _dma_)
+#define dwc_dma_alloc_atomic(_ctx_,_size_,_dma_) DWC_DMA_ALLOC_ATOMIC(_size_, _dma_)
+#define dwc_dma_free(_ctx_,_size_,_virt_,_dma_) DWC_DMA_FREE(_size_, _virt_, _dma_)
+#endif
+
+#if defined(DWC_FREEBSD) || defined(DWC_NETBSD)
+/** BSD needs several extra parameters for DMA buffer allocation, so we pass
+ * them in using the DMA context parameter.
+ */
+#define dwc_dma_alloc DWC_DMA_ALLOC
+#define dwc_dma_free DWC_DMA_FREE
+#endif
+
+
+/** @name Memory and String Processing */
+
+/** memset() clone */
+extern void *DWC_MEMSET(void *dest, uint8_t byte, uint32_t size);
+#define dwc_memset DWC_MEMSET
+
+/** memcpy() clone */
+extern void *DWC_MEMCPY(void *dest, void const *src, uint32_t size);
+#define dwc_memcpy DWC_MEMCPY
+
+/** memmove() clone */
+extern void *DWC_MEMMOVE(void *dest, void *src, uint32_t size);
+#define dwc_memmove DWC_MEMMOVE
+
+/** memcmp() clone */
+extern int DWC_MEMCMP(void *m1, void *m2, uint32_t size);
+#define dwc_memcmp DWC_MEMCMP
+
+/** strcmp() clone */
+extern int DWC_STRCMP(void *s1, void *s2);
+#define dwc_strcmp DWC_STRCMP
+
+/** strncmp() clone */
+extern int DWC_STRNCMP(void *s1, void *s2, uint32_t size);
+#define dwc_strncmp DWC_STRNCMP
+
+/** strlen() clone, for NULL terminated ASCII strings */
+extern int DWC_STRLEN(char const *str);
+#define dwc_strlen DWC_STRLEN
+
+/** strcpy() clone, for NULL terminated ASCII strings */
+extern char *DWC_STRCPY(char *to, const char *from);
+#define dwc_strcpy DWC_STRCPY
+
+/** strdup() clone.  If you wish to use memory allocation debugging, this
+ * implementation of strdup should use the DWC_* memory routines instead of
+ * calling a predefined strdup.  Otherwise the memory allocated by this routine
+ * will not be seen by the debugging routines. */
+extern char *DWC_STRDUP(char const *str);
+#define dwc_strdup(_ctx_,_str_) DWC_STRDUP(_str_)
+
+/** NOT an atoi() clone.  Read the description carefully.  Returns an integer
+ * converted from the string str in base 10 unless the string begins with a "0x"
+ * in which case it is base 16.  String must be a NULL terminated sequence of
+ * ASCII characters and may optionally begin with whitespace, a + or -, and a
+ * "0x" prefix if base 16.  The remaining characters must be valid digits for
+ * the number and end with a NULL character.  If any invalid characters are
+ * encountered or it returns with a negative error code and the results of the
+ * conversion are undefined.  On sucess it returns 0.  Overflow conditions are
+ * undefined.  An example implementation using atoi() can be referenced from the
+ * Linux implementation. */
+extern int DWC_ATOI(const char *str, int32_t *value);
+#define dwc_atoi DWC_ATOI
+
+/** Same as above but for unsigned. */
+extern int DWC_ATOUI(const char *str, uint32_t *value);
+#define dwc_atoui DWC_ATOUI
+
+#ifdef DWC_UTFLIB
+/** This routine returns a UTF16LE unicode encoded string from a UTF8 string. */
+extern int DWC_UTF8_TO_UTF16LE(uint8_t const *utf8string, uint16_t *utf16string, unsigned len);
+#define dwc_utf8_to_utf16le DWC_UTF8_TO_UTF16LE
+#endif
+
+
+/** @name Wait queues
+ *
+ * Wait queues provide a means of synchronizing between threads or processes.  A
+ * process can block on a waitq if some condition is not true, waiting for it to
+ * become true.  When the waitq is triggered all waiting process will get
+ * unblocked and the condition will be check again.  Waitqs should be triggered
+ * every time a condition can potentially change.*/
+struct dwc_waitq;
+
+/** Type for a waitq */
+typedef struct dwc_waitq dwc_waitq_t;
+
+/** The type of waitq condition callback function.  This is called every time
+ * condition is evaluated. */
+typedef int (*dwc_waitq_condition_t)(void *data);
+
+/** Allocate a waitq */
+extern dwc_waitq_t *DWC_WAITQ_ALLOC(void);
+#define dwc_waitq_alloc(_ctx_) DWC_WAITQ_ALLOC()
+
+/** Free a waitq */
+extern void DWC_WAITQ_FREE(dwc_waitq_t *wq);
+#define dwc_waitq_free DWC_WAITQ_FREE
+
+/** Check the condition and if it is false, block on the waitq.  When unblocked, check the
+ * condition again.  The function returns when the condition becomes true.  The return value
+ * is 0 on condition true, DWC_WAITQ_ABORTED on abort or killed, or DWC_WAITQ_UNKNOWN on error. */
+extern int32_t DWC_WAITQ_WAIT(dwc_waitq_t *wq, dwc_waitq_condition_t cond, void *data);
+#define dwc_waitq_wait DWC_WAITQ_WAIT
+
+/** Check the condition and if it is false, block on the waitq.  When unblocked,
+ * check the condition again.  The function returns when the condition become
+ * true or the timeout has passed.  The return value is 0 on condition true or
+ * DWC_TIMED_OUT on timeout, or DWC_WAITQ_ABORTED, or DWC_WAITQ_UNKNOWN on
+ * error. */
+extern int32_t DWC_WAITQ_WAIT_TIMEOUT(dwc_waitq_t *wq, dwc_waitq_condition_t cond,
+				      void *data, int32_t msecs);
+#define dwc_waitq_wait_timeout DWC_WAITQ_WAIT_TIMEOUT
+
+/** Trigger a waitq, unblocking all processes.  This should be called whenever a condition
+ * has potentially changed. */
+extern void DWC_WAITQ_TRIGGER(dwc_waitq_t *wq);
+#define dwc_waitq_trigger DWC_WAITQ_TRIGGER
+
+/** Unblock all processes waiting on the waitq with an ABORTED result. */
+extern void DWC_WAITQ_ABORT(dwc_waitq_t *wq);
+#define dwc_waitq_abort DWC_WAITQ_ABORT
+
+
+/** @name Threads
+ *
+ * A thread must be explicitly stopped.  It must check DWC_THREAD_SHOULD_STOP
+ * whenever it is woken up, and then return.  The DWC_THREAD_STOP function
+ * returns the value from the thread.
+ */
+
+struct dwc_thread;
+
+/** Type for a thread */
+typedef struct dwc_thread dwc_thread_t;
+
+/** The thread function */
+typedef int (*dwc_thread_function_t)(void *data);
+
+/** Create a thread and start it running the thread_function.  Returns a handle
+ * to the thread */
+extern dwc_thread_t *DWC_THREAD_RUN(dwc_thread_function_t func, char *name, void *data);
+#define dwc_thread_run(_ctx_,_func_,_name_,_data_) DWC_THREAD_RUN(_func_, _name_, _data_)
+
+/** Stops a thread.  Return the value returned by the thread.  Or will return
+ * DWC_ABORT if the thread never started. */
+extern int DWC_THREAD_STOP(dwc_thread_t *thread);
+#define dwc_thread_stop DWC_THREAD_STOP
+
+/** Signifies to the thread that it must stop. */
+#ifdef DWC_LINUX
+/* Linux doesn't need any parameters for kthread_should_stop() */
+extern dwc_bool_t DWC_THREAD_SHOULD_STOP(void);
+#define dwc_thread_should_stop(_thrd_) DWC_THREAD_SHOULD_STOP()
+
+/* No thread_exit function in Linux */
+#define dwc_thread_exit(_thrd_)
+#endif
+
+#if defined(DWC_FREEBSD) || defined(DWC_NETBSD)
+/** BSD needs the thread pointer for kthread_suspend_check() */
+extern dwc_bool_t DWC_THREAD_SHOULD_STOP(dwc_thread_t *thread);
+#define dwc_thread_should_stop DWC_THREAD_SHOULD_STOP
+
+/** The thread must call this to exit. */
+extern void DWC_THREAD_EXIT(dwc_thread_t *thread);
+#define dwc_thread_exit DWC_THREAD_EXIT
+#endif
+
+
+/** @name Work queues
+ *
+ * Workqs are used to queue a callback function to be called at some later time,
+ * in another thread. */
+struct dwc_workq;
+
+/** Type for a workq */
+typedef struct dwc_workq dwc_workq_t;
+
+/** The type of the callback function to be called. */
+typedef void (*dwc_work_callback_t)(void *data);
+
+/** Allocate a workq */
+extern dwc_workq_t *DWC_WORKQ_ALLOC(char *name);
+#define dwc_workq_alloc(_ctx_,_name_) DWC_WORKQ_ALLOC(_name_)
+
+/** Free a workq.  All work must be completed before being freed. */
+extern void DWC_WORKQ_FREE(dwc_workq_t *workq);
+#define dwc_workq_free DWC_WORKQ_FREE
+
+/** Schedule a callback on the workq, passing in data.  The function will be
+ * scheduled at some later time. */
+extern void DWC_WORKQ_SCHEDULE(dwc_workq_t *workq, dwc_work_callback_t cb,
+			       void *data, char *format, ...)
+#ifdef __GNUC__
+	__attribute__ ((format(printf, 4, 5)));
+#else
+	;
+#endif
+#define dwc_workq_schedule DWC_WORKQ_SCHEDULE
+
+/** Schedule a callback on the workq, that will be called until at least
+ * given number miliseconds have passed. */
+extern void DWC_WORKQ_SCHEDULE_DELAYED(dwc_workq_t *workq, dwc_work_callback_t cb,
+				       void *data, uint32_t time, char *format, ...)
+#ifdef __GNUC__
+	__attribute__ ((format(printf, 5, 6)));
+#else
+	;
+#endif
+#define dwc_workq_schedule_delayed DWC_WORKQ_SCHEDULE_DELAYED
+
+/** The number of processes in the workq */
+extern int DWC_WORKQ_PENDING(dwc_workq_t *workq);
+#define dwc_workq_pending DWC_WORKQ_PENDING
+
+/** Blocks until all the work in the workq is complete or timed out.  Returns <
+ * 0 on timeout. */
+extern int DWC_WORKQ_WAIT_WORK_DONE(dwc_workq_t *workq, int timeout);
+#define dwc_workq_wait_work_done DWC_WORKQ_WAIT_WORK_DONE
+
+
+/** @name Tasklets
+ *
+ */
+struct dwc_tasklet;
+
+/** Type for a tasklet */
+typedef struct dwc_tasklet dwc_tasklet_t;
+
+/** The type of the callback function to be called */
+typedef void (*dwc_tasklet_callback_t)(void *data);
+
+/** Allocates a tasklet */
+extern dwc_tasklet_t *DWC_TASK_ALLOC(char *name, dwc_tasklet_callback_t cb, void *data);
+#define dwc_task_alloc(_ctx_,_name_,_cb_,_data_) DWC_TASK_ALLOC(_name_, _cb_, _data_)
+
+/** Frees a tasklet */
+extern void DWC_TASK_FREE(dwc_tasklet_t *task);
+#define dwc_task_free DWC_TASK_FREE
+
+/** Schedules a tasklet to run */
+extern void DWC_TASK_SCHEDULE(dwc_tasklet_t *task);
+#define dwc_task_schedule DWC_TASK_SCHEDULE
+
+
+/** @name Timer
+ *
+ * Callbacks must be small and atomic.
+ */
+struct dwc_timer;
+
+/** Type for a timer */
+typedef struct dwc_timer dwc_timer_t;
+
+/** The type of the callback function to be called */
+typedef void (*dwc_timer_callback_t)(void *data);
+
+/** Allocates a timer */
+extern dwc_timer_t *DWC_TIMER_ALLOC(char *name, dwc_timer_callback_t cb, void *data);
+#define dwc_timer_alloc(_ctx_,_name_,_cb_,_data_) DWC_TIMER_ALLOC(_name_,_cb_,_data_)
+
+/** Frees a timer */
+extern void DWC_TIMER_FREE(dwc_timer_t *timer);
+#define dwc_timer_free DWC_TIMER_FREE
+
+/** Schedules the timer to run at time ms from now.  And will repeat at every
+ * repeat_interval msec therafter
+ *
+ * Modifies a timer that is still awaiting execution to a new expiration time.
+ * The mod_time is added to the old time.  */
+extern void DWC_TIMER_SCHEDULE(dwc_timer_t *timer, uint32_t time);
+#define dwc_timer_schedule DWC_TIMER_SCHEDULE
+
+/** Disables the timer from execution. */
+extern void DWC_TIMER_CANCEL(dwc_timer_t *timer);
+#define dwc_timer_cancel DWC_TIMER_CANCEL
+
+
+/** @name Spinlocks
+ *
+ * These locks are used when the work between the lock/unlock is atomic and
+ * short.  Interrupts are also disabled during the lock/unlock and thus they are
+ * suitable to lock between interrupt/non-interrupt context.  They also lock
+ * between processes if you have multiple CPUs or Preemption.  If you don't have
+ * multiple CPUS or Preemption, then the you can simply implement the
+ * DWC_SPINLOCK and DWC_SPINUNLOCK to disable and enable interrupts.  Because
+ * the work between the lock/unlock is atomic, the process context will never
+ * change, and so you never have to lock between processes.  */
+
+struct dwc_spinlock;
+
+/** Type for a spinlock */
+typedef struct dwc_spinlock dwc_spinlock_t;
+
+/** Type for the 'flags' argument to spinlock funtions */
+typedef unsigned long dwc_irqflags_t;
+
+/** Returns an initialized lock variable.  This function should allocate and
+ * initialize the OS-specific data structure used for locking.  This data
+ * structure is to be used for the DWC_LOCK and DWC_UNLOCK functions and should
+ * be freed by the DWC_FREE_LOCK when it is no longer used. */
+extern dwc_spinlock_t *DWC_SPINLOCK_ALLOC(void);
+#define dwc_spinlock_alloc(_ctx_) DWC_SPINLOCK_ALLOC()
+
+/** Frees an initialized lock variable. */
+extern void DWC_SPINLOCK_FREE(dwc_spinlock_t *lock);
+#define dwc_spinlock_free(_ctx_,_lock_) DWC_SPINLOCK_FREE(_lock_)
+
+/** Disables interrupts and blocks until it acquires the lock.
+ *
+ * @param lock Pointer to the spinlock.
+ * @param flags Unsigned long for irq flags storage.
+ */
+extern void DWC_SPINLOCK_IRQSAVE(dwc_spinlock_t *lock, dwc_irqflags_t *flags);
+#define dwc_spinlock_irqsave DWC_SPINLOCK_IRQSAVE
+
+/** Re-enables the interrupt and releases the lock.
+ *
+ * @param lock Pointer to the spinlock.
+ * @param flags Unsigned long for irq flags storage.  Must be the same as was
+ * passed into DWC_LOCK.
+ */
+extern void DWC_SPINUNLOCK_IRQRESTORE(dwc_spinlock_t *lock, dwc_irqflags_t flags);
+#define dwc_spinunlock_irqrestore DWC_SPINUNLOCK_IRQRESTORE
+
+/** Blocks until it acquires the lock.
+ *
+ * @param lock Pointer to the spinlock.
+ */
+extern void DWC_SPINLOCK(dwc_spinlock_t *lock);
+#define dwc_spinlock DWC_SPINLOCK
+
+/** Releases the lock.
+ *
+ * @param lock Pointer to the spinlock.
+ */
+extern void DWC_SPINUNLOCK(dwc_spinlock_t *lock);
+#define dwc_spinunlock DWC_SPINUNLOCK
+
+
+/** @name Mutexes
+ *
+ * Unlike spinlocks Mutexes lock only between processes and the work between the
+ * lock/unlock CAN block, therefore it CANNOT be called from interrupt context.
+ */
+
+struct dwc_mutex;
+
+/** Type for a mutex */
+typedef struct dwc_mutex dwc_mutex_t;
+
+/* For Linux Mutex Debugging make it inline because the debugging routines use
+ * the symbol to determine recursive locking.  This makes it falsely think
+ * recursive locking occurs. */
+#if defined(DWC_LINUX) && defined(CONFIG_DEBUG_MUTEXES)
+#define DWC_MUTEX_ALLOC_LINUX_DEBUG(__mutexp) ({ \
+	__mutexp = (dwc_mutex_t *)DWC_ALLOC(sizeof(struct mutex)); \
+	mutex_init((struct mutex *)__mutexp); \
+})
+#endif
+
+/** Allocate a mutex */
+extern dwc_mutex_t *DWC_MUTEX_ALLOC(void);
+#define dwc_mutex_alloc(_ctx_) DWC_MUTEX_ALLOC()
+
+/* For memory leak debugging when using Linux Mutex Debugging */
+#if defined(DWC_LINUX) && defined(CONFIG_DEBUG_MUTEXES)
+#define DWC_MUTEX_FREE(__mutexp) do { \
+	mutex_destroy((struct mutex *)__mutexp); \
+	DWC_FREE(__mutexp); \
+} while(0)
+#else
+/** Free a mutex */
+extern void DWC_MUTEX_FREE(dwc_mutex_t *mutex);
+#define dwc_mutex_free(_ctx_,_mutex_) DWC_MUTEX_FREE(_mutex_)
+#endif
+
+/** Lock a mutex */
+extern void DWC_MUTEX_LOCK(dwc_mutex_t *mutex);
+#define dwc_mutex_lock DWC_MUTEX_LOCK
+
+/** Non-blocking lock returns 1 on successful lock. */
+extern int DWC_MUTEX_TRYLOCK(dwc_mutex_t *mutex);
+#define dwc_mutex_trylock DWC_MUTEX_TRYLOCK
+
+/** Unlock a mutex */
+extern void DWC_MUTEX_UNLOCK(dwc_mutex_t *mutex);
+#define dwc_mutex_unlock DWC_MUTEX_UNLOCK
+
+
+/** @name Time */
+
+/** Microsecond delay.
+ *
+ * @param usecs  Microseconds to delay.
+ */
+extern void DWC_UDELAY(uint32_t usecs);
+#define dwc_udelay DWC_UDELAY
+
+/** Millisecond delay.
+ *
+ * @param msecs  Milliseconds to delay.
+ */
+extern void DWC_MDELAY(uint32_t msecs);
+#define dwc_mdelay DWC_MDELAY
+
+/** Non-busy waiting.
+ * Sleeps for specified number of milliseconds.
+ *
+ * @param msecs Milliseconds to sleep.
+ */
+extern void DWC_MSLEEP(uint32_t msecs);
+#define dwc_msleep DWC_MSLEEP
+
+/**
+ * Returns number of milliseconds since boot.
+ */
+extern uint32_t DWC_TIME(void);
+#define dwc_time DWC_TIME
+
+
+
+
+/* @mainpage DWC Portability and Common Library
+ *
+ * This is the documentation for the DWC Portability and Common Library.
+ *
+ * @section intro Introduction
+ *
+ * The DWC Portability library consists of wrapper calls and data structures to
+ * all low-level functions which are typically provided by the OS.  The WUDEV
+ * driver uses only these functions.  In order to port the WUDEV driver, only
+ * the functions in this library need to be re-implemented, with the same
+ * behavior as documented here.
+ *
+ * The Common library consists of higher level functions, which rely only on
+ * calling the functions from the DWC Portability library.  These common
+ * routines are shared across modules.  Some of the common libraries need to be
+ * used directly by the driver programmer when porting WUDEV.  Such as the
+ * parameter and notification libraries.
+ *
+ * @section low Portability Library OS Wrapper Functions
+ *
+ * Any function starting with DWC and in all CAPS is a low-level OS-wrapper that
+ * needs to be implemented when porting, for example DWC_MUTEX_ALLOC().  All of
+ * these functions are included in the dwc_os.h file.
+ *
+ * There are many functions here covering a wide array of OS services.  Please
+ * see dwc_os.h for details, and implementation notes for each function.
+ *
+ * @section common Common Library Functions
+ *
+ * Any function starting with dwc and in all lowercase is a common library
+ * routine.  These functions have a portable implementation and do not need to
+ * be reimplemented when porting.  The common routines can be used by any
+ * driver, and some must be used by the end user to control the drivers.  For
+ * example, you must use the Parameter common library in order to set the
+ * parameters in the WUDEV module.
+ *
+ * The common libraries consist of the following:
+ *
+ * - Connection Contexts - Used internally and can be used by end-user.  See dwc_cc.h
+ * - Parameters - Used internally and can be used by end-user.  See dwc_params.h
+ * - Notifications - Used internally and can be used by end-user.  See dwc_notifier.h
+ * - Lists - Used internally and can be used by end-user.  See dwc_list.h
+ * - Memory Debugging - Used internally and can be used by end-user.  See dwc_os.h
+ * - Modpow - Used internally only.  See dwc_modpow.h
+ * - DH - Used internally only.  See dwc_dh.h
+ * - Crypto - Used internally only.  See dwc_crypto.h
+ *
+ *
+ * @section prereq Prerequistes For dwc_os.h
+ * @subsection types Data Types
+ *
+ * The dwc_os.h file assumes that several low-level data types are pre defined for the
+ * compilation environment.  These data types are:
+ *
+ * - uint8_t - unsigned 8-bit data type
+ * - int8_t - signed 8-bit data type
+ * - uint16_t - unsigned 16-bit data type
+ * - int16_t - signed 16-bit data type
+ * - uint32_t - unsigned 32-bit data type
+ * - int32_t - signed 32-bit data type
+ * - uint64_t - unsigned 64-bit data type
+ * - int64_t - signed 64-bit data type
+ *
+ * Ensure that these are defined before using dwc_os.h.  The easiest way to do
+ * that is to modify the top of the file to include the appropriate header.
+ * This is already done for the Linux environment.  If the DWC_LINUX macro is
+ * defined, the correct header will be added.  A standard header <stdint.h> is
+ * also used for environments where standard C headers are available.
+ *
+ * @subsection stdarg Variable Arguments
+ *
+ * Variable arguments are provided by a standard C header <stdarg.h>.  it is
+ * available in Both the Linux and ANSI C enviornment.  An equivalent must be
+ * provided in your enviornment in order to use dwc_os.h with the debug and
+ * tracing message functionality.
+ *
+ * @subsection thread Threading
+ *
+ * WUDEV Core must be run on an operating system that provides for multiple
+ * threads/processes.  Threading can be implemented in many ways, even in
+ * embedded systems without an operating system.  At the bare minimum, the
+ * system should be able to start any number of processes at any time to handle
+ * special work.  It need not be a pre-emptive system.  Process context can
+ * change upon a call to a blocking function.  The hardware interrupt context
+ * that calls the module's ISR() function must be differentiable from process
+ * context, even if your processes are impemented via a hardware interrupt.
+ * Further locking mechanism between process must exist (or be implemented), and
+ * process context must have a way to disable interrupts for a period of time to
+ * lock them out.  If all of this exists, the functions in dwc_os.h related to
+ * threading should be able to be implemented with the defined behavior.
+ *
+ */
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* _DWC_OS_H_ */
diff --git a/drivers/usb/dwc_otg/dwc_otg_adp.c b/drivers/usb/dwc_otg/dwc_otg_adp.c
new file mode 100644
index 0000000..0877472
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_otg_adp.c
@@ -0,0 +1,854 @@
+/* ==========================================================================
+ * $File: //dwh/usb_iip/dev/software/otg/linux/drivers/dwc_otg_adp.c $
+ * $Revision: #12 $
+ * $Date: 2011/10/26 $
+ * $Change: 1873028 $
+ *
+ * Synopsys HS OTG Linux Software Driver and documentation (hereinafter,
+ * "Software") is an Unsupported proprietary work of Synopsys, Inc. unless
+ * otherwise expressly agreed to in writing between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product under
+ * any End User Software License Agreement or Agreement for Licensed Product
+ * with Synopsys or any supplement thereto. You are permitted to use and
+ * redistribute this Software in source and binary forms, with or without
+ * modification, provided that redistributions of source code must retain this
+ * notice. You may not view, use, disclose, copy or distribute this file or
+ * any information contained herein except pursuant to this license grant from
+ * Synopsys. If you do not agree with this notice, including the disclaimer
+ * below, then you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS" BASIS
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE HEREBY DISCLAIMED. IN NO EVENT SHALL SYNOPSYS BE LIABLE FOR ANY DIRECT,
+ * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================== */
+
+#include "dwc_os.h"
+#include "dwc_otg_regs.h"
+#include "dwc_otg_cil.h"
+#include "dwc_otg_adp.h"
+
+/** @file
+ *
+ * This file contains the most of the Attach Detect Protocol implementation for
+ * the driver to support OTG Rev2.0.
+ *
+ */
+
+void dwc_otg_adp_write_reg(dwc_otg_core_if_t * core_if, uint32_t value)
+{
+	adpctl_data_t adpctl;
+
+	adpctl.d32 = value;
+	adpctl.b.ar = 0x2;
+
+	DWC_WRITE_REG32(&core_if->core_global_regs->adpctl, adpctl.d32);
+
+	while (adpctl.b.ar) {
+		adpctl.d32 = DWC_READ_REG32(&core_if->core_global_regs->adpctl);
+	}
+
+}
+
+/**
+ * Function is called to read ADP registers
+ */
+uint32_t dwc_otg_adp_read_reg(dwc_otg_core_if_t * core_if)
+{
+	adpctl_data_t adpctl;
+
+	adpctl.d32 = 0;
+	adpctl.b.ar = 0x1;
+
+	DWC_WRITE_REG32(&core_if->core_global_regs->adpctl, adpctl.d32);
+
+	while (adpctl.b.ar) {
+		adpctl.d32 = DWC_READ_REG32(&core_if->core_global_regs->adpctl);
+	}
+
+	return adpctl.d32;
+}
+
+/**
+ * Function is called to read ADPCTL register and filter Write-clear bits
+ */
+uint32_t dwc_otg_adp_read_reg_filter(dwc_otg_core_if_t * core_if)
+{
+	adpctl_data_t adpctl;
+
+	adpctl.d32 = dwc_otg_adp_read_reg(core_if);
+	adpctl.b.adp_tmout_int = 0;
+	adpctl.b.adp_prb_int = 0;
+	adpctl.b.adp_tmout_int = 0;
+		
+	return adpctl.d32;
+}
+
+/**
+ * Function is called to write ADP registers
+ */
+void dwc_otg_adp_modify_reg(dwc_otg_core_if_t * core_if, uint32_t clr,
+			    uint32_t set)
+{
+	dwc_otg_adp_write_reg(core_if,
+			      (dwc_otg_adp_read_reg(core_if) & (~clr)) | set);
+}
+
+static void adp_sense_timeout(void *ptr)
+{
+	dwc_otg_core_if_t *core_if = (dwc_otg_core_if_t *) ptr;
+	core_if->adp.sense_timer_started = 0;
+	DWC_PRINTF("ADP SENSE TIMEOUT\n");
+	if (core_if->adp_enable) {
+		dwc_otg_adp_sense_stop(core_if);
+		dwc_otg_adp_probe_start(core_if);
+	}
+}
+
+/**
+ * This function is called when the ADP vbus timer expires. Timeout is 1.1s.
+ */
+static void adp_vbuson_timeout(void *ptr)
+{
+	gpwrdn_data_t gpwrdn;
+	dwc_otg_core_if_t *core_if = (dwc_otg_core_if_t *) ptr;
+	hprt0_data_t hprt0 = {.d32 = 0 };
+	pcgcctl_data_t pcgcctl = {.d32 = 0 };
+	DWC_PRINTF("%s: 1.1 seconds expire after turning on VBUS\n",__FUNCTION__);
+	if (core_if) {
+		core_if->adp.vbuson_timer_started = 0;
+		/* Turn off vbus */
+		hprt0.b.prtpwr = 1;
+		DWC_MODIFY_REG32(core_if->host_if->hprt0, hprt0.d32, 0);
+		gpwrdn.d32 = 0;
+
+		/* Power off the core */
+		if (core_if->power_down == 2) {
+			/* Enable Wakeup Logic */
+//                      gpwrdn.b.wkupactiv = 1;
+			gpwrdn.b.pmuactv = 0;
+			gpwrdn.b.pwrdnrstn = 1;
+			gpwrdn.b.pwrdnclmp = 1;
+			DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, 0,
+					 gpwrdn.d32);
+
+			/* Suspend the Phy Clock */
+			pcgcctl.b.stoppclk = 1;
+			DWC_MODIFY_REG32(core_if->pcgcctl, 0, pcgcctl.d32);
+
+			/* Switch on VDD */
+//                      gpwrdn.b.wkupactiv = 1;
+			gpwrdn.b.pmuactv = 1;
+			gpwrdn.b.pwrdnrstn = 1;
+			gpwrdn.b.pwrdnclmp = 1;
+			DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, 0,
+					 gpwrdn.d32);
+		} else {
+			/* Enable Power Down Logic */
+			gpwrdn.b.pmuintsel = 1;
+			gpwrdn.b.pmuactv = 1;
+			DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, 0, gpwrdn.d32);
+		}
+
+		/* Power off the core */
+		if (core_if->power_down == 2) {
+			gpwrdn.d32 = 0;
+			gpwrdn.b.pwrdnswtch = 1;
+			DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn,
+					 gpwrdn.d32, 0);
+		}
+
+		/* Unmask SRP detected interrupt from Power Down Logic */
+		gpwrdn.d32 = 0;
+		gpwrdn.b.srp_det_msk = 1;
+		DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, 0, gpwrdn.d32);
+
+		dwc_otg_adp_probe_start(core_if);
+		dwc_otg_dump_global_registers(core_if);
+		dwc_otg_dump_host_registers(core_if);
+	}
+
+}
+
+/**
+ * Start the ADP Initial Probe timer to detect if Port Connected interrupt is 
+ * not asserted within 1.1 seconds.
+ *
+ * @param core_if the pointer to core_if strucure.
+ */
+void dwc_otg_adp_vbuson_timer_start(dwc_otg_core_if_t * core_if)
+{
+	core_if->adp.vbuson_timer_started = 1;
+	if (core_if->adp.vbuson_timer)
+	{
+		DWC_PRINTF("SCHEDULING VBUSON TIMER\n");
+		/* 1.1 secs + 60ms necessary for cil_hcd_start*/
+		DWC_TIMER_SCHEDULE(core_if->adp.vbuson_timer, 1160);
+	} else {
+		DWC_WARN("VBUSON_TIMER = %p\n",core_if->adp.vbuson_timer);
+	}
+}
+
+#if 0
+/**
+ * Masks all DWC OTG core interrupts
+ *
+ */
+static void mask_all_interrupts(dwc_otg_core_if_t * core_if)
+{
+	int i;
+	gahbcfg_data_t ahbcfg = {.d32 = 0 };
+
+	/* Mask Host Interrupts */
+
+	/* Clear and disable HCINTs */
+	for (i = 0; i < core_if->core_params->host_channels; i++) {
+		DWC_WRITE_REG32(&core_if->host_if->hc_regs[i]->hcintmsk, 0);
+		DWC_WRITE_REG32(&core_if->host_if->hc_regs[i]->hcint, 0xFFFFFFFF);
+
+	}
+
+	/* Clear and disable HAINT */
+	DWC_WRITE_REG32(&core_if->host_if->host_global_regs->haintmsk, 0x0000);
+	DWC_WRITE_REG32(&core_if->host_if->host_global_regs->haint, 0xFFFFFFFF);
+
+	/* Mask Device Interrupts */
+	if (!core_if->multiproc_int_enable) {
+		/* Clear and disable IN Endpoint interrupts */
+		DWC_WRITE_REG32(&core_if->dev_if->dev_global_regs->diepmsk, 0);
+		for (i = 0; i <= core_if->dev_if->num_in_eps; i++) {
+			DWC_WRITE_REG32(&core_if->dev_if->in_ep_regs[i]->
+					diepint, 0xFFFFFFFF);
+		}
+
+		/* Clear and disable OUT Endpoint interrupts */
+		DWC_WRITE_REG32(&core_if->dev_if->dev_global_regs->doepmsk, 0);
+		for (i = 0; i <= core_if->dev_if->num_out_eps; i++) {
+			DWC_WRITE_REG32(&core_if->dev_if->out_ep_regs[i]->
+					doepint, 0xFFFFFFFF);
+		}
+
+		/* Clear and disable DAINT */
+		DWC_WRITE_REG32(&core_if->dev_if->dev_global_regs->daint,
+				0xFFFFFFFF);
+		DWC_WRITE_REG32(&core_if->dev_if->dev_global_regs->daintmsk, 0);
+	} else {
+		for (i = 0; i < core_if->dev_if->num_in_eps; ++i) {
+			DWC_WRITE_REG32(&core_if->dev_if->dev_global_regs->
+					diepeachintmsk[i], 0);
+			DWC_WRITE_REG32(&core_if->dev_if->in_ep_regs[i]->
+					diepint, 0xFFFFFFFF);
+		}
+
+		for (i = 0; i < core_if->dev_if->num_out_eps; ++i) {
+			DWC_WRITE_REG32(&core_if->dev_if->dev_global_regs->
+					doepeachintmsk[i], 0);
+			DWC_WRITE_REG32(&core_if->dev_if->out_ep_regs[i]->
+					doepint, 0xFFFFFFFF);
+		}
+
+		DWC_WRITE_REG32(&core_if->dev_if->dev_global_regs->deachintmsk,
+				0);
+		DWC_WRITE_REG32(&core_if->dev_if->dev_global_regs->deachint,
+				0xFFFFFFFF);
+
+	}
+
+	/* Disable interrupts */
+	ahbcfg.b.glblintrmsk = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gahbcfg, ahbcfg.d32, 0);
+
+	/* Disable all interrupts. */
+	DWC_WRITE_REG32(&core_if->core_global_regs->gintmsk, 0);
+
+	/* Clear any pending interrupts */
+	DWC_WRITE_REG32(&core_if->core_global_regs->gintsts, 0xFFFFFFFF);
+
+	/* Clear any pending OTG Interrupts */
+	DWC_WRITE_REG32(&core_if->core_global_regs->gotgint, 0xFFFFFFFF);
+}
+
+/**
+ * Unmask Port Connection Detected interrupt
+ *
+ */
+static void unmask_conn_det_intr(dwc_otg_core_if_t * core_if)
+{
+	gintmsk_data_t gintmsk = {.d32 = 0,.b.portintr = 1 };
+
+	DWC_WRITE_REG32(&core_if->core_global_regs->gintmsk, gintmsk.d32);
+}
+#endif
+
+/**
+ * Starts the ADP Probing
+ *
+ * @param core_if the pointer to core_if structure.
+ */
+uint32_t dwc_otg_adp_probe_start(dwc_otg_core_if_t * core_if)
+{
+
+	adpctl_data_t adpctl = {.d32 = 0};
+	gpwrdn_data_t gpwrdn;
+#if 0
+	adpctl_data_t adpctl_int = {.d32 = 0, .b.adp_prb_int = 1,
+								.b.adp_sns_int = 1, b.adp_tmout_int};
+#endif
+	dwc_otg_disable_global_interrupts(core_if);
+	DWC_PRINTF("ADP Probe Start\n");
+	core_if->adp.probe_enabled = 1;
+
+	adpctl.b.adpres = 1;
+	dwc_otg_adp_write_reg(core_if, adpctl.d32);
+
+	while (adpctl.b.adpres) {
+		adpctl.d32 = dwc_otg_adp_read_reg(core_if);
+	}
+
+	adpctl.d32 = 0;
+	gpwrdn.d32 = DWC_READ_REG32(&core_if->core_global_regs->gpwrdn);
+
+	/* In Host mode unmask SRP detected interrupt */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.sts_chngint_msk = 1;
+	if (!gpwrdn.b.idsts) {
+		gpwrdn.b.srp_det_msk = 1;
+	}
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, 0, gpwrdn.d32);
+
+	adpctl.b.adp_tmout_int_msk = 1;
+	adpctl.b.adp_prb_int_msk = 1;
+	adpctl.b.prb_dschg = 1;
+	adpctl.b.prb_delta = 1;
+	adpctl.b.prb_per = 1;
+	adpctl.b.adpen = 1;
+	adpctl.b.enaprb = 1;
+
+	dwc_otg_adp_write_reg(core_if, adpctl.d32);
+	DWC_PRINTF("ADP Probe Finish\n");
+	return 0;
+}
+
+/**
+ * Starts the ADP Sense timer to detect if ADP Sense interrupt is not asserted 
+ * within 3 seconds.
+ *
+ * @param core_if the pointer to core_if strucure.
+ */
+void dwc_otg_adp_sense_timer_start(dwc_otg_core_if_t * core_if)
+{
+	core_if->adp.sense_timer_started = 1;
+	DWC_TIMER_SCHEDULE(core_if->adp.sense_timer, 3000 /* 3 secs */ );
+}
+
+/**
+ * Starts the ADP Sense
+ *
+ * @param core_if the pointer to core_if strucure.
+ */
+uint32_t dwc_otg_adp_sense_start(dwc_otg_core_if_t * core_if)
+{
+	adpctl_data_t adpctl;
+
+	DWC_PRINTF("ADP Sense Start\n");
+
+	/* Unmask ADP sense interrupt and mask all other from the core */
+	adpctl.d32 = dwc_otg_adp_read_reg_filter(core_if);
+	adpctl.b.adp_sns_int_msk = 1;
+	dwc_otg_adp_write_reg(core_if, adpctl.d32);
+	dwc_otg_disable_global_interrupts(core_if); // vahrama 
+
+	/* Set ADP reset bit*/
+	adpctl.d32 = dwc_otg_adp_read_reg_filter(core_if);
+	adpctl.b.adpres = 1;
+	dwc_otg_adp_write_reg(core_if, adpctl.d32);
+
+	while (adpctl.b.adpres) {
+		adpctl.d32 = dwc_otg_adp_read_reg(core_if);
+	}
+
+	adpctl.b.adpres = 0;
+	adpctl.b.adpen = 1;
+	adpctl.b.enasns = 1;
+	dwc_otg_adp_write_reg(core_if, adpctl.d32);
+
+	dwc_otg_adp_sense_timer_start(core_if);
+
+	return 0;
+}
+
+/**
+ * Stops the ADP Probing
+ *
+ * @param core_if the pointer to core_if strucure.
+ */
+uint32_t dwc_otg_adp_probe_stop(dwc_otg_core_if_t * core_if)
+{
+
+	adpctl_data_t adpctl;
+	DWC_PRINTF("Stop ADP probe\n");
+	core_if->adp.probe_enabled = 0;
+	core_if->adp.probe_counter = 0;
+	adpctl.d32 = dwc_otg_adp_read_reg(core_if);
+
+	adpctl.b.adpen = 0;
+	adpctl.b.adp_prb_int = 1;
+	adpctl.b.adp_tmout_int = 1;
+	adpctl.b.adp_sns_int = 1;
+	dwc_otg_adp_write_reg(core_if, adpctl.d32);
+
+	return 0;
+}
+
+/**
+ * Stops the ADP Sensing
+ *
+ * @param core_if the pointer to core_if strucure.
+ */
+uint32_t dwc_otg_adp_sense_stop(dwc_otg_core_if_t * core_if)
+{
+	adpctl_data_t adpctl;
+
+	core_if->adp.sense_enabled = 0;
+
+	adpctl.d32 = dwc_otg_adp_read_reg_filter(core_if);
+	adpctl.b.enasns = 0;
+	adpctl.b.adp_sns_int = 1;
+	dwc_otg_adp_write_reg(core_if, adpctl.d32);
+
+	return 0;
+}
+
+/**
+ * Called to turn on the VBUS after initial ADP probe in host mode.
+ * If port power was already enabled in cil_hcd_start function then
+ * only schedule a timer.
+ *
+ * @param core_if the pointer to core_if structure.
+ */
+void dwc_otg_adp_turnon_vbus(dwc_otg_core_if_t * core_if)
+{
+	hprt0_data_t hprt0 = {.d32 = 0 };
+	hprt0.d32 = dwc_otg_read_hprt0(core_if);
+	DWC_PRINTF("Turn on VBUS for 1.1s, port power is %d\n", hprt0.b.prtpwr);
+
+	if (hprt0.b.prtpwr == 0) {
+		hprt0.b.prtpwr = 1;
+		//DWC_WRITE_REG32(core_if->host_if->hprt0, hprt0.d32);
+	}
+	
+	dwc_otg_adp_vbuson_timer_start(core_if);
+}
+
+/**
+ * Called right after driver is loaded
+ * to perform initial actions for ADP
+ *
+ * @param core_if the pointer to core_if structure.
+ * @param is_host - flag for current mode of operation either from GINTSTS or GPWRDN
+ */
+void dwc_otg_adp_start(dwc_otg_core_if_t * core_if, uint8_t is_host)
+{
+	gpwrdn_data_t gpwrdn;
+
+	DWC_PRINTF("ADP Initial Start\n");
+	core_if->adp.adp_started = 1;
+
+	DWC_WRITE_REG32(&core_if->core_global_regs->gintsts, 0xFFFFFFFF);
+	dwc_otg_disable_global_interrupts(core_if);
+	if (is_host) {
+		DWC_PRINTF("HOST MODE\n");
+		/* Enable Power Down Logic Interrupt*/
+		gpwrdn.d32 = 0;
+		gpwrdn.b.pmuintsel = 1;
+		gpwrdn.b.pmuactv = 1;
+		DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, 0, gpwrdn.d32);
+		/* Initialize first ADP probe to obtain Ramp Time value */
+		core_if->adp.initial_probe = 1;
+		dwc_otg_adp_probe_start(core_if);
+	} else {
+		gotgctl_data_t gotgctl;
+		gotgctl.d32 = DWC_READ_REG32(&core_if->core_global_regs->gotgctl);
+		DWC_PRINTF("DEVICE MODE\n");
+		if (gotgctl.b.bsesvld == 0) {
+			/* Enable Power Down Logic Interrupt*/
+			gpwrdn.d32 = 0;
+			DWC_PRINTF("VBUS is not valid - start ADP probe\n");
+			gpwrdn.b.pmuintsel = 1;
+			gpwrdn.b.pmuactv = 1;
+			DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, 0, gpwrdn.d32);
+			core_if->adp.initial_probe = 1;
+			dwc_otg_adp_probe_start(core_if);
+		} else {
+			DWC_PRINTF("VBUS is valid - initialize core as a Device\n");
+			core_if->op_state = B_PERIPHERAL;
+			dwc_otg_core_init(core_if);
+			dwc_otg_enable_global_interrupts(core_if);
+			cil_pcd_start(core_if);
+			dwc_otg_dump_global_registers(core_if);
+			dwc_otg_dump_dev_registers(core_if);
+		}
+	}
+}
+
+void dwc_otg_adp_init(dwc_otg_core_if_t * core_if)
+{
+	core_if->adp.adp_started = 0;
+	core_if->adp.initial_probe = 0;
+	core_if->adp.probe_timer_values[0] = -1;
+	core_if->adp.probe_timer_values[1] = -1;
+	core_if->adp.probe_enabled = 0;
+	core_if->adp.sense_enabled = 0;
+	core_if->adp.sense_timer_started = 0;
+	core_if->adp.vbuson_timer_started = 0;
+	core_if->adp.probe_counter = 0;
+	core_if->adp.gpwrdn = 0;
+	core_if->adp.attached = DWC_OTG_ADP_UNKOWN;
+	/* Initialize timers */
+	core_if->adp.sense_timer =
+	    DWC_TIMER_ALLOC("ADP SENSE TIMER", adp_sense_timeout, core_if);
+	core_if->adp.vbuson_timer =
+	    DWC_TIMER_ALLOC("ADP VBUS ON TIMER", adp_vbuson_timeout, core_if);
+	if (!core_if->adp.sense_timer || !core_if->adp.vbuson_timer)
+	{
+		DWC_ERROR("Could not allocate memory for ADP timers\n");
+	}
+}
+
+void dwc_otg_adp_remove(dwc_otg_core_if_t * core_if)
+{
+	gpwrdn_data_t gpwrdn = { .d32 = 0 };
+	gpwrdn.b.pmuintsel = 1;
+	gpwrdn.b.pmuactv = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+
+	if (core_if->adp.probe_enabled)		
+		dwc_otg_adp_probe_stop(core_if);
+	if (core_if->adp.sense_enabled)		
+		dwc_otg_adp_sense_stop(core_if);
+	if (core_if->adp.sense_timer_started)		
+		DWC_TIMER_CANCEL(core_if->adp.sense_timer);
+	if (core_if->adp.vbuson_timer_started)		
+		DWC_TIMER_CANCEL(core_if->adp.vbuson_timer);
+	DWC_TIMER_FREE(core_if->adp.sense_timer);
+	DWC_TIMER_FREE(core_if->adp.vbuson_timer);
+}
+
+/////////////////////////////////////////////////////////////////////
+////////////// ADP Interrupt Handlers ///////////////////////////////
+/////////////////////////////////////////////////////////////////////
+/**
+ * This function sets Ramp Timer values
+ */
+static uint32_t set_timer_value(dwc_otg_core_if_t * core_if, uint32_t val)
+{
+	if (core_if->adp.probe_timer_values[0] == -1) {
+		core_if->adp.probe_timer_values[0] = val;
+		core_if->adp.probe_timer_values[1] = -1;
+		return 1;
+	} else {
+		core_if->adp.probe_timer_values[1] =
+		    core_if->adp.probe_timer_values[0];
+		core_if->adp.probe_timer_values[0] = val;
+		return 0;
+	}
+}
+
+/**
+ * This function compares Ramp Timer values
+ */
+static uint32_t compare_timer_values(dwc_otg_core_if_t * core_if)
+{
+	uint32_t diff;
+	if (core_if->adp.probe_timer_values[0]>=core_if->adp.probe_timer_values[1])
+			diff = core_if->adp.probe_timer_values[0]-core_if->adp.probe_timer_values[1];
+	else
+			diff = core_if->adp.probe_timer_values[1]-core_if->adp.probe_timer_values[0];   	
+	if(diff < 2) {
+		return 0;
+	} else {
+		return 1;
+	}
+}
+
+/**
+ * This function handles ADP Probe Interrupts
+ */
+static int32_t dwc_otg_adp_handle_prb_intr(dwc_otg_core_if_t * core_if,
+						 uint32_t val)
+{
+	adpctl_data_t adpctl = {.d32 = 0 };
+	gpwrdn_data_t gpwrdn, temp;
+	adpctl.d32 = val;
+
+	temp.d32 = DWC_READ_REG32(&core_if->core_global_regs->gpwrdn);
+	core_if->adp.probe_counter++;
+	core_if->adp.gpwrdn = DWC_READ_REG32(&core_if->core_global_regs->gpwrdn);
+	if (adpctl.b.rtim == 0 && !temp.b.idsts){
+		DWC_PRINTF("RTIM value is 0\n");	
+		goto exit;
+	}
+	if (set_timer_value(core_if, adpctl.b.rtim) &&
+	    core_if->adp.initial_probe) {
+		core_if->adp.initial_probe = 0;
+		dwc_otg_adp_probe_stop(core_if);
+		gpwrdn.d32 = 0;
+		gpwrdn.b.pmuactv = 1;
+		gpwrdn.b.pmuintsel = 1;
+		DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+		DWC_WRITE_REG32(&core_if->core_global_regs->gintsts, 0xFFFFFFFF);
+
+		/* check which value is for device mode and which for Host mode */
+		if (!temp.b.idsts) {	/* considered host mode value is 0 */
+			/*
+			 * Turn on VBUS after initial ADP probe.
+			 */
+			core_if->op_state = A_HOST;
+			dwc_otg_enable_global_interrupts(core_if);
+			DWC_SPINUNLOCK(core_if->lock);
+			cil_hcd_start(core_if);
+			dwc_otg_adp_turnon_vbus(core_if);
+			DWC_SPINLOCK(core_if->lock);
+		} else {
+			/*
+			 * Initiate SRP after initial ADP probe.
+			 */
+			dwc_otg_enable_global_interrupts(core_if);
+			dwc_otg_initiate_srp(core_if);
+		}
+	} else if (core_if->adp.probe_counter > 2){
+		gpwrdn.d32 = DWC_READ_REG32(&core_if->core_global_regs->gpwrdn);
+		if (compare_timer_values(core_if)) {
+			DWC_PRINTF("Difference in timer values !!! \n");
+//                      core_if->adp.attached = DWC_OTG_ADP_ATTACHED;
+			dwc_otg_adp_probe_stop(core_if);
+
+			/* Power on the core */
+			if (core_if->power_down == 2) {
+				gpwrdn.b.pwrdnswtch = 1;
+				DWC_MODIFY_REG32(&core_if->core_global_regs->
+						 gpwrdn, 0, gpwrdn.d32);
+			}
+
+			/* check which value is for device mode and which for Host mode */
+			if (!temp.b.idsts) {	/* considered host mode value is 0 */
+				/* Disable Interrupt from Power Down Logic */
+				gpwrdn.d32 = 0;
+				gpwrdn.b.pmuintsel = 1;
+				gpwrdn.b.pmuactv = 1;
+				DWC_MODIFY_REG32(&core_if->core_global_regs->
+						 gpwrdn, gpwrdn.d32, 0);
+
+				/*
+				 * Initialize the Core for Host mode.
+				 */
+				core_if->op_state = A_HOST;
+				dwc_otg_core_init(core_if);
+				dwc_otg_enable_global_interrupts(core_if);
+				cil_hcd_start(core_if);
+			} else {
+				gotgctl_data_t gotgctl;
+				/* Mask SRP detected interrupt from Power Down Logic */
+				gpwrdn.d32 = 0;
+				gpwrdn.b.srp_det_msk = 1;
+				DWC_MODIFY_REG32(&core_if->core_global_regs->
+						 gpwrdn, gpwrdn.d32, 0);
+
+				/* Disable Power Down Logic */
+				gpwrdn.d32 = 0;
+				gpwrdn.b.pmuintsel = 1;
+				gpwrdn.b.pmuactv = 1;
+				DWC_MODIFY_REG32(&core_if->core_global_regs->
+						 gpwrdn, gpwrdn.d32, 0);
+
+				/*
+				 * Initialize the Core for Device mode.
+				 */
+				core_if->op_state = B_PERIPHERAL;
+				dwc_otg_core_init(core_if);
+				dwc_otg_enable_global_interrupts(core_if);
+				cil_pcd_start(core_if);
+
+				gotgctl.d32 = DWC_READ_REG32(&core_if->core_global_regs->gotgctl);
+				if (!gotgctl.b.bsesvld) {
+					dwc_otg_initiate_srp(core_if);
+				}
+			}
+		}
+		if (core_if->power_down == 2) {
+			if (gpwrdn.b.bsessvld) {
+				/* Mask SRP detected interrupt from Power Down Logic */
+				gpwrdn.d32 = 0;
+				gpwrdn.b.srp_det_msk = 1;
+				DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+				
+				/* Disable Power Down Logic */
+				gpwrdn.d32 = 0;
+				gpwrdn.b.pmuactv = 1;
+				DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+
+				/*
+				 * Initialize the Core for Device mode.
+				 */
+				core_if->op_state = B_PERIPHERAL;
+				dwc_otg_core_init(core_if);
+				dwc_otg_enable_global_interrupts(core_if);
+				cil_pcd_start(core_if);
+			}
+		}
+	}
+exit:
+	/* Clear interrupt */
+	adpctl.d32 = dwc_otg_adp_read_reg(core_if);
+	adpctl.b.adp_prb_int = 1;
+	dwc_otg_adp_write_reg(core_if, adpctl.d32);
+
+	return 0;
+}
+
+/**
+ * This function hadles ADP Sense Interrupt
+ */
+static int32_t dwc_otg_adp_handle_sns_intr(dwc_otg_core_if_t * core_if)
+{
+	adpctl_data_t adpctl;
+	/* Stop ADP Sense timer */
+	DWC_TIMER_CANCEL(core_if->adp.sense_timer);
+
+	/* Restart ADP Sense timer */
+	dwc_otg_adp_sense_timer_start(core_if);
+	
+	/* Clear interrupt */
+	adpctl.d32 = dwc_otg_adp_read_reg(core_if);
+	adpctl.b.adp_sns_int = 1;
+	dwc_otg_adp_write_reg(core_if, adpctl.d32);
+
+	return 0;
+}
+
+/**
+ * This function handles ADP Probe Interrupts
+ */
+static int32_t dwc_otg_adp_handle_prb_tmout_intr(dwc_otg_core_if_t * core_if,
+						 uint32_t val)
+{
+	adpctl_data_t adpctl = {.d32 = 0 };
+	adpctl.d32 = val;
+	set_timer_value(core_if, adpctl.b.rtim);
+	
+	/* Clear interrupt */
+	adpctl.d32 = dwc_otg_adp_read_reg(core_if);
+	adpctl.b.adp_tmout_int = 1;
+	dwc_otg_adp_write_reg(core_if, adpctl.d32);
+
+	return 0;
+}
+
+/**
+ * ADP Interrupt handler.
+ *
+ */
+int32_t dwc_otg_adp_handle_intr(dwc_otg_core_if_t * core_if)
+{
+	int retval = 0;
+	adpctl_data_t adpctl = {.d32 = 0};
+
+	adpctl.d32 = dwc_otg_adp_read_reg(core_if);
+	DWC_PRINTF("ADPCTL = %08x\n",adpctl.d32);
+
+	if (adpctl.b.adp_sns_int & adpctl.b.adp_sns_int_msk) {
+		DWC_PRINTF("ADP Sense interrupt\n");
+		retval |= dwc_otg_adp_handle_sns_intr(core_if);
+	}
+	if (adpctl.b.adp_tmout_int & adpctl.b.adp_tmout_int_msk) {
+		DWC_PRINTF("ADP timeout interrupt\n");
+		retval |= dwc_otg_adp_handle_prb_tmout_intr(core_if, adpctl.d32);
+	}
+	if (adpctl.b.adp_prb_int & adpctl.b.adp_prb_int_msk) {
+		DWC_PRINTF("ADP Probe interrupt\n");
+		adpctl.b.adp_prb_int = 1;	
+		retval |= dwc_otg_adp_handle_prb_intr(core_if, adpctl.d32);
+	}
+
+//	dwc_otg_adp_modify_reg(core_if, adpctl.d32, 0);
+	//dwc_otg_adp_write_reg(core_if, adpctl.d32);
+	DWC_PRINTF("RETURN FROM ADP ISR\n");
+
+	return retval;
+}
+
+/**
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ */
+int32_t dwc_otg_adp_handle_srp_intr(dwc_otg_core_if_t * core_if)
+{
+
+#ifndef DWC_HOST_ONLY
+	hprt0_data_t hprt0;
+	gpwrdn_data_t gpwrdn;
+	DWC_DEBUGPL(DBG_ANY, "++ Power Down Logic Session Request Interrupt++\n");
+
+	gpwrdn.d32 = DWC_READ_REG32(&core_if->core_global_regs->gpwrdn);
+	/* check which value is for device mode and which for Host mode */
+	if (!gpwrdn.b.idsts) {	/* considered host mode value is 0 */
+		DWC_PRINTF("SRP: Host mode\n");
+
+		if (core_if->adp_enable) {
+			dwc_otg_adp_probe_stop(core_if);
+
+			/* Power on the core */
+			if (core_if->power_down == 2) {
+				gpwrdn.b.pwrdnswtch = 1;
+				DWC_MODIFY_REG32(&core_if->core_global_regs->
+						 gpwrdn, 0, gpwrdn.d32);
+			}
+
+			core_if->op_state = A_HOST;
+			dwc_otg_core_init(core_if);
+			dwc_otg_enable_global_interrupts(core_if);
+			cil_hcd_start(core_if);
+		}
+
+		/* Turn on the port power bit. */
+		hprt0.d32 = dwc_otg_read_hprt0(core_if);
+		hprt0.b.prtpwr = 1;
+		DWC_WRITE_REG32(core_if->host_if->hprt0, hprt0.d32);
+
+		/* Start the Connection timer. So a message can be displayed
+		 * if connect does not occur within 10 seconds. */
+		cil_hcd_session_start(core_if);
+	} else {
+		DWC_PRINTF("SRP: Device mode %s\n", __FUNCTION__);
+		if (core_if->adp_enable) {
+			dwc_otg_adp_probe_stop(core_if);
+
+			/* Power on the core */
+			if (core_if->power_down == 2) {
+				gpwrdn.b.pwrdnswtch = 1;
+				DWC_MODIFY_REG32(&core_if->core_global_regs->
+						 gpwrdn, 0, gpwrdn.d32);
+			}
+
+			gpwrdn.d32 = 0;
+			gpwrdn.b.pmuactv = 0;
+			DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, 0,
+					 gpwrdn.d32);
+
+			core_if->op_state = B_PERIPHERAL;
+			dwc_otg_core_init(core_if);
+			dwc_otg_enable_global_interrupts(core_if);
+			cil_pcd_start(core_if);
+		}
+	}
+#endif
+	return 1;
+}
diff --git a/drivers/usb/dwc_otg/dwc_otg_adp.h b/drivers/usb/dwc_otg/dwc_otg_adp.h
new file mode 100644
index 0000000..d8c3f85
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_otg_adp.h
@@ -0,0 +1,80 @@
+/* ==========================================================================
+ * $File: //dwh/usb_iip/dev/software/otg/linux/drivers/dwc_otg_adp.h $
+ * $Revision: #7 $
+ * $Date: 2011/10/24 $
+ * $Change: 1871159 $
+ *
+ * Synopsys HS OTG Linux Software Driver and documentation (hereinafter,
+ * "Software") is an Unsupported proprietary work of Synopsys, Inc. unless
+ * otherwise expressly agreed to in writing between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product under
+ * any End User Software License Agreement or Agreement for Licensed Product
+ * with Synopsys or any supplement thereto. You are permitted to use and
+ * redistribute this Software in source and binary forms, with or without
+ * modification, provided that redistributions of source code must retain this
+ * notice. You may not view, use, disclose, copy or distribute this file or
+ * any information contained herein except pursuant to this license grant from
+ * Synopsys. If you do not agree with this notice, including the disclaimer
+ * below, then you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS" BASIS
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE HEREBY DISCLAIMED. IN NO EVENT SHALL SYNOPSYS BE LIABLE FOR ANY DIRECT,
+ * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================== */
+
+#ifndef __DWC_OTG_ADP_H__
+#define __DWC_OTG_ADP_H__
+
+/**
+ * @file
+ *
+ * This file contains the Attach Detect Protocol interfaces and defines
+ * (functions) and structures for Linux.
+ *
+ */
+
+#define DWC_OTG_ADP_UNATTACHED	0
+#define DWC_OTG_ADP_ATTACHED	1
+#define DWC_OTG_ADP_UNKOWN	2
+
+typedef struct dwc_otg_adp {
+	uint32_t adp_started;	
+	uint32_t initial_probe;
+	int32_t probe_timer_values[2];
+	uint32_t probe_enabled;
+	uint32_t sense_enabled;
+	dwc_timer_t *sense_timer;
+	uint32_t sense_timer_started;
+	dwc_timer_t *vbuson_timer;
+	uint32_t vbuson_timer_started;
+	uint32_t attached;
+	uint32_t probe_counter;
+	uint32_t gpwrdn;
+} dwc_otg_adp_t;
+
+/**
+ * Attach Detect Protocol functions
+ */
+
+extern void dwc_otg_adp_write_reg(dwc_otg_core_if_t * core_if, uint32_t value);
+extern uint32_t dwc_otg_adp_read_reg(dwc_otg_core_if_t * core_if);
+extern uint32_t dwc_otg_adp_probe_start(dwc_otg_core_if_t * core_if);
+extern uint32_t dwc_otg_adp_sense_start(dwc_otg_core_if_t * core_if);
+extern uint32_t dwc_otg_adp_probe_stop(dwc_otg_core_if_t * core_if);
+extern uint32_t dwc_otg_adp_sense_stop(dwc_otg_core_if_t * core_if);
+extern void dwc_otg_adp_start(dwc_otg_core_if_t * core_if, uint8_t is_host);
+extern void dwc_otg_adp_init(dwc_otg_core_if_t * core_if);
+extern void dwc_otg_adp_remove(dwc_otg_core_if_t * core_if);
+extern int32_t dwc_otg_adp_handle_intr(dwc_otg_core_if_t * core_if);
+extern int32_t dwc_otg_adp_handle_srp_intr(dwc_otg_core_if_t * core_if);
+
+#endif //__DWC_OTG_ADP_H__
diff --git a/drivers/usb/dwc_otg/dwc_otg_attr.c b/drivers/usb/dwc_otg/dwc_otg_attr.c
new file mode 100644
index 0000000..41448b5
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_otg_attr.c
@@ -0,0 +1,1441 @@
+/* ==========================================================================
+ * $File: //dwh/usb_iip/dev/software/otg/linux/drivers/dwc_otg_attr.c $
+ * $Revision: #44 $
+ * $Date: 2010/11/29 $
+ * $Change: 1636033 $
+ *
+ * Synopsys HS OTG Linux Software Driver and documentation (hereinafter,
+ * "Software") is an Unsupported proprietary work of Synopsys, Inc. unless
+ * otherwise expressly agreed to in writing between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product under
+ * any End User Software License Agreement or Agreement for Licensed Product
+ * with Synopsys or any supplement thereto. You are permitted to use and
+ * redistribute this Software in source and binary forms, with or without
+ * modification, provided that redistributions of source code must retain this
+ * notice. You may not view, use, disclose, copy or distribute this file or
+ * any information contained herein except pursuant to this license grant from
+ * Synopsys. If you do not agree with this notice, including the disclaimer
+ * below, then you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS" BASIS
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE HEREBY DISCLAIMED. IN NO EVENT SHALL SYNOPSYS BE LIABLE FOR ANY DIRECT,
+ * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================== */
+
+/** @file
+ *
+ * The diagnostic interface will provide access to the controller for
+ * bringing up the hardware and testing.  The Linux driver attributes
+ * feature will be used to provide the Linux Diagnostic
+ * Interface. These attributes are accessed through sysfs.
+ */
+
+/** @page "Linux Module Attributes"
+ *
+ * The Linux module attributes feature is used to provide the Linux
+ * Diagnostic Interface.  These attributes are accessed through sysfs.
+ * The diagnostic interface will provide access to the controller for
+ * bringing up the hardware and testing.
+
+ The following table shows the attributes.
+ <table>
+ <tr>
+ <td><b> Name</b></td>
+ <td><b> Description</b></td>
+ <td><b> Access</b></td>
+ </tr>
+
+ <tr>
+ <td> mode </td>
+ <td> Returns the current mode: 0 for device mode, 1 for host mode</td>
+ <td> Read</td>
+ </tr>
+
+ <tr>
+ <td> hnpcapable </td>
+ <td> Gets or sets the "HNP-capable" bit in the Core USB Configuraton Register.
+ Read returns the current value.</td>
+ <td> Read/Write</td>
+ </tr>
+
+ <tr>
+ <td> srpcapable </td>
+ <td> Gets or sets the "SRP-capable" bit in the Core USB Configuraton Register.
+ Read returns the current value.</td>
+ <td> Read/Write</td>
+ </tr>
+
+ <tr>
+ <td> hsic_connect </td>
+ <td> Gets or sets the "HSIC-Connect" bit in the GLPMCFG Register.
+ Read returns the current value.</td>
+ <td> Read/Write</td>
+ </tr>
+
+ <tr>
+ <td> inv_sel_hsic </td>
+ <td> Gets or sets the "Invert Select HSIC" bit in the GLPMFG Register.
+ Read returns the current value.</td>
+ <td> Read/Write</td>
+ </tr>
+
+ <tr>
+ <td> hnp </td>
+ <td> Initiates the Host Negotiation Protocol.  Read returns the status.</td>
+ <td> Read/Write</td>
+ </tr>
+
+ <tr>
+ <td> srp </td>
+ <td> Initiates the Session Request Protocol.  Read returns the status.</td>
+ <td> Read/Write</td>
+ </tr>
+
+ <tr>
+ <td> buspower </td>
+ <td> Gets or sets the Power State of the bus (0 - Off or 1 - On)</td>
+ <td> Read/Write</td>
+ </tr>
+
+ <tr>
+ <td> bussuspend </td>
+ <td> Suspends the USB bus.</td>
+ <td> Read/Write</td>
+ </tr>
+
+ <tr>
+ <td> busconnected </td>
+ <td> Gets the connection status of the bus</td>
+ <td> Read</td>
+ </tr>
+
+ <tr>
+ <td> gotgctl </td>
+ <td> Gets or sets the Core Control Status Register.</td>
+ <td> Read/Write</td>
+ </tr>
+
+ <tr>
+ <td> gusbcfg </td>
+ <td> Gets or sets the Core USB Configuration Register</td>
+ <td> Read/Write</td>
+ </tr>
+
+ <tr>
+ <td> grxfsiz </td>
+ <td> Gets or sets the Receive FIFO Size Register</td>
+ <td> Read/Write</td>
+ </tr>
+
+ <tr>
+ <td> gnptxfsiz </td>
+ <td> Gets or sets the non-periodic Transmit Size Register</td>
+ <td> Read/Write</td>
+ </tr>
+
+ <tr>
+ <td> gpvndctl </td>
+ <td> Gets or sets the PHY Vendor Control Register</td>
+ <td> Read/Write</td>
+ </tr>
+
+ <tr>
+ <td> ggpio </td>
+ <td> Gets the value in the lower 16-bits of the General Purpose IO Register
+ or sets the upper 16 bits.</td>
+ <td> Read/Write</td>
+ </tr>
+
+ <tr>
+ <td> guid </td>
+ <td> Gets or sets the value of the User ID Register</td>
+ <td> Read/Write</td>
+ </tr>
+
+ <tr>
+ <td> gsnpsid </td>
+ <td> Gets the value of the Synopsys ID Regester</td>
+ <td> Read</td>
+ </tr>
+
+ <tr>
+ <td> devspeed </td>
+ <td> Gets or sets the device speed setting in the DCFG register</td>
+ <td> Read/Write</td>
+ </tr>
+
+ <tr>
+ <td> enumspeed </td>
+ <td> Gets the device enumeration Speed.</td>
+ <td> Read</td>
+ </tr>
+
+ <tr>
+ <td> hptxfsiz </td>
+ <td> Gets the value of the Host Periodic Transmit FIFO</td>
+ <td> Read</td>
+ </tr>
+
+ <tr>
+ <td> hprt0 </td>
+ <td> Gets or sets the value in the Host Port Control and Status Register</td>
+ <td> Read/Write</td>
+ </tr>
+
+ <tr>
+ <td> regoffset </td>
+ <td> Sets the register offset for the next Register Access</td>
+ <td> Read/Write</td>
+ </tr>
+
+ <tr>
+ <td> regvalue </td>
+ <td> Gets or sets the value of the register at the offset in the regoffset attribute.</td>
+ <td> Read/Write</td>
+ </tr>
+
+ <tr>
+ <td> remote_wakeup </td>
+ <td> On read, shows the status of Remote Wakeup. On write, initiates a remote
+ wakeup of the host. When bit 0 is 1 and Remote Wakeup is enabled, the Remote
+ Wakeup signalling bit in the Device Control Register is set for 1
+ milli-second.</td>
+ <td> Read/Write</td>
+ </tr>
+
+ <tr>
+ <td> rem_wakeup_pwrdn </td>
+ <td> On read, shows the status core - hibernated or not. On write, initiates 
+ a remote wakeup of the device from Hibernation. </td>
+ <td> Read/Write</td>
+ </tr>
+
+ <tr>
+ <td> mode_ch_tim_en </td>
+ <td> This bit is used to enable or disable the host core to wait for 200 PHY 
+ clock cycles at the end of Resume to change the opmode signal to the PHY to 00
+ after Suspend or LPM. </td>
+ <td> Read/Write</td>
+ </tr>
+ 
+ <tr>
+ <td> fr_interval </td>
+ <td> On read, shows the value of HFIR Frame Interval. On write, dynamically 
+ reload HFIR register during runtime. The application can write a value to this
+ register only after the Port Enable bit of the Host Port Control and Status 
+ register (HPRT.PrtEnaPort) has been set </td>
+ <td> Read/Write</td>
+ </tr>
+ 
+ <tr>
+ <td> disconnect_us </td>
+ <td> On read, shows the status of disconnect_device_us. On write, sets disconnect_us
+ which causes soft disconnect for 100us. Applicable only for device mode of operation.</td>
+ <td> Read/Write</td>
+ </tr>
+
+ <tr>
+ <td> regdump </td>
+ <td> Dumps the contents of core registers.</td>
+ <td> Read</td>
+ </tr>
+
+ <tr>
+ <td> spramdump </td>
+ <td> Dumps the contents of core registers.</td>
+ <td> Read</td>
+ </tr>
+
+ <tr>
+ <td> hcddump </td>
+ <td> Dumps the current HCD state.</td>
+ <td> Read</td>
+ </tr>
+
+ <tr>
+ <td> hcd_frrem </td>
+ <td> Shows the average value of the Frame Remaining
+ field in the Host Frame Number/Frame Remaining register when an SOF interrupt
+ occurs. This can be used to determine the average interrupt latency. Also
+ shows the average Frame Remaining value for start_transfer and the "a" and
+ "b" sample points. The "a" and "b" sample points may be used during debugging
+ bto determine how long it takes to execute a section of the HCD code.</td>
+ <td> Read</td>
+ </tr>
+
+ <tr>
+ <td> rd_reg_test </td>
+ <td> Displays the time required to read the GNPTXFSIZ register many times
+ (the output shows the number of times the register is read).
+ <td> Read</td>
+ </tr>
+
+ <tr>
+ <td> wr_reg_test </td>
+ <td> Displays the time required to write the GNPTXFSIZ register many times
+ (the output shows the number of times the register is written).
+ <td> Read</td>
+ </tr>
+
+ <tr>
+ <td> lpm_response </td>
+ <td> Gets or sets lpm_response mode. Applicable only in device mode.
+ <td> Write</td>
+ </tr>
+
+ <tr>
+ <td> sleep_status </td>
+ <td> Shows sleep status of device.
+ <td> Read</td>
+ </tr>
+
+ </table>
+
+ Example usage:
+ To get the current mode:
+ cat /sys/devices/lm0/mode
+
+ To power down the USB:
+ echo 0 > /sys/devices/lm0/buspower
+ */
+
+#include "dwc_otg_os_dep.h"
+#include "dwc_os.h"
+#include "dwc_otg_driver.h"
+#include "dwc_otg_attr.h"
+#include "dwc_otg_core_if.h"
+#include "dwc_otg_pcd_if.h"
+#include "dwc_otg_hcd_if.h"
+
+/*
+ * MACROs for defining sysfs attribute
+ */
+#ifdef LM_INTERFACE
+
+#define DWC_OTG_DEVICE_ATTR_BITFIELD_SHOW(_otg_attr_name_,_string_) \
+static ssize_t _otg_attr_name_##_show (struct device *_dev, struct device_attribute *attr, char *buf) \
+{ \
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev); \
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);		\
+	uint32_t val; \
+	val = dwc_otg_get_##_otg_attr_name_ (otg_dev->core_if); \
+	return sprintf (buf, "%s = 0x%x\n", _string_, val); \
+}
+#define DWC_OTG_DEVICE_ATTR_BITFIELD_STORE(_otg_attr_name_,_string_) \
+static ssize_t _otg_attr_name_##_store (struct device *_dev, struct device_attribute *attr, \
+					const char *buf, size_t count) \
+{ \
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev); \
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev); \
+	uint32_t set = simple_strtoul(buf, NULL, 16); \
+	dwc_otg_set_##_otg_attr_name_(otg_dev->core_if, set);\
+	return count; \
+}
+
+#elif defined(PCI_INTERFACE)
+
+#define DWC_OTG_DEVICE_ATTR_BITFIELD_SHOW(_otg_attr_name_,_string_) \
+static ssize_t _otg_attr_name_##_show (struct device *_dev, struct device_attribute *attr, char *buf) \
+{ \
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);	\
+	uint32_t val; \
+	val = dwc_otg_get_##_otg_attr_name_ (otg_dev->core_if); \
+	return sprintf (buf, "%s = 0x%x\n", _string_, val); \
+}
+#define DWC_OTG_DEVICE_ATTR_BITFIELD_STORE(_otg_attr_name_,_string_) \
+static ssize_t _otg_attr_name_##_store (struct device *_dev, struct device_attribute *attr, \
+					const char *buf, size_t count) \
+{ \
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);  \
+	uint32_t set = simple_strtoul(buf, NULL, 16); \
+	dwc_otg_set_##_otg_attr_name_(otg_dev->core_if, set);\
+	return count; \
+}
+#else
+
+#define DWC_OTG_DEVICE_ATTR_BITFIELD_SHOW(_otg_attr_name_,_string_) \
+static ssize_t _otg_attr_name_##_show (struct device *_dev, struct device_attribute *attr, char *buf) \
+{ \
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);		\
+	uint32_t val; \
+	val = dwc_otg_get_##_otg_attr_name_ (otg_dev->core_if); \
+	return sprintf (buf, "%s = 0x%x\n", _string_, val); \
+}
+#define DWC_OTG_DEVICE_ATTR_BITFIELD_STORE(_otg_attr_name_,_string_) \
+static ssize_t _otg_attr_name_##_store (struct device *_dev, struct device_attribute *attr, \
+					const char *buf, size_t count) \
+{ \
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev); \
+	uint32_t set = simple_strtoul(buf, NULL, 16); \
+	dwc_otg_set_##_otg_attr_name_(otg_dev->core_if, set);\
+	return count; \
+}
+
+
+#endif
+
+/*
+ * MACROs for defining sysfs attribute for 32-bit registers
+ */
+#ifdef LM_INTERFACE
+#define DWC_OTG_DEVICE_ATTR_REG_SHOW(_otg_attr_name_,_string_) \
+static ssize_t _otg_attr_name_##_show (struct device *_dev, struct device_attribute *attr, char *buf) \
+{ \
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev); \
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev); \
+	uint32_t val; \
+	val = dwc_otg_get_##_otg_attr_name_ (otg_dev->core_if); \
+	return sprintf (buf, "%s = 0x%08x\n", _string_, val); \
+}
+#define DWC_OTG_DEVICE_ATTR_REG_STORE(_otg_attr_name_,_string_) \
+static ssize_t _otg_attr_name_##_store (struct device *_dev, struct device_attribute *attr, \
+					const char *buf, size_t count) \
+{ \
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev); \
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev); \
+	uint32_t val = simple_strtoul(buf, NULL, 16); \
+	dwc_otg_set_##_otg_attr_name_ (otg_dev->core_if, val); \
+	return count; \
+}
+#elif defined(PCI_INTERFACE)
+#define DWC_OTG_DEVICE_ATTR_REG_SHOW(_otg_attr_name_,_string_) \
+static ssize_t _otg_attr_name_##_show (struct device *_dev, struct device_attribute *attr, char *buf) \
+{ \
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);  \
+	uint32_t val; \
+	val = dwc_otg_get_##_otg_attr_name_ (otg_dev->core_if); \
+	return sprintf (buf, "%s = 0x%08x\n", _string_, val); \
+}
+#define DWC_OTG_DEVICE_ATTR_REG_STORE(_otg_attr_name_,_string_) \
+static ssize_t _otg_attr_name_##_store (struct device *_dev, struct device_attribute *attr, \
+					const char *buf, size_t count) \
+{ \
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);  \
+	uint32_t val = simple_strtoul(buf, NULL, 16); \
+	dwc_otg_set_##_otg_attr_name_ (otg_dev->core_if, val); \
+	return count; \
+}
+#else
+#define DWC_OTG_DEVICE_ATTR_REG_SHOW(_otg_attr_name_,_string_) \
+static ssize_t _otg_attr_name_##_show (struct device *_dev, struct device_attribute *attr, char *buf) \
+{ \
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev); \
+	uint32_t val; \
+	val = dwc_otg_get_##_otg_attr_name_ (otg_dev->core_if); \
+	return sprintf (buf, "%s = 0x%08x\n", _string_, val); \
+}
+#define DWC_OTG_DEVICE_ATTR_REG_STORE(_otg_attr_name_,_string_) \
+static ssize_t _otg_attr_name_##_store (struct device *_dev, struct device_attribute *attr, \
+					const char *buf, size_t count) \
+{ \
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev); \
+	uint32_t val = simple_strtoul(buf, NULL, 16); \
+	dwc_otg_set_##_otg_attr_name_ (otg_dev->core_if, val); \
+	return count; \
+}
+#endif
+
+#define DWC_OTG_DEVICE_ATTR_BITFIELD_RW(_otg_attr_name_,_string_) \
+DWC_OTG_DEVICE_ATTR_BITFIELD_SHOW(_otg_attr_name_,_string_) \
+DWC_OTG_DEVICE_ATTR_BITFIELD_STORE(_otg_attr_name_,_string_) \
+DEVICE_ATTR(_otg_attr_name_,0644,_otg_attr_name_##_show,_otg_attr_name_##_store);
+
+#define DWC_OTG_DEVICE_ATTR_BITFIELD_RO(_otg_attr_name_,_string_) \
+DWC_OTG_DEVICE_ATTR_BITFIELD_SHOW(_otg_attr_name_,_string_) \
+DEVICE_ATTR(_otg_attr_name_,0444,_otg_attr_name_##_show,NULL);
+
+#define DWC_OTG_DEVICE_ATTR_REG32_RW(_otg_attr_name_,_addr_,_string_) \
+DWC_OTG_DEVICE_ATTR_REG_SHOW(_otg_attr_name_,_string_) \
+DWC_OTG_DEVICE_ATTR_REG_STORE(_otg_attr_name_,_string_) \
+DEVICE_ATTR(_otg_attr_name_,0644,_otg_attr_name_##_show,_otg_attr_name_##_store);
+
+#define DWC_OTG_DEVICE_ATTR_REG32_RO(_otg_attr_name_,_addr_,_string_) \
+DWC_OTG_DEVICE_ATTR_REG_SHOW(_otg_attr_name_,_string_) \
+DEVICE_ATTR(_otg_attr_name_,0444,_otg_attr_name_##_show,NULL);
+
+/** @name Functions for Show/Store of Attributes */
+/**@{*/
+
+/**
+ * Show the register offset of the Register Access.
+ */
+static ssize_t regoffset_show(struct device *_dev,
+			      struct device_attribute *attr, char *buf)
+{
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+
+	return snprintf(buf, sizeof("0xFFFFFFFF\n") + 1, "0x%08x\n",
+			otg_dev->os_dep.reg_offset);
+}
+
+/**
+ * Set the register offset for the next Register Access 	Read/Write
+ */
+static ssize_t regoffset_store(struct device *_dev,
+			       struct device_attribute *attr,
+			       const char *buf, size_t count)
+{
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+
+	uint32_t offset = simple_strtoul(buf, NULL, 16);
+#ifdef LM_INTERFACE
+	if (offset < SZ_256K) {
+#elif  defined(PCI_INTERFACE)
+	if (offset < 0x00040000) {
+#else
+	if (offset < 0x00040000) {		
+#endif
+		otg_dev->os_dep.reg_offset = offset;
+	} else {
+		dev_err(_dev, "invalid offset\n");
+	}
+
+	return count;
+}
+
+DEVICE_ATTR(regoffset, S_IRUGO | S_IWUSR, regoffset_show, regoffset_store);
+
+/**
+ * Show the value of the register at the offset in the reg_offset
+ * attribute.
+ */
+static ssize_t regvalue_show(struct device *_dev,
+			     struct device_attribute *attr, char *buf)
+{
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+
+	uint32_t val;
+	volatile uint32_t *addr;
+
+	if (otg_dev->os_dep.reg_offset != 0xFFFFFFFF && 0 != otg_dev->os_dep.base) {
+		/* Calculate the address */
+		addr = (uint32_t *) (otg_dev->os_dep.reg_offset +
+				     (uint8_t *) otg_dev->os_dep.base);
+		val = DWC_READ_REG32(addr);
+		return snprintf(buf,
+				sizeof("Reg@0xFFFFFFFF = 0xFFFFFFFF\n") + 1,
+				"Reg@0x%06x = 0x%08x\n", otg_dev->os_dep.reg_offset,
+				val);
+	} else {
+		dev_err(_dev, "Invalid offset (0x%0x)\n", otg_dev->os_dep.reg_offset);
+		return sprintf(buf, "invalid offset\n");
+	}
+}
+
+/**
+ * Store the value in the register at the offset in the reg_offset
+ * attribute.
+ *
+ */
+static ssize_t regvalue_store(struct device *_dev,
+			      struct device_attribute *attr,
+			      const char *buf, size_t count)
+{
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+
+	volatile uint32_t *addr;
+	uint32_t val = simple_strtoul(buf, NULL, 16);
+	//dev_dbg(_dev, "Offset=0x%08x Val=0x%08x\n", otg_dev->reg_offset, val);
+	if (otg_dev->os_dep.reg_offset != 0xFFFFFFFF && 0 != otg_dev->os_dep.base) {
+		/* Calculate the address */
+		addr = (uint32_t *) (otg_dev->os_dep.reg_offset +
+				     (uint8_t *) otg_dev->os_dep.base);
+		DWC_WRITE_REG32(addr, val);
+	} else {
+		dev_err(_dev, "Invalid Register Offset (0x%08x)\n",
+			otg_dev->os_dep.reg_offset);
+	}
+	return count;
+}
+
+DEVICE_ATTR(regvalue, S_IRUGO | S_IWUSR, regvalue_show, regvalue_store);
+
+/*
+ * Attributes
+ */
+DWC_OTG_DEVICE_ATTR_BITFIELD_RO(mode, "Mode");
+DWC_OTG_DEVICE_ATTR_BITFIELD_RW(hnpcapable, "HNPCapable");
+DWC_OTG_DEVICE_ATTR_BITFIELD_RW(srpcapable, "SRPCapable");
+DWC_OTG_DEVICE_ATTR_BITFIELD_RW(hsic_connect, "HSIC Connect");
+DWC_OTG_DEVICE_ATTR_BITFIELD_RW(inv_sel_hsic, "Invert Select HSIC");
+
+//DWC_OTG_DEVICE_ATTR_BITFIELD_RW(buspower,&(otg_dev->core_if->core_global_regs->gotgctl),(1<<8),8,"Mode");
+//DWC_OTG_DEVICE_ATTR_BITFIELD_RW(bussuspend,&(otg_dev->core_if->core_global_regs->gotgctl),(1<<8),8,"Mode");
+DWC_OTG_DEVICE_ATTR_BITFIELD_RO(busconnected, "Bus Connected");
+
+#ifndef CONFIG_MACH_M822XX
+/* Base value of GOTGCTL with VBUS checking enabled (forced to zero) */
+DWC_OTG_DEVICE_ATTR_REG32_RW(gotgctl, 0, "GOTGCTL");
+#else  /* CONFIG_MACH_M822XX */
+/* Use actual register if VBUS checking not enabled */
+DWC_OTG_DEVICE_ATTR_REG32_RW(gotgctl,
+			     &(otg_dev->core_if->core_global_regs->gotgctl),
+			     "GOTGCTL");
+#endif	/* CONFIG_MACH_M822XX */
+DWC_OTG_DEVICE_ATTR_REG32_RW(gusbcfg,
+			     &(otg_dev->core_if->core_global_regs->gusbcfg),
+			     "GUSBCFG");
+DWC_OTG_DEVICE_ATTR_REG32_RW(grxfsiz,
+			     &(otg_dev->core_if->core_global_regs->grxfsiz),
+			     "GRXFSIZ");
+DWC_OTG_DEVICE_ATTR_REG32_RW(gnptxfsiz,
+			     &(otg_dev->core_if->core_global_regs->gnptxfsiz),
+			     "GNPTXFSIZ");
+DWC_OTG_DEVICE_ATTR_REG32_RW(gpvndctl,
+			     &(otg_dev->core_if->core_global_regs->gpvndctl),
+			     "GPVNDCTL");
+DWC_OTG_DEVICE_ATTR_REG32_RW(ggpio,
+			     &(otg_dev->core_if->core_global_regs->ggpio),
+			     "GGPIO");
+DWC_OTG_DEVICE_ATTR_REG32_RW(guid, &(otg_dev->core_if->core_global_regs->guid),
+			     "GUID");
+DWC_OTG_DEVICE_ATTR_REG32_RO(gsnpsid,
+			     &(otg_dev->core_if->core_global_regs->gsnpsid),
+			     "GSNPSID");
+DWC_OTG_DEVICE_ATTR_BITFIELD_RW(devspeed, "Device Speed");
+DWC_OTG_DEVICE_ATTR_BITFIELD_RO(enumspeed, "Device Enumeration Speed");
+
+DWC_OTG_DEVICE_ATTR_REG32_RO(hptxfsiz,
+			     &(otg_dev->core_if->core_global_regs->hptxfsiz),
+			     "HPTXFSIZ");
+DWC_OTG_DEVICE_ATTR_REG32_RW(hprt0, otg_dev->core_if->host_if->hprt0, "HPRT0");
+
+/**
+ * @todo Add code to initiate the HNP.
+ */
+/**
+ * Show the HNP status bit
+ */
+static ssize_t hnp_show(struct device *_dev,
+			struct device_attribute *attr, char *buf)
+{
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+	return sprintf(buf, "HstNegScs = 0x%x\n",
+		       dwc_otg_get_hnpstatus(otg_dev->core_if));
+}
+
+/**
+ * Set the HNP Request bit
+ */
+static ssize_t hnp_store(struct device *_dev,
+			 struct device_attribute *attr,
+			 const char *buf, size_t count)
+{
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+	uint32_t in = simple_strtoul(buf, NULL, 16);
+	dwc_otg_set_hnpreq(otg_dev->core_if, in);
+	return count;
+}
+
+DEVICE_ATTR(hnp, 0644, hnp_show, hnp_store);
+
+/**
+ * @todo Add code to initiate the SRP.
+ */
+/**
+ * Show the SRP status bit
+ */
+static ssize_t srp_show(struct device *_dev,
+			struct device_attribute *attr, char *buf)
+{
+#ifndef DWC_HOST_ONLY
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+	return sprintf(buf, "SesReqScs = 0x%x\n",
+		       dwc_otg_get_srpstatus(otg_dev->core_if));
+#else
+	return sprintf(buf, "Host Only Mode!\n");
+#endif
+}
+
+/**
+ * Set the SRP Request bit
+ */
+static ssize_t srp_store(struct device *_dev,
+			 struct device_attribute *attr,
+			 const char *buf, size_t count)
+{
+#ifndef DWC_HOST_ONLY
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+	dwc_otg_pcd_initiate_srp(otg_dev->pcd);
+#endif
+	return count;
+}
+
+DEVICE_ATTR(srp, 0644, srp_show, srp_store);
+
+/**
+ * @todo Need to do more for power on/off?
+ */
+/**
+ * Show the Bus Power status
+ */
+static ssize_t buspower_show(struct device *_dev,
+			     struct device_attribute *attr, char *buf)
+{
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+	return sprintf(buf, "Bus Power = 0x%x\n",
+		       dwc_otg_get_prtpower(otg_dev->core_if));
+}
+
+/**
+ * Set the Bus Power status
+ */
+static ssize_t buspower_store(struct device *_dev,
+			      struct device_attribute *attr,
+			      const char *buf, size_t count)
+{
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+	uint32_t on = simple_strtoul(buf, NULL, 16);
+	dwc_otg_set_prtpower(otg_dev->core_if, on);
+	return count;
+}
+
+DEVICE_ATTR(buspower, 0644, buspower_show, buspower_store);
+
+/**
+ * @todo Need to do more for suspend?
+ */
+/**
+ * Show the Bus Suspend status
+ */
+static ssize_t bussuspend_show(struct device *_dev,
+			       struct device_attribute *attr, char *buf)
+{
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+
+	return sprintf(buf, "Bus Suspend = 0x%x\n",
+		       dwc_otg_get_prtsuspend(otg_dev->core_if));
+}
+
+/**
+ * Set the Bus Suspend status
+ */
+static ssize_t bussuspend_store(struct device *_dev,
+				struct device_attribute *attr,
+				const char *buf, size_t count)
+{
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+
+	uint32_t in = simple_strtoul(buf, NULL, 16);
+	dwc_otg_set_prtsuspend(otg_dev->core_if, in);
+	return count;
+}
+
+DEVICE_ATTR(bussuspend, 0644, bussuspend_show, bussuspend_store);
+
+/**
+ * Show the Mode Change Ready Timer status
+ */
+static ssize_t mode_ch_tim_en_show(struct device *_dev,
+				   struct device_attribute *attr, char *buf)
+{
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+
+	return sprintf(buf, "Mode Change Ready Timer Enable = 0x%x\n",
+		       dwc_otg_get_mode_ch_tim(otg_dev->core_if));
+}
+
+/**
+ * Set the Mode Change Ready Timer status
+ */
+static ssize_t mode_ch_tim_en_store(struct device *_dev,
+				    struct device_attribute *attr,
+				    const char *buf, size_t count)
+{
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+
+	uint32_t in = simple_strtoul(buf, NULL, 16);
+	dwc_otg_set_mode_ch_tim(otg_dev->core_if, in);
+	return count;
+}
+
+DEVICE_ATTR(mode_ch_tim_en, 0644, mode_ch_tim_en_show, mode_ch_tim_en_store);
+
+/**
+ * Show the value of HFIR Frame Interval bitfield
+ */
+static ssize_t fr_interval_show(struct device *_dev,
+				struct device_attribute *attr, char *buf)
+{
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+
+	return sprintf(buf, "Frame Interval = 0x%x\n",
+		       dwc_otg_get_fr_interval(otg_dev->core_if));
+}
+
+/**
+ * Set the HFIR Frame Interval value
+ */
+static ssize_t fr_interval_store(struct device *_dev,
+				 struct device_attribute *attr,
+				 const char *buf, size_t count)
+{
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+
+	uint32_t in = simple_strtoul(buf, NULL, 10);
+	dwc_otg_set_fr_interval(otg_dev->core_if, in);
+	return count;
+}
+
+DEVICE_ATTR(fr_interval, 0644, fr_interval_show, fr_interval_store);
+
+/**
+ * Show the status of Remote Wakeup.
+ */
+static ssize_t remote_wakeup_show(struct device *_dev,
+				  struct device_attribute *attr, char *buf)
+{
+#ifndef DWC_HOST_ONLY
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+
+	return sprintf(buf,
+		       "Remote Wakeup Sig = %d Enabled = %d LPM Remote Wakeup = %d\n",
+		       dwc_otg_get_remotewakesig(otg_dev->core_if),
+		       dwc_otg_pcd_get_rmwkup_enable(otg_dev->pcd),
+		       dwc_otg_get_lpm_remotewakeenabled(otg_dev->core_if));
+#else
+	return sprintf(buf, "Host Only Mode!\n");
+#endif /* DWC_HOST_ONLY */
+}
+
+/**
+ * Initiate a remote wakeup of the host.  The Device control register
+ * Remote Wakeup Signal bit is written if the PCD Remote wakeup enable
+ * flag is set.
+ *
+ */
+static ssize_t remote_wakeup_store(struct device *_dev,
+				   struct device_attribute *attr,
+				   const char *buf, size_t count)
+{
+#ifndef DWC_HOST_ONLY
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+
+	uint32_t val = simple_strtoul(buf, NULL, 16);
+
+	if (val & 1) {
+		dwc_otg_pcd_remote_wakeup(otg_dev->pcd, 1);
+	} else {
+		dwc_otg_pcd_remote_wakeup(otg_dev->pcd, 0);
+	}
+#endif /* DWC_HOST_ONLY */
+	return count;
+}
+
+DEVICE_ATTR(remote_wakeup, S_IRUGO | S_IWUSR, remote_wakeup_show,
+	    remote_wakeup_store);
+
+/**
+ * Show the whether core is hibernated or not. 					
+ */
+static ssize_t rem_wakeup_pwrdn_show(struct device *_dev,
+				     struct device_attribute *attr, char *buf)
+{
+#ifndef DWC_HOST_ONLY
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+	if (dwc_otg_get_core_state(otg_dev->core_if)) {
+		DWC_PRINTF("Core is in hibernation\n");
+	} else {
+		DWC_PRINTF("Core is not in hibernation\n");
+	}
+#endif /* DWC_HOST_ONLY */
+	return 0;
+}
+
+extern int dwc_otg_device_hibernation_restore(dwc_otg_core_if_t * core_if,
+					      int rem_wakeup, int reset);
+
+/**
+ * Initiate a remote wakeup of the device to exit from hibernation.
+ */
+static ssize_t rem_wakeup_pwrdn_store(struct device *_dev,
+				      struct device_attribute *attr,
+				      const char *buf, size_t count)
+{
+#ifndef DWC_HOST_ONLY
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+	dwc_otg_device_hibernation_restore(otg_dev->core_if, 1, 0);
+#endif
+	return count;
+}
+
+DEVICE_ATTR(rem_wakeup_pwrdn, S_IRUGO | S_IWUSR, rem_wakeup_pwrdn_show,
+	    rem_wakeup_pwrdn_store);
+
+static ssize_t disconnect_us(struct device *_dev,
+			     struct device_attribute *attr,
+			     const char *buf, size_t count)
+{
+
+#ifndef DWC_HOST_ONLY
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+	uint32_t val = simple_strtoul(buf, NULL, 16);
+	DWC_PRINTF("The Passed value is %04x\n", val);
+
+	dwc_otg_pcd_disconnect_us(otg_dev->pcd, 50);
+
+#endif /* DWC_HOST_ONLY */
+	return count;
+}
+
+DEVICE_ATTR(disconnect_us, S_IWUSR, 0, disconnect_us);
+
+/**
+ * Dump global registers and either host or device registers (depending on the
+ * current mode of the core).
+ */
+static ssize_t regdump_show(struct device *_dev,
+			    struct device_attribute *attr, char *buf)
+{
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+
+	dwc_otg_dump_global_registers(otg_dev->core_if);
+	if (dwc_otg_is_host_mode(otg_dev->core_if)) {
+		dwc_otg_dump_host_registers(otg_dev->core_if);
+	} else {
+		dwc_otg_dump_dev_registers(otg_dev->core_if);
+
+	}
+	return sprintf(buf, "Register Dump\n");
+}
+
+DEVICE_ATTR(regdump, S_IRUGO | S_IWUSR, regdump_show, 0);
+
+/**
+ * Dump global registers and either host or device registers (depending on the
+ * current mode of the core).
+ */
+static ssize_t spramdump_show(struct device *_dev,
+			      struct device_attribute *attr, char *buf)
+{
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+
+	dwc_otg_dump_spram(otg_dev->core_if);
+
+	return sprintf(buf, "SPRAM Dump\n");
+}
+
+DEVICE_ATTR(spramdump, S_IRUGO | S_IWUSR, spramdump_show, 0);
+
+/**
+ * Dump the current hcd state.
+ */
+static ssize_t hcddump_show(struct device *_dev,
+			    struct device_attribute *attr, char *buf)
+{
+#ifndef DWC_DEVICE_ONLY
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+
+	dwc_otg_hcd_dump_state(otg_dev->hcd);
+#endif /* DWC_DEVICE_ONLY */
+	return sprintf(buf, "HCD Dump\n");
+}
+
+DEVICE_ATTR(hcddump, S_IRUGO | S_IWUSR, hcddump_show, 0);
+
+/**
+ * Dump the average frame remaining at SOF. This can be used to
+ * determine average interrupt latency. Frame remaining is also shown for
+ * start transfer and two additional sample points.
+ */
+static ssize_t hcd_frrem_show(struct device *_dev,
+			      struct device_attribute *attr, char *buf)
+{
+#ifndef DWC_DEVICE_ONLY
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+
+	dwc_otg_hcd_dump_frrem(otg_dev->hcd);
+#endif /* DWC_DEVICE_ONLY */
+	return sprintf(buf, "HCD Dump Frame Remaining\n");
+}
+
+DEVICE_ATTR(hcd_frrem, S_IRUGO | S_IWUSR, hcd_frrem_show, 0);
+
+/**
+ * Displays the time required to read the GNPTXFSIZ register many times (the
+ * output shows the number of times the register is read).
+ */
+#define RW_REG_COUNT 10000000
+#define MSEC_PER_JIFFIE 1000/HZ
+static ssize_t rd_reg_test_show(struct device *_dev,
+				struct device_attribute *attr, char *buf)
+{
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+
+	int i;
+	int time;
+	int start_jiffies;
+
+	printk("HZ %d, MSEC_PER_JIFFIE %d, loops_per_jiffy %lu\n",
+	       HZ, MSEC_PER_JIFFIE, loops_per_jiffy);
+	start_jiffies = jiffies;
+	for (i = 0; i < RW_REG_COUNT; i++) {
+		dwc_otg_get_gnptxfsiz(otg_dev->core_if);
+	}
+	time = jiffies - start_jiffies;
+	return sprintf(buf,
+		       "Time to read GNPTXFSIZ reg %d times: %d msecs (%d jiffies)\n",
+		       RW_REG_COUNT, time * MSEC_PER_JIFFIE, time);
+}
+
+DEVICE_ATTR(rd_reg_test, S_IRUGO | S_IWUSR, rd_reg_test_show, 0);
+
+/**
+ * Displays the time required to write the GNPTXFSIZ register many times (the
+ * output shows the number of times the register is written).
+ */
+static ssize_t wr_reg_test_show(struct device *_dev,
+				struct device_attribute *attr, char *buf)
+{
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+
+
+	uint32_t reg_val;
+	int i;
+	int time;
+	int start_jiffies;
+
+	printk("HZ %d, MSEC_PER_JIFFIE %d, loops_per_jiffy %lu\n",
+	       HZ, MSEC_PER_JIFFIE, loops_per_jiffy);
+	reg_val = dwc_otg_get_gnptxfsiz(otg_dev->core_if);
+	start_jiffies = jiffies;
+	for (i = 0; i < RW_REG_COUNT; i++) {
+		dwc_otg_set_gnptxfsiz(otg_dev->core_if, reg_val);
+	}
+	time = jiffies - start_jiffies;
+	return sprintf(buf,
+		       "Time to write GNPTXFSIZ reg %d times: %d msecs (%d jiffies)\n",
+		       RW_REG_COUNT, time * MSEC_PER_JIFFIE, time);
+}
+
+DEVICE_ATTR(wr_reg_test, S_IRUGO | S_IWUSR, wr_reg_test_show, 0);
+
+#ifdef CONFIG_USB_DWC_OTG_LPM
+
+/**
+* Show the lpm_response attribute.
+*/
+static ssize_t lpmresp_show(struct device *_dev,
+			    struct device_attribute *attr, char *buf)
+{
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+
+
+	if (!dwc_otg_get_param_lpm_enable(otg_dev->core_if))
+		return sprintf(buf, "** LPM is DISABLED **\n");
+
+	if (!dwc_otg_is_device_mode(otg_dev->core_if)) {
+		return sprintf(buf, "** Current mode is not device mode\n");
+	}
+	return sprintf(buf, "lpm_response = %d\n",
+		       dwc_otg_get_lpmresponse(otg_dev->core_if));
+}
+
+/**
+* Store the lpm_response attribute.
+*/
+static ssize_t lpmresp_store(struct device *_dev,
+			     struct device_attribute *attr,
+			     const char *buf, size_t count)
+{
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+
+
+	uint32_t val = simple_strtoul(buf, NULL, 16);
+
+	if (!dwc_otg_get_param_lpm_enable(otg_dev->core_if)) {
+		return 0;
+	}
+
+	if (!dwc_otg_is_device_mode(otg_dev->core_if)) {
+		return 0;
+	}
+
+	dwc_otg_set_lpmresponse(otg_dev->core_if, val);
+	return count;
+}
+
+DEVICE_ATTR(lpm_response, S_IRUGO | S_IWUSR, lpmresp_show, lpmresp_store);
+
+/**
+* Show the sleep_status attribute.
+*/
+static ssize_t sleepstatus_show(struct device *_dev,
+				struct device_attribute *attr, char *buf)
+{
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+
+
+	return sprintf(buf, "Sleep Status = %d\n",
+		       dwc_otg_get_lpm_portsleepstatus(otg_dev->core_if));
+}
+
+/**
+ * Store the sleep_status attribure.
+ */
+static ssize_t sleepstatus_store(struct device *_dev,
+				 struct device_attribute *attr,
+				 const char *buf, size_t count)
+{
+#ifdef LM_INTERFACE
+	struct lm_device *lm_dev = container_of(_dev, struct lm_device, dev);
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(lm_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = dev_get_drvdata(_dev);
+#endif
+
+
+	dwc_otg_core_if_t *core_if = otg_dev->core_if;
+
+	if (dwc_otg_get_lpm_portsleepstatus(otg_dev->core_if)) {
+		if (dwc_otg_is_host_mode(core_if)) {
+
+			DWC_PRINTF("Host initiated resume\n");
+			dwc_otg_set_prtresume(otg_dev->core_if, 1);
+		}
+	}
+
+	return count;
+}
+
+DEVICE_ATTR(sleep_status, S_IRUGO | S_IWUSR, sleepstatus_show,
+	    sleepstatus_store);
+
+#endif /* CONFIG_USB_DWC_OTG_LPM_ENABLE */
+
+/**@}*/
+
+/**
+ * Create the device files
+ */
+void dwc_otg_attr_create(
+#ifdef LM_INTERFACE
+				struct lm_device *dev
+#elif  defined(PCI_INTERFACE)
+				struct pci_dev *dev
+#else 
+//				struct device *dev
+				struct platform_device *dev
+#endif
+
+    )
+{
+	int error;
+
+	error = device_create_file(&dev->dev, &dev_attr_regoffset);
+	error = device_create_file(&dev->dev, &dev_attr_regvalue);
+	error = device_create_file(&dev->dev, &dev_attr_mode);
+	error = device_create_file(&dev->dev, &dev_attr_hnpcapable);
+	error = device_create_file(&dev->dev, &dev_attr_srpcapable);
+	error = device_create_file(&dev->dev, &dev_attr_hsic_connect);
+	error = device_create_file(&dev->dev, &dev_attr_inv_sel_hsic);
+	error = device_create_file(&dev->dev, &dev_attr_hnp);
+	error = device_create_file(&dev->dev, &dev_attr_srp);
+	error = device_create_file(&dev->dev, &dev_attr_buspower);
+	error = device_create_file(&dev->dev, &dev_attr_bussuspend);
+	error = device_create_file(&dev->dev, &dev_attr_mode_ch_tim_en);
+	error = device_create_file(&dev->dev, &dev_attr_fr_interval);
+	error = device_create_file(&dev->dev, &dev_attr_busconnected);
+	error = device_create_file(&dev->dev, &dev_attr_gotgctl);
+	error = device_create_file(&dev->dev, &dev_attr_gusbcfg);
+	error = device_create_file(&dev->dev, &dev_attr_grxfsiz);
+	error = device_create_file(&dev->dev, &dev_attr_gnptxfsiz);
+	error = device_create_file(&dev->dev, &dev_attr_gpvndctl);
+	error = device_create_file(&dev->dev, &dev_attr_ggpio);
+	error = device_create_file(&dev->dev, &dev_attr_guid);
+	error = device_create_file(&dev->dev, &dev_attr_gsnpsid);
+	error = device_create_file(&dev->dev, &dev_attr_devspeed);
+	error = device_create_file(&dev->dev, &dev_attr_enumspeed);
+	error = device_create_file(&dev->dev, &dev_attr_hptxfsiz);
+	error = device_create_file(&dev->dev, &dev_attr_hprt0);
+	error = device_create_file(&dev->dev, &dev_attr_remote_wakeup);
+	error = device_create_file(&dev->dev, &dev_attr_rem_wakeup_pwrdn);
+	error = device_create_file(&dev->dev, &dev_attr_disconnect_us);
+	error = device_create_file(&dev->dev, &dev_attr_regdump);
+	error = device_create_file(&dev->dev, &dev_attr_spramdump);
+	error = device_create_file(&dev->dev, &dev_attr_hcddump);
+	error = device_create_file(&dev->dev, &dev_attr_hcd_frrem);
+	error = device_create_file(&dev->dev, &dev_attr_rd_reg_test);
+	error = device_create_file(&dev->dev, &dev_attr_wr_reg_test);
+#ifdef CONFIG_USB_DWC_OTG_LPM
+	error = device_create_file(&dev->dev, &dev_attr_lpm_response);
+	error = device_create_file(&dev->dev, &dev_attr_sleep_status);
+#endif
+}
+
+/**
+ * Remove the device files
+ */
+void dwc_otg_attr_remove(
+#ifdef LM_INTERFACE
+				struct lm_device *dev
+#elif  defined(PCI_INTERFACE)
+				struct pci_dev *dev
+#else 
+				//struct device *dev
+				struct platform_device *dev
+#endif
+
+    )
+{
+	device_remove_file(&dev->dev, &dev_attr_regoffset);
+	device_remove_file(&dev->dev, &dev_attr_regvalue);
+	device_remove_file(&dev->dev, &dev_attr_mode);
+	device_remove_file(&dev->dev, &dev_attr_hnpcapable);
+	device_remove_file(&dev->dev, &dev_attr_srpcapable);
+	device_remove_file(&dev->dev, &dev_attr_hsic_connect);
+	device_remove_file(&dev->dev, &dev_attr_inv_sel_hsic);
+	device_remove_file(&dev->dev, &dev_attr_hnp);
+	device_remove_file(&dev->dev, &dev_attr_srp);
+	device_remove_file(&dev->dev, &dev_attr_buspower);
+	device_remove_file(&dev->dev, &dev_attr_bussuspend);
+	device_remove_file(&dev->dev, &dev_attr_mode_ch_tim_en);
+	device_remove_file(&dev->dev, &dev_attr_fr_interval);
+	device_remove_file(&dev->dev, &dev_attr_busconnected);
+	device_remove_file(&dev->dev, &dev_attr_gotgctl);
+	device_remove_file(&dev->dev, &dev_attr_gusbcfg);
+	device_remove_file(&dev->dev, &dev_attr_grxfsiz);
+	device_remove_file(&dev->dev, &dev_attr_gnptxfsiz);
+	device_remove_file(&dev->dev, &dev_attr_gpvndctl);
+	device_remove_file(&dev->dev, &dev_attr_ggpio);
+	device_remove_file(&dev->dev, &dev_attr_guid);
+	device_remove_file(&dev->dev, &dev_attr_gsnpsid);
+	device_remove_file(&dev->dev, &dev_attr_devspeed);
+	device_remove_file(&dev->dev, &dev_attr_enumspeed);
+	device_remove_file(&dev->dev, &dev_attr_hptxfsiz);
+	device_remove_file(&dev->dev, &dev_attr_hprt0);
+	device_remove_file(&dev->dev, &dev_attr_remote_wakeup);
+	device_remove_file(&dev->dev, &dev_attr_rem_wakeup_pwrdn);
+	device_remove_file(&dev->dev, &dev_attr_disconnect_us);
+	device_remove_file(&dev->dev, &dev_attr_regdump);
+	device_remove_file(&dev->dev, &dev_attr_spramdump);
+	device_remove_file(&dev->dev, &dev_attr_hcddump);
+	device_remove_file(&dev->dev, &dev_attr_hcd_frrem);
+	device_remove_file(&dev->dev, &dev_attr_rd_reg_test);
+	device_remove_file(&dev->dev, &dev_attr_wr_reg_test);
+#ifdef CONFIG_USB_DWC_OTG_LPM
+	device_remove_file(&dev->dev, &dev_attr_lpm_response);
+	device_remove_file(&dev->dev, &dev_attr_sleep_status);
+#endif
+}
diff --git a/drivers/usb/dwc_otg/dwc_otg_attr.h b/drivers/usb/dwc_otg/dwc_otg_attr.h
new file mode 100644
index 0000000..ab46f2e
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_otg_attr.h
@@ -0,0 +1,91 @@
+/* ==========================================================================
+ * $File: //dwh/usb_iip/dev/software/otg/linux/drivers/dwc_otg_attr.h $
+ * $Revision: #13 $
+ * $Date: 2010/06/21 $
+ * $Change: 1532021 $
+ *
+ * Synopsys HS OTG Linux Software Driver and documentation (hereinafter,
+ * "Software") is an Unsupported proprietary work of Synopsys, Inc. unless
+ * otherwise expressly agreed to in writing between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product under
+ * any End User Software License Agreement or Agreement for Licensed Product
+ * with Synopsys or any supplement thereto. You are permitted to use and
+ * redistribute this Software in source and binary forms, with or without
+ * modification, provided that redistributions of source code must retain this
+ * notice. You may not view, use, disclose, copy or distribute this file or
+ * any information contained herein except pursuant to this license grant from
+ * Synopsys. If you do not agree with this notice, including the disclaimer
+ * below, then you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS" BASIS
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE HEREBY DISCLAIMED. IN NO EVENT SHALL SYNOPSYS BE LIABLE FOR ANY DIRECT,
+ * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================== */
+
+#if !defined(__DWC_OTG_ATTR_H__)
+#define __DWC_OTG_ATTR_H__
+
+/** @file
+ * This file contains the interface to the Linux device attributes.
+ */
+extern struct device_attribute dev_attr_regoffset;
+extern struct device_attribute dev_attr_regvalue;
+
+extern struct device_attribute dev_attr_mode;
+extern struct device_attribute dev_attr_hnpcapable;
+extern struct device_attribute dev_attr_srpcapable;
+extern struct device_attribute dev_attr_hnp;
+extern struct device_attribute dev_attr_srp;
+extern struct device_attribute dev_attr_buspower;
+extern struct device_attribute dev_attr_bussuspend;
+extern struct device_attribute dev_attr_mode_ch_tim_en;
+extern struct device_attribute dev_attr_fr_interval;
+extern struct device_attribute dev_attr_busconnected;
+extern struct device_attribute dev_attr_gotgctl;
+extern struct device_attribute dev_attr_gusbcfg;
+extern struct device_attribute dev_attr_grxfsiz;
+extern struct device_attribute dev_attr_gnptxfsiz;
+extern struct device_attribute dev_attr_gpvndctl;
+extern struct device_attribute dev_attr_ggpio;
+extern struct device_attribute dev_attr_guid;
+extern struct device_attribute dev_attr_gsnpsid;
+extern struct device_attribute dev_attr_devspeed;
+extern struct device_attribute dev_attr_enumspeed;
+extern struct device_attribute dev_attr_hptxfsiz;
+extern struct device_attribute dev_attr_hprt0;
+#ifdef CONFIG_USB_DWC_OTG_LPM
+extern struct device_attribute dev_attr_lpm_response;
+extern struct device_attribute devi_attr_sleep_status;
+#endif
+
+void dwc_otg_attr_create(
+#ifdef LM_INTERFACE
+				struct lm_device *dev
+#elif defined(PCI_INTERFACE)
+				struct pci_dev *dev
+#else
+				//struct device *dev
+				struct platform_device *dev
+#endif
+    );
+
+void dwc_otg_attr_remove(
+#ifdef LM_INTERFACE
+				struct lm_device *dev
+#elif defined(PCI_INTERFACE)
+				struct pci_dev *dev
+#else
+				//struct device *dev
+				struct platform_device *dev
+#endif
+    );
+#endif
diff --git a/drivers/usb/dwc_otg/dwc_otg_cfi.c b/drivers/usb/dwc_otg/dwc_otg_cfi.c
new file mode 100644
index 0000000..0956e8e
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_otg_cfi.c
@@ -0,0 +1,1877 @@
+/* ==========================================================================
+ * Synopsys HS OTG Linux Software Driver and documentation (hereinafter,
+ * "Software") is an Unsupported proprietary work of Synopsys, Inc. unless
+ * otherwise expressly agreed to in writing between Synopsys and you.
+ * 
+ * The Software IS NOT an item of Licensed Software or Licensed Product under
+ * any End User Software License Agreement or Agreement for Licensed Product
+ * with Synopsys or any supplement thereto. You are permitted to use and
+ * redistribute this Software in source and binary forms, with or without
+ * modification, provided that redistributions of source code must retain this
+ * notice. You may not view, use, disclose, copy or distribute this file or
+ * any information contained herein except pursuant to this license grant from
+ * Synopsys. If you do not agree with this notice, including the disclaimer
+ * below, then you are not authorized to use the Software.
+ * 
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS" BASIS
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE HEREBY DISCLAIMED. IN NO EVENT SHALL SYNOPSYS BE LIABLE FOR ANY DIRECT,
+ * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================== */
+
+/** @file 
+ *
+ * This file contains the most of the CFI(Core Feature Interface) 
+ * implementation for the OTG. 
+ */
+
+#ifdef DWC_UTE_CFI
+
+#include "dwc_otg_pcd.h"
+#include "dwc_otg_cfi.h"
+
+/** This definition should actually migrate to the Portability Library */
+#define DWC_CONSTANT_CPU_TO_LE16(x) (x)
+
+extern dwc_otg_pcd_ep_t *get_ep_by_addr(dwc_otg_pcd_t * pcd, u16 wIndex);
+
+static int cfi_core_features_buf(uint8_t * buf, uint16_t buflen);
+static int cfi_get_feature_value(uint8_t * buf, uint16_t buflen,
+				 struct dwc_otg_pcd *pcd,
+				 struct cfi_usb_ctrlrequest *ctrl_req);
+static int cfi_set_feature_value(struct dwc_otg_pcd *pcd);
+static int cfi_ep_get_sg_val(uint8_t * buf, struct dwc_otg_pcd *pcd,
+			     struct cfi_usb_ctrlrequest *req);
+static int cfi_ep_get_concat_val(uint8_t * buf, struct dwc_otg_pcd *pcd,
+				 struct cfi_usb_ctrlrequest *req);
+static int cfi_ep_get_align_val(uint8_t * buf, struct dwc_otg_pcd *pcd,
+				struct cfi_usb_ctrlrequest *req);
+static int cfi_preproc_reset(struct dwc_otg_pcd *pcd,
+			     struct cfi_usb_ctrlrequest *req);
+static void cfi_free_ep_bs_dyn_data(cfi_ep_t * cfiep);
+
+static uint16_t get_dfifo_size(dwc_otg_core_if_t * core_if);
+static int32_t get_rxfifo_size(dwc_otg_core_if_t * core_if, uint16_t wValue);
+static int32_t get_txfifo_size(struct dwc_otg_pcd *pcd, uint16_t wValue);
+
+static uint8_t resize_fifos(dwc_otg_core_if_t * core_if);
+
+/** This is the header of the all features descriptor */
+static cfi_all_features_header_t all_props_desc_header = {
+	.wVersion = DWC_CONSTANT_CPU_TO_LE16(0x100),
+	.wCoreID = DWC_CONSTANT_CPU_TO_LE16(CFI_CORE_ID_OTG),
+	.wNumFeatures = DWC_CONSTANT_CPU_TO_LE16(9),
+};
+
+/** This is an array of statically allocated feature descriptors */
+static cfi_feature_desc_header_t prop_descs[] = {
+
+	/* FT_ID_DMA_MODE */
+	{
+	 .wFeatureID = DWC_CONSTANT_CPU_TO_LE16(FT_ID_DMA_MODE),
+	 .bmAttributes = CFI_FEATURE_ATTR_RW,
+	 .wDataLength = DWC_CONSTANT_CPU_TO_LE16(1),
+	 },
+
+	/* FT_ID_DMA_BUFFER_SETUP */
+	{
+	 .wFeatureID = DWC_CONSTANT_CPU_TO_LE16(FT_ID_DMA_BUFFER_SETUP),
+	 .bmAttributes = CFI_FEATURE_ATTR_RW,
+	 .wDataLength = DWC_CONSTANT_CPU_TO_LE16(6),
+	 },
+
+	/* FT_ID_DMA_BUFF_ALIGN */
+	{
+	 .wFeatureID = DWC_CONSTANT_CPU_TO_LE16(FT_ID_DMA_BUFF_ALIGN),
+	 .bmAttributes = CFI_FEATURE_ATTR_RW,
+	 .wDataLength = DWC_CONSTANT_CPU_TO_LE16(2),
+	 },
+
+	/* FT_ID_DMA_CONCAT_SETUP */
+	{
+	 .wFeatureID = DWC_CONSTANT_CPU_TO_LE16(FT_ID_DMA_CONCAT_SETUP),
+	 .bmAttributes = CFI_FEATURE_ATTR_RW,
+	 //.wDataLength  = DWC_CONSTANT_CPU_TO_LE16(6),
+	 },
+
+	/* FT_ID_DMA_CIRCULAR */
+	{
+	 .wFeatureID = DWC_CONSTANT_CPU_TO_LE16(FT_ID_DMA_CIRCULAR),
+	 .bmAttributes = CFI_FEATURE_ATTR_RW,
+	 .wDataLength = DWC_CONSTANT_CPU_TO_LE16(6),
+	 },
+
+	/* FT_ID_THRESHOLD_SETUP */
+	{
+	 .wFeatureID = DWC_CONSTANT_CPU_TO_LE16(FT_ID_THRESHOLD_SETUP),
+	 .bmAttributes = CFI_FEATURE_ATTR_RW,
+	 .wDataLength = DWC_CONSTANT_CPU_TO_LE16(6),
+	 },
+
+	/* FT_ID_DFIFO_DEPTH */
+	{
+	 .wFeatureID = DWC_CONSTANT_CPU_TO_LE16(FT_ID_DFIFO_DEPTH),
+	 .bmAttributes = CFI_FEATURE_ATTR_RO,
+	 .wDataLength = DWC_CONSTANT_CPU_TO_LE16(2),
+	 },
+
+	/* FT_ID_TX_FIFO_DEPTH */
+	{
+	 .wFeatureID = DWC_CONSTANT_CPU_TO_LE16(FT_ID_TX_FIFO_DEPTH),
+	 .bmAttributes = CFI_FEATURE_ATTR_RW,
+	 .wDataLength = DWC_CONSTANT_CPU_TO_LE16(2),
+	 },
+
+	/* FT_ID_RX_FIFO_DEPTH */
+	{
+	 .wFeatureID = DWC_CONSTANT_CPU_TO_LE16(FT_ID_RX_FIFO_DEPTH),
+	 .bmAttributes = CFI_FEATURE_ATTR_RW,
+	 .wDataLength = DWC_CONSTANT_CPU_TO_LE16(2),
+	 }
+};
+
+/** The table of feature names */
+cfi_string_t prop_name_table[] = {
+	{FT_ID_DMA_MODE, "dma_mode"},
+	{FT_ID_DMA_BUFFER_SETUP, "buffer_setup"},
+	{FT_ID_DMA_BUFF_ALIGN, "buffer_align"},
+	{FT_ID_DMA_CONCAT_SETUP, "concat_setup"},
+	{FT_ID_DMA_CIRCULAR, "buffer_circular"},
+	{FT_ID_THRESHOLD_SETUP, "threshold_setup"},
+	{FT_ID_DFIFO_DEPTH, "dfifo_depth"},
+	{FT_ID_TX_FIFO_DEPTH, "txfifo_depth"},
+	{FT_ID_RX_FIFO_DEPTH, "rxfifo_depth"},
+	{}
+};
+
+/************************************************************************/
+
+/** 
+ * Returns the name of the feature by its ID 
+ * or NULL if no featute ID matches.
+ * 
+ */
+const uint8_t *get_prop_name(uint16_t prop_id, int *len)
+{
+	cfi_string_t *pstr;
+	*len = 0;
+
+	for (pstr = prop_name_table; pstr && pstr->s; pstr++) {
+		if (pstr->id == prop_id) {
+			*len = DWC_STRLEN(pstr->s);
+			return pstr->s;
+		}
+	}
+	return NULL;
+}
+
+/**
+ * This function handles all CFI specific control requests.
+ * 
+ * Return a negative value to stall the DCE.
+ */
+int cfi_setup(struct dwc_otg_pcd *pcd, struct cfi_usb_ctrlrequest *ctrl)
+{
+	int retval = 0;
+	dwc_otg_pcd_ep_t *ep = NULL;
+	cfiobject_t *cfi = pcd->cfi;
+	struct dwc_otg_core_if *coreif = GET_CORE_IF(pcd);
+	uint16_t wLen = DWC_LE16_TO_CPU(&ctrl->wLength);
+	uint16_t wValue = DWC_LE16_TO_CPU(&ctrl->wValue);
+	uint16_t wIndex = DWC_LE16_TO_CPU(&ctrl->wIndex);
+	uint32_t regaddr = 0;
+	uint32_t regval = 0;
+
+	/* Save this Control Request in the CFI object. 
+	 * The data field will be assigned in the data stage completion CB function.
+	 */
+	cfi->ctrl_req = *ctrl;
+	cfi->ctrl_req.data = NULL;
+
+	cfi->need_gadget_att = 0;
+	cfi->need_status_in_complete = 0;
+
+	switch (ctrl->bRequest) {
+	case VEN_CORE_GET_FEATURES:
+		retval = cfi_core_features_buf(cfi->buf_in.buf, CFI_IN_BUF_LEN);
+		if (retval >= 0) {
+			//dump_msg(cfi->buf_in.buf, retval);
+			ep = &pcd->ep0;
+
+			retval = min((uint16_t) retval, wLen);
+			/* Transfer this buffer to the host through the EP0-IN EP */
+			ep->dwc_ep.dma_addr = cfi->buf_in.addr;
+			ep->dwc_ep.start_xfer_buff = cfi->buf_in.buf;
+			ep->dwc_ep.xfer_buff = cfi->buf_in.buf;
+			ep->dwc_ep.xfer_len = retval;
+			ep->dwc_ep.xfer_count = 0;
+			ep->dwc_ep.sent_zlp = 0;
+			ep->dwc_ep.total_len = ep->dwc_ep.xfer_len;
+
+			pcd->ep0_pending = 1;
+			dwc_otg_ep0_start_transfer(coreif, &ep->dwc_ep);
+		}
+		retval = 0;
+		break;
+
+	case VEN_CORE_GET_FEATURE:
+		CFI_INFO("VEN_CORE_GET_FEATURE\n");
+		retval = cfi_get_feature_value(cfi->buf_in.buf, CFI_IN_BUF_LEN,
+					       pcd, ctrl);
+		if (retval >= 0) {
+			ep = &pcd->ep0;
+
+			retval = min((uint16_t) retval, wLen);
+			/* Transfer this buffer to the host through the EP0-IN EP */
+			ep->dwc_ep.dma_addr = cfi->buf_in.addr;
+			ep->dwc_ep.start_xfer_buff = cfi->buf_in.buf;
+			ep->dwc_ep.xfer_buff = cfi->buf_in.buf;
+			ep->dwc_ep.xfer_len = retval;
+			ep->dwc_ep.xfer_count = 0;
+			ep->dwc_ep.sent_zlp = 0;
+			ep->dwc_ep.total_len = ep->dwc_ep.xfer_len;
+
+			pcd->ep0_pending = 1;
+			dwc_otg_ep0_start_transfer(coreif, &ep->dwc_ep);
+		}
+		CFI_INFO("VEN_CORE_GET_FEATURE=%d\n", retval);
+		dump_msg(cfi->buf_in.buf, retval);
+		break;
+
+	case VEN_CORE_SET_FEATURE:
+		CFI_INFO("VEN_CORE_SET_FEATURE\n");
+		/* Set up an XFER to get the data stage of the control request,
+		 * which is the new value of the feature to be modified.
+		 */
+		ep = &pcd->ep0;
+		ep->dwc_ep.is_in = 0;
+		ep->dwc_ep.dma_addr = cfi->buf_out.addr;
+		ep->dwc_ep.start_xfer_buff = cfi->buf_out.buf;
+		ep->dwc_ep.xfer_buff = cfi->buf_out.buf;
+		ep->dwc_ep.xfer_len = wLen;
+		ep->dwc_ep.xfer_count = 0;
+		ep->dwc_ep.sent_zlp = 0;
+		ep->dwc_ep.total_len = ep->dwc_ep.xfer_len;
+
+		pcd->ep0_pending = 1;
+		/* Read the control write's data stage */
+		dwc_otg_ep0_start_transfer(coreif, &ep->dwc_ep);
+		retval = 0;
+		break;
+
+	case VEN_CORE_RESET_FEATURES:
+		CFI_INFO("VEN_CORE_RESET_FEATURES\n");
+		cfi->need_gadget_att = 1;
+		cfi->need_status_in_complete = 1;
+		retval = cfi_preproc_reset(pcd, ctrl);
+		CFI_INFO("VEN_CORE_RESET_FEATURES = (%d)\n", retval);
+		break;
+
+	case VEN_CORE_ACTIVATE_FEATURES:
+		CFI_INFO("VEN_CORE_ACTIVATE_FEATURES\n");
+		break;
+
+	case VEN_CORE_READ_REGISTER:
+		CFI_INFO("VEN_CORE_READ_REGISTER\n");
+		/* wValue optionally contains the HI WORD of the register offset and
+		 * wIndex contains the LOW WORD of the register offset 
+		 */
+		if (wValue == 0) {
+			/* @TODO - MAS - fix the access to the base field */
+			regaddr = 0;
+			//regaddr = (uint32_t) pcd->otg_dev->os_dep.base;
+			//GET_CORE_IF(pcd)->co
+			regaddr |= wIndex;
+		} else {
+			regaddr = (wValue << 16) | wIndex;
+		}
+
+		/* Read a 32-bit value of the memory at the regaddr */
+		regval = DWC_READ_REG32((uint32_t *) regaddr);
+
+		ep = &pcd->ep0;
+		dwc_memcpy(cfi->buf_in.buf, &regval, sizeof(uint32_t));
+		ep->dwc_ep.is_in = 1;
+		ep->dwc_ep.dma_addr = cfi->buf_in.addr;
+		ep->dwc_ep.start_xfer_buff = cfi->buf_in.buf;
+		ep->dwc_ep.xfer_buff = cfi->buf_in.buf;
+		ep->dwc_ep.xfer_len = wLen;
+		ep->dwc_ep.xfer_count = 0;
+		ep->dwc_ep.sent_zlp = 0;
+		ep->dwc_ep.total_len = ep->dwc_ep.xfer_len;
+
+		pcd->ep0_pending = 1;
+		dwc_otg_ep0_start_transfer(coreif, &ep->dwc_ep);
+		cfi->need_gadget_att = 0;
+		retval = 0;
+		break;
+
+	case VEN_CORE_WRITE_REGISTER:
+		CFI_INFO("VEN_CORE_WRITE_REGISTER\n");
+		/* Set up an XFER to get the data stage of the control request,
+		 * which is the new value of the register to be modified.
+		 */
+		ep = &pcd->ep0;
+		ep->dwc_ep.is_in = 0;
+		ep->dwc_ep.dma_addr = cfi->buf_out.addr;
+		ep->dwc_ep.start_xfer_buff = cfi->buf_out.buf;
+		ep->dwc_ep.xfer_buff = cfi->buf_out.buf;
+		ep->dwc_ep.xfer_len = wLen;
+		ep->dwc_ep.xfer_count = 0;
+		ep->dwc_ep.sent_zlp = 0;
+		ep->dwc_ep.total_len = ep->dwc_ep.xfer_len;
+
+		pcd->ep0_pending = 1;
+		/* Read the control write's data stage */
+		dwc_otg_ep0_start_transfer(coreif, &ep->dwc_ep);
+		retval = 0;
+		break;
+
+	default:
+		retval = -DWC_E_NOT_SUPPORTED;
+		break;
+	}
+
+	return retval;
+}
+
+/**
+ * This function prepares the core features descriptors and copies its
+ * raw representation into the buffer <buf>.
+ * 
+ * The buffer structure is as follows:
+ *	all_features_header (8 bytes)
+ *	features_#1 (8 bytes + feature name string length)
+ *	features_#2 (8 bytes + feature name string length)
+ *	.....
+ *	features_#n - where n=the total count of feature descriptors
+ */
+static int cfi_core_features_buf(uint8_t * buf, uint16_t buflen)
+{
+	cfi_feature_desc_header_t *prop_hdr = prop_descs;
+	cfi_feature_desc_header_t *prop;
+	cfi_all_features_header_t *all_props_hdr = &all_props_desc_header;
+	cfi_all_features_header_t *tmp;
+	uint8_t *tmpbuf = buf;
+	const uint8_t *pname = NULL;
+	int i, j, namelen = 0, totlen;
+
+	/* Prepare and copy the core features into the buffer */
+	CFI_INFO("%s:\n", __func__);
+
+	tmp = (cfi_all_features_header_t *) tmpbuf;
+	*tmp = *all_props_hdr;
+	tmpbuf += CFI_ALL_FEATURES_HDR_LEN;
+
+	j = sizeof(prop_descs) / sizeof(cfi_all_features_header_t);
+	for (i = 0; i < j; i++, prop_hdr++) {
+		pname = get_prop_name(prop_hdr->wFeatureID, &namelen);
+		prop = (cfi_feature_desc_header_t *) tmpbuf;
+		*prop = *prop_hdr;
+
+		prop->bNameLen = namelen;
+		prop->wLength =
+		    DWC_CONSTANT_CPU_TO_LE16(CFI_FEATURE_DESC_HDR_LEN +
+					     namelen);
+
+		tmpbuf += CFI_FEATURE_DESC_HDR_LEN;
+		dwc_memcpy(tmpbuf, pname, namelen);
+		tmpbuf += namelen;
+	}
+
+	totlen = tmpbuf - buf;
+
+	if (totlen > 0) {
+		tmp = (cfi_all_features_header_t *) buf;
+		tmp->wTotalLen = DWC_CONSTANT_CPU_TO_LE16(totlen);
+	}
+
+	return totlen;
+}
+
+/**
+ * This function releases all the dynamic memory in the CFI object.
+ */
+static void cfi_release(cfiobject_t * cfiobj)
+{
+	cfi_ep_t *cfiep;
+	dwc_list_link_t *tmp;
+
+	CFI_INFO("%s\n", __func__);
+
+	if (cfiobj->buf_in.buf) {
+		DWC_DMA_FREE(CFI_IN_BUF_LEN, cfiobj->buf_in.buf,
+			     cfiobj->buf_in.addr);
+		cfiobj->buf_in.buf = NULL;
+	}
+
+	if (cfiobj->buf_out.buf) {
+		DWC_DMA_FREE(CFI_OUT_BUF_LEN, cfiobj->buf_out.buf,
+			     cfiobj->buf_out.addr);
+		cfiobj->buf_out.buf = NULL;
+	}
+
+	/* Free the Buffer Setup values for each EP */
+	//list_for_each_entry(cfiep, &cfiobj->active_eps, lh) {
+	DWC_LIST_FOREACH(tmp, &cfiobj->active_eps) {
+		cfiep = DWC_LIST_ENTRY(tmp, struct cfi_ep, lh);
+		cfi_free_ep_bs_dyn_data(cfiep);
+	}
+}
+
+/**
+ * This function frees the dynamically allocated EP buffer setup data.
+ */
+static void cfi_free_ep_bs_dyn_data(cfi_ep_t * cfiep)
+{
+	if (cfiep->bm_sg) {
+		DWC_FREE(cfiep->bm_sg);
+		cfiep->bm_sg = NULL;
+	}
+
+	if (cfiep->bm_align) {
+		DWC_FREE(cfiep->bm_align);
+		cfiep->bm_align = NULL;
+	}
+
+	if (cfiep->bm_concat) {
+		if (NULL != cfiep->bm_concat->wTxBytes) {
+			DWC_FREE(cfiep->bm_concat->wTxBytes);
+			cfiep->bm_concat->wTxBytes = NULL;
+		}
+		DWC_FREE(cfiep->bm_concat);
+		cfiep->bm_concat = NULL;
+	}
+}
+
+/**
+ * This function initializes the default values of the features
+ * for a specific endpoint and should be called only once when
+ * the EP is enabled first time.
+ */
+static int cfi_ep_init_defaults(struct dwc_otg_pcd *pcd, cfi_ep_t * cfiep)
+{
+	int retval = 0;
+
+	cfiep->bm_sg = DWC_ALLOC(sizeof(ddma_sg_buffer_setup_t));
+	if (NULL == cfiep->bm_sg) {
+		CFI_INFO("Failed to allocate memory for SG feature value\n");
+		return -DWC_E_NO_MEMORY;
+	}
+	dwc_memset(cfiep->bm_sg, 0, sizeof(ddma_sg_buffer_setup_t));
+
+	/* For the Concatenation feature's default value we do not allocate
+	 * memory for the wTxBytes field - it will be done in the set_feature_value
+	 * request handler.
+	 */
+	cfiep->bm_concat = DWC_ALLOC(sizeof(ddma_concat_buffer_setup_t));
+	if (NULL == cfiep->bm_concat) {
+		CFI_INFO
+		    ("Failed to allocate memory for CONCATENATION feature value\n");
+		DWC_FREE(cfiep->bm_sg);
+		return -DWC_E_NO_MEMORY;
+	}
+	dwc_memset(cfiep->bm_concat, 0, sizeof(ddma_concat_buffer_setup_t));
+
+	cfiep->bm_align = DWC_ALLOC(sizeof(ddma_align_buffer_setup_t));
+	if (NULL == cfiep->bm_align) {
+		CFI_INFO
+		    ("Failed to allocate memory for Alignment feature value\n");
+		DWC_FREE(cfiep->bm_sg);
+		DWC_FREE(cfiep->bm_concat);
+		return -DWC_E_NO_MEMORY;
+	}
+	dwc_memset(cfiep->bm_align, 0, sizeof(ddma_align_buffer_setup_t));
+
+	return retval;
+}
+
+/**
+ * The callback function that notifies the CFI on the activation of
+ * an endpoint in the PCD. The following steps are done in this function:
+ *
+ *	Create a dynamically allocated cfi_ep_t object (a CFI wrapper to the PCD's 
+ *		active endpoint)
+ *	Create MAX_DMA_DESCS_PER_EP count DMA Descriptors for the EP
+ *	Set the Buffer Mode to standard
+ *	Initialize the default values for all EP modes (SG, Circular, Concat, Align)
+ *	Add the cfi_ep_t object to the list of active endpoints in the CFI object
+ */
+static int cfi_ep_enable(struct cfiobject *cfi, struct dwc_otg_pcd *pcd,
+			 struct dwc_otg_pcd_ep *ep)
+{
+	cfi_ep_t *cfiep;
+	int retval = -DWC_E_NOT_SUPPORTED;
+
+	CFI_INFO("%s: epname=%s; epnum=0x%02x\n", __func__,
+		 "EP_" /*ep->ep.name */ , ep->desc->bEndpointAddress);
+	/* MAS - Check whether this endpoint already is in the list */
+	cfiep = get_cfi_ep_by_pcd_ep(cfi, ep);
+
+	if (NULL == cfiep) {
+		/* Allocate a cfi_ep_t object */
+		cfiep = DWC_ALLOC(sizeof(cfi_ep_t));
+		if (NULL == cfiep) {
+			CFI_INFO
+			    ("Unable to allocate memory for <cfiep> in function %s\n",
+			     __func__);
+			return -DWC_E_NO_MEMORY;
+		}
+		dwc_memset(cfiep, 0, sizeof(cfi_ep_t));
+
+		/* Save the dwc_otg_pcd_ep pointer in the cfiep object */
+		cfiep->ep = ep;
+
+		/* Allocate the DMA Descriptors chain of MAX_DMA_DESCS_PER_EP count */
+		ep->dwc_ep.descs =
+		    DWC_DMA_ALLOC(MAX_DMA_DESCS_PER_EP *
+				  sizeof(dwc_otg_dma_desc_t),
+				  &ep->dwc_ep.descs_dma_addr);
+
+		if (NULL == ep->dwc_ep.descs) {
+			DWC_FREE(cfiep);
+			return -DWC_E_NO_MEMORY;
+		}
+
+		DWC_LIST_INIT(&cfiep->lh);
+
+		/* Set the buffer mode to BM_STANDARD. It will be modified 
+		 * when building descriptors for a specific buffer mode */
+		ep->dwc_ep.buff_mode = BM_STANDARD;
+
+		/* Create and initialize the default values for this EP's Buffer modes */
+		if ((retval = cfi_ep_init_defaults(pcd, cfiep)) < 0)
+			return retval;
+
+		/* Add the cfi_ep_t object to the CFI object's list of active endpoints */
+		DWC_LIST_INSERT_TAIL(&cfi->active_eps, &cfiep->lh);
+		retval = 0;
+	} else {		/* The sought EP already is in the list */
+		CFI_INFO("%s: The sought EP already is in the list\n",
+			 __func__);
+	}
+
+	return retval;
+}
+
+/**
+ * This function is called when the data stage of a 3-stage Control Write request
+ * is complete.
+ * 
+ */
+static int cfi_ctrl_write_complete(struct cfiobject *cfi,
+				   struct dwc_otg_pcd *pcd)
+{
+	uint32_t addr, reg_value;
+	uint16_t wIndex, wValue;
+	uint8_t bRequest;
+	uint8_t *buf = cfi->buf_out.buf;
+	//struct usb_ctrlrequest *ctrl_req = &cfi->ctrl_req_saved;
+	struct cfi_usb_ctrlrequest *ctrl_req = &cfi->ctrl_req;
+	int retval = -DWC_E_NOT_SUPPORTED;
+
+	CFI_INFO("%s\n", __func__);
+
+	bRequest = ctrl_req->bRequest;
+	wIndex = DWC_CONSTANT_CPU_TO_LE16(ctrl_req->wIndex);
+	wValue = DWC_CONSTANT_CPU_TO_LE16(ctrl_req->wValue);
+
+	/* 
+	 * Save the pointer to the data stage in the ctrl_req's <data> field.
+	 * The request should be already saved in the command stage by now.
+	 */
+	ctrl_req->data = cfi->buf_out.buf;
+	cfi->need_status_in_complete = 0;
+	cfi->need_gadget_att = 0;
+
+	switch (bRequest) {
+	case VEN_CORE_WRITE_REGISTER:
+		/* The buffer contains raw data of the new value for the register */
+		reg_value = *((uint32_t *) buf);
+		if (wValue == 0) {
+			addr = 0;
+			//addr = (uint32_t) pcd->otg_dev->os_dep.base;
+			addr += wIndex;
+		} else {
+			addr = (wValue << 16) | wIndex;
+		}
+
+		//writel(reg_value, addr);
+
+		retval = 0;
+		cfi->need_status_in_complete = 1;
+		break;
+
+	case VEN_CORE_SET_FEATURE:
+		/* The buffer contains raw data of the new value of the feature */
+		retval = cfi_set_feature_value(pcd);
+		if (retval < 0)
+			return retval;
+
+		cfi->need_status_in_complete = 1;
+		break;
+
+	default:
+		break;
+	}
+
+	return retval;
+}
+
+/**
+ * This function builds the DMA descriptors for the SG buffer mode.
+ */
+static void cfi_build_sg_descs(struct cfiobject *cfi, cfi_ep_t * cfiep,
+			       dwc_otg_pcd_request_t * req)
+{
+	struct dwc_otg_pcd_ep *ep = cfiep->ep;
+	ddma_sg_buffer_setup_t *sgval = cfiep->bm_sg;
+	struct dwc_otg_dma_desc *desc = cfiep->ep->dwc_ep.descs;
+	struct dwc_otg_dma_desc *desc_last = cfiep->ep->dwc_ep.descs;
+	dma_addr_t buff_addr = req->dma;
+	int i;
+	uint32_t txsize, off;
+
+	txsize = sgval->wSize;
+	off = sgval->bOffset;
+
+//      CFI_INFO("%s: %s TXSIZE=0x%08x; OFFSET=0x%08x\n", 
+//              __func__, cfiep->ep->ep.name, txsize, off);
+
+	for (i = 0; i < sgval->bCount; i++) {
+		desc->status.b.bs = BS_HOST_BUSY;
+		desc->buf = buff_addr;
+		desc->status.b.l = 0;
+		desc->status.b.ioc = 0;
+		desc->status.b.sp = 0;
+		desc->status.b.bytes = txsize;
+		desc->status.b.bs = BS_HOST_READY;
+
+		/* Set the next address of the buffer */
+		buff_addr += txsize + off;
+		desc_last = desc;
+		desc++;
+	}
+
+	/* Set the last, ioc and sp bits on the Last DMA Descriptor */
+	desc_last->status.b.l = 1;
+	desc_last->status.b.ioc = 1;
+	desc_last->status.b.sp = ep->dwc_ep.sent_zlp;
+	/* Save the last DMA descriptor pointer */
+	cfiep->dma_desc_last = desc_last;
+	cfiep->desc_count = sgval->bCount;
+}
+
+/**
+ * This function builds the DMA descriptors for the Concatenation buffer mode.
+ */
+static void cfi_build_concat_descs(struct cfiobject *cfi, cfi_ep_t * cfiep,
+				   dwc_otg_pcd_request_t * req)
+{
+	struct dwc_otg_pcd_ep *ep = cfiep->ep;
+	ddma_concat_buffer_setup_t *concatval = cfiep->bm_concat;
+	struct dwc_otg_dma_desc *desc = cfiep->ep->dwc_ep.descs;
+	struct dwc_otg_dma_desc *desc_last = cfiep->ep->dwc_ep.descs;
+	dma_addr_t buff_addr = req->dma;
+	int i;
+	uint16_t *txsize;
+
+	txsize = concatval->wTxBytes;
+
+	for (i = 0; i < concatval->hdr.bDescCount; i++) {
+		desc->buf = buff_addr;
+		desc->status.b.bs = BS_HOST_BUSY;
+		desc->status.b.l = 0;
+		desc->status.b.ioc = 0;
+		desc->status.b.sp = 0;
+		desc->status.b.bytes = *txsize;
+		desc->status.b.bs = BS_HOST_READY;
+
+		txsize++;
+		/* Set the next address of the buffer */
+		buff_addr += UGETW(ep->desc->wMaxPacketSize);
+		desc_last = desc;
+		desc++;
+	}
+
+	/* Set the last, ioc and sp bits on the Last DMA Descriptor */
+	desc_last->status.b.l = 1;
+	desc_last->status.b.ioc = 1;
+	desc_last->status.b.sp = ep->dwc_ep.sent_zlp;
+	cfiep->dma_desc_last = desc_last;
+	cfiep->desc_count = concatval->hdr.bDescCount;
+}
+
+/**
+ * This function builds the DMA descriptors for the Circular buffer mode
+ */
+static void cfi_build_circ_descs(struct cfiobject *cfi, cfi_ep_t * cfiep,
+				 dwc_otg_pcd_request_t * req)
+{
+	/* @todo: MAS - add implementation when this feature needs to be tested */
+}
+
+/**
+ * This function builds the DMA descriptors for the Alignment buffer mode
+ */
+static void cfi_build_align_descs(struct cfiobject *cfi, cfi_ep_t * cfiep,
+				  dwc_otg_pcd_request_t * req)
+{
+	struct dwc_otg_pcd_ep *ep = cfiep->ep;
+	ddma_align_buffer_setup_t *alignval = cfiep->bm_align;
+	struct dwc_otg_dma_desc *desc = cfiep->ep->dwc_ep.descs;
+	dma_addr_t buff_addr = req->dma;
+
+	desc->status.b.bs = BS_HOST_BUSY;
+	desc->status.b.l = 1;
+	desc->status.b.ioc = 1;
+	desc->status.b.sp = ep->dwc_ep.sent_zlp;
+	desc->status.b.bytes = req->length;
+	/* Adjust the buffer alignment */
+	desc->buf = (buff_addr + alignval->bAlign);
+	desc->status.b.bs = BS_HOST_READY;
+	cfiep->dma_desc_last = desc;
+	cfiep->desc_count = 1;
+}
+
+/**
+ * This function builds the DMA descriptors chain for different modes of the
+ * buffer setup of an endpoint.
+ */
+static void cfi_build_descriptors(struct cfiobject *cfi,
+				  struct dwc_otg_pcd *pcd,
+				  struct dwc_otg_pcd_ep *ep,
+				  dwc_otg_pcd_request_t * req)
+{
+	cfi_ep_t *cfiep;
+
+	/* Get the cfiep by the dwc_otg_pcd_ep */
+	cfiep = get_cfi_ep_by_pcd_ep(cfi, ep);
+	if (NULL == cfiep) {
+		CFI_INFO("%s: Unable to find a matching active endpoint\n",
+			 __func__);
+		return;
+	}
+
+	cfiep->xfer_len = req->length;
+
+	/* Iterate through all the DMA descriptors */
+	switch (cfiep->ep->dwc_ep.buff_mode) {
+	case BM_SG:
+		cfi_build_sg_descs(cfi, cfiep, req);
+		break;
+
+	case BM_CONCAT:
+		cfi_build_concat_descs(cfi, cfiep, req);
+		break;
+
+	case BM_CIRCULAR:
+		cfi_build_circ_descs(cfi, cfiep, req);
+		break;
+
+	case BM_ALIGN:
+		cfi_build_align_descs(cfi, cfiep, req);
+		break;
+
+	default:
+		break;
+	}
+}
+
+/**
+ * Allocate DMA buffer for different Buffer modes.
+ */
+static void *cfi_ep_alloc_buf(struct cfiobject *cfi, struct dwc_otg_pcd *pcd,
+			      struct dwc_otg_pcd_ep *ep, dma_addr_t * dma,
+			      unsigned size, gfp_t flags)
+{
+	return DWC_DMA_ALLOC(size, dma);
+}
+
+/**
+ * This function initializes the CFI object.
+ */
+int init_cfi(cfiobject_t * cfiobj)
+{
+	CFI_INFO("%s\n", __func__);
+
+	/* Allocate a buffer for IN XFERs */
+	cfiobj->buf_in.buf =
+	    DWC_DMA_ALLOC(CFI_IN_BUF_LEN, &cfiobj->buf_in.addr);
+	if (NULL == cfiobj->buf_in.buf) {
+		CFI_INFO("Unable to allocate buffer for INs\n");
+		return -DWC_E_NO_MEMORY;
+	}
+
+	/* Allocate a buffer for OUT XFERs */
+	cfiobj->buf_out.buf =
+	    DWC_DMA_ALLOC(CFI_OUT_BUF_LEN, &cfiobj->buf_out.addr);
+	if (NULL == cfiobj->buf_out.buf) {
+		CFI_INFO("Unable to allocate buffer for OUT\n");
+		return -DWC_E_NO_MEMORY;
+	}
+
+	/* Initialize the callback function pointers */
+	cfiobj->ops.release = cfi_release;
+	cfiobj->ops.ep_enable = cfi_ep_enable;
+	cfiobj->ops.ctrl_write_complete = cfi_ctrl_write_complete;
+	cfiobj->ops.build_descriptors = cfi_build_descriptors;
+	cfiobj->ops.ep_alloc_buf = cfi_ep_alloc_buf;
+
+	/* Initialize the list of active endpoints in the CFI object */
+	DWC_LIST_INIT(&cfiobj->active_eps);
+
+	return 0;
+}
+
+/**
+ * This function reads the required feature's current value into the buffer
+ *
+ * @retval: Returns negative as error, or the data length of the feature  
+ */
+static int cfi_get_feature_value(uint8_t * buf, uint16_t buflen,
+				 struct dwc_otg_pcd *pcd,
+				 struct cfi_usb_ctrlrequest *ctrl_req)
+{
+	int retval = -DWC_E_NOT_SUPPORTED;
+	struct dwc_otg_core_if *coreif = GET_CORE_IF(pcd);
+	uint16_t dfifo, rxfifo, txfifo;
+
+	switch (ctrl_req->wIndex) {
+		/* Whether the DDMA is enabled or not */
+	case FT_ID_DMA_MODE:
+		*buf = (coreif->dma_enable && coreif->dma_desc_enable) ? 1 : 0;
+		retval = 1;
+		break;
+
+	case FT_ID_DMA_BUFFER_SETUP:
+		retval = cfi_ep_get_sg_val(buf, pcd, ctrl_req);
+		break;
+
+	case FT_ID_DMA_BUFF_ALIGN:
+		retval = cfi_ep_get_align_val(buf, pcd, ctrl_req);
+		break;
+
+	case FT_ID_DMA_CONCAT_SETUP:
+		retval = cfi_ep_get_concat_val(buf, pcd, ctrl_req);
+		break;
+
+	case FT_ID_DMA_CIRCULAR:
+		CFI_INFO("GetFeature value (FT_ID_DMA_CIRCULAR)\n");
+		break;
+
+	case FT_ID_THRESHOLD_SETUP:
+		CFI_INFO("GetFeature value (FT_ID_THRESHOLD_SETUP)\n");
+		break;
+
+	case FT_ID_DFIFO_DEPTH:
+		dfifo = get_dfifo_size(coreif);
+		*((uint16_t *) buf) = dfifo;
+		retval = sizeof(uint16_t);
+		break;
+
+	case FT_ID_TX_FIFO_DEPTH:
+		retval = get_txfifo_size(pcd, ctrl_req->wValue);
+		if (retval >= 0) {
+			txfifo = retval;
+			*((uint16_t *) buf) = txfifo;
+			retval = sizeof(uint16_t);
+		}
+		break;
+
+	case FT_ID_RX_FIFO_DEPTH:
+		retval = get_rxfifo_size(coreif, ctrl_req->wValue);
+		if (retval >= 0) {
+			rxfifo = retval;
+			*((uint16_t *) buf) = rxfifo;
+			retval = sizeof(uint16_t);
+		}
+		break;
+	}
+
+	return retval;
+}
+
+/**
+ * This function resets the SG for the specified EP to its default value
+ */
+static int cfi_reset_sg_val(cfi_ep_t * cfiep)
+{
+	dwc_memset(cfiep->bm_sg, 0, sizeof(ddma_sg_buffer_setup_t));
+	return 0;
+}
+
+/**
+ * This function resets the Alignment for the specified EP to its default value
+ */
+static int cfi_reset_align_val(cfi_ep_t * cfiep)
+{
+	dwc_memset(cfiep->bm_sg, 0, sizeof(ddma_sg_buffer_setup_t));
+	return 0;
+}
+
+/**
+ * This function resets the Concatenation for the specified EP to its default value
+ * This function will also set the value of the wTxBytes field to NULL after 
+ * freeing the memory previously allocated for this field.
+ */
+static int cfi_reset_concat_val(cfi_ep_t * cfiep)
+{
+	/* First we need to free the wTxBytes field */
+	if (cfiep->bm_concat->wTxBytes) {
+		DWC_FREE(cfiep->bm_concat->wTxBytes);
+		cfiep->bm_concat->wTxBytes = NULL;
+	}
+
+	dwc_memset(cfiep->bm_concat, 0, sizeof(ddma_concat_buffer_setup_t));
+	return 0;
+}
+
+/**
+ * This function resets all the buffer setups of the specified endpoint
+ */
+static int cfi_ep_reset_all_setup_vals(cfi_ep_t * cfiep)
+{
+	cfi_reset_sg_val(cfiep);
+	cfi_reset_align_val(cfiep);
+	cfi_reset_concat_val(cfiep);
+	return 0;
+}
+
+static int cfi_handle_reset_fifo_val(struct dwc_otg_pcd *pcd, uint8_t ep_addr,
+				     uint8_t rx_rst, uint8_t tx_rst)
+{
+	int retval = -DWC_E_INVALID;
+	uint16_t tx_siz[15];
+	uint16_t rx_siz = 0;
+	dwc_otg_pcd_ep_t *ep = NULL;
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(pcd);
+	dwc_otg_core_params_t *params = GET_CORE_IF(pcd)->core_params;
+
+	if (rx_rst) {
+		rx_siz = params->dev_rx_fifo_size;
+		params->dev_rx_fifo_size = GET_CORE_IF(pcd)->init_rxfsiz;
+	}
+
+	if (tx_rst) {
+		if (ep_addr == 0) {
+			int i;
+
+			for (i = 0; i < core_if->hwcfg4.b.num_in_eps; i++) {
+				tx_siz[i] =
+				    core_if->core_params->dev_tx_fifo_size[i];
+				core_if->core_params->dev_tx_fifo_size[i] =
+				    core_if->init_txfsiz[i];
+			}
+		} else {
+
+			ep = get_ep_by_addr(pcd, ep_addr);
+
+			if (NULL == ep) {
+				CFI_INFO
+				    ("%s: Unable to get the endpoint addr=0x%02x\n",
+				     __func__, ep_addr);
+				return -DWC_E_INVALID;
+			}
+
+			tx_siz[0] =
+			    params->dev_tx_fifo_size[ep->dwc_ep.tx_fifo_num -
+						     1];
+			params->dev_tx_fifo_size[ep->dwc_ep.tx_fifo_num - 1] =
+			    GET_CORE_IF(pcd)->init_txfsiz[ep->
+							  dwc_ep.tx_fifo_num -
+							  1];
+		}
+	}
+
+	if (resize_fifos(GET_CORE_IF(pcd))) {
+		retval = 0;
+	} else {
+		CFI_INFO
+		    ("%s: Error resetting the feature Reset All(FIFO size)\n",
+		     __func__);
+		if (rx_rst) {
+			params->dev_rx_fifo_size = rx_siz;
+		}
+
+		if (tx_rst) {
+			if (ep_addr == 0) {
+				int i;
+				for (i = 0; i < core_if->hwcfg4.b.num_in_eps;
+				     i++) {
+					core_if->
+					    core_params->dev_tx_fifo_size[i] =
+					    tx_siz[i];
+				}
+			} else {
+				params->dev_tx_fifo_size[ep->
+							 dwc_ep.tx_fifo_num -
+							 1] = tx_siz[0];
+			}
+		}
+		retval = -DWC_E_INVALID;
+	}
+	return retval;
+}
+
+static int cfi_handle_reset_all(struct dwc_otg_pcd *pcd, uint8_t addr)
+{
+	int retval = 0;
+	cfi_ep_t *cfiep;
+	cfiobject_t *cfi = pcd->cfi;
+	dwc_list_link_t *tmp;
+
+	retval = cfi_handle_reset_fifo_val(pcd, addr, 1, 1);
+	if (retval < 0) {
+		return retval;
+	}
+
+	/* If the EP address is known then reset the features for only that EP */
+	if (addr) {
+		cfiep = get_cfi_ep_by_addr(pcd->cfi, addr);
+		if (NULL == cfiep) {
+			CFI_INFO("%s: Error getting the EP address 0x%02x\n",
+				 __func__, addr);
+			return -DWC_E_INVALID;
+		}
+		retval = cfi_ep_reset_all_setup_vals(cfiep);
+		cfiep->ep->dwc_ep.buff_mode = BM_STANDARD;
+	}
+	/* Otherwise (wValue == 0), reset all features of all EP's */
+	else {
+		/* Traverse all the active EP's and reset the feature(s) value(s) */
+		//list_for_each_entry(cfiep, &cfi->active_eps, lh) {
+		DWC_LIST_FOREACH(tmp, &cfi->active_eps) {
+			cfiep = DWC_LIST_ENTRY(tmp, struct cfi_ep, lh);
+			retval = cfi_ep_reset_all_setup_vals(cfiep);
+			cfiep->ep->dwc_ep.buff_mode = BM_STANDARD;
+			if (retval < 0) {
+				CFI_INFO
+				    ("%s: Error resetting the feature Reset All\n",
+				     __func__);
+				return retval;
+			}
+		}
+	}
+	return retval;
+}
+
+static int cfi_handle_reset_dma_buff_setup(struct dwc_otg_pcd *pcd,
+					   uint8_t addr)
+{
+	int retval = 0;
+	cfi_ep_t *cfiep;
+	cfiobject_t *cfi = pcd->cfi;
+	dwc_list_link_t *tmp;
+
+	/* If the EP address is known then reset the features for only that EP */
+	if (addr) {
+		cfiep = get_cfi_ep_by_addr(pcd->cfi, addr);
+		if (NULL == cfiep) {
+			CFI_INFO("%s: Error getting the EP address 0x%02x\n",
+				 __func__, addr);
+			return -DWC_E_INVALID;
+		}
+		retval = cfi_reset_sg_val(cfiep);
+	}
+	/* Otherwise (wValue == 0), reset all features of all EP's */
+	else {
+		/* Traverse all the active EP's and reset the feature(s) value(s) */
+		//list_for_each_entry(cfiep, &cfi->active_eps, lh) {
+		DWC_LIST_FOREACH(tmp, &cfi->active_eps) {
+			cfiep = DWC_LIST_ENTRY(tmp, struct cfi_ep, lh);
+			retval = cfi_reset_sg_val(cfiep);
+			if (retval < 0) {
+				CFI_INFO
+				    ("%s: Error resetting the feature Buffer Setup\n",
+				     __func__);
+				return retval;
+			}
+		}
+	}
+	return retval;
+}
+
+static int cfi_handle_reset_concat_val(struct dwc_otg_pcd *pcd, uint8_t addr)
+{
+	int retval = 0;
+	cfi_ep_t *cfiep;
+	cfiobject_t *cfi = pcd->cfi;
+	dwc_list_link_t *tmp;
+
+	/* If the EP address is known then reset the features for only that EP */
+	if (addr) {
+		cfiep = get_cfi_ep_by_addr(pcd->cfi, addr);
+		if (NULL == cfiep) {
+			CFI_INFO("%s: Error getting the EP address 0x%02x\n",
+				 __func__, addr);
+			return -DWC_E_INVALID;
+		}
+		retval = cfi_reset_concat_val(cfiep);
+	}
+	/* Otherwise (wValue == 0), reset all features of all EP's */
+	else {
+		/* Traverse all the active EP's and reset the feature(s) value(s) */
+		//list_for_each_entry(cfiep, &cfi->active_eps, lh) {
+		DWC_LIST_FOREACH(tmp, &cfi->active_eps) {
+			cfiep = DWC_LIST_ENTRY(tmp, struct cfi_ep, lh);
+			retval = cfi_reset_concat_val(cfiep);
+			if (retval < 0) {
+				CFI_INFO
+				    ("%s: Error resetting the feature Concatenation Value\n",
+				     __func__);
+				return retval;
+			}
+		}
+	}
+	return retval;
+}
+
+static int cfi_handle_reset_align_val(struct dwc_otg_pcd *pcd, uint8_t addr)
+{
+	int retval = 0;
+	cfi_ep_t *cfiep;
+	cfiobject_t *cfi = pcd->cfi;
+	dwc_list_link_t *tmp;
+
+	/* If the EP address is known then reset the features for only that EP */
+	if (addr) {
+		cfiep = get_cfi_ep_by_addr(pcd->cfi, addr);
+		if (NULL == cfiep) {
+			CFI_INFO("%s: Error getting the EP address 0x%02x\n",
+				 __func__, addr);
+			return -DWC_E_INVALID;
+		}
+		retval = cfi_reset_align_val(cfiep);
+	}
+	/* Otherwise (wValue == 0), reset all features of all EP's */
+	else {
+		/* Traverse all the active EP's and reset the feature(s) value(s) */
+		//list_for_each_entry(cfiep, &cfi->active_eps, lh) {
+		DWC_LIST_FOREACH(tmp, &cfi->active_eps) {
+			cfiep = DWC_LIST_ENTRY(tmp, struct cfi_ep, lh);
+			retval = cfi_reset_align_val(cfiep);
+			if (retval < 0) {
+				CFI_INFO
+				    ("%s: Error resetting the feature Aliignment Value\n",
+				     __func__);
+				return retval;
+			}
+		}
+	}
+	return retval;
+
+}
+
+static int cfi_preproc_reset(struct dwc_otg_pcd *pcd,
+			     struct cfi_usb_ctrlrequest *req)
+{
+	int retval = 0;
+
+	switch (req->wIndex) {
+	case 0:
+		/* Reset all features */
+		retval = cfi_handle_reset_all(pcd, req->wValue & 0xff);
+		break;
+
+	case FT_ID_DMA_BUFFER_SETUP:
+		/* Reset the SG buffer setup */
+		retval =
+		    cfi_handle_reset_dma_buff_setup(pcd, req->wValue & 0xff);
+		break;
+
+	case FT_ID_DMA_CONCAT_SETUP:
+		/* Reset the Concatenation buffer setup */
+		retval = cfi_handle_reset_concat_val(pcd, req->wValue & 0xff);
+		break;
+
+	case FT_ID_DMA_BUFF_ALIGN:
+		/* Reset the Alignment buffer setup */
+		retval = cfi_handle_reset_align_val(pcd, req->wValue & 0xff);
+		break;
+
+	case FT_ID_TX_FIFO_DEPTH:
+		retval =
+		    cfi_handle_reset_fifo_val(pcd, req->wValue & 0xff, 0, 1);
+		pcd->cfi->need_gadget_att = 0;
+		break;
+
+	case FT_ID_RX_FIFO_DEPTH:
+		retval = cfi_handle_reset_fifo_val(pcd, 0, 1, 0);
+		pcd->cfi->need_gadget_att = 0;
+		break;
+	default:
+		break;
+	}
+	return retval;
+}
+
+/**
+ * This function sets a new value for the SG buffer setup.
+ */
+static int cfi_ep_set_sg_val(uint8_t * buf, struct dwc_otg_pcd *pcd)
+{
+	uint8_t inaddr, outaddr;
+	cfi_ep_t *epin, *epout;
+	ddma_sg_buffer_setup_t *psgval;
+	uint32_t desccount, size;
+
+	CFI_INFO("%s\n", __func__);
+
+	psgval = (ddma_sg_buffer_setup_t *) buf;
+	desccount = (uint32_t) psgval->bCount;
+	size = (uint32_t) psgval->wSize;
+
+	/* Check the DMA descriptor count */
+	if ((desccount > MAX_DMA_DESCS_PER_EP) || (desccount == 0)) {
+		CFI_INFO
+		    ("%s: The count of DMA Descriptors should be between 1 and %d\n",
+		     __func__, MAX_DMA_DESCS_PER_EP);
+		return -DWC_E_INVALID;
+	}
+
+	/* Check the DMA descriptor count */
+
+	if (size == 0) {
+
+		CFI_INFO("%s: The transfer size should be at least 1 byte\n",
+			 __func__);
+
+		return -DWC_E_INVALID;
+
+	}
+
+	inaddr = psgval->bInEndpointAddress;
+	outaddr = psgval->bOutEndpointAddress;
+
+	epin = get_cfi_ep_by_addr(pcd->cfi, inaddr);
+	epout = get_cfi_ep_by_addr(pcd->cfi, outaddr);
+
+	if (NULL == epin || NULL == epout) {
+		CFI_INFO
+		    ("%s: Unable to get the endpoints inaddr=0x%02x outaddr=0x%02x\n",
+		     __func__, inaddr, outaddr);
+		return -DWC_E_INVALID;
+	}
+
+	epin->ep->dwc_ep.buff_mode = BM_SG;
+	dwc_memcpy(epin->bm_sg, psgval, sizeof(ddma_sg_buffer_setup_t));
+
+	epout->ep->dwc_ep.buff_mode = BM_SG;
+	dwc_memcpy(epout->bm_sg, psgval, sizeof(ddma_sg_buffer_setup_t));
+
+	return 0;
+}
+
+/**
+ * This function sets a new value for the buffer Alignment setup.
+ */
+static int cfi_ep_set_alignment_val(uint8_t * buf, struct dwc_otg_pcd *pcd)
+{
+	cfi_ep_t *ep;
+	uint8_t addr;
+	ddma_align_buffer_setup_t *palignval;
+
+	palignval = (ddma_align_buffer_setup_t *) buf;
+	addr = palignval->bEndpointAddress;
+
+	ep = get_cfi_ep_by_addr(pcd->cfi, addr);
+
+	if (NULL == ep) {
+		CFI_INFO("%s: Unable to get the endpoint addr=0x%02x\n",
+			 __func__, addr);
+		return -DWC_E_INVALID;
+	}
+
+	ep->ep->dwc_ep.buff_mode = BM_ALIGN;
+	dwc_memcpy(ep->bm_align, palignval, sizeof(ddma_align_buffer_setup_t));
+
+	return 0;
+}
+
+/**
+ * This function sets a new value for the Concatenation buffer setup.
+ */
+static int cfi_ep_set_concat_val(uint8_t * buf, struct dwc_otg_pcd *pcd)
+{
+	uint8_t addr;
+	cfi_ep_t *ep;
+	struct _ddma_concat_buffer_setup_hdr *pConcatValHdr;
+	uint16_t *pVals;
+	uint32_t desccount;
+	int i;
+	uint16_t mps;
+
+	pConcatValHdr = (struct _ddma_concat_buffer_setup_hdr *)buf;
+	desccount = (uint32_t) pConcatValHdr->bDescCount;
+	pVals = (uint16_t *) (buf + BS_CONCAT_VAL_HDR_LEN);
+
+	/* Check the DMA descriptor count */
+	if (desccount > MAX_DMA_DESCS_PER_EP) {
+		CFI_INFO("%s: Maximum DMA Descriptor count should be %d\n",
+			 __func__, MAX_DMA_DESCS_PER_EP);
+		return -DWC_E_INVALID;
+	}
+
+	addr = pConcatValHdr->bEndpointAddress;
+	ep = get_cfi_ep_by_addr(pcd->cfi, addr);
+	if (NULL == ep) {
+		CFI_INFO("%s: Unable to get the endpoint addr=0x%02x\n",
+			 __func__, addr);
+		return -DWC_E_INVALID;
+	}
+
+	mps = UGETW(ep->ep->desc->wMaxPacketSize);
+
+#if 0
+	for (i = 0; i < desccount; i++) {
+		CFI_INFO("%s: wTxSize[%d]=0x%04x\n", __func__, i, pVals[i]);
+	}
+	CFI_INFO("%s: epname=%s; mps=%d\n", __func__, ep->ep->ep.name, mps);
+#endif
+
+	/* Check the wTxSizes to be less than or equal to the mps */
+	for (i = 0; i < desccount; i++) {
+		if (pVals[i] > mps) {
+			CFI_INFO
+			    ("%s: ERROR - the wTxSize[%d] should be <= MPS (wTxSize=%d)\n",
+			     __func__, i, pVals[i]);
+			return -DWC_E_INVALID;
+		}
+	}
+
+	ep->ep->dwc_ep.buff_mode = BM_CONCAT;
+	dwc_memcpy(ep->bm_concat, pConcatValHdr, BS_CONCAT_VAL_HDR_LEN);
+
+	/* Free the previously allocated storage for the wTxBytes */
+	if (ep->bm_concat->wTxBytes) {
+		DWC_FREE(ep->bm_concat->wTxBytes);
+	}
+
+	/* Allocate a new storage for the wTxBytes field */
+	ep->bm_concat->wTxBytes =
+	    DWC_ALLOC(sizeof(uint16_t) * pConcatValHdr->bDescCount);
+	if (NULL == ep->bm_concat->wTxBytes) {
+		CFI_INFO("%s: Unable to allocate memory\n", __func__);
+		return -DWC_E_NO_MEMORY;
+	}
+
+	/* Copy the new values into the wTxBytes filed */
+	dwc_memcpy(ep->bm_concat->wTxBytes, buf + BS_CONCAT_VAL_HDR_LEN,
+		   sizeof(uint16_t) * pConcatValHdr->bDescCount);
+
+	return 0;
+}
+
+/**
+ * This function calculates the total of all FIFO sizes
+ * 
+ * @param core_if Programming view of DWC_otg controller
+ *
+ * @return The total of data FIFO sizes.
+ *
+ */
+static uint16_t get_dfifo_size(dwc_otg_core_if_t * core_if)
+{
+	dwc_otg_core_params_t *params = core_if->core_params;
+	uint16_t dfifo_total = 0;
+	int i;
+
+	/* The shared RxFIFO size */
+	dfifo_total =
+	    params->dev_rx_fifo_size + params->dev_nperio_tx_fifo_size;
+
+	/* Add up each TxFIFO size to the total */
+	for (i = 0; i < core_if->hwcfg4.b.num_in_eps; i++) {
+		dfifo_total += params->dev_tx_fifo_size[i];
+	}
+
+	return dfifo_total;
+}
+
+/**
+ * This function returns Rx FIFO size
+ * 
+ * @param core_if Programming view of DWC_otg controller
+ *
+ * @return The total of data FIFO sizes.
+ *
+ */
+static int32_t get_rxfifo_size(dwc_otg_core_if_t * core_if, uint16_t wValue)
+{
+	switch (wValue >> 8) {
+	case 0:
+		return (core_if->pwron_rxfsiz <
+			32768) ? core_if->pwron_rxfsiz : 32768;
+		break;
+	case 1:
+		return core_if->core_params->dev_rx_fifo_size;
+		break;
+	default:
+		return -DWC_E_INVALID;
+		break;
+	}
+}
+
+/**
+ * This function returns Tx FIFO size for IN EP
+ * 
+ * @param core_if Programming view of DWC_otg controller
+ *
+ * @return The total of data FIFO sizes.
+ *
+ */
+static int32_t get_txfifo_size(struct dwc_otg_pcd *pcd, uint16_t wValue)
+{
+	dwc_otg_pcd_ep_t *ep;
+
+	ep = get_ep_by_addr(pcd, wValue & 0xff);
+
+	if (NULL == ep) {
+		CFI_INFO("%s: Unable to get the endpoint addr=0x%02x\n",
+			 __func__, wValue & 0xff);
+		return -DWC_E_INVALID;
+	}
+
+	if (!ep->dwc_ep.is_in) {
+		CFI_INFO
+		    ("%s: No Tx FIFO assingned to the Out endpoint addr=0x%02x\n",
+		     __func__, wValue & 0xff);
+		return -DWC_E_INVALID;
+	}
+
+	switch (wValue >> 8) {
+	case 0:
+		return (GET_CORE_IF(pcd)->pwron_txfsiz
+			[ep->dwc_ep.tx_fifo_num - 1] <
+			768) ? GET_CORE_IF(pcd)->pwron_txfsiz[ep->
+							      dwc_ep.tx_fifo_num
+							      - 1] : 32768;
+		break;
+	case 1:
+		return GET_CORE_IF(pcd)->core_params->
+		    dev_tx_fifo_size[ep->dwc_ep.num - 1];
+		break;
+	default:
+		return -DWC_E_INVALID;
+		break;
+	}
+}
+
+/**
+ * This function checks if the submitted combination of 
+ * device mode FIFO sizes is possible or not.
+ * 
+ * @param core_if Programming view of DWC_otg controller
+ *
+ * @return 1 if possible, 0 otherwise.
+ *
+ */
+static uint8_t check_fifo_sizes(dwc_otg_core_if_t * core_if)
+{
+	uint16_t dfifo_actual = 0;
+	dwc_otg_core_params_t *params = core_if->core_params;
+	uint16_t start_addr = 0;
+	int i;
+
+	dfifo_actual =
+	    params->dev_rx_fifo_size + params->dev_nperio_tx_fifo_size;
+
+	for (i = 0; i < core_if->hwcfg4.b.num_in_eps; i++) {
+		dfifo_actual += params->dev_tx_fifo_size[i];
+	}
+
+	if (dfifo_actual > core_if->total_fifo_size) {
+		return 0;
+	}
+
+	if (params->dev_rx_fifo_size > 32768 || params->dev_rx_fifo_size < 16)
+		return 0;
+
+	if (params->dev_nperio_tx_fifo_size > 32768
+	    || params->dev_nperio_tx_fifo_size < 16)
+		return 0;
+
+	for (i = 0; i < core_if->hwcfg4.b.num_in_eps; i++) {
+
+		if (params->dev_tx_fifo_size[i] > 768
+		    || params->dev_tx_fifo_size[i] < 4)
+			return 0;
+	}
+
+	if (params->dev_rx_fifo_size > core_if->pwron_rxfsiz)
+		return 0;
+	start_addr = params->dev_rx_fifo_size;
+
+	if (params->dev_nperio_tx_fifo_size > core_if->pwron_gnptxfsiz)
+		return 0;
+	start_addr += params->dev_nperio_tx_fifo_size;
+
+	for (i = 0; i < core_if->hwcfg4.b.num_in_eps; i++) {
+
+		if (params->dev_tx_fifo_size[i] > core_if->pwron_txfsiz[i])
+			return 0;
+		start_addr += params->dev_tx_fifo_size[i];
+	}
+
+	return 1;
+}
+
+/**
+ * This function resizes Device mode FIFOs
+ * 
+ * @param core_if Programming view of DWC_otg controller
+ *
+ * @return 1 if successful, 0 otherwise
+ *
+ */
+static uint8_t resize_fifos(dwc_otg_core_if_t * core_if)
+{
+	int i = 0;
+	dwc_otg_core_global_regs_t *global_regs = core_if->core_global_regs;
+	dwc_otg_core_params_t *params = core_if->core_params;
+	uint32_t rx_fifo_size;
+	fifosize_data_t nptxfifosize;
+	fifosize_data_t txfifosize[15];
+
+	uint32_t rx_fsz_bak;
+	uint32_t nptxfsz_bak;
+	uint32_t txfsz_bak[15];
+
+	uint16_t start_address;
+	uint8_t retval = 1;
+
+	if (!check_fifo_sizes(core_if)) {
+		return 0;
+	}
+
+	/* Configure data FIFO sizes */
+	if (core_if->hwcfg2.b.dynamic_fifo && params->enable_dynamic_fifo) {
+		rx_fsz_bak = DWC_READ_REG32(&global_regs->grxfsiz);
+		rx_fifo_size = params->dev_rx_fifo_size;
+		DWC_WRITE_REG32(&global_regs->grxfsiz, rx_fifo_size);
+
+		/*
+		 * Tx FIFOs These FIFOs are numbered from 1 to 15.
+		 * Indexes of the FIFO size module parameters in the
+		 * dev_tx_fifo_size array and the FIFO size registers in
+		 * the dtxfsiz array run from 0 to 14.
+		 */
+
+		/* Non-periodic Tx FIFO */
+		nptxfsz_bak = DWC_READ_REG32(&global_regs->gnptxfsiz);
+		nptxfifosize.b.depth = params->dev_nperio_tx_fifo_size;
+		//start_address = params->dev_rx_fifo_size;
+		start_address = 0x400; // Makarand: USB2 DEBUG
+		nptxfifosize.b.startaddr = start_address;
+
+		DWC_WRITE_REG32(&global_regs->gnptxfsiz, nptxfifosize.d32);
+
+		start_address += nptxfifosize.b.depth;
+
+		for (i = 0; i < core_if->hwcfg4.b.num_in_eps; i++) {
+			txfsz_bak[i] = DWC_READ_REG32(&global_regs->dtxfsiz[i]);
+
+			txfifosize[i].b.depth = params->dev_tx_fifo_size[i];
+			txfifosize[i].b.startaddr = start_address;
+			DWC_WRITE_REG32(&global_regs->dtxfsiz[i],
+					txfifosize[i].d32);
+
+			start_address += txfifosize[i].b.depth;
+		}
+
+		/** Check if register values are set correctly */
+		if (rx_fifo_size != DWC_READ_REG32(&global_regs->grxfsiz)) {
+			retval = 0;
+		}
+
+		if (nptxfifosize.d32 != DWC_READ_REG32(&global_regs->gnptxfsiz)) {
+			retval = 0;
+		}
+
+		for (i = 0; i < core_if->hwcfg4.b.num_in_eps; i++) {
+			if (txfifosize[i].d32 !=
+			    DWC_READ_REG32(&global_regs->dtxfsiz[i])) {
+				retval = 0;
+			}
+		}
+
+		/** If register values are not set correctly, reset old values */
+		if (retval == 0) {
+			DWC_WRITE_REG32(&global_regs->grxfsiz, rx_fsz_bak);
+
+			/* Non-periodic Tx FIFO */
+			DWC_WRITE_REG32(&global_regs->gnptxfsiz, nptxfsz_bak);
+
+			for (i = 0; i < core_if->hwcfg4.b.num_in_eps; i++) {
+				DWC_WRITE_REG32(&global_regs->dtxfsiz[i],
+						txfsz_bak[i]);
+			}
+		}
+	} else {
+		return 0;
+	}
+
+	/* Flush the FIFOs */
+	dwc_otg_flush_tx_fifo(core_if, 0x10);	/* all Tx FIFOs */
+	dwc_otg_flush_rx_fifo(core_if);
+
+	return retval;
+}
+
+/**
+ * This function sets a new value for the buffer Alignment setup.
+ */
+static int cfi_ep_set_tx_fifo_val(uint8_t * buf, dwc_otg_pcd_t * pcd)
+{
+	int retval;
+	uint32_t fsiz;
+	uint16_t size;
+	uint16_t ep_addr;
+	dwc_otg_pcd_ep_t *ep;
+	dwc_otg_core_params_t *params = GET_CORE_IF(pcd)->core_params;
+	tx_fifo_size_setup_t *ptxfifoval;
+
+	ptxfifoval = (tx_fifo_size_setup_t *) buf;
+	ep_addr = ptxfifoval->bEndpointAddress;
+	size = ptxfifoval->wDepth;
+
+	ep = get_ep_by_addr(pcd, ep_addr);
+
+	CFI_INFO
+	    ("%s: Set Tx FIFO size: endpoint addr=0x%02x, depth=%d, FIFO Num=%d\n",
+	     __func__, ep_addr, size, ep->dwc_ep.tx_fifo_num);
+
+	if (NULL == ep) {
+		CFI_INFO("%s: Unable to get the endpoint addr=0x%02x\n",
+			 __func__, ep_addr);
+		return -DWC_E_INVALID;
+	}
+
+	fsiz = params->dev_tx_fifo_size[ep->dwc_ep.tx_fifo_num - 1];
+	params->dev_tx_fifo_size[ep->dwc_ep.tx_fifo_num - 1] = size;
+
+	if (resize_fifos(GET_CORE_IF(pcd))) {
+		retval = 0;
+	} else {
+		CFI_INFO
+		    ("%s: Error setting the feature Tx FIFO Size for EP%d\n",
+		     __func__, ep_addr);
+		params->dev_tx_fifo_size[ep->dwc_ep.tx_fifo_num - 1] = fsiz;
+		retval = -DWC_E_INVALID;
+	}
+
+	return retval;
+}
+
+/**
+ * This function sets a new value for the buffer Alignment setup.
+ */
+static int cfi_set_rx_fifo_val(uint8_t * buf, dwc_otg_pcd_t * pcd)
+{
+	int retval;
+	uint32_t fsiz;
+	uint16_t size;
+	dwc_otg_core_params_t *params = GET_CORE_IF(pcd)->core_params;
+	rx_fifo_size_setup_t *prxfifoval;
+
+	prxfifoval = (rx_fifo_size_setup_t *) buf;
+	size = prxfifoval->wDepth;
+
+	fsiz = params->dev_rx_fifo_size;
+	params->dev_rx_fifo_size = size;
+
+	if (resize_fifos(GET_CORE_IF(pcd))) {
+		retval = 0;
+	} else {
+		CFI_INFO("%s: Error setting the feature Rx FIFO Size\n",
+			 __func__);
+		params->dev_rx_fifo_size = fsiz;
+		retval = -DWC_E_INVALID;
+	}
+
+	return retval;
+}
+
+/**
+ * This function reads the SG of an EP's buffer setup into the buffer buf
+ */
+static int cfi_ep_get_sg_val(uint8_t * buf, struct dwc_otg_pcd *pcd,
+			     struct cfi_usb_ctrlrequest *req)
+{
+	int retval = -DWC_E_INVALID;
+	uint8_t addr;
+	cfi_ep_t *ep;
+
+	/* The Low Byte of the wValue contains a non-zero address of the endpoint */
+	addr = req->wValue & 0xFF;
+	if (addr == 0)		/* The address should be non-zero */
+		return retval;
+
+	ep = get_cfi_ep_by_addr(pcd->cfi, addr);
+	if (NULL == ep) {
+		CFI_INFO("%s: Unable to get the endpoint address(0x%02x)\n",
+			 __func__, addr);
+		return retval;
+	}
+
+	dwc_memcpy(buf, ep->bm_sg, BS_SG_VAL_DESC_LEN);
+	retval = BS_SG_VAL_DESC_LEN;
+	return retval;
+}
+
+/**
+ * This function reads the Concatenation value of an EP's buffer mode into 
+ * the buffer buf
+ */
+static int cfi_ep_get_concat_val(uint8_t * buf, struct dwc_otg_pcd *pcd,
+				 struct cfi_usb_ctrlrequest *req)
+{
+	int retval = -DWC_E_INVALID;
+	uint8_t addr;
+	cfi_ep_t *ep;
+	uint8_t desc_count;
+
+	/* The Low Byte of the wValue contains a non-zero address of the endpoint */
+	addr = req->wValue & 0xFF;
+	if (addr == 0)		/* The address should be non-zero */
+		return retval;
+
+	ep = get_cfi_ep_by_addr(pcd->cfi, addr);
+	if (NULL == ep) {
+		CFI_INFO("%s: Unable to get the endpoint address(0x%02x)\n",
+			 __func__, addr);
+		return retval;
+	}
+
+	/* Copy the header to the buffer */
+	dwc_memcpy(buf, ep->bm_concat, BS_CONCAT_VAL_HDR_LEN);
+	/* Advance the buffer pointer by the header size */
+	buf += BS_CONCAT_VAL_HDR_LEN;
+
+	desc_count = ep->bm_concat->hdr.bDescCount;
+	/* Copy alll the wTxBytes to the buffer */
+	dwc_memcpy(buf, ep->bm_concat->wTxBytes, sizeof(uid16_t) * desc_count);
+
+	retval = BS_CONCAT_VAL_HDR_LEN + sizeof(uid16_t) * desc_count;
+	return retval;
+}
+
+/**
+ * This function reads the buffer Alignment value of an EP's buffer mode into 
+ * the buffer buf
+ *
+ * @return The total number of bytes copied to the buffer or negative error code.
+ */
+static int cfi_ep_get_align_val(uint8_t * buf, struct dwc_otg_pcd *pcd,
+				struct cfi_usb_ctrlrequest *req)
+{
+	int retval = -DWC_E_INVALID;
+	uint8_t addr;
+	cfi_ep_t *ep;
+
+	/* The Low Byte of the wValue contains a non-zero address of the endpoint */
+	addr = req->wValue & 0xFF;
+	if (addr == 0)		/* The address should be non-zero */
+		return retval;
+
+	ep = get_cfi_ep_by_addr(pcd->cfi, addr);
+	if (NULL == ep) {
+		CFI_INFO("%s: Unable to get the endpoint address(0x%02x)\n",
+			 __func__, addr);
+		return retval;
+	}
+
+	dwc_memcpy(buf, ep->bm_align, BS_ALIGN_VAL_HDR_LEN);
+	retval = BS_ALIGN_VAL_HDR_LEN;
+
+	return retval;
+}
+
+/**
+ * This function sets a new value for the specified feature
+ * 
+ * @param	pcd	A pointer to the PCD object
+ * 
+ * @return 0 if successful, negative error code otherwise to stall the DCE.
+ */
+static int cfi_set_feature_value(struct dwc_otg_pcd *pcd)
+{
+	int retval = -DWC_E_NOT_SUPPORTED;
+	uint16_t wIndex, wValue;
+	uint8_t bRequest;
+	struct dwc_otg_core_if *coreif;
+	cfiobject_t *cfi = pcd->cfi;
+	struct cfi_usb_ctrlrequest *ctrl_req;
+	uint8_t *buf;
+	ctrl_req = &cfi->ctrl_req;
+
+	buf = pcd->cfi->ctrl_req.data;
+
+	coreif = GET_CORE_IF(pcd);
+	bRequest = ctrl_req->bRequest;
+	wIndex = DWC_CONSTANT_CPU_TO_LE16(ctrl_req->wIndex);
+	wValue = DWC_CONSTANT_CPU_TO_LE16(ctrl_req->wValue);
+
+	/* See which feature is to be modified */
+	switch (wIndex) {
+	case FT_ID_DMA_BUFFER_SETUP:
+		/* Modify the feature */
+		if ((retval = cfi_ep_set_sg_val(buf, pcd)) < 0)
+			return retval;
+
+		/* And send this request to the gadget */
+		cfi->need_gadget_att = 1;
+		break;
+
+	case FT_ID_DMA_BUFF_ALIGN:
+		if ((retval = cfi_ep_set_alignment_val(buf, pcd)) < 0)
+			return retval;
+		cfi->need_gadget_att = 1;
+		break;
+
+	case FT_ID_DMA_CONCAT_SETUP:
+		/* Modify the feature */
+		if ((retval = cfi_ep_set_concat_val(buf, pcd)) < 0)
+			return retval;
+		cfi->need_gadget_att = 1;
+		break;
+
+	case FT_ID_DMA_CIRCULAR:
+		CFI_INFO("FT_ID_DMA_CIRCULAR\n");
+		break;
+
+	case FT_ID_THRESHOLD_SETUP:
+		CFI_INFO("FT_ID_THRESHOLD_SETUP\n");
+		break;
+
+	case FT_ID_DFIFO_DEPTH:
+		CFI_INFO("FT_ID_DFIFO_DEPTH\n");
+		break;
+
+	case FT_ID_TX_FIFO_DEPTH:
+		CFI_INFO("FT_ID_TX_FIFO_DEPTH\n");
+		if ((retval = cfi_ep_set_tx_fifo_val(buf, pcd)) < 0)
+			return retval;
+		cfi->need_gadget_att = 0;
+		break;
+
+	case FT_ID_RX_FIFO_DEPTH:
+		CFI_INFO("FT_ID_RX_FIFO_DEPTH\n");
+		if ((retval = cfi_set_rx_fifo_val(buf, pcd)) < 0)
+			return retval;
+		cfi->need_gadget_att = 0;
+		break;
+	}
+
+	return retval;
+}
+
+#endif //DWC_UTE_CFI
diff --git a/drivers/usb/dwc_otg/dwc_otg_cfi.h b/drivers/usb/dwc_otg/dwc_otg_cfi.h
new file mode 100644
index 0000000..cbccdab
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_otg_cfi.h
@@ -0,0 +1,320 @@
+/* ==========================================================================
+ * Synopsys HS OTG Linux Software Driver and documentation (hereinafter,
+ * "Software") is an Unsupported proprietary work of Synopsys, Inc. unless
+ * otherwise expressly agreed to in writing between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product under
+ * any End User Software License Agreement or Agreement for Licensed Product
+ * with Synopsys or any supplement thereto. You are permitted to use and
+ * redistribute this Software in source and binary forms, with or without
+ * modification, provided that redistributions of source code must retain this
+ * notice. You may not view, use, disclose, copy or distribute this file or
+ * any information contained herein except pursuant to this license grant from
+ * Synopsys. If you do not agree with this notice, including the disclaimer
+ * below, then you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS" BASIS
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE HEREBY DISCLAIMED. IN NO EVENT SHALL SYNOPSYS BE LIABLE FOR ANY DIRECT,
+ * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================== */
+
+#if !defined(__DWC_OTG_CFI_H__)
+#define __DWC_OTG_CFI_H__
+
+#include "dwc_otg_pcd.h"
+#include "dwc_cfi_common.h"
+
+/**
+ * @file
+ * This file contains the CFI related OTG PCD specific common constants, 
+ * interfaces(functions and macros) and data structures.The CFI Protocol is an 
+ * optional interface for internal testing purposes that a DUT may implement to 
+ * support testing of configurable features.
+ *
+ */
+
+struct dwc_otg_pcd;
+struct dwc_otg_pcd_ep;
+
+/** OTG CFI Features (properties) ID constants */
+/** This is a request for all Core Features */
+#define FT_ID_DMA_MODE					0x0001
+#define FT_ID_DMA_BUFFER_SETUP			0x0002
+#define FT_ID_DMA_BUFF_ALIGN			0x0003
+#define FT_ID_DMA_CONCAT_SETUP			0x0004
+#define FT_ID_DMA_CIRCULAR				0x0005
+#define FT_ID_THRESHOLD_SETUP			0x0006
+#define FT_ID_DFIFO_DEPTH				0x0007
+#define FT_ID_TX_FIFO_DEPTH				0x0008
+#define FT_ID_RX_FIFO_DEPTH				0x0009
+
+/**********************************************************/
+#define CFI_INFO_DEF
+
+#ifdef CFI_INFO_DEF
+#define CFI_INFO(fmt...)	DWC_PRINTF("CFI: " fmt);
+#else
+#define CFI_INFO(fmt...)
+#endif
+
+#define min(x,y) ({ \
+	x < y ? x : y; })
+
+#define max(x,y) ({ \
+	x > y ? x : y; })
+
+/**
+ * Descriptor DMA SG Buffer setup structure (SG buffer). This structure is
+ * also used for setting up a buffer for Circular DDMA.
+ */
+struct _ddma_sg_buffer_setup {
+#define BS_SG_VAL_DESC_LEN	6
+	/* The OUT EP address */
+	uint8_t bOutEndpointAddress;
+	/* The IN EP address */
+	uint8_t bInEndpointAddress;
+	/* Number of bytes to put between transfer segments (must be DWORD boundaries) */
+	uint8_t bOffset;
+	/* The number of transfer segments (a DMA descriptors per each segment) */
+	uint8_t bCount;
+	/* Size (in byte) of each transfer segment */
+	uint16_t wSize;
+} __attribute__ ((packed));
+typedef struct _ddma_sg_buffer_setup ddma_sg_buffer_setup_t;
+
+/** Descriptor DMA Concatenation Buffer setup structure */
+struct _ddma_concat_buffer_setup_hdr {
+#define BS_CONCAT_VAL_HDR_LEN	4
+	/* The endpoint for which the buffer is to be set up */
+	uint8_t bEndpointAddress;
+	/* The count of descriptors to be used */
+	uint8_t bDescCount;
+	/* The total size of the transfer */
+	uint16_t wSize;
+} __attribute__ ((packed));
+typedef struct _ddma_concat_buffer_setup_hdr ddma_concat_buffer_setup_hdr_t;
+
+/** Descriptor DMA Concatenation Buffer setup structure */
+struct _ddma_concat_buffer_setup {
+	/* The SG header */
+	ddma_concat_buffer_setup_hdr_t hdr;
+
+	/* The XFER sizes pointer (allocated dynamically) */
+	uint16_t *wTxBytes;
+} __attribute__ ((packed));
+typedef struct _ddma_concat_buffer_setup ddma_concat_buffer_setup_t;
+
+/** Descriptor DMA Alignment Buffer setup structure */
+struct _ddma_align_buffer_setup {
+#define BS_ALIGN_VAL_HDR_LEN	2
+	uint8_t bEndpointAddress;
+	uint8_t bAlign;
+} __attribute__ ((packed));
+typedef struct _ddma_align_buffer_setup ddma_align_buffer_setup_t;
+
+/** Transmit FIFO Size setup structure */
+struct _tx_fifo_size_setup {
+	uint8_t bEndpointAddress;
+	uint16_t wDepth;
+} __attribute__ ((packed));
+typedef struct _tx_fifo_size_setup tx_fifo_size_setup_t;
+
+/** Transmit FIFO Size setup structure */
+struct _rx_fifo_size_setup {
+	uint16_t wDepth;
+} __attribute__ ((packed));
+typedef struct _rx_fifo_size_setup rx_fifo_size_setup_t;
+
+/**
+ * struct cfi_usb_ctrlrequest - the CFI implementation of the struct usb_ctrlrequest
+ * This structure encapsulates the standard usb_ctrlrequest and adds a pointer
+ * to the data returned in the data stage of a 3-stage Control Write requests.
+ */
+struct cfi_usb_ctrlrequest {
+	uint8_t bRequestType;
+	uint8_t bRequest;
+	uint16_t wValue;
+	uint16_t wIndex;
+	uint16_t wLength;
+	uint8_t *data;
+} UPACKED;
+
+/*---------------------------------------------------------------------------*/
+
+/**
+ * The CFI wrapper of the enabled and activated dwc_otg_pcd_ep structures.
+ * This structure is used to store the buffer setup data for any
+ * enabled endpoint in the PCD.
+ */
+struct cfi_ep {
+	/* Entry for the list container */
+	dwc_list_link_t lh;
+	/* Pointer to the active PCD endpoint structure */
+	struct dwc_otg_pcd_ep *ep;
+	/* The last descriptor in the chain of DMA descriptors of the endpoint */
+	struct dwc_otg_dma_desc *dma_desc_last;
+	/* The SG feature value */
+	ddma_sg_buffer_setup_t *bm_sg;
+	/* The Circular feature value */
+	ddma_sg_buffer_setup_t *bm_circ;
+	/* The Concatenation feature value */
+	ddma_concat_buffer_setup_t *bm_concat;
+	/* The Alignment feature value */
+	ddma_align_buffer_setup_t *bm_align;
+	/* XFER length */
+	uint32_t xfer_len;
+	/*
+	 * Count of DMA descriptors currently used.
+	 * The total should not exceed the MAX_DMA_DESCS_PER_EP value
+	 * defined in the dwc_otg_cil.h
+	 */
+	uint32_t desc_count;
+};
+typedef struct cfi_ep cfi_ep_t;
+
+typedef struct cfi_dma_buff {
+#define CFI_IN_BUF_LEN	1024
+#define CFI_OUT_BUF_LEN	1024
+	dma_addr_t addr;
+	uint8_t *buf;
+} cfi_dma_buff_t;
+
+struct cfiobject;
+
+/**
+ * This is the interface for the CFI operations.
+ *
+ * @param	ep_enable			Called when any endpoint is enabled and activated.
+ * @param	release				Called when the CFI object is released and it needs to correctly
+ *								deallocate the dynamic memory
+ * @param	ctrl_write_complete	Called when the data stage of the request is complete
+ */
+typedef struct cfi_ops {
+	int (*ep_enable) (struct cfiobject * cfi, struct dwc_otg_pcd * pcd,
+			  struct dwc_otg_pcd_ep * ep);
+	void *(*ep_alloc_buf) (struct cfiobject * cfi, struct dwc_otg_pcd * pcd,
+			       struct dwc_otg_pcd_ep * ep, dma_addr_t * dma,
+			       unsigned size, gfp_t flags);
+	void (*release) (struct cfiobject * cfi);
+	int (*ctrl_write_complete) (struct cfiobject * cfi,
+				    struct dwc_otg_pcd * pcd);
+	void (*build_descriptors) (struct cfiobject * cfi,
+				   struct dwc_otg_pcd * pcd,
+				   struct dwc_otg_pcd_ep * ep,
+				   dwc_otg_pcd_request_t * req);
+} cfi_ops_t;
+
+struct cfiobject {
+	cfi_ops_t ops;
+	struct dwc_otg_pcd *pcd;
+	struct usb_gadget *gadget;
+
+	/* Buffers used to send/receive CFI-related request data */
+	cfi_dma_buff_t buf_in;
+	cfi_dma_buff_t buf_out;
+
+	/* CFI specific Control request wrapper */
+	struct cfi_usb_ctrlrequest ctrl_req;
+
+	/* The list of active EP's in the PCD of type cfi_ep_t */
+	dwc_list_link_t active_eps;
+
+	/* This flag shall control the propagation of a specific request
+	 * to the gadget's processing routines.
+	 * 0 - no gadget handling
+	 * 1 - the gadget needs to know about this request (w/o completing a status
+	 * phase - just return a 0 to the _setup callback)
+	 */
+	uint8_t need_gadget_att;
+
+	/* Flag indicating whether the status IN phase needs to be
+	 * completed by the PCD
+	 */
+	uint8_t need_status_in_complete;
+};
+typedef struct cfiobject cfiobject_t;
+
+#define DUMP_MSG
+
+#if defined(DUMP_MSG)
+static inline void dump_msg(const u8 * buf, unsigned int length)
+{
+	unsigned int start, num, i;
+	char line[52], *p;
+
+	if (length >= 512)
+		return;
+
+	start = 0;
+	while (length > 0) {
+		num = min(length, 16u);
+		p = line;
+		for (i = 0; i < num; ++i) {
+			if (i == 8)
+				*p++ = ' ';
+			DWC_SPRINTF(p, " %02x", buf[i]);
+			p += 3;
+		}
+		*p = 0;
+		DWC_DEBUG("%6x: %s\n", start, line);
+		buf += num;
+		start += num;
+		length -= num;
+	}
+}
+#else
+static inline void dump_msg(const u8 * buf, unsigned int length)
+{
+}
+#endif
+
+/**
+ * This function returns a pointer to cfi_ep_t object with the addr address.
+ */
+static inline struct cfi_ep *get_cfi_ep_by_addr(struct cfiobject *cfi,
+						uint8_t addr)
+{
+	struct cfi_ep *pcfiep;
+	dwc_list_link_t *tmp;
+
+	DWC_LIST_FOREACH(tmp, &cfi->active_eps) {
+		pcfiep = DWC_LIST_ENTRY(tmp, struct cfi_ep, lh);
+
+		if (pcfiep->ep->desc->bEndpointAddress == addr) {
+			return pcfiep;
+		}
+	}
+
+	return NULL;
+}
+
+/**
+ * This function returns a pointer to cfi_ep_t object that matches
+ * the dwc_otg_pcd_ep object.
+ */
+static inline struct cfi_ep *get_cfi_ep_by_pcd_ep(struct cfiobject *cfi,
+						  struct dwc_otg_pcd_ep *ep)
+{
+	struct cfi_ep *pcfiep = NULL;
+	dwc_list_link_t *tmp;
+
+	DWC_LIST_FOREACH(tmp, &cfi->active_eps) {
+		pcfiep = DWC_LIST_ENTRY(tmp, struct cfi_ep, lh);
+		if (pcfiep->ep == ep) {
+			return pcfiep;
+		}
+	}
+	return NULL;
+}
+
+int cfi_setup(struct dwc_otg_pcd *pcd, struct cfi_usb_ctrlrequest *ctrl);
+
+#endif /* (__DWC_OTG_CFI_H__) */
diff --git a/drivers/usb/dwc_otg/dwc_otg_cil.c b/drivers/usb/dwc_otg/dwc_otg_cil.c
new file mode 100644
index 0000000..db8d6be
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_otg_cil.c
@@ -0,0 +1,7087 @@
+/* ==========================================================================
+ * $File: //dwh/usb_iip/dev/software/otg/linux/drivers/dwc_otg_cil.c $
+ * $Revision: #189 $
+ * $Date: 2011/10/24 $
+ * $Change: 1871160 $
+ *
+ * Synopsys HS OTG Linux Software Driver and documentation (hereinafter,
+ * "Software") is an Unsupported proprietary work of Synopsys, Inc. unless
+ * otherwise expressly agreed to in writing between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product under
+ * any End User Software License Agreement or Agreement for Licensed Product
+ * with Synopsys or any supplement thereto. You are permitted to use and
+ * redistribute this Software in source and binary forms, with or without
+ * modification, provided that redistributions of source code must retain this
+ * notice. You may not view, use, disclose, copy or distribute this file or
+ * any information contained herein except pursuant to this license grant from
+ * Synopsys. If you do not agree with this notice, including the disclaimer
+ * below, then you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS" BASIS
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE HEREBY DISCLAIMED. IN NO EVENT SHALL SYNOPSYS BE LIABLE FOR ANY DIRECT,
+ * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================== */
+
+/** @file
+ *
+ * The Core Interface Layer provides basic services for accessing and
+ * managing the DWC_otg hardware. These services are used by both the
+ * Host Controller Driver and the Peripheral Controller Driver.
+ *
+ * The CIL manages the memory map for the core so that the HCD and PCD
+ * don't have to do this separately. It also handles basic tasks like
+ * reading/writing the registers and data FIFOs in the controller.
+ * Some of the data access functions provide encapsulation of several
+ * operations required to perform a task, such as writing multiple
+ * registers to start a transfer. Finally, the CIL performs basic
+ * services that are not specific to either the host or device modes
+ * of operation. These services include management of the OTG Host
+ * Negotiation Protocol (HNP) and Session Request Protocol (SRP). A
+ * Diagnostic API is also provided to allow testing of the controller
+ * hardware.
+ *
+ * The Core Interface Layer has the following requirements:
+ * - Provides basic controller operations.
+ * - Minimal use of OS services.
+ * - The OS services used will be abstracted by using inline functions
+ *	 or macros.
+ *
+ */
+
+#include "dwc_os.h"
+#include "dwc_otg_regs.h"
+#include "dwc_otg_cil.h"
+
+static int dwc_otg_setup_params(dwc_otg_core_if_t * core_if);
+
+/**
+ * This function is called to initialize the DWC_otg CSR data
+ * structures. The register addresses in the device and host
+ * structures are initialized from the base address supplied by the
+ * caller. The calling function must make the OS calls to get the
+ * base address of the DWC_otg controller registers. The core_params
+ * argument holds the parameters that specify how the core should be
+ * configured.
+ *
+ * @param reg_base_addr Base address of DWC_otg core registers
+ *
+ */
+dwc_otg_core_if_t *dwc_otg_cil_init(const uint32_t * reg_base_addr)
+{
+	dwc_otg_core_if_t *core_if = 0;
+	dwc_otg_dev_if_t *dev_if = 0;
+	dwc_otg_host_if_t *host_if = 0;
+	uint8_t *reg_base = (uint8_t *) reg_base_addr;
+	int i = 0;
+
+	DWC_DEBUGPL(DBG_CILV, "%s(%p)\n", __func__, reg_base_addr);
+
+	core_if = DWC_ALLOC(sizeof(dwc_otg_core_if_t));
+
+	if (core_if == NULL) {
+		DWC_DEBUGPL(DBG_CIL,
+			    "Allocation of dwc_otg_core_if_t failed\n");
+		return 0;
+	}
+	core_if->core_global_regs = (dwc_otg_core_global_regs_t *) reg_base;
+
+	/*
+	 * Allocate the Device Mode structures.
+	 */
+	dev_if = DWC_ALLOC(sizeof(dwc_otg_dev_if_t));
+
+	if (dev_if == NULL) {
+		DWC_DEBUGPL(DBG_CIL, "Allocation of dwc_otg_dev_if_t failed\n");
+		DWC_FREE(core_if);
+		return 0;
+	}
+
+	dev_if->dev_global_regs =
+	    (dwc_otg_device_global_regs_t *) (reg_base +
+					      DWC_DEV_GLOBAL_REG_OFFSET);
+
+	for (i = 0; i < MAX_EPS_CHANNELS; i++) {
+		dev_if->in_ep_regs[i] = (dwc_otg_dev_in_ep_regs_t *)
+		    (reg_base + DWC_DEV_IN_EP_REG_OFFSET +
+		     (i * DWC_EP_REG_OFFSET));
+
+		dev_if->out_ep_regs[i] = (dwc_otg_dev_out_ep_regs_t *)
+		    (reg_base + DWC_DEV_OUT_EP_REG_OFFSET +
+		     (i * DWC_EP_REG_OFFSET));
+		DWC_DEBUGPL(DBG_CILV, "in_ep_regs[%d]->diepctl=%p\n",
+			    i, &dev_if->in_ep_regs[i]->diepctl);
+		DWC_DEBUGPL(DBG_CILV, "out_ep_regs[%d]->doepctl=%p\n",
+			    i, &dev_if->out_ep_regs[i]->doepctl);
+	}
+
+	dev_if->speed = 0;	// unknown
+
+	core_if->dev_if = dev_if;
+
+	/*
+	 * Allocate the Host Mode structures.
+	 */
+	host_if = DWC_ALLOC(sizeof(dwc_otg_host_if_t));
+
+	if (host_if == NULL) {
+		DWC_DEBUGPL(DBG_CIL,
+			    "Allocation of dwc_otg_host_if_t failed\n");
+		DWC_FREE(dev_if);
+		DWC_FREE(core_if);
+		return 0;
+	}
+
+	host_if->host_global_regs = (dwc_otg_host_global_regs_t *)
+	    (reg_base + DWC_OTG_HOST_GLOBAL_REG_OFFSET);
+
+	host_if->hprt0 =
+	    (uint32_t *) (reg_base + DWC_OTG_HOST_PORT_REGS_OFFSET);
+
+	for (i = 0; i < MAX_EPS_CHANNELS; i++) {
+		host_if->hc_regs[i] = (dwc_otg_hc_regs_t *)
+		    (reg_base + DWC_OTG_HOST_CHAN_REGS_OFFSET +
+		     (i * DWC_OTG_CHAN_REGS_OFFSET));
+		DWC_DEBUGPL(DBG_CILV, "hc_reg[%d]->hcchar=%p\n",
+			    i, &host_if->hc_regs[i]->hcchar);
+	}
+
+	host_if->num_host_channels = MAX_EPS_CHANNELS;
+	core_if->host_if = host_if;
+
+	for (i = 0; i < MAX_EPS_CHANNELS; i++) {
+		core_if->data_fifo[i] =
+		    (uint32_t *) (reg_base + DWC_OTG_DATA_FIFO_OFFSET +
+				  (i * DWC_OTG_DATA_FIFO_SIZE));
+		DWC_DEBUGPL(DBG_CILV, "data_fifo[%d]=0x%08lx\n",
+			    i, (unsigned long)core_if->data_fifo[i]);
+	}
+
+	core_if->pcgcctl = (uint32_t *) (reg_base + DWC_OTG_PCGCCTL_OFFSET);
+
+	/* Initiate lx_state to L3 disconnected state */
+	core_if->lx_state = DWC_OTG_L3;
+	/*
+	 * Store the contents of the hardware configuration registers here for
+	 * easy access later.
+	 */
+	core_if->hwcfg1.d32 =
+	    DWC_READ_REG32(&core_if->core_global_regs->ghwcfg1);
+	core_if->hwcfg2.d32 =
+	    DWC_READ_REG32(&core_if->core_global_regs->ghwcfg2);
+	core_if->hwcfg3.d32 =
+	    DWC_READ_REG32(&core_if->core_global_regs->ghwcfg3);
+	core_if->hwcfg4.d32 =
+	    DWC_READ_REG32(&core_if->core_global_regs->ghwcfg4);
+
+	/* Force host mode to get HPTXFSIZ exact power on value */
+	{
+		gusbcfg_data_t gusbcfg = {.d32 = 0 };
+		gusbcfg.d32 =  DWC_READ_REG32(&core_if->core_global_regs->gusbcfg);
+		gusbcfg.b.force_host_mode = 1;
+		DWC_WRITE_REG32(&core_if->core_global_regs->gusbcfg, gusbcfg.d32);
+		dwc_mdelay(100);
+		core_if->hptxfsiz.d32 =
+		DWC_READ_REG32(&core_if->core_global_regs->hptxfsiz);
+		gusbcfg.d32 =  DWC_READ_REG32(&core_if->core_global_regs->gusbcfg);
+		gusbcfg.b.force_host_mode = 0;
+		DWC_WRITE_REG32(&core_if->core_global_regs->gusbcfg, gusbcfg.d32);
+		dwc_mdelay(100);
+	}
+
+#ifdef CONFIG_MACH_M822XX
+	{
+		/* force gotgctl for VBUS override as always valid */
+		gotgctl_data_t gotgctl = {.d32 = 0 };
+
+		gotgctl.d32 =  DWC_READ_REG32(&core_if->core_global_regs->gotgctl);
+		gotgctl.d32 |= ((1<<3) | (1<<2)); /* force bits 3 and 2 to override VBUS check as always valid */
+		DWC_WRITE_REG32(&core_if->core_global_regs->gotgctl, gotgctl.d32);
+
+		DWC_DEBUGPL(DBG_CILV, "Set forced VBUS check off\n");
+	}
+#endif	/* CONFIG_MACH_M822XX */
+
+	DWC_DEBUGPL(DBG_CILV, "hwcfg1=%08x\n", core_if->hwcfg1.d32);
+	DWC_DEBUGPL(DBG_CILV, "hwcfg2=%08x\n", core_if->hwcfg2.d32);
+	DWC_DEBUGPL(DBG_CILV, "hwcfg3=%08x\n", core_if->hwcfg3.d32);
+	DWC_DEBUGPL(DBG_CILV, "hwcfg4=%08x\n", core_if->hwcfg4.d32);
+
+	core_if->hcfg.d32 =
+	    DWC_READ_REG32(&core_if->host_if->host_global_regs->hcfg);
+	core_if->dcfg.d32 =
+	    DWC_READ_REG32(&core_if->dev_if->dev_global_regs->dcfg);
+
+	DWC_DEBUGPL(DBG_CILV, "hcfg=%08x\n", core_if->hcfg.d32);
+	DWC_DEBUGPL(DBG_CILV, "dcfg=%08x\n", core_if->dcfg.d32);
+
+	DWC_DEBUGPL(DBG_CILV, "op_mode=%0x\n", core_if->hwcfg2.b.op_mode);
+	DWC_DEBUGPL(DBG_CILV, "arch=%0x\n", core_if->hwcfg2.b.architecture);
+	DWC_DEBUGPL(DBG_CILV, "num_dev_ep=%d\n", core_if->hwcfg2.b.num_dev_ep);
+	DWC_DEBUGPL(DBG_CILV, "num_host_chan=%d\n",
+		    core_if->hwcfg2.b.num_host_chan);
+	DWC_DEBUGPL(DBG_CILV, "nonperio_tx_q_depth=0x%0x\n",
+		    core_if->hwcfg2.b.nonperio_tx_q_depth);
+	DWC_DEBUGPL(DBG_CILV, "host_perio_tx_q_depth=0x%0x\n",
+		    core_if->hwcfg2.b.host_perio_tx_q_depth);
+	DWC_DEBUGPL(DBG_CILV, "dev_token_q_depth=0x%0x\n",
+		    core_if->hwcfg2.b.dev_token_q_depth);
+
+	DWC_DEBUGPL(DBG_CILV, "Total FIFO SZ=%d\n",
+		    core_if->hwcfg3.b.dfifo_depth);
+	DWC_DEBUGPL(DBG_CILV, "xfer_size_cntr_width=%0x\n",
+		    core_if->hwcfg3.b.xfer_size_cntr_width);
+
+	/*
+	 * Set the SRP sucess bit for FS-I2c
+	 */
+	core_if->srp_success = 0;
+	core_if->srp_timer_started = 0;
+
+	/*
+	 * Create new workqueue and init works
+	 */
+	core_if->wq_otg = DWC_WORKQ_ALLOC("dwc_otg");
+	if (core_if->wq_otg == 0) {
+		DWC_WARN("DWC_WORKQ_ALLOC failed\n");
+		DWC_FREE(host_if);
+		DWC_FREE(dev_if);
+		DWC_FREE(core_if);
+		return 0;
+	}
+
+	core_if->snpsid = DWC_READ_REG32(&core_if->core_global_regs->gsnpsid);
+
+	DWC_PRINTF("Core Release: %x.%x%x%x\n",
+		   (core_if->snpsid >> 12 & 0xF),
+		   (core_if->snpsid >> 8 & 0xF),
+		   (core_if->snpsid >> 4 & 0xF), (core_if->snpsid & 0xF));
+
+	core_if->wkp_timer = DWC_TIMER_ALLOC("Wake Up Timer",
+					     w_wakeup_detected, core_if);
+	if (core_if->wkp_timer == 0) {
+		DWC_WARN("DWC_TIMER_ALLOC failed\n");
+		DWC_FREE(host_if);
+		DWC_FREE(dev_if);
+		DWC_WORKQ_FREE(core_if->wq_otg);
+		DWC_FREE(core_if);
+		return 0;
+	}
+
+	if (dwc_otg_setup_params(core_if)) {
+		DWC_WARN("Error while setting core params\n");
+	}
+
+	core_if->hibernation_suspend = 0;
+
+	/** ADP initialization */
+	dwc_otg_adp_init(core_if);
+
+	return core_if;
+}
+
+/**
+ * This function frees the structures allocated by dwc_otg_cil_init().
+ *
+ * @param core_if The core interface pointer returned from
+ * 		  dwc_otg_cil_init().
+ *
+ */
+void dwc_otg_cil_remove(dwc_otg_core_if_t * core_if)
+{
+	/* Disable all interrupts */
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gahbcfg, 1, 0);
+	DWC_WRITE_REG32(&core_if->core_global_regs->gintmsk, 0);
+
+	if (core_if->wq_otg) {
+		DWC_WORKQ_WAIT_WORK_DONE(core_if->wq_otg, 500);
+		DWC_WORKQ_FREE(core_if->wq_otg);
+	}
+	if (core_if->dev_if) {
+		DWC_FREE(core_if->dev_if);
+	}
+	if (core_if->host_if) {
+		DWC_FREE(core_if->host_if);
+	}
+
+	/** Remove ADP Stuff  */
+	dwc_otg_adp_remove(core_if);
+	if (core_if->core_params) {
+		DWC_FREE(core_if->core_params);
+	}
+	if (core_if->wkp_timer) {
+		DWC_TIMER_FREE(core_if->wkp_timer);
+	}
+	if (core_if->srp_timer) {
+		DWC_TIMER_FREE(core_if->srp_timer);
+	}
+	DWC_FREE(core_if);
+}
+
+/**
+ * This function enables the controller's Global Interrupt in the AHB Config
+ * register.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ */
+void dwc_otg_enable_global_interrupts(dwc_otg_core_if_t * core_if)
+{
+	gahbcfg_data_t ahbcfg = {.d32 = 0 };
+	ahbcfg.b.glblintrmsk = 1;	/* Enable interrupts */
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gahbcfg, 0, ahbcfg.d32);
+}
+
+/**
+ * This function disables the controller's Global Interrupt in the AHB Config
+ * register.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ */
+void dwc_otg_disable_global_interrupts(dwc_otg_core_if_t * core_if)
+{
+	gahbcfg_data_t ahbcfg = {.d32 = 0 };
+	ahbcfg.b.glblintrmsk = 1;	/* Disable interrupts */
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gahbcfg, ahbcfg.d32, 0);
+}
+
+/**
+ * This function initializes the commmon interrupts, used in both
+ * device and host modes.
+ *
+ * @param core_if Programming view of the DWC_otg controller
+ *
+ */
+static void dwc_otg_enable_common_interrupts(dwc_otg_core_if_t * core_if)
+{
+	dwc_otg_core_global_regs_t *global_regs = core_if->core_global_regs;
+	gintmsk_data_t intr_mask = {.d32 = 0 };
+
+	/* Clear any pending OTG Interrupts */
+	DWC_WRITE_REG32(&global_regs->gotgint, 0xFFFFFFFF);
+
+	/* Clear any pending interrupts */
+	DWC_WRITE_REG32(&global_regs->gintsts, 0xFFFFFFFF);
+
+	/*
+	 * Enable the interrupts in the GINTMSK.
+	 */
+	intr_mask.b.modemismatch = 1;
+	intr_mask.b.otgintr = 1;
+
+	if (!core_if->dma_enable) {
+		intr_mask.b.rxstsqlvl = 1;
+	}
+
+	intr_mask.b.conidstschng = 1;
+	intr_mask.b.wkupintr = 1;
+	intr_mask.b.disconnect = 0;
+	intr_mask.b.usbsuspend = 1;
+	intr_mask.b.sessreqintr = 1;
+#ifdef CONFIG_USB_DWC_OTG_LPM
+	if (core_if->core_params->lpm_enable) {
+		intr_mask.b.lpmtranrcvd = 1;
+	}
+#endif
+	DWC_WRITE_REG32(&global_regs->gintmsk, intr_mask.d32);
+}
+
+/*
+ * The restore operation is modified to support Synopsys Emulated Powerdown and
+ * Hibernation. This function is for exiting from Device mode hibernation by
+ * Host Initiated Resume/Reset and Device Initiated Remote-Wakeup.
+ * @param core_if Programming view of DWC_otg controller.
+ * @param rem_wakeup - indicates whether resume is initiated by Device or Host.
+ * @param reset - indicates whether resume is initiated by Reset.
+ */
+int dwc_otg_device_hibernation_restore(dwc_otg_core_if_t * core_if,
+				       int rem_wakeup, int reset)
+{
+	gpwrdn_data_t gpwrdn = {.d32 = 0 };
+	pcgcctl_data_t pcgcctl = {.d32 = 0 };
+	dctl_data_t dctl = {.d32 = 0 };
+
+	int timeout = 2000;
+
+	if (!core_if->hibernation_suspend) {
+		DWC_PRINTF("Already exited from Hibernation\n");
+		return 1;
+	}
+
+	DWC_DEBUGPL(DBG_PCD, "%s called\n", __FUNCTION__);
+	/* Switch-on voltage to the core */
+	gpwrdn.b.pwrdnswtch = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+	dwc_udelay(10);
+
+	/* Reset core */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.pwrdnrstn = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+	dwc_udelay(10);
+
+	/* Assert Restore signal */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.restore = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, 0, gpwrdn.d32);
+	dwc_udelay(10);
+
+	/* Disable power clamps */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.pwrdnclmp = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+
+	if (rem_wakeup) {
+		dwc_udelay(70);
+	}
+
+	/* Deassert Reset core */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.pwrdnrstn = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, 0, gpwrdn.d32);
+	dwc_udelay(10);
+
+	/* Disable PMU interrupt */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.pmuintsel = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+
+	/* Mask interrupts from gpwrdn */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.connect_det_msk = 1;
+	gpwrdn.b.srp_det_msk = 1;
+	gpwrdn.b.disconn_det_msk = 1;
+	gpwrdn.b.rst_det_msk = 1;
+	gpwrdn.b.lnstchng_msk = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+
+	/* Indicates that we are going out from hibernation */
+	core_if->hibernation_suspend = 0;
+
+	/*
+	 * Set Restore Essential Regs bit in PCGCCTL register, restore_mode = 1
+	 * indicates restore from remote_wakeup
+	 */
+	restore_essential_regs(core_if, rem_wakeup, 0);
+
+	/*
+	 * Wait a little for seeing new value of variable hibernation_suspend if
+	 * Restore done interrupt received before polling
+	 */
+	dwc_udelay(10);
+
+	if (core_if->hibernation_suspend == 0) {
+		/*
+		 * Wait For Restore_done Interrupt. This mechanism of polling the
+		 * interrupt is introduced to avoid any possible race conditions
+		 */
+		do {
+			gintsts_data_t gintsts;
+			gintsts.d32 =
+			    DWC_READ_REG32(&core_if->core_global_regs->gintsts);
+			if (gintsts.b.restoredone) {
+				gintsts.d32 = 0;
+				gintsts.b.restoredone = 1;
+				DWC_WRITE_REG32(&core_if->core_global_regs->
+						gintsts, gintsts.d32);
+				DWC_PRINTF("Restore Done Interrupt seen\n");
+				break;
+			}
+			dwc_udelay(10);
+		} while (--timeout);
+		if (!timeout) {
+			DWC_PRINTF("Restore Done interrupt wasn't generated here\n");
+		}
+	}
+	/* Clear all pending interupts */
+	DWC_WRITE_REG32(&core_if->core_global_regs->gintsts, 0xFFFFFFFF);
+
+	/* De-assert Restore */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.restore = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+	dwc_udelay(10);
+
+	if (!rem_wakeup) {
+		pcgcctl.d32 = 0;
+		pcgcctl.b.rstpdwnmodule = 1;
+		DWC_MODIFY_REG32(core_if->pcgcctl, pcgcctl.d32, 0);
+	}
+
+	/* Restore GUSBCFG and DCFG */
+	DWC_WRITE_REG32(&core_if->core_global_regs->gusbcfg,
+			core_if->gr_backup->gusbcfg_local);
+	DWC_WRITE_REG32(&core_if->dev_if->dev_global_regs->dcfg,
+			core_if->dr_backup->dcfg);
+
+	/* De-assert Wakeup Logic */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.pmuactv = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+	dwc_udelay(10);
+
+	if (!rem_wakeup) {
+		/* Set Device programming done bit */
+		dctl.b.pwronprgdone = 1;
+		DWC_MODIFY_REG32(&core_if->dev_if->dev_global_regs->dctl, 0, dctl.d32);
+	} else {
+		/* Start Remote Wakeup Signaling */
+		dctl.d32 = core_if->dr_backup->dctl;
+		dctl.b.rmtwkupsig = 1;
+		DWC_WRITE_REG32(&core_if->dev_if->dev_global_regs->dctl, dctl.d32);
+	}
+
+	dwc_mdelay(2);
+	/* Clear all pending interupts */
+	DWC_WRITE_REG32(&core_if->core_global_regs->gintsts, 0xFFFFFFFF);
+
+	/* Restore global registers */
+	dwc_otg_restore_global_regs(core_if);
+	/* Restore device global registers */
+	dwc_otg_restore_dev_regs(core_if, rem_wakeup);
+
+	if (rem_wakeup) {
+		dwc_mdelay(7);
+		dctl.d32 = 0;
+		dctl.b.rmtwkupsig = 1;
+		DWC_MODIFY_REG32(&core_if->dev_if->dev_global_regs->dctl, dctl.d32, 0);
+	}
+
+	core_if->hibernation_suspend = 0;
+	/* The core will be in ON STATE */
+	core_if->lx_state = DWC_OTG_L0;
+	DWC_PRINTF("Hibernation recovery completes here\n");
+
+	return 1;
+}
+
+/*
+ * The restore operation is modified to support Synopsys Emulated Powerdown and
+ * Hibernation. This function is for exiting from Host mode hibernation by
+ * Host Initiated Resume/Reset and Device Initiated Remote-Wakeup.
+ * @param core_if Programming view of DWC_otg controller.
+ * @param rem_wakeup - indicates whether resume is initiated by Device or Host.
+ * @param reset - indicates whether resume is initiated by Reset.
+ */
+int dwc_otg_host_hibernation_restore(dwc_otg_core_if_t * core_if,
+				     int rem_wakeup, int reset)
+{
+	gpwrdn_data_t gpwrdn = {.d32 = 0 };
+	hprt0_data_t hprt0 = {.d32 = 0 };
+
+	int timeout = 2000;
+
+	DWC_DEBUGPL(DBG_HCD, "%s called\n", __FUNCTION__);
+	/* Switch-on voltage to the core */
+	gpwrdn.b.pwrdnswtch = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+	dwc_udelay(10);
+
+	/* Reset core */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.pwrdnrstn = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+	dwc_udelay(10);
+
+	/* Assert Restore signal */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.restore = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, 0, gpwrdn.d32);
+	dwc_udelay(10);
+
+	/* Disable power clamps */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.pwrdnclmp = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+
+	if (!rem_wakeup) {
+		dwc_udelay(50);
+	}
+
+	/* Deassert Reset core */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.pwrdnrstn = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, 0, gpwrdn.d32);
+	dwc_udelay(10);
+
+	/* Disable PMU interrupt */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.pmuintsel = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+
+	gpwrdn.d32 = 0;
+	gpwrdn.b.connect_det_msk = 1;
+	gpwrdn.b.srp_det_msk = 1;
+	gpwrdn.b.disconn_det_msk = 1;
+	gpwrdn.b.rst_det_msk = 1;
+	gpwrdn.b.lnstchng_msk = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+
+	/* Indicates that we are going out from hibernation */
+	core_if->hibernation_suspend = 0;
+
+	/* Set Restore Essential Regs bit in PCGCCTL register */
+	restore_essential_regs(core_if, rem_wakeup, 1);
+
+	/* Wait a little for seeing new value of variable hibernation_suspend if
+	 * Restore done interrupt received before polling */
+	dwc_udelay(10);
+
+	if (core_if->hibernation_suspend == 0) {
+		/* Wait For Restore_done Interrupt. This mechanism of polling the
+		 * interrupt is introduced to avoid any possible race conditions
+		 */
+		do {
+			gintsts_data_t gintsts;
+			gintsts.d32 = DWC_READ_REG32(&core_if->core_global_regs->gintsts);
+			if (gintsts.b.restoredone) {
+				gintsts.d32 = 0;
+				gintsts.b.restoredone = 1;
+         		DWC_WRITE_REG32(&core_if->core_global_regs->gintsts, gintsts.d32);
+				DWC_DEBUGPL(DBG_HCD,"Restore Done Interrupt seen\n");
+				break;
+			}
+			dwc_udelay(10);
+		} while (--timeout);
+		if (!timeout) {
+			DWC_WARN("Restore Done interrupt wasn't generated\n");
+		}
+	}
+
+	/* Set the flag's value to 0 again after receiving restore done interrupt */
+	core_if->hibernation_suspend = 0;
+
+	/* This step is not described in functional spec but if not wait for this
+	 * delay, mismatch interrupts occurred because just after restore core is
+	 * in Device mode(gintsts.curmode == 0) */
+	dwc_mdelay(100);
+
+	/* Clear all pending interrupts */
+	DWC_WRITE_REG32(&core_if->core_global_regs->gintsts, 0xFFFFFFFF);
+
+	/* De-assert Restore */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.restore = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+	dwc_udelay(10);
+
+	/* Restore GUSBCFG and HCFG */
+	DWC_WRITE_REG32(&core_if->core_global_regs->gusbcfg,
+			core_if->gr_backup->gusbcfg_local);
+	DWC_WRITE_REG32(&core_if->host_if->host_global_regs->hcfg,
+			core_if->hr_backup->hcfg_local);
+
+	/* De-assert Wakeup Logic */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.pmuactv = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+	dwc_udelay(10);
+
+	/* Start the Resume operation by programming HPRT0 */
+	hprt0.d32 = core_if->hr_backup->hprt0_local;
+	hprt0.b.prtpwr = 1;
+	hprt0.b.prtena = 0;
+	hprt0.b.prtsusp = 0;
+	DWC_WRITE_REG32(core_if->host_if->hprt0, hprt0.d32);
+
+	DWC_PRINTF("Resume Starts Now\n");
+	if (!reset) {		// Indicates it is Resume Operation
+		hprt0.d32 = core_if->hr_backup->hprt0_local;
+		hprt0.b.prtres = 1;
+		hprt0.b.prtpwr = 1;
+		hprt0.b.prtena = 0;
+		hprt0.b.prtsusp = 0;
+		DWC_WRITE_REG32(core_if->host_if->hprt0, hprt0.d32);
+
+		if (!rem_wakeup)
+			hprt0.b.prtres = 0;
+		/* Wait for Resume time and then program HPRT again */
+		dwc_mdelay(100);
+		DWC_WRITE_REG32(core_if->host_if->hprt0, hprt0.d32);
+
+	} else {		// Indicates it is Reset Operation
+		hprt0.d32 = core_if->hr_backup->hprt0_local;
+		hprt0.b.prtrst = 1;
+		hprt0.b.prtpwr = 1;
+		hprt0.b.prtena = 0;
+		hprt0.b.prtsusp = 0;
+		DWC_WRITE_REG32(core_if->host_if->hprt0, hprt0.d32);
+		/* Wait for Reset time and then program HPRT again */
+		dwc_mdelay(60);
+		hprt0.b.prtrst = 0;
+		DWC_WRITE_REG32(core_if->host_if->hprt0, hprt0.d32);
+	}
+	/* Clear all interrupt status */
+	hprt0.d32 = dwc_otg_read_hprt0(core_if);
+	hprt0.b.prtconndet = 1;
+	hprt0.b.prtenchng = 1;
+	DWC_WRITE_REG32(core_if->host_if->hprt0, hprt0.d32);
+
+	/* Clear all pending interupts */
+	DWC_WRITE_REG32(&core_if->core_global_regs->gintsts, 0xFFFFFFFF);
+
+	/* Restore global registers */
+	dwc_otg_restore_global_regs(core_if);
+	/* Restore host global registers */
+	dwc_otg_restore_host_regs(core_if, reset);
+
+	/* The core will be in ON STATE */
+	core_if->lx_state = DWC_OTG_L0;
+	DWC_PRINTF("Hibernation recovery is complete here\n");
+	return 0;
+}
+
+/** Saves some register values into system memory. */
+int dwc_otg_save_global_regs(dwc_otg_core_if_t * core_if)
+{
+	struct dwc_otg_global_regs_backup *gr;
+	int i;
+
+	gr = core_if->gr_backup;
+	if (!gr) {
+		gr = DWC_ALLOC(sizeof(*gr));
+		if (!gr) {
+			return -DWC_E_NO_MEMORY;
+		}
+		core_if->gr_backup = gr;
+	}
+
+	gr->gotgctl_local = DWC_READ_REG32(&core_if->core_global_regs->gotgctl);
+	gr->gintmsk_local = DWC_READ_REG32(&core_if->core_global_regs->gintmsk);
+	gr->gahbcfg_local = DWC_READ_REG32(&core_if->core_global_regs->gahbcfg);
+	gr->gusbcfg_local = DWC_READ_REG32(&core_if->core_global_regs->gusbcfg);
+	gr->grxfsiz_local = DWC_READ_REG32(&core_if->core_global_regs->grxfsiz);
+	gr->gnptxfsiz_local = DWC_READ_REG32(&core_if->core_global_regs->gnptxfsiz);
+	gr->hptxfsiz_local = DWC_READ_REG32(&core_if->core_global_regs->hptxfsiz);
+#ifdef CONFIG_USB_DWC_OTG_LPM
+	gr->glpmcfg_local = DWC_READ_REG32(&core_if->core_global_regs->glpmcfg);
+#endif
+	gr->gi2cctl_local = DWC_READ_REG32(&core_if->core_global_regs->gi2cctl);
+	gr->pcgcctl_local = DWC_READ_REG32(core_if->pcgcctl);
+	gr->gdfifocfg_local =
+	    DWC_READ_REG32(&core_if->core_global_regs->gdfifocfg);
+	for (i = 0; i < MAX_EPS_CHANNELS; i++) {
+		gr->dtxfsiz_local[i] =
+		    DWC_READ_REG32(&(core_if->core_global_regs->dtxfsiz[i]));
+	}
+
+	DWC_DEBUGPL(DBG_ANY, "===========Backing Global registers==========\n");
+	DWC_DEBUGPL(DBG_ANY, "Backed up gotgctl   = %08x\n", gr->gotgctl_local);
+	DWC_DEBUGPL(DBG_ANY, "Backed up gintmsk   = %08x\n", gr->gintmsk_local);
+	DWC_DEBUGPL(DBG_ANY, "Backed up gahbcfg   = %08x\n", gr->gahbcfg_local);
+	DWC_DEBUGPL(DBG_ANY, "Backed up gusbcfg   = %08x\n", gr->gusbcfg_local);
+	DWC_DEBUGPL(DBG_ANY, "Backed up grxfsiz   = %08x\n", gr->grxfsiz_local);
+	DWC_DEBUGPL(DBG_ANY, "Backed up gnptxfsiz = %08x\n",
+		    gr->gnptxfsiz_local);
+	DWC_DEBUGPL(DBG_ANY, "Backed up hptxfsiz  = %08x\n",
+		    gr->hptxfsiz_local);
+#ifdef CONFIG_USB_DWC_OTG_LPM
+	DWC_DEBUGPL(DBG_ANY, "Backed up glpmcfg   = %08x\n", gr->glpmcfg_local);
+#endif
+	DWC_DEBUGPL(DBG_ANY, "Backed up gi2cctl   = %08x\n", gr->gi2cctl_local);
+	DWC_DEBUGPL(DBG_ANY, "Backed up pcgcctl   = %08x\n", gr->pcgcctl_local);
+	DWC_DEBUGPL(DBG_ANY,"Backed up gdfifocfg   = %08x\n",gr->gdfifocfg_local);
+
+	return 0;
+}
+
+/** Saves GINTMSK register before setting the msk bits. */
+int dwc_otg_save_gintmsk_reg(dwc_otg_core_if_t * core_if)
+{
+	struct dwc_otg_global_regs_backup *gr;
+
+	gr = core_if->gr_backup;
+	if (!gr) {
+		gr = DWC_ALLOC(sizeof(*gr));
+		if (!gr) {
+			return -DWC_E_NO_MEMORY;
+		}
+		core_if->gr_backup = gr;
+	}
+
+	gr->gintmsk_local = DWC_READ_REG32(&core_if->core_global_regs->gintmsk);
+
+	DWC_DEBUGPL(DBG_ANY,"=============Backing GINTMSK registers============\n");
+	DWC_DEBUGPL(DBG_ANY, "Backed up gintmsk   = %08x\n", gr->gintmsk_local);
+
+	return 0;
+}
+
+int dwc_otg_save_dev_regs(dwc_otg_core_if_t * core_if)
+{
+	struct dwc_otg_dev_regs_backup *dr;
+	int i;
+
+	dr = core_if->dr_backup;
+	if (!dr) {
+		dr = DWC_ALLOC(sizeof(*dr));
+		if (!dr) {
+			return -DWC_E_NO_MEMORY;
+		}
+		core_if->dr_backup = dr;
+	}
+
+	dr->dcfg = DWC_READ_REG32(&core_if->dev_if->dev_global_regs->dcfg);
+	dr->dctl = DWC_READ_REG32(&core_if->dev_if->dev_global_regs->dctl);
+	dr->daintmsk =
+	    DWC_READ_REG32(&core_if->dev_if->dev_global_regs->daintmsk);
+	dr->diepmsk =
+	    DWC_READ_REG32(&core_if->dev_if->dev_global_regs->diepmsk);
+	dr->doepmsk =
+	    DWC_READ_REG32(&core_if->dev_if->dev_global_regs->doepmsk);
+
+	for (i = 0; i < core_if->dev_if->num_in_eps; ++i) {
+		dr->diepctl[i] =
+		    DWC_READ_REG32(&core_if->dev_if->in_ep_regs[i]->diepctl);
+		dr->dieptsiz[i] =
+		    DWC_READ_REG32(&core_if->dev_if->in_ep_regs[i]->dieptsiz);
+		dr->diepdma[i] =
+		    DWC_READ_REG32(&core_if->dev_if->in_ep_regs[i]->diepdma);
+	}
+
+	DWC_DEBUGPL(DBG_ANY,
+		    "=============Backing Host registers==============\n");
+	DWC_DEBUGPL(DBG_ANY, "Backed up dcfg            = %08x\n", dr->dcfg);
+	DWC_DEBUGPL(DBG_ANY, "Backed up dctl        = %08x\n", dr->dctl);
+	DWC_DEBUGPL(DBG_ANY, "Backed up daintmsk            = %08x\n",
+		    dr->daintmsk);
+	DWC_DEBUGPL(DBG_ANY, "Backed up diepmsk        = %08x\n", dr->diepmsk);
+	DWC_DEBUGPL(DBG_ANY, "Backed up doepmsk        = %08x\n", dr->doepmsk);
+	for (i = 0; i < core_if->dev_if->num_in_eps; ++i) {
+		DWC_DEBUGPL(DBG_ANY, "Backed up diepctl[%d]        = %08x\n", i,
+			    dr->diepctl[i]);
+		DWC_DEBUGPL(DBG_ANY, "Backed up dieptsiz[%d]        = %08x\n",
+			    i, dr->dieptsiz[i]);
+		DWC_DEBUGPL(DBG_ANY, "Backed up diepdma[%d]        = %08x\n", i,
+			    dr->diepdma[i]);
+	}
+
+	return 0;
+}
+
+int dwc_otg_save_host_regs(dwc_otg_core_if_t * core_if)
+{
+	struct dwc_otg_host_regs_backup *hr;
+	int i;
+
+	hr = core_if->hr_backup;
+	if (!hr) {
+		hr = DWC_ALLOC(sizeof(*hr));
+		if (!hr) {
+			return -DWC_E_NO_MEMORY;
+		}
+		core_if->hr_backup = hr;
+	}
+
+	hr->hcfg_local =
+	    DWC_READ_REG32(&core_if->host_if->host_global_regs->hcfg);
+	hr->haintmsk_local =
+	    DWC_READ_REG32(&core_if->host_if->host_global_regs->haintmsk);
+	for (i = 0; i < dwc_otg_get_param_host_channels(core_if); ++i) {
+		hr->hcintmsk_local[i] =
+		    DWC_READ_REG32(&core_if->host_if->hc_regs[i]->hcintmsk);
+	}
+	hr->hprt0_local = DWC_READ_REG32(core_if->host_if->hprt0);
+	hr->hfir_local =
+	    DWC_READ_REG32(&core_if->host_if->host_global_regs->hfir);
+
+	DWC_DEBUGPL(DBG_ANY,
+		    "=============Backing Host registers===============\n");
+	DWC_DEBUGPL(DBG_ANY, "Backed up hcfg		= %08x\n",
+		    hr->hcfg_local);
+	DWC_DEBUGPL(DBG_ANY, "Backed up haintmsk = %08x\n", hr->haintmsk_local);
+	for (i = 0; i < dwc_otg_get_param_host_channels(core_if); ++i) {
+		DWC_DEBUGPL(DBG_ANY, "Backed up hcintmsk[%02d]=%08x\n", i,
+			    hr->hcintmsk_local[i]);
+	}
+	DWC_DEBUGPL(DBG_ANY, "Backed up hprt0           = %08x\n",
+		    hr->hprt0_local);
+	DWC_DEBUGPL(DBG_ANY, "Backed up hfir           = %08x\n",
+		    hr->hfir_local);
+
+	return 0;
+}
+
+int dwc_otg_restore_global_regs(dwc_otg_core_if_t *core_if)
+{
+	struct dwc_otg_global_regs_backup *gr;
+	int i;
+
+	gr = core_if->gr_backup;
+	if (!gr) {
+		return -DWC_E_INVALID;
+	}
+
+	DWC_WRITE_REG32(&core_if->core_global_regs->gotgctl, gr->gotgctl_local);
+	DWC_WRITE_REG32(&core_if->core_global_regs->gintmsk, gr->gintmsk_local);
+	DWC_WRITE_REG32(&core_if->core_global_regs->gusbcfg, gr->gusbcfg_local);
+	DWC_WRITE_REG32(&core_if->core_global_regs->gahbcfg, gr->gahbcfg_local);
+	DWC_WRITE_REG32(&core_if->core_global_regs->grxfsiz, gr->grxfsiz_local);
+	DWC_WRITE_REG32(&core_if->core_global_regs->gnptxfsiz,
+			gr->gnptxfsiz_local);
+	DWC_WRITE_REG32(&core_if->core_global_regs->hptxfsiz,
+			gr->hptxfsiz_local);
+	DWC_WRITE_REG32(&core_if->core_global_regs->gdfifocfg,
+			gr->gdfifocfg_local);
+	for (i = 0; i < MAX_EPS_CHANNELS; i++) {
+		DWC_WRITE_REG32(&core_if->core_global_regs->dtxfsiz[i],
+				gr->dtxfsiz_local[i]);
+	}
+
+	DWC_WRITE_REG32(&core_if->core_global_regs->gintsts, 0xFFFFFFFF);
+	DWC_WRITE_REG32(core_if->host_if->hprt0, 0x0000100A);
+	DWC_WRITE_REG32(&core_if->core_global_regs->gahbcfg,
+			(gr->gahbcfg_local));
+	return 0;
+}
+
+int dwc_otg_restore_dev_regs(dwc_otg_core_if_t * core_if, int rem_wakeup)
+{
+	struct dwc_otg_dev_regs_backup *dr;
+	int i;
+
+	dr = core_if->dr_backup;
+
+	if (!dr) {
+		return -DWC_E_INVALID;
+	}
+
+	if (!rem_wakeup)
+	{
+		DWC_WRITE_REG32(&core_if->dev_if->dev_global_regs->dctl, dr->dctl);
+	}
+
+	DWC_WRITE_REG32(&core_if->dev_if->dev_global_regs->daintmsk, dr->daintmsk);
+	DWC_WRITE_REG32(&core_if->dev_if->dev_global_regs->diepmsk, dr->diepmsk);
+	DWC_WRITE_REG32(&core_if->dev_if->dev_global_regs->doepmsk, dr->doepmsk);
+
+	for (i = 0; i < core_if->dev_if->num_in_eps; ++i) {
+		DWC_WRITE_REG32(&core_if->dev_if->in_ep_regs[i]->diepctl, dr->diepctl[i]);
+		DWC_WRITE_REG32(&core_if->dev_if->in_ep_regs[i]->dieptsiz, dr->dieptsiz[i]);
+		DWC_WRITE_REG32(&core_if->dev_if->in_ep_regs[i]->diepdma, dr->diepdma[i]);
+	}
+
+	return 0;
+}
+
+int dwc_otg_restore_host_regs(dwc_otg_core_if_t * core_if, int reset)
+{
+	struct dwc_otg_host_regs_backup *hr;
+	int i;
+	hr = core_if->hr_backup;
+
+	if (!hr) {
+		return -DWC_E_INVALID;
+	}
+
+	DWC_WRITE_REG32(&core_if->host_if->host_global_regs->hcfg, hr->hcfg_local);
+	//if (!reset)
+	//{
+	//      DWC_WRITE_REG32(&core_if->host_if->host_global_regs->hfir, hr->hfir_local);
+	//}
+
+	DWC_WRITE_REG32(&core_if->host_if->host_global_regs->haintmsk,
+			hr->haintmsk_local);
+	for (i = 0; i < dwc_otg_get_param_host_channels(core_if); ++i) {
+		DWC_WRITE_REG32(&core_if->host_if->hc_regs[i]->hcintmsk,
+				hr->hcintmsk_local[i]);
+	}
+
+	return 0;
+}
+
+int restore_lpm_i2c_regs(dwc_otg_core_if_t * core_if)
+{
+	struct dwc_otg_global_regs_backup *gr;
+
+	gr = core_if->gr_backup;
+
+	/* Restore values for LPM and I2C */
+#ifdef CONFIG_USB_DWC_OTG_LPM
+	DWC_WRITE_REG32(&core_if->core_global_regs->glpmcfg, gr->glpmcfg_local);
+#endif
+	DWC_WRITE_REG32(&core_if->core_global_regs->gi2cctl, gr->gi2cctl_local);
+
+	return 0;
+}
+
+int restore_essential_regs(dwc_otg_core_if_t * core_if, int rmode, int is_host)
+{
+	struct dwc_otg_global_regs_backup *gr;
+	pcgcctl_data_t pcgcctl = {.d32 = 0 };
+	gahbcfg_data_t gahbcfg = {.d32 = 0 };
+	gusbcfg_data_t gusbcfg = {.d32 = 0 };
+	gintmsk_data_t gintmsk = {.d32 = 0 };
+
+	/* Restore LPM and I2C registers */
+	restore_lpm_i2c_regs(core_if);
+
+	/* Set PCGCCTL to 0 */
+	DWC_WRITE_REG32(core_if->pcgcctl, 0x00000000);
+
+	gr = core_if->gr_backup;
+	/* Load restore values for [31:14] bits */
+	DWC_WRITE_REG32(core_if->pcgcctl,
+			((gr->pcgcctl_local & 0xffffc000) | 0x00020000));
+
+	/* Umnask global Interrupt in GAHBCFG and restore it */
+	gahbcfg.d32 = gr->gahbcfg_local;
+	gahbcfg.b.glblintrmsk = 1;
+	DWC_WRITE_REG32(&core_if->core_global_regs->gahbcfg, gahbcfg.d32);
+
+	/* Clear all pending interupts */
+	DWC_WRITE_REG32(&core_if->core_global_regs->gintsts, 0xFFFFFFFF);
+
+	/* Unmask restore done interrupt */
+	gintmsk.b.restoredone = 1;
+	DWC_WRITE_REG32(&core_if->core_global_regs->gintmsk, gintmsk.d32);
+
+	/* Restore GUSBCFG and HCFG/DCFG */
+	gusbcfg.d32 = core_if->gr_backup->gusbcfg_local;
+	DWC_WRITE_REG32(&core_if->core_global_regs->gusbcfg, gusbcfg.d32);
+
+	if (is_host) {
+		hcfg_data_t hcfg = {.d32 = 0 };
+		hcfg.d32 = core_if->hr_backup->hcfg_local;
+		DWC_WRITE_REG32(&core_if->host_if->host_global_regs->hcfg,
+				hcfg.d32);
+
+		/* Load restore values for [31:14] bits */
+		pcgcctl.d32 = gr->pcgcctl_local & 0xffffc000;
+		pcgcctl.d32 = gr->pcgcctl_local | 0x00020000;
+
+		if (rmode)
+			pcgcctl.b.restoremode = 1;
+		DWC_WRITE_REG32(core_if->pcgcctl, pcgcctl.d32);
+		dwc_udelay(10);
+
+		/* Load restore values for [31:14] bits and set EssRegRestored bit */
+		pcgcctl.d32 = gr->pcgcctl_local | 0xffffc000;
+		pcgcctl.d32 = gr->pcgcctl_local & 0xffffc000;
+		pcgcctl.b.ess_reg_restored = 1;
+		if (rmode)
+			pcgcctl.b.restoremode = 1;
+		DWC_WRITE_REG32(core_if->pcgcctl, pcgcctl.d32);
+	} else {
+		dcfg_data_t dcfg = {.d32 = 0 };
+		dcfg.d32 = core_if->dr_backup->dcfg;
+		DWC_WRITE_REG32(&core_if->dev_if->dev_global_regs->dcfg, dcfg.d32);
+
+		/* Load restore values for [31:14] bits */
+		pcgcctl.d32 = gr->pcgcctl_local & 0xffffc000;
+		pcgcctl.d32 = gr->pcgcctl_local | 0x00020000;
+		if (!rmode) {
+			pcgcctl.d32 |= 0x208;
+		}
+		DWC_WRITE_REG32(core_if->pcgcctl, pcgcctl.d32);
+		dwc_udelay(10);
+
+		/* Load restore values for [31:14] bits */
+		pcgcctl.d32 = gr->pcgcctl_local & 0xffffc000;
+		pcgcctl.d32 = gr->pcgcctl_local | 0x00020000;
+		pcgcctl.b.ess_reg_restored = 1;
+		if (!rmode)
+			pcgcctl.d32 |= 0x208;
+		DWC_WRITE_REG32(core_if->pcgcctl, pcgcctl.d32);
+	}
+
+	return 0;
+}
+
+/**
+ * Initializes the FSLSPClkSel field of the HCFG register depending on the PHY
+ * type.
+ */
+static void init_fslspclksel(dwc_otg_core_if_t * core_if)
+{
+	uint32_t val;
+	hcfg_data_t hcfg;
+
+	if (((core_if->hwcfg2.b.hs_phy_type == 2) &&
+	     (core_if->hwcfg2.b.fs_phy_type == 1) &&
+	     (core_if->core_params->ulpi_fs_ls)) ||
+	    (core_if->core_params->phy_type == DWC_PHY_TYPE_PARAM_FS)) {
+		/* Full speed PHY */
+		val = DWC_HCFG_48_MHZ;
+	} else {
+		/* High speed PHY running at full speed or high speed */
+		val = DWC_HCFG_30_60_MHZ;
+	}
+
+	DWC_DEBUGPL(DBG_CIL, "Initializing HCFG.FSLSPClkSel to 0x%1x\n", val);
+	hcfg.d32 = DWC_READ_REG32(&core_if->host_if->host_global_regs->hcfg);
+	hcfg.b.fslspclksel = val;
+	DWC_WRITE_REG32(&core_if->host_if->host_global_regs->hcfg, hcfg.d32);
+}
+
+/**
+ * Initializes the DevSpd field of the DCFG register depending on the PHY type
+ * and the enumeration speed of the device.
+ */
+static void init_devspd(dwc_otg_core_if_t * core_if)
+{
+	uint32_t val;
+	dcfg_data_t dcfg;
+
+	if (((core_if->hwcfg2.b.hs_phy_type == 2) &&
+	     (core_if->hwcfg2.b.fs_phy_type == 1) &&
+	     (core_if->core_params->ulpi_fs_ls)) ||
+	    (core_if->core_params->phy_type == DWC_PHY_TYPE_PARAM_FS)) {
+		/* Full speed PHY */
+		val = 0x3;
+	} else if (core_if->core_params->speed == DWC_SPEED_PARAM_FULL) {
+		/* High speed PHY running at full speed */
+		val = 0x1;
+	} else {
+		/* High speed PHY running at high speed */
+		val = 0x0;
+	}
+
+	DWC_DEBUGPL(DBG_CIL, "Initializing DCFG.DevSpd to 0x%1x\n", val);
+
+	dcfg.d32 = DWC_READ_REG32(&core_if->dev_if->dev_global_regs->dcfg);
+	dcfg.b.devspd = val;
+	DWC_WRITE_REG32(&core_if->dev_if->dev_global_regs->dcfg, dcfg.d32);
+}
+
+/**
+ * This function calculates the number of IN EPS
+ * using GHWCFG1 and GHWCFG2 registers values
+ *
+ * @param core_if Programming view of the DWC_otg controller
+ */
+static uint32_t calc_num_in_eps(dwc_otg_core_if_t * core_if)
+{
+	uint32_t num_in_eps = 0;
+	uint32_t num_eps = core_if->hwcfg2.b.num_dev_ep;
+	uint32_t hwcfg1 = core_if->hwcfg1.d32 >> 3;
+	uint32_t num_tx_fifos = core_if->hwcfg4.b.num_in_eps;
+	int i;
+
+	for (i = 0; i < num_eps; ++i) {
+		if (!(hwcfg1 & 0x1))
+			num_in_eps++;
+
+		hwcfg1 >>= 2;
+	}
+
+	if (core_if->hwcfg4.b.ded_fifo_en) {
+		num_in_eps =
+		    (num_in_eps > num_tx_fifos) ? num_tx_fifos : num_in_eps;
+	}
+
+	return num_in_eps;
+}
+
+/**
+ * This function calculates the number of OUT EPS
+ * using GHWCFG1 and GHWCFG2 registers values
+ *
+ * @param core_if Programming view of the DWC_otg controller
+ */
+static uint32_t calc_num_out_eps(dwc_otg_core_if_t * core_if)
+{
+	uint32_t num_out_eps = 0;
+	uint32_t num_eps = core_if->hwcfg2.b.num_dev_ep;
+	uint32_t hwcfg1 = core_if->hwcfg1.d32 >> 2;
+	int i;
+
+	for (i = 0; i < num_eps; ++i) {
+		if (!(hwcfg1 & 0x1))
+			num_out_eps++;
+
+		hwcfg1 >>= 2;
+	}
+	return num_out_eps;
+}
+
+/**
+ * This function initializes the DWC_otg controller registers and
+ * prepares the core for device mode or host mode operation.
+ *
+ * @param core_if Programming view of the DWC_otg controller
+ *
+ */
+void dwc_otg_core_init(dwc_otg_core_if_t * core_if)
+{
+	int i = 0;
+	dwc_otg_core_global_regs_t *global_regs = core_if->core_global_regs;
+	dwc_otg_dev_if_t *dev_if = core_if->dev_if;
+	gahbcfg_data_t ahbcfg = {.d32 = 0 };
+	gusbcfg_data_t usbcfg = {.d32 = 0 };
+	gi2cctl_data_t i2cctl = {.d32 = 0 };
+
+	DWC_DEBUGPL(DBG_CILV, "dwc_otg_core_init(%p)\n", core_if);
+
+	/* Common Initialization */
+	usbcfg.d32 = DWC_READ_REG32(&global_regs->gusbcfg);
+
+	/* Program the ULPI External VBUS bit if needed */
+#ifndef CONFIG_MACH_M822XX
+	usbcfg.b.ulpi_ext_vbus_drv =
+	    (core_if->core_params->phy_ulpi_ext_vbus ==
+	     DWC_PHY_ULPI_EXTERNAL_VBUS) ? 1 : 0;
+
+#else  /* CONFIG_MACH_M822XX */
+    /* Force external VBUS for T2200 EVM */
+	usbcfg.b.ulpi_ext_vbus_drv       = 1;
+	usbcfg.b.ulpi_int_vbus_indicator = 1;
+	usbcfg.b.indicator_pass_through  = 0;
+	usbcfg.b.indicator_complement    = 0;
+#endif	/* CONFIG_MACH_M822XX */
+
+	/* Set external TS Dline pulsing */
+	usbcfg.b.term_sel_dl_pulse =
+	    (core_if->core_params->ts_dline == 1) ? 1 : 0;
+	DWC_WRITE_REG32(&global_regs->gusbcfg, usbcfg.d32);
+
+	/* Reset the Controller */
+	dwc_otg_core_reset(core_if);
+
+#ifndef CONFIG_MACH_M822XX
+	/* FIXME: it appears gusbcfg register, some bits are reset */
+	DWC_WRITE_REG32(&global_regs->gusbcfg, usbcfg.d32);
+#endif
+	core_if->adp_enable = core_if->core_params->adp_supp_enable;
+	core_if->power_down = core_if->core_params->power_down;
+	core_if->otg_sts = 0;
+
+	/* Initialize parameters from Hardware configuration registers. */
+	dev_if->num_in_eps = calc_num_in_eps(core_if);
+	dev_if->num_out_eps = calc_num_out_eps(core_if);
+
+	DWC_DEBUGPL(DBG_CIL, "num_dev_perio_in_ep=%d\n",
+		    core_if->hwcfg4.b.num_dev_perio_in_ep);
+
+	for (i = 0; i < core_if->hwcfg4.b.num_dev_perio_in_ep; i++) {
+		dev_if->perio_tx_fifo_size[i] =
+		    DWC_READ_REG32(&global_regs->dtxfsiz[i]) >> 16;
+		DWC_DEBUGPL(DBG_CIL, "Periodic Tx FIFO SZ #%d=0x%0x\n",
+			    i, dev_if->perio_tx_fifo_size[i]);
+	}
+
+	for (i = 0; i < core_if->hwcfg4.b.num_in_eps; i++) {
+		dev_if->tx_fifo_size[i] =
+		    DWC_READ_REG32(&global_regs->dtxfsiz[i]) >> 16;
+		DWC_DEBUGPL(DBG_CIL, "Tx FIFO SZ #%d=0x%0x\n",
+			    i, dev_if->tx_fifo_size[i]);
+	}
+
+	core_if->total_fifo_size = core_if->hwcfg3.b.dfifo_depth;
+	core_if->rx_fifo_size = DWC_READ_REG32(&global_regs->grxfsiz);
+	core_if->nperio_tx_fifo_size =
+	    DWC_READ_REG32(&global_regs->gnptxfsiz) >> 16;
+
+	DWC_DEBUGPL(DBG_CIL, "Total FIFO SZ=%d\n", core_if->total_fifo_size);
+	DWC_DEBUGPL(DBG_CIL, "Rx FIFO SZ=%d\n", core_if->rx_fifo_size);
+	DWC_DEBUGPL(DBG_CIL, "NP Tx FIFO SZ=%d\n",
+		    core_if->nperio_tx_fifo_size);
+
+	/* This programming sequence needs to happen in FS mode before any other
+	 * programming occurs */
+	if ((core_if->core_params->speed == DWC_SPEED_PARAM_FULL) &&
+	    (core_if->core_params->phy_type == DWC_PHY_TYPE_PARAM_FS)) {
+		/* If FS mode with FS PHY */
+
+		/* core_init() is now called on every switch so only call the
+		 * following for the first time through. */
+		if (!core_if->phy_init_done) {
+			core_if->phy_init_done = 1;
+			DWC_DEBUGPL(DBG_CIL, "FS_PHY detected\n");
+			usbcfg.d32 = DWC_READ_REG32(&global_regs->gusbcfg);
+			usbcfg.b.physel = 1;
+			DWC_WRITE_REG32(&global_regs->gusbcfg, usbcfg.d32);
+
+			/* Reset after a PHY select */
+			dwc_otg_core_reset(core_if);
+		}
+
+		/* Program DCFG.DevSpd or HCFG.FSLSPclkSel to 48Mhz in FS.      Also
+		 * do this on HNP Dev/Host mode switches (done in dev_init and
+		 * host_init). */
+		if (dwc_otg_is_host_mode(core_if)) {
+			init_fslspclksel(core_if);
+		} else {
+			init_devspd(core_if);
+		}
+
+		if (core_if->core_params->i2c_enable) {
+			DWC_DEBUGPL(DBG_CIL, "FS_PHY Enabling I2c\n");
+			/* Program GUSBCFG.OtgUtmifsSel to I2C */
+			usbcfg.d32 = DWC_READ_REG32(&global_regs->gusbcfg);
+			usbcfg.b.otgutmifssel = 1;
+			DWC_WRITE_REG32(&global_regs->gusbcfg, usbcfg.d32);
+
+			/* Program GI2CCTL.I2CEn */
+			i2cctl.d32 = DWC_READ_REG32(&global_regs->gi2cctl);
+			i2cctl.b.i2cdevaddr = 1;
+			i2cctl.b.i2cen = 0;
+			DWC_WRITE_REG32(&global_regs->gi2cctl, i2cctl.d32);
+			i2cctl.b.i2cen = 1;
+			DWC_WRITE_REG32(&global_regs->gi2cctl, i2cctl.d32);
+		}
+
+	} /* endif speed == DWC_SPEED_PARAM_FULL */
+	else {
+		/* High speed PHY. */
+		if (!core_if->phy_init_done) {
+			core_if->phy_init_done = 1;
+			/* HS PHY parameters.  These parameters are preserved
+			 * during soft reset so only program the first time.  Do
+			 * a soft reset immediately after setting phyif.  */
+
+			if (core_if->core_params->phy_type == 2) {
+				/* ULPI interface */
+				usbcfg.b.ulpi_utmi_sel = 1;
+				usbcfg.b.phyif = 0;
+				usbcfg.b.ddrsel =
+				    core_if->core_params->phy_ulpi_ddr;
+			} else if (core_if->core_params->phy_type == 1) {
+				/* UTMI+ interface */
+				usbcfg.b.ulpi_utmi_sel = 0;
+				if (core_if->core_params->phy_utmi_width == 16) {
+					usbcfg.b.phyif = 1;
+
+				} else {
+					usbcfg.b.phyif = 0;
+				}
+			} else {
+				DWC_ERROR("FS PHY TYPE\n");
+			}
+			DWC_WRITE_REG32(&global_regs->gusbcfg, usbcfg.d32);
+			/* Reset after setting the PHY parameters */
+			dwc_otg_core_reset(core_if);
+		}
+	}
+
+	if ((core_if->hwcfg2.b.hs_phy_type == 2) &&
+	    (core_if->hwcfg2.b.fs_phy_type == 1) &&
+	    (core_if->core_params->ulpi_fs_ls)) {
+		DWC_DEBUGPL(DBG_CIL, "Setting ULPI FSLS\n");
+		usbcfg.d32 = DWC_READ_REG32(&global_regs->gusbcfg);
+		usbcfg.b.ulpi_fsls = 1;
+		usbcfg.b.ulpi_clk_sus_m = 1;
+		DWC_WRITE_REG32(&global_regs->gusbcfg, usbcfg.d32);
+	} else {
+		usbcfg.d32 = DWC_READ_REG32(&global_regs->gusbcfg);
+		usbcfg.b.ulpi_fsls = 0;
+		usbcfg.b.ulpi_clk_sus_m = 0;
+		DWC_WRITE_REG32(&global_regs->gusbcfg, usbcfg.d32);
+	}
+
+	/* Program the GAHBCFG Register. */
+	switch (core_if->hwcfg2.b.architecture) {
+
+	case DWC_SLAVE_ONLY_ARCH:
+		DWC_DEBUGPL(DBG_CIL, "Slave Only Mode\n");
+		ahbcfg.b.nptxfemplvl_txfemplvl =
+		    DWC_GAHBCFG_TXFEMPTYLVL_HALFEMPTY;
+		ahbcfg.b.ptxfemplvl = DWC_GAHBCFG_TXFEMPTYLVL_HALFEMPTY;
+		core_if->dma_enable = 0;
+		core_if->dma_desc_enable = 0;
+		break;
+
+	case DWC_EXT_DMA_ARCH:
+		DWC_DEBUGPL(DBG_CIL, "External DMA Mode\n");
+		{
+			uint8_t brst_sz = core_if->core_params->dma_burst_size;
+			ahbcfg.b.hburstlen = 0;
+			while (brst_sz > 1) {
+				ahbcfg.b.hburstlen++;
+				brst_sz >>= 1;
+			}
+		}
+		core_if->dma_enable = (core_if->core_params->dma_enable != 0);
+		core_if->dma_desc_enable =
+		    (core_if->core_params->dma_desc_enable != 0);
+		break;
+
+	case DWC_INT_DMA_ARCH:
+		DWC_DEBUGPL(DBG_CIL, "Internal DMA Mode\n");
+		/* Old value was DWC_GAHBCFG_INT_DMA_BURST_INCR - done for
+		  Host mode ISOC in issue fix - vahrama */
+		ahbcfg.b.hburstlen = DWC_GAHBCFG_INT_DMA_BURST_INCR4;
+		core_if->dma_enable = (core_if->core_params->dma_enable != 0);
+		core_if->dma_desc_enable =
+		    (core_if->core_params->dma_desc_enable != 0);
+		break;
+
+	}
+	if (core_if->dma_enable) {
+		if (core_if->dma_desc_enable) {
+			DWC_PRINTF("Using Descriptor DMA mode\n");
+		} else {
+			DWC_PRINTF("Using Buffer DMA mode\n");
+
+		}
+	} else {
+		DWC_PRINTF("Using Slave mode\n");
+		core_if->dma_desc_enable = 0;
+	}
+
+	if (core_if->core_params->ahb_single) {
+		ahbcfg.b.ahbsingle = 1;
+	}
+
+	ahbcfg.b.dmaenable = core_if->dma_enable;
+	DWC_WRITE_REG32(&global_regs->gahbcfg, ahbcfg.d32);
+
+	core_if->en_multiple_tx_fifo = core_if->hwcfg4.b.ded_fifo_en;
+
+	core_if->pti_enh_enable = core_if->core_params->pti_enable != 0;
+	core_if->multiproc_int_enable = core_if->core_params->mpi_enable;
+	DWC_PRINTF("Periodic Transfer Interrupt Enhancement - %s\n",
+		   ((core_if->pti_enh_enable) ? "enabled" : "disabled"));
+	DWC_PRINTF("Multiprocessor Interrupt Enhancement - %s\n",
+		   ((core_if->multiproc_int_enable) ? "enabled" : "disabled"));
+
+	/*
+	 * Program the GUSBCFG register.
+	 */
+	usbcfg.d32 = DWC_READ_REG32(&global_regs->gusbcfg);
+
+	switch (core_if->hwcfg2.b.op_mode) {
+	case DWC_MODE_HNP_SRP_CAPABLE:
+		usbcfg.b.hnpcap = (core_if->core_params->otg_cap ==
+				   DWC_OTG_CAP_PARAM_HNP_SRP_CAPABLE);
+		usbcfg.b.srpcap = (core_if->core_params->otg_cap !=
+				   DWC_OTG_CAP_PARAM_NO_HNP_SRP_CAPABLE);
+		break;
+
+	case DWC_MODE_SRP_ONLY_CAPABLE:
+		usbcfg.b.hnpcap = 0;
+		usbcfg.b.srpcap = (core_if->core_params->otg_cap !=
+				   DWC_OTG_CAP_PARAM_NO_HNP_SRP_CAPABLE);
+		break;
+
+	case DWC_MODE_NO_HNP_SRP_CAPABLE:
+		usbcfg.b.hnpcap = 0;
+		usbcfg.b.srpcap = 0;
+		break;
+
+	case DWC_MODE_SRP_CAPABLE_DEVICE:
+		usbcfg.b.hnpcap = 0;
+		usbcfg.b.srpcap = (core_if->core_params->otg_cap !=
+				   DWC_OTG_CAP_PARAM_NO_HNP_SRP_CAPABLE);
+		break;
+
+	case DWC_MODE_NO_SRP_CAPABLE_DEVICE:
+		usbcfg.b.hnpcap = 0;
+		usbcfg.b.srpcap = 0;
+		break;
+
+	case DWC_MODE_SRP_CAPABLE_HOST:
+		usbcfg.b.hnpcap = 0;
+		usbcfg.b.srpcap = (core_if->core_params->otg_cap !=
+				   DWC_OTG_CAP_PARAM_NO_HNP_SRP_CAPABLE);
+		break;
+
+	case DWC_MODE_NO_SRP_CAPABLE_HOST:
+		usbcfg.b.hnpcap = 0;
+		usbcfg.b.srpcap = 0;
+		break;
+	}
+
+	DWC_WRITE_REG32(&global_regs->gusbcfg, usbcfg.d32);
+
+#ifdef CONFIG_USB_DWC_OTG_LPM
+	if (core_if->core_params->lpm_enable) {
+		glpmcfg_data_t lpmcfg = {.d32 = 0 };
+
+		/* To enable LPM support set lpm_cap_en bit */
+		lpmcfg.b.lpm_cap_en = 1;
+
+		/* Make AppL1Res ACK */
+		lpmcfg.b.appl_resp = 1;
+
+		/* Retry 3 times */
+		lpmcfg.b.retry_count = 3;
+
+		DWC_MODIFY_REG32(&core_if->core_global_regs->glpmcfg,
+				 0, lpmcfg.d32);
+
+	}
+#endif
+	if (core_if->core_params->ic_usb_cap) {
+		gusbcfg_data_t gusbcfg = {.d32 = 0 };
+		gusbcfg.b.ic_usb_cap = 1;
+		DWC_MODIFY_REG32(&core_if->core_global_regs->gusbcfg,
+				 0, gusbcfg.d32);
+	}
+	{
+#ifndef CONFIG_MACH_M822XX
+		/* Base value of GOTGCTL, VBUS checking enabled */
+		gotgctl_data_t gotgctl = {.d32 = 0 };
+#else  /* CONFIG_MACH_M822XX */
+		/* Base value of GOTGCTL, VBUS checking disabled */
+		gotgctl_data_t gotgctl = {.d32 = ((1<<3) | (1<<2)) };
+#endif	/* CONFIG_MACH_M822XX */
+		gotgctl.b.otgver = core_if->core_params->otg_ver;
+		DWC_MODIFY_REG32(&core_if->core_global_regs->gotgctl, 0,
+				 gotgctl.d32);
+		/* Set OTG version supported */
+		core_if->otg_ver = core_if->core_params->otg_ver;
+		DWC_PRINTF("OTG VER PARAM: %d, OTG VER FLAG: %d\n",
+			   core_if->core_params->otg_ver, core_if->otg_ver);
+	}
+
+
+	/* Enable common interrupts */
+	dwc_otg_enable_common_interrupts(core_if);
+
+	/* Do device or host intialization based on mode during PCD
+	 * and HCD initialization  */
+	if (dwc_otg_is_host_mode(core_if)) {
+		DWC_DEBUGPL(DBG_ANY, "Host Mode\n");
+		core_if->op_state = A_HOST;
+	} else {
+		DWC_DEBUGPL(DBG_ANY, "Device Mode\n");
+		core_if->op_state = B_PERIPHERAL;
+#ifdef DWC_DEVICE_ONLY
+		dwc_otg_core_dev_init(core_if);
+#endif
+	}
+}
+
+/**
+ * This function enables the Device mode interrupts.
+ *
+ * @param core_if Programming view of DWC_otg controller
+ */
+void dwc_otg_enable_device_interrupts(dwc_otg_core_if_t * core_if)
+{
+	gintmsk_data_t intr_mask = {.d32 = 0 };
+	dwc_otg_core_global_regs_t *global_regs = core_if->core_global_regs;
+
+	DWC_DEBUGPL(DBG_CIL, "%s()\n", __func__);
+
+	/* Disable all interrupts. */
+	DWC_WRITE_REG32(&global_regs->gintmsk, 0);
+
+	/* Clear any pending interrupts */
+	DWC_WRITE_REG32(&global_regs->gintsts, 0xFFFFFFFF);
+
+	/* Enable the common interrupts */
+	dwc_otg_enable_common_interrupts(core_if);
+
+	/* Enable interrupts */
+	intr_mask.b.usbreset = 1;
+	intr_mask.b.enumdone = 1;
+	/* Disable Disconnect interrupt in Device mode */
+	intr_mask.b.disconnect = 0;
+
+	if (!core_if->multiproc_int_enable) {
+		intr_mask.b.inepintr = 1;
+		intr_mask.b.outepintr = 1;
+	}
+
+	intr_mask.b.erlysuspend = 1;
+
+	if (core_if->en_multiple_tx_fifo == 0) {
+		intr_mask.b.epmismatch = 1;
+	}
+
+	//intr_mask.b.incomplisoout = 1;
+	intr_mask.b.incomplisoin = 1;
+
+/* Enable the ignore frame number for ISOC xfers - MAS */
+/* Disable to support high bandwith ISOC transfers - manukz */
+#if 0
+#ifdef DWC_UTE_PER_IO
+	if (core_if->dma_enable) {
+		if (core_if->dma_desc_enable) {
+			dctl_data_t dctl1 = {.d32 = 0 };
+			dctl1.b.ifrmnum = 1;
+			DWC_MODIFY_REG32(&core_if->dev_if->dev_global_regs->
+					 dctl, 0, dctl1.d32);
+			DWC_DEBUG("----Enabled Ignore frame number (0x%08x)",
+				  DWC_READ_REG32(&core_if->dev_if->
+						 dev_global_regs->dctl));
+		}
+	}
+#endif
+#endif
+#ifdef DWC_EN_ISOC
+	if (core_if->dma_enable) {
+		if (core_if->dma_desc_enable == 0) {
+			if (core_if->pti_enh_enable) {
+				dctl_data_t dctl = {.d32 = 0 };
+				dctl.b.ifrmnum = 1;
+				DWC_MODIFY_REG32(&core_if->
+						 dev_if->dev_global_regs->dctl,
+						 0, dctl.d32);
+			} else {
+				intr_mask.b.incomplisoin = 1;
+				intr_mask.b.incomplisoout = 1;
+			}
+		}
+	} else {
+		intr_mask.b.incomplisoin = 1;
+		intr_mask.b.incomplisoout = 1;
+	}
+#endif /* DWC_EN_ISOC */
+
+	/** @todo NGS: Should this be a module parameter? */
+#ifdef USE_PERIODIC_EP
+	intr_mask.b.isooutdrop = 1;
+	intr_mask.b.eopframe = 1;
+	intr_mask.b.incomplisoin = 1;
+	intr_mask.b.incomplisoout = 1;
+#endif
+
+	DWC_MODIFY_REG32(&global_regs->gintmsk, intr_mask.d32, intr_mask.d32);
+
+	DWC_DEBUGPL(DBG_CIL, "%s() gintmsk=%0x\n", __func__,
+		    DWC_READ_REG32(&global_regs->gintmsk));
+}
+
+/**
+ * This function initializes the DWC_otg controller registers for
+ * device mode.
+ *
+ * @param core_if Programming view of DWC_otg controller
+ *
+ */
+void dwc_otg_core_dev_init(dwc_otg_core_if_t * core_if)
+{
+	int i;
+	dwc_otg_core_global_regs_t *global_regs = core_if->core_global_regs;
+	dwc_otg_dev_if_t *dev_if = core_if->dev_if;
+	dwc_otg_core_params_t *params = core_if->core_params;
+	dcfg_data_t dcfg = {.d32 = 0 };
+	depctl_data_t diepctl = {.d32 = 0 };
+	grstctl_t resetctl = {.d32 = 0 };
+	uint32_t rx_fifo_size;
+	fifosize_data_t nptxfifosize;
+	fifosize_data_t txfifosize;
+	dthrctl_data_t dthrctl;
+	fifosize_data_t ptxfifosize;
+	uint16_t rxfsiz, nptxfsiz;
+	gdfifocfg_data_t gdfifocfg = {.d32 = 0 };
+	hwcfg3_data_t hwcfg3 = {.d32 = 0 };
+
+	/* Restart the Phy Clock */
+	DWC_WRITE_REG32(core_if->pcgcctl, 0);
+
+	/* Device configuration register */
+	init_devspd(core_if);
+	dcfg.d32 = DWC_READ_REG32(&dev_if->dev_global_regs->dcfg);
+	dcfg.b.descdma = (core_if->dma_desc_enable) ? 1 : 0;
+	dcfg.b.perfrint = DWC_DCFG_FRAME_INTERVAL_80;
+	/* Enable Device OUT NAK in case of DDMA mode*/
+	if (core_if->core_params->dev_out_nak) {
+		dcfg.b.endevoutnak = 1;
+	}
+
+	if (core_if->core_params->cont_on_bna) {
+		dctl_data_t dctl = {.d32 = 0 };
+		dctl.b.encontonbna = 1;
+		DWC_MODIFY_REG32(&dev_if->dev_global_regs->dctl, 0, dctl.d32);
+	}
+
+
+	DWC_WRITE_REG32(&dev_if->dev_global_regs->dcfg, dcfg.d32);
+
+	/* Configure data FIFO sizes */
+	if (core_if->hwcfg2.b.dynamic_fifo && params->enable_dynamic_fifo) {
+		DWC_DEBUGPL(DBG_CIL, "Total FIFO Size=%d\n",
+			    core_if->total_fifo_size);
+		DWC_DEBUGPL(DBG_CIL, "Rx FIFO Size=%d\n",
+			    params->dev_rx_fifo_size);
+		DWC_DEBUGPL(DBG_CIL, "NP Tx FIFO Size=%d\n",
+			    params->dev_nperio_tx_fifo_size);
+
+		/* Rx FIFO */
+		DWC_DEBUGPL(DBG_CIL, "initial grxfsiz=%08x\n",
+			    DWC_READ_REG32(&global_regs->grxfsiz));
+
+#ifdef DWC_UTE_CFI
+		core_if->pwron_rxfsiz = DWC_READ_REG32(&global_regs->grxfsiz);
+		core_if->init_rxfsiz = params->dev_rx_fifo_size;
+#endif
+		rx_fifo_size = params->dev_rx_fifo_size;
+		DWC_WRITE_REG32(&global_regs->grxfsiz, rx_fifo_size);
+
+		DWC_DEBUGPL(DBG_CIL, "new grxfsiz=%08x\n",
+			    DWC_READ_REG32(&global_regs->grxfsiz));
+
+		/** Set Periodic Tx FIFO Mask all bits 0 */
+		core_if->p_tx_msk = 0;
+
+		/** Set Tx FIFO Mask all bits 0 */
+		core_if->tx_msk = 0;
+
+		if (core_if->en_multiple_tx_fifo == 0) {
+			/* Non-periodic Tx FIFO */
+			DWC_DEBUGPL(DBG_CIL, "initial gnptxfsiz=%08x\n",
+				    DWC_READ_REG32(&global_regs->gnptxfsiz));
+
+			nptxfifosize.b.depth = params->dev_nperio_tx_fifo_size;
+			//nptxfifosize.b.startaddr = params->dev_rx_fifo_size;
+			nptxfifosize.b.startaddr = 0x400; // Makarand: USB2 DEBUG
+
+			DWC_WRITE_REG32(&global_regs->gnptxfsiz,
+					nptxfifosize.d32);
+
+			DWC_DEBUGPL(DBG_CIL, "new gnptxfsiz=%08x\n",
+				    DWC_READ_REG32(&global_regs->gnptxfsiz));
+
+			/**@todo NGS: Fix Periodic FIFO Sizing! */
+			/*
+			 * Periodic Tx FIFOs These FIFOs are numbered from 1 to 15.
+			 * Indexes of the FIFO size module parameters in the
+			 * dev_perio_tx_fifo_size array and the FIFO size registers in
+			 * the dptxfsiz array run from 0 to 14.
+			 */
+			/** @todo Finish debug of this */
+			ptxfifosize.b.startaddr =
+			    nptxfifosize.b.startaddr + nptxfifosize.b.depth;
+			for (i = 0; i < core_if->hwcfg4.b.num_dev_perio_in_ep; i++) {
+				ptxfifosize.b.depth =
+				    params->dev_perio_tx_fifo_size[i];
+				DWC_DEBUGPL(DBG_CIL,
+					    "initial dtxfsiz[%d]=%08x\n", i,
+					    DWC_READ_REG32(&global_regs->dtxfsiz
+							   [i]));
+				DWC_WRITE_REG32(&global_regs->dtxfsiz[i],
+						ptxfifosize.d32);
+				DWC_DEBUGPL(DBG_CIL, "new dtxfsiz[%d]=%08x\n",
+					    i,
+					    DWC_READ_REG32(&global_regs->dtxfsiz
+							   [i]));
+				ptxfifosize.b.startaddr += ptxfifosize.b.depth;
+			}
+		} else {
+			/*
+			 * Tx FIFOs These FIFOs are numbered from 1 to 15.
+			 * Indexes of the FIFO size module parameters in the
+			 * dev_tx_fifo_size array and the FIFO size registers in
+			 * the dtxfsiz array run from 0 to 14.
+			 */
+
+			/* Non-periodic Tx FIFO */
+			DWC_DEBUGPL(DBG_CIL, "initial gnptxfsiz=%08x\n",
+				    DWC_READ_REG32(&global_regs->gnptxfsiz));
+
+#ifdef DWC_UTE_CFI
+			core_if->pwron_gnptxfsiz =
+			    (DWC_READ_REG32(&global_regs->gnptxfsiz) >> 16);
+			core_if->init_gnptxfsiz =
+			    params->dev_nperio_tx_fifo_size;
+#endif
+			nptxfifosize.b.depth = params->dev_nperio_tx_fifo_size;
+			//nptxfifosize.b.startaddr = params->dev_rx_fifo_size;
+			nptxfifosize.b.startaddr = 0x400; // Makarand: USB2 DEBUG
+
+			DWC_WRITE_REG32(&global_regs->gnptxfsiz,
+					nptxfifosize.d32);
+
+			DWC_DEBUGPL(DBG_CIL, "new gnptxfsiz=%08x\n",
+				    DWC_READ_REG32(&global_regs->gnptxfsiz));
+
+			txfifosize.b.startaddr =
+			    nptxfifosize.b.startaddr + nptxfifosize.b.depth;
+
+			for (i = 0; i < core_if->hwcfg4.b.num_in_eps; i++) {
+
+				txfifosize.b.depth =
+				    params->dev_tx_fifo_size[i];
+
+				DWC_DEBUGPL(DBG_CIL,
+					    "initial dtxfsiz[%d]=%08x\n",
+					    i,
+					    DWC_READ_REG32(&global_regs->dtxfsiz
+							   [i]));
+
+#ifdef DWC_UTE_CFI
+				core_if->pwron_txfsiz[i] =
+				    (DWC_READ_REG32
+				     (&global_regs->dtxfsiz[i]) >> 16);
+				core_if->init_txfsiz[i] =
+				    params->dev_tx_fifo_size[i];
+#endif
+				DWC_WRITE_REG32(&global_regs->dtxfsiz[i],
+						txfifosize.d32);
+
+				DWC_DEBUGPL(DBG_CIL,
+					    "new dtxfsiz[%d]=%08x\n",
+					    i,
+					    DWC_READ_REG32(&global_regs->dtxfsiz
+							   [i]));
+
+				txfifosize.b.startaddr += txfifosize.b.depth;
+			}
+		        /* Calculating DFIFOCFG for Device mode to include RxFIFO and NPTXFIFO */
+			gdfifocfg.d32 = DWC_READ_REG32(&global_regs->gdfifocfg);
+			hwcfg3.d32 = DWC_READ_REG32(&global_regs->ghwcfg3);
+			gdfifocfg.b.gdfifocfg = (DWC_READ_REG32(&global_regs->ghwcfg3) >> 16);
+			DWC_WRITE_REG32(&global_regs->gdfifocfg, gdfifocfg.d32);
+			rxfsiz = (DWC_READ_REG32(&global_regs->grxfsiz) & 0x0000ffff);
+			nptxfsiz = (DWC_READ_REG32(&global_regs->gnptxfsiz) >> 16);
+			gdfifocfg.b.epinfobase = rxfsiz + nptxfsiz;
+			DWC_WRITE_REG32(&global_regs->gdfifocfg, gdfifocfg.d32);
+		}
+	}
+
+	/* Flush the FIFOs */
+	dwc_otg_flush_tx_fifo(core_if, 0x10);	/* all Tx FIFOs */
+	dwc_otg_flush_rx_fifo(core_if);
+
+	/* Flush the Learning Queue. */
+	resetctl.b.intknqflsh = 1;
+	DWC_WRITE_REG32(&core_if->core_global_regs->grstctl, resetctl.d32);
+
+	if (!core_if->core_params->en_multiple_tx_fifo && core_if->dma_enable) {
+		core_if->start_predict = 0;
+		for (i = 0; i<= core_if->dev_if->num_in_eps; ++i) {
+			core_if->nextep_seq[i] = 0xff;	// 0xff - EP not active
+		}
+		core_if->nextep_seq[0] = 0;
+		core_if->first_in_nextep_seq = 0;
+		diepctl.d32 = DWC_READ_REG32(&dev_if->in_ep_regs[0]->diepctl);
+		diepctl.b.nextep = 0;
+		DWC_WRITE_REG32(&dev_if->in_ep_regs[0]->diepctl, diepctl.d32);
+
+		/* Update IN Endpoint Mismatch Count by active IN NP EP count + 1 */
+		dcfg.d32 = DWC_READ_REG32(&dev_if->dev_global_regs->dcfg);
+		dcfg.b.epmscnt = 2;
+		DWC_WRITE_REG32(&dev_if->dev_global_regs->dcfg, dcfg.d32);
+
+		DWC_DEBUGPL(DBG_CILV,"%s first_in_nextep_seq= %2d; nextep_seq[]:\n",
+			__func__, core_if->first_in_nextep_seq);
+		for (i=0; i <= core_if->dev_if->num_in_eps; i++) {
+			DWC_DEBUGPL(DBG_CILV, "%2d ", core_if->nextep_seq[i]);
+		}
+		DWC_DEBUGPL(DBG_CILV,"\n");
+	}
+
+	/* Clear all pending Device Interrupts */
+	/** @todo - if the condition needed to be checked
+	 *  or in any case all pending interrutps should be cleared?
+     */
+	if (core_if->multiproc_int_enable) {
+		for (i = 0; i < core_if->dev_if->num_in_eps; ++i) {
+			DWC_WRITE_REG32(&dev_if->
+					dev_global_regs->diepeachintmsk[i], 0);
+		}
+
+		for (i = 0; i < core_if->dev_if->num_out_eps; ++i) {
+			DWC_WRITE_REG32(&dev_if->
+					dev_global_regs->doepeachintmsk[i], 0);
+		}
+
+		DWC_WRITE_REG32(&dev_if->dev_global_regs->deachint, 0xFFFFFFFF);
+		DWC_WRITE_REG32(&dev_if->dev_global_regs->deachintmsk, 0);
+	} else {
+		DWC_WRITE_REG32(&dev_if->dev_global_regs->diepmsk, 0);
+		DWC_WRITE_REG32(&dev_if->dev_global_regs->doepmsk, 0);
+		DWC_WRITE_REG32(&dev_if->dev_global_regs->daint, 0xFFFFFFFF);
+		DWC_WRITE_REG32(&dev_if->dev_global_regs->daintmsk, 0);
+	}
+
+	for (i = 0; i <= dev_if->num_in_eps; i++) {
+		depctl_data_t depctl;
+		depctl.d32 = DWC_READ_REG32(&dev_if->in_ep_regs[i]->diepctl);
+		if (depctl.b.epena) {
+			depctl.d32 = 0;
+			depctl.b.epdis = 1;
+			depctl.b.snak = 1;
+		} else {
+			depctl.d32 = 0;
+		}
+
+		DWC_WRITE_REG32(&dev_if->in_ep_regs[i]->diepctl, depctl.d32);
+
+		DWC_WRITE_REG32(&dev_if->in_ep_regs[i]->dieptsiz, 0);
+		DWC_WRITE_REG32(&dev_if->in_ep_regs[i]->diepdma, 0);
+		DWC_WRITE_REG32(&dev_if->in_ep_regs[i]->diepint, 0xFF);
+	}
+
+	for (i = 0; i <= dev_if->num_out_eps; i++) {
+		depctl_data_t depctl;
+		depctl.d32 = DWC_READ_REG32(&dev_if->out_ep_regs[i]->doepctl);
+		if (depctl.b.epena) {
+			depctl.d32 = 0;
+			depctl.b.epdis = 1;
+			depctl.b.snak = 1;
+		} else {
+			depctl.d32 = 0;
+		}
+
+		DWC_WRITE_REG32(&dev_if->out_ep_regs[i]->doepctl, depctl.d32);
+
+		DWC_WRITE_REG32(&dev_if->out_ep_regs[i]->doeptsiz, 0);
+		DWC_WRITE_REG32(&dev_if->out_ep_regs[i]->doepdma, 0);
+		DWC_WRITE_REG32(&dev_if->out_ep_regs[i]->doepint, 0xFF);
+	}
+
+	if (core_if->en_multiple_tx_fifo && core_if->dma_enable) {
+		dev_if->non_iso_tx_thr_en = params->thr_ctl & 0x1;
+		dev_if->iso_tx_thr_en = (params->thr_ctl >> 1) & 0x1;
+		dev_if->rx_thr_en = (params->thr_ctl >> 2) & 0x1;
+
+		dev_if->rx_thr_length = params->rx_thr_length;
+		dev_if->tx_thr_length = params->tx_thr_length;
+
+		dev_if->setup_desc_index = 0;
+
+		dthrctl.d32 = 0;
+		dthrctl.b.non_iso_thr_en = dev_if->non_iso_tx_thr_en;
+		dthrctl.b.iso_thr_en = dev_if->iso_tx_thr_en;
+		dthrctl.b.tx_thr_len = dev_if->tx_thr_length;
+		dthrctl.b.rx_thr_en = dev_if->rx_thr_en;
+		dthrctl.b.rx_thr_len = dev_if->rx_thr_length;
+		dthrctl.b.ahb_thr_ratio = params->ahb_thr_ratio;
+
+		DWC_WRITE_REG32(&dev_if->dev_global_regs->dtknqr3_dthrctl,
+				dthrctl.d32);
+
+		DWC_DEBUGPL(DBG_CIL,
+			    "Non ISO Tx Thr - %d\nISO Tx Thr - %d\nRx Thr - %d\nTx Thr Len - %d\nRx Thr Len - %d\n",
+			    dthrctl.b.non_iso_thr_en, dthrctl.b.iso_thr_en,
+			    dthrctl.b.rx_thr_en, dthrctl.b.tx_thr_len,
+			    dthrctl.b.rx_thr_len);
+
+	}
+
+	dwc_otg_enable_device_interrupts(core_if);
+
+	{
+		diepmsk_data_t msk = {.d32 = 0 };
+		msk.b.txfifoundrn = 1;
+		if (core_if->multiproc_int_enable) {
+			DWC_MODIFY_REG32(&dev_if->
+					 dev_global_regs->diepeachintmsk[0],
+					 msk.d32, msk.d32);
+		} else {
+			DWC_MODIFY_REG32(&dev_if->dev_global_regs->diepmsk,
+					 msk.d32, msk.d32);
+		}
+	}
+
+	if (core_if->multiproc_int_enable) {
+		/* Set NAK on Babble */
+		dctl_data_t dctl = {.d32 = 0 };
+		dctl.b.nakonbble = 1;
+		DWC_MODIFY_REG32(&dev_if->dev_global_regs->dctl, 0, dctl.d32);
+	}
+
+	if (core_if->snpsid >= OTG_CORE_REV_2_94a) {
+		dctl_data_t dctl = {.d32 = 0 };
+		dctl.d32 = DWC_READ_REG32(&dev_if->dev_global_regs->dctl);
+		dctl.b.sftdiscon = 0;
+		DWC_WRITE_REG32(&dev_if->dev_global_regs->dctl, dctl.d32);
+	}
+}
+
+/**
+ * This function enables the Host mode interrupts.
+ *
+ * @param core_if Programming view of DWC_otg controller
+ */
+void dwc_otg_enable_host_interrupts(dwc_otg_core_if_t * core_if)
+{
+	dwc_otg_core_global_regs_t *global_regs = core_if->core_global_regs;
+	gintmsk_data_t intr_mask = {.d32 = 0 };
+
+	DWC_DEBUGPL(DBG_CIL, "%s()\n", __func__);
+
+	/* Disable all interrupts. */
+	DWC_WRITE_REG32(&global_regs->gintmsk, 0);
+
+	/* Clear any pending interrupts. */
+	DWC_WRITE_REG32(&global_regs->gintsts, 0xFFFFFFFF);
+
+	/* Enable the common interrupts */
+	dwc_otg_enable_common_interrupts(core_if);
+
+	/*
+	 * Enable host mode interrupts without disturbing common
+	 * interrupts.
+	 */
+
+	intr_mask.b.disconnect = 1;
+	intr_mask.b.portintr = 1;
+	intr_mask.b.hcintr = 1;
+
+	DWC_MODIFY_REG32(&global_regs->gintmsk, intr_mask.d32, intr_mask.d32);
+}
+
+/**
+ * This function disables the Host Mode interrupts.
+ *
+ * @param core_if Programming view of DWC_otg controller
+ */
+void dwc_otg_disable_host_interrupts(dwc_otg_core_if_t * core_if)
+{
+	dwc_otg_core_global_regs_t *global_regs = core_if->core_global_regs;
+	gintmsk_data_t intr_mask = {.d32 = 0 };
+
+	DWC_DEBUGPL(DBG_CILV, "%s()\n", __func__);
+
+	/*
+	 * Disable host mode interrupts without disturbing common
+	 * interrupts.
+	 */
+	intr_mask.b.sofintr = 1;
+	intr_mask.b.portintr = 1;
+	intr_mask.b.hcintr = 1;
+	intr_mask.b.ptxfempty = 1;
+	intr_mask.b.nptxfempty = 1;
+
+	DWC_MODIFY_REG32(&global_regs->gintmsk, intr_mask.d32, 0);
+}
+
+/**
+ * This function initializes the DWC_otg controller registers for
+ * host mode.
+ *
+ * This function flushes the Tx and Rx FIFOs and it flushes any entries in the
+ * request queues. Host channels are reset to ensure that they are ready for
+ * performing transfers.
+ *
+ * @param core_if Programming view of DWC_otg controller
+ *
+ */
+void dwc_otg_core_host_init(dwc_otg_core_if_t * core_if)
+{
+	dwc_otg_core_global_regs_t *global_regs = core_if->core_global_regs;
+	dwc_otg_host_if_t *host_if = core_if->host_if;
+	dwc_otg_core_params_t *params = core_if->core_params;
+	hprt0_data_t hprt0 = {.d32 = 0 };
+	fifosize_data_t nptxfifosize;
+	fifosize_data_t ptxfifosize;
+	uint16_t rxfsiz, nptxfsiz, hptxfsiz;
+	gdfifocfg_data_t gdfifocfg = {.d32 = 0 };
+	int i;
+	hcchar_data_t hcchar;
+	hcfg_data_t hcfg;
+	hfir_data_t hfir;
+	dwc_otg_hc_regs_t *hc_regs;
+	int num_channels;
+	gotgctl_data_t gotgctl = {.d32 = 0 };
+
+	DWC_DEBUGPL(DBG_CILV, "%s(%p)\n", __func__, core_if);
+
+	/* Restart the Phy Clock */
+	DWC_WRITE_REG32(core_if->pcgcctl, 0);
+
+	/* Initialize Host Configuration Register */
+	init_fslspclksel(core_if);
+	if (core_if->core_params->speed == DWC_SPEED_PARAM_FULL) {
+		hcfg.d32 = DWC_READ_REG32(&host_if->host_global_regs->hcfg);
+		hcfg.b.fslssupp = 1;
+		DWC_WRITE_REG32(&host_if->host_global_regs->hcfg, hcfg.d32);
+
+	}
+
+	/* This bit allows dynamic reloading of the HFIR register
+	 * during runtime. This bit needs to be programmed during
+	 * initial configuration and its value must not be changed
+	 * during runtime.*/
+	if (core_if->core_params->reload_ctl == 1) {
+		hfir.d32 = DWC_READ_REG32(&host_if->host_global_regs->hfir);
+		hfir.b.hfirrldctrl = 1;
+		DWC_WRITE_REG32(&host_if->host_global_regs->hfir, hfir.d32);
+	}
+
+	if (core_if->core_params->dma_desc_enable) {
+		uint8_t op_mode = core_if->hwcfg2.b.op_mode;
+		if (!
+		    (core_if->hwcfg4.b.desc_dma
+		     && (core_if->snpsid >= OTG_CORE_REV_2_90a)
+		     && ((op_mode == DWC_HWCFG2_OP_MODE_HNP_SRP_CAPABLE_OTG)
+			 || (op_mode == DWC_HWCFG2_OP_MODE_SRP_ONLY_CAPABLE_OTG)
+			 || (op_mode ==
+			     DWC_HWCFG2_OP_MODE_NO_HNP_SRP_CAPABLE_OTG)
+			 || (op_mode == DWC_HWCFG2_OP_MODE_SRP_CAPABLE_HOST)
+			 || (op_mode ==
+			     DWC_HWCFG2_OP_MODE_NO_SRP_CAPABLE_HOST)))) {
+
+			DWC_ERROR("Host can't operate in Descriptor DMA mode.\n"
+				  "Either core version is below 2.90a or "
+				  "GHWCFG2, GHWCFG4 registers' values do not allow Descriptor DMA in host mode.\n"
+				  "To run the driver in Buffer DMA host mode set dma_desc_enable "
+				  "module parameter to 0.\n");
+			return;
+		}
+		hcfg.d32 = DWC_READ_REG32(&host_if->host_global_regs->hcfg);
+		hcfg.b.descdma = 1;
+		DWC_WRITE_REG32(&host_if->host_global_regs->hcfg, hcfg.d32);
+	}
+
+	/* Configure data FIFO sizes */
+	if (core_if->hwcfg2.b.dynamic_fifo && params->enable_dynamic_fifo) {
+		DWC_DEBUGPL(DBG_CIL, "Total FIFO Size=%d\n",
+			    core_if->total_fifo_size);
+		DWC_DEBUGPL(DBG_CIL, "Rx FIFO Size=%d\n",
+			    params->host_rx_fifo_size);
+		DWC_DEBUGPL(DBG_CIL, "NP Tx FIFO Size=%d\n",
+			    params->host_nperio_tx_fifo_size);
+		DWC_DEBUGPL(DBG_CIL, "P Tx FIFO Size=%d\n",
+			    params->host_perio_tx_fifo_size);
+
+		/* Rx FIFO */
+		DWC_DEBUGPL(DBG_CIL, "initial grxfsiz=%08x\n",
+			    DWC_READ_REG32(&global_regs->grxfsiz));
+		DWC_WRITE_REG32(&global_regs->grxfsiz,
+				params->host_rx_fifo_size);
+		DWC_DEBUGPL(DBG_CIL, "new grxfsiz=%08x\n",
+			    DWC_READ_REG32(&global_regs->grxfsiz));
+
+		/* Non-periodic Tx FIFO */
+		DWC_DEBUGPL(DBG_CIL, "initial gnptxfsiz=%08x\n",
+			    DWC_READ_REG32(&global_regs->gnptxfsiz));
+		nptxfifosize.b.depth = params->host_nperio_tx_fifo_size;
+		//nptxfifosize.b.startaddr = params->host_rx_fifo_size;
+		nptxfifosize.b.startaddr = 0x400; // Makarand: USB2 DEBUG
+
+		DWC_WRITE_REG32(&global_regs->gnptxfsiz, nptxfifosize.d32);
+		DWC_DEBUGPL(DBG_CIL, "new gnptxfsiz=%08x\n",
+			    DWC_READ_REG32(&global_regs->gnptxfsiz));
+
+		/* Periodic Tx FIFO */
+		DWC_DEBUGPL(DBG_CIL, "initial hptxfsiz=%08x\n",
+			    DWC_READ_REG32(&global_regs->hptxfsiz));
+		ptxfifosize.b.depth = params->host_perio_tx_fifo_size;
+		/*ptxfifosize.b.startaddr =
+			nptxfifosize.b.startaddr + nptxfifosize.b.depth;*/
+		ptxfifosize.b.startaddr = 0xC00; // Makarand: USB2 DEBUG
+
+		DWC_WRITE_REG32(&global_regs->hptxfsiz, ptxfifosize.d32);
+		DWC_DEBUGPL(DBG_CIL, "new hptxfsiz=%08x\n",
+			    DWC_READ_REG32(&global_regs->hptxfsiz));
+
+		if (core_if->en_multiple_tx_fifo) {
+			/* Global DFIFOCFG calculation for Host mode - include RxFIFO, NPTXFIFO and HPTXFIFO */
+			gdfifocfg.d32 = DWC_READ_REG32(&global_regs->gdfifocfg);
+			rxfsiz = (DWC_READ_REG32(&global_regs->grxfsiz) & 0x0000ffff);
+			nptxfsiz = (DWC_READ_REG32(&global_regs->gnptxfsiz) >> 16);
+			hptxfsiz = (DWC_READ_REG32(&global_regs->hptxfsiz) >> 16);
+			gdfifocfg.b.epinfobase = rxfsiz + nptxfsiz + hptxfsiz;
+			DWC_WRITE_REG32(&global_regs->gdfifocfg, gdfifocfg.d32);
+		}
+	}
+
+	/* TODO - check this */
+	/* Clear Host Set HNP Enable in the OTG Control Register */
+	gotgctl.b.hstsethnpen = 1;
+	DWC_MODIFY_REG32(&global_regs->gotgctl, gotgctl.d32, 0);
+	/* Make sure the FIFOs are flushed. */
+	dwc_otg_flush_tx_fifo(core_if, 0x10 /* all TX FIFOs */ );
+	dwc_otg_flush_rx_fifo(core_if);
+
+	/* Clear Host Set HNP Enable in the OTG Control Register */
+	gotgctl.b.hstsethnpen = 1;
+	DWC_MODIFY_REG32(&global_regs->gotgctl, gotgctl.d32, 0);
+
+	if (!core_if->core_params->dma_desc_enable) {
+		/* Flush out any leftover queued requests. */
+		num_channels = core_if->core_params->host_channels;
+
+		for (i = 0; i < num_channels; i++) {
+			hc_regs = core_if->host_if->hc_regs[i];
+			hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+			hcchar.b.chen = 0;
+			hcchar.b.chdis = 1;
+			hcchar.b.epdir = 0;
+			DWC_WRITE_REG32(&hc_regs->hcchar, hcchar.d32);
+		}
+
+		/* Halt all channels to put them into a known state. */
+		for (i = 0; i < num_channels; i++) {
+			int count = 0;
+			hc_regs = core_if->host_if->hc_regs[i];
+			hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+			hcchar.b.chen = 1;
+			hcchar.b.chdis = 1;
+			hcchar.b.epdir = 0;
+			DWC_WRITE_REG32(&hc_regs->hcchar, hcchar.d32);
+			DWC_DEBUGPL(DBG_HCDV, "%s: Halt channel %d\n", __func__, i);
+			do {
+				hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+				if (++count > 1000) {
+					DWC_ERROR
+					    ("%s: Unable to clear halt on channel %d\n",
+					     __func__, i);
+					break;
+				}
+				dwc_udelay(1);
+			} while (hcchar.b.chen);
+		}
+	}
+
+	/* Turn on the vbus power. */
+	DWC_PRINTF("Init: Port Power? op_state=%d\n", core_if->op_state);
+	if (core_if->op_state == A_HOST) {
+		hprt0.d32 = dwc_otg_read_hprt0(core_if);
+		DWC_PRINTF("Init: Power Port (%d)\n", hprt0.b.prtpwr);
+		if (hprt0.b.prtpwr == 0) {
+			hprt0.b.prtpwr = 1;
+			DWC_WRITE_REG32(host_if->hprt0, hprt0.d32);
+		}
+	}
+
+	dwc_otg_enable_host_interrupts(core_if);
+}
+
+/**
+ * Prepares a host channel for transferring packets to/from a specific
+ * endpoint. The HCCHARn register is set up with the characteristics specified
+ * in _hc. Host channel interrupts that may need to be serviced while this
+ * transfer is in progress are enabled.
+ *
+ * @param core_if Programming view of DWC_otg controller
+ * @param hc Information needed to initialize the host channel
+ */
+void dwc_otg_hc_init(dwc_otg_core_if_t * core_if, dwc_hc_t * hc)
+{
+	uint32_t intr_enable;
+	hcintmsk_data_t hc_intr_mask;
+	gintmsk_data_t gintmsk = {.d32 = 0 };
+	hcchar_data_t hcchar;
+	hcsplt_data_t hcsplt;
+
+	uint8_t hc_num = hc->hc_num;
+	dwc_otg_host_if_t *host_if = core_if->host_if;
+	dwc_otg_hc_regs_t *hc_regs = host_if->hc_regs[hc_num];
+
+	/* Clear old interrupt conditions for this host channel. */
+	hc_intr_mask.d32 = 0xFFFFFFFF;
+	hc_intr_mask.b.reserved14_31 = 0;
+	DWC_WRITE_REG32(&hc_regs->hcint, hc_intr_mask.d32);
+
+	/* Enable channel interrupts required for this transfer. */
+	hc_intr_mask.d32 = 0;
+	hc_intr_mask.b.chhltd = 1;
+	if (core_if->dma_enable) {
+		/* For Descriptor DMA mode core halts the channel on AHB error. Interrupt is not required */
+		if (!core_if->dma_desc_enable)
+			hc_intr_mask.b.ahberr = 1;
+		else {
+			if (hc->ep_type == DWC_OTG_EP_TYPE_ISOC)
+				hc_intr_mask.b.xfercompl = 1;
+		}
+
+		if (hc->error_state && !hc->do_split &&
+		    hc->ep_type != DWC_OTG_EP_TYPE_ISOC) {
+			hc_intr_mask.b.ack = 1;
+			if (hc->ep_is_in) {
+				hc_intr_mask.b.datatglerr = 1;
+				if (hc->ep_type != DWC_OTG_EP_TYPE_INTR) {
+					hc_intr_mask.b.nak = 1;
+				}
+			}
+		}
+	} else {
+		switch (hc->ep_type) {
+		case DWC_OTG_EP_TYPE_CONTROL:
+		case DWC_OTG_EP_TYPE_BULK:
+			hc_intr_mask.b.xfercompl = 1;
+			hc_intr_mask.b.stall = 1;
+			hc_intr_mask.b.xacterr = 1;
+			hc_intr_mask.b.datatglerr = 1;
+			if (hc->ep_is_in) {
+				hc_intr_mask.b.bblerr = 1;
+			} else {
+				hc_intr_mask.b.nak = 1;
+				hc_intr_mask.b.nyet = 1;
+				if (hc->do_ping) {
+					hc_intr_mask.b.ack = 1;
+				}
+			}
+
+			if (hc->do_split) {
+				hc_intr_mask.b.nak = 1;
+				if (hc->complete_split) {
+					hc_intr_mask.b.nyet = 1;
+				} else {
+					hc_intr_mask.b.ack = 1;
+				}
+			}
+
+			if (hc->error_state) {
+				hc_intr_mask.b.ack = 1;
+			}
+			break;
+		case DWC_OTG_EP_TYPE_INTR:
+			hc_intr_mask.b.xfercompl = 1;
+			hc_intr_mask.b.nak = 1;
+			hc_intr_mask.b.stall = 1;
+			hc_intr_mask.b.xacterr = 1;
+			hc_intr_mask.b.datatglerr = 1;
+			hc_intr_mask.b.frmovrun = 1;
+
+			if (hc->ep_is_in) {
+				hc_intr_mask.b.bblerr = 1;
+			}
+			if (hc->error_state) {
+				hc_intr_mask.b.ack = 1;
+			}
+			if (hc->do_split) {
+				if (hc->complete_split) {
+					hc_intr_mask.b.nyet = 1;
+				} else {
+					hc_intr_mask.b.ack = 1;
+				}
+			}
+			break;
+		case DWC_OTG_EP_TYPE_ISOC:
+			hc_intr_mask.b.xfercompl = 1;
+			hc_intr_mask.b.frmovrun = 1;
+			hc_intr_mask.b.ack = 1;
+
+			if (hc->ep_is_in) {
+				hc_intr_mask.b.xacterr = 1;
+				hc_intr_mask.b.bblerr = 1;
+			}
+			break;
+		}
+	}
+	DWC_WRITE_REG32(&hc_regs->hcintmsk, hc_intr_mask.d32);
+
+	/* Enable the top level host channel interrupt. */
+	intr_enable = (1 << hc_num);
+	DWC_MODIFY_REG32(&host_if->host_global_regs->haintmsk, 0, intr_enable);
+
+	/* Make sure host channel interrupts are enabled. */
+	gintmsk.b.hcintr = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gintmsk, 0, gintmsk.d32);
+
+	/*
+	 * Program the HCCHARn register with the endpoint characteristics for
+	 * the current transfer.
+	 */
+	hcchar.d32 = 0;
+	hcchar.b.devaddr = hc->dev_addr;
+	hcchar.b.epnum = hc->ep_num;
+	hcchar.b.epdir = hc->ep_is_in;
+	hcchar.b.lspddev = (hc->speed == DWC_OTG_EP_SPEED_LOW);
+	hcchar.b.eptype = hc->ep_type;
+	hcchar.b.mps = hc->max_packet;
+
+	DWC_WRITE_REG32(&host_if->hc_regs[hc_num]->hcchar, hcchar.d32);
+
+	DWC_DEBUGPL(DBG_HCDV, "%s: Channel %d\n", __func__, hc->hc_num);
+	DWC_DEBUGPL(DBG_HCDV, "	 Dev Addr: %d\n", hcchar.b.devaddr);
+	DWC_DEBUGPL(DBG_HCDV, "	 Ep Num: %d\n", hcchar.b.epnum);
+	DWC_DEBUGPL(DBG_HCDV, "	 Is In: %d\n", hcchar.b.epdir);
+	DWC_DEBUGPL(DBG_HCDV, "	 Is Low Speed: %d\n", hcchar.b.lspddev);
+	DWC_DEBUGPL(DBG_HCDV, "	 Ep Type: %d\n", hcchar.b.eptype);
+	DWC_DEBUGPL(DBG_HCDV, "	 Max Pkt: %d\n", hcchar.b.mps);
+	DWC_DEBUGPL(DBG_HCDV, "	 Multi Cnt: %d\n", hcchar.b.multicnt);
+
+	/*
+	 * Program the HCSPLIT register for SPLITs
+	 */
+	hcsplt.d32 = 0;
+	if (hc->do_split) {
+		DWC_DEBUGPL(DBG_HCDV, "Programming HC %d with split --> %s\n",
+			    hc->hc_num,
+			    hc->complete_split ? "CSPLIT" : "SSPLIT");
+		hcsplt.b.compsplt = hc->complete_split;
+		hcsplt.b.xactpos = hc->xact_pos;
+		hcsplt.b.hubaddr = hc->hub_addr;
+		hcsplt.b.prtaddr = hc->port_addr;
+		DWC_DEBUGPL(DBG_HCDV, "	  comp split %d\n", hc->complete_split);
+		DWC_DEBUGPL(DBG_HCDV, "	  xact pos %d\n", hc->xact_pos);
+		DWC_DEBUGPL(DBG_HCDV, "	  hub addr %d\n", hc->hub_addr);
+		DWC_DEBUGPL(DBG_HCDV, "	  port addr %d\n", hc->port_addr);
+		DWC_DEBUGPL(DBG_HCDV, "	  is_in %d\n", hc->ep_is_in);
+		DWC_DEBUGPL(DBG_HCDV, "	  Max Pkt: %d\n", hcchar.b.mps);
+		DWC_DEBUGPL(DBG_HCDV, "	  xferlen: %d\n", hc->xfer_len);
+	}
+	DWC_WRITE_REG32(&host_if->hc_regs[hc_num]->hcsplt, hcsplt.d32);
+
+}
+
+/**
+ * Attempts to halt a host channel. This function should only be called in
+ * Slave mode or to abort a transfer in either Slave mode or DMA mode. Under
+ * normal circumstances in DMA mode, the controller halts the channel when the
+ * transfer is complete or a condition occurs that requires application
+ * intervention.
+ *
+ * In slave mode, checks for a free request queue entry, then sets the Channel
+ * Enable and Channel Disable bits of the Host Channel Characteristics
+ * register of the specified channel to intiate the halt. If there is no free
+ * request queue entry, sets only the Channel Disable bit of the HCCHARn
+ * register to flush requests for this channel. In the latter case, sets a
+ * flag to indicate that the host channel needs to be halted when a request
+ * queue slot is open.
+ *
+ * In DMA mode, always sets the Channel Enable and Channel Disable bits of the
+ * HCCHARn register. The controller ensures there is space in the request
+ * queue before submitting the halt request.
+ *
+ * Some time may elapse before the core flushes any posted requests for this
+ * host channel and halts. The Channel Halted interrupt handler completes the
+ * deactivation of the host channel.
+ *
+ * @param core_if Controller register interface.
+ * @param hc Host channel to halt.
+ * @param halt_status Reason for halting the channel.
+ */
+void dwc_otg_hc_halt(dwc_otg_core_if_t * core_if,
+		     dwc_hc_t * hc, dwc_otg_halt_status_e halt_status)
+{
+	gnptxsts_data_t nptxsts;
+	hptxsts_data_t hptxsts;
+	hcchar_data_t hcchar;
+	dwc_otg_hc_regs_t *hc_regs;
+	dwc_otg_core_global_regs_t *global_regs;
+	dwc_otg_host_global_regs_t *host_global_regs;
+
+	hc_regs = core_if->host_if->hc_regs[hc->hc_num];
+	global_regs = core_if->core_global_regs;
+	host_global_regs = core_if->host_if->host_global_regs;
+
+	DWC_ASSERT(!(halt_status == DWC_OTG_HC_XFER_NO_HALT_STATUS),
+		   "halt_status = %d\n", halt_status);
+
+	if (halt_status == DWC_OTG_HC_XFER_URB_DEQUEUE ||
+	    halt_status == DWC_OTG_HC_XFER_AHB_ERR) {
+		/*
+		 * Disable all channel interrupts except Ch Halted. The QTD
+		 * and QH state associated with this transfer has been cleared
+		 * (in the case of URB_DEQUEUE), so the channel needs to be
+		 * shut down carefully to prevent crashes.
+		 */
+		hcintmsk_data_t hcintmsk;
+		hcintmsk.d32 = 0;
+		hcintmsk.b.chhltd = 1;
+		DWC_WRITE_REG32(&hc_regs->hcintmsk, hcintmsk.d32);
+
+		/*
+		 * Make sure no other interrupts besides halt are currently
+		 * pending. Handling another interrupt could cause a crash due
+		 * to the QTD and QH state.
+		 */
+		DWC_WRITE_REG32(&hc_regs->hcint, ~hcintmsk.d32);
+
+		/*
+		 * Make sure the halt status is set to URB_DEQUEUE or AHB_ERR
+		 * even if the channel was already halted for some other
+		 * reason.
+		 */
+		hc->halt_status = halt_status;
+
+		hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+		if (hcchar.b.chen == 0) {
+			/*
+			 * The channel is either already halted or it hasn't
+			 * started yet. In DMA mode, the transfer may halt if
+			 * it finishes normally or a condition occurs that
+			 * requires driver intervention. Don't want to halt
+			 * the channel again. In either Slave or DMA mode,
+			 * it's possible that the transfer has been assigned
+			 * to a channel, but not started yet when an URB is
+			 * dequeued. Don't want to halt a channel that hasn't
+			 * started yet.
+			 */
+			return;
+		}
+	}
+	if (hc->halt_pending) {
+		/*
+		 * A halt has already been issued for this channel. This might
+		 * happen when a transfer is aborted by a higher level in
+		 * the stack.
+		 */
+#ifdef DEBUG
+		DWC_PRINTF
+		    ("*** %s: Channel %d, _hc->halt_pending already set ***\n",
+		     __func__, hc->hc_num);
+
+#endif
+		return;
+	}
+
+	hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+
+	/* No need to set the bit in DDMA for disabling the channel */
+	//TODO check it everywhere channel is disabled
+	if (!core_if->core_params->dma_desc_enable)
+		hcchar.b.chen = 1;
+	hcchar.b.chdis = 1;
+
+	if (!core_if->dma_enable) {
+		/* Check for space in the request queue to issue the halt. */
+		if (hc->ep_type == DWC_OTG_EP_TYPE_CONTROL ||
+		    hc->ep_type == DWC_OTG_EP_TYPE_BULK) {
+			nptxsts.d32 = DWC_READ_REG32(&global_regs->gnptxsts);
+			if (nptxsts.b.nptxqspcavail == 0) {
+				hcchar.b.chen = 0;
+			}
+		} else {
+			hptxsts.d32 =
+			    DWC_READ_REG32(&host_global_regs->hptxsts);
+			if ((hptxsts.b.ptxqspcavail == 0)
+			    || (core_if->queuing_high_bandwidth)) {
+				hcchar.b.chen = 0;
+			}
+		}
+	}
+	DWC_WRITE_REG32(&hc_regs->hcchar, hcchar.d32);
+
+	hc->halt_status = halt_status;
+
+	if (hcchar.b.chen) {
+		hc->halt_pending = 1;
+		hc->halt_on_queue = 0;
+	} else {
+		hc->halt_on_queue = 1;
+	}
+
+	DWC_DEBUGPL(DBG_HCDV, "%s: Channel %d\n", __func__, hc->hc_num);
+	DWC_DEBUGPL(DBG_HCDV, "	 hcchar: 0x%08x\n", hcchar.d32);
+	DWC_DEBUGPL(DBG_HCDV, "	 halt_pending: %d\n", hc->halt_pending);
+	DWC_DEBUGPL(DBG_HCDV, "	 halt_on_queue: %d\n", hc->halt_on_queue);
+	DWC_DEBUGPL(DBG_HCDV, "	 halt_status: %d\n", hc->halt_status);
+
+	return;
+}
+
+/**
+ * Clears the transfer state for a host channel. This function is normally
+ * called after a transfer is done and the host channel is being released.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param hc Identifies the host channel to clean up.
+ */
+void dwc_otg_hc_cleanup(dwc_otg_core_if_t * core_if, dwc_hc_t * hc)
+{
+	dwc_otg_hc_regs_t *hc_regs;
+
+	hc->xfer_started = 0;
+
+	/*
+	 * Clear channel interrupt enables and any unhandled channel interrupt
+	 * conditions.
+	 */
+	hc_regs = core_if->host_if->hc_regs[hc->hc_num];
+	DWC_WRITE_REG32(&hc_regs->hcintmsk, 0);
+	DWC_WRITE_REG32(&hc_regs->hcint, 0xFFFFFFFF);
+#ifdef DEBUG
+	DWC_TIMER_CANCEL(core_if->hc_xfer_timer[hc->hc_num]);
+#endif
+}
+
+/**
+ * Sets the channel property that indicates in which frame a periodic transfer
+ * should occur. This is always set to the _next_ frame. This function has no
+ * effect on non-periodic transfers.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param hc Identifies the host channel to set up and its properties.
+ * @param hcchar Current value of the HCCHAR register for the specified host
+ * channel.
+ */
+static inline void hc_set_even_odd_frame(dwc_otg_core_if_t * core_if,
+					 dwc_hc_t * hc, hcchar_data_t * hcchar)
+{
+	if (hc->ep_type == DWC_OTG_EP_TYPE_INTR ||
+	    hc->ep_type == DWC_OTG_EP_TYPE_ISOC) {
+		hfnum_data_t hfnum;
+		hfnum.d32 =
+		    DWC_READ_REG32(&core_if->host_if->host_global_regs->hfnum);
+
+		/* 1 if _next_ frame is odd, 0 if it's even */
+		hcchar->b.oddfrm = (hfnum.b.frnum & 0x1) ? 0 : 1;
+#ifdef DEBUG
+		if (hc->ep_type == DWC_OTG_EP_TYPE_INTR && hc->do_split
+		    && !hc->complete_split) {
+			switch (hfnum.b.frnum & 0x7) {
+			case 7:
+				core_if->hfnum_7_samples++;
+				core_if->hfnum_7_frrem_accum += hfnum.b.frrem;
+				break;
+			case 0:
+				core_if->hfnum_0_samples++;
+				core_if->hfnum_0_frrem_accum += hfnum.b.frrem;
+				break;
+			default:
+				core_if->hfnum_other_samples++;
+				core_if->hfnum_other_frrem_accum +=
+				    hfnum.b.frrem;
+				break;
+			}
+		}
+#endif
+	}
+}
+
+#ifdef DEBUG
+void hc_xfer_timeout(void *ptr)
+{
+	hc_xfer_info_t *xfer_info = NULL;
+	int hc_num = 0;
+
+	if (ptr)
+		xfer_info = (hc_xfer_info_t *) ptr;
+
+	if (!xfer_info->hc) {
+		DWC_ERROR("xfer_info->hc = %p\n", xfer_info->hc);
+		return;
+	}
+
+	hc_num = xfer_info->hc->hc_num;
+	DWC_WARN("%s: timeout on channel %d\n", __func__, hc_num);
+	DWC_WARN("	start_hcchar_val 0x%08x\n",
+		 xfer_info->core_if->start_hcchar_val[hc_num]);
+}
+#endif
+
+void ep_xfer_timeout(void *ptr)
+{
+	ep_xfer_info_t *xfer_info = NULL;
+	int ep_num = 0;
+	dctl_data_t dctl = {.d32 = 0 };
+	gintsts_data_t gintsts = {.d32 = 0 };
+	gintmsk_data_t gintmsk = {.d32 = 0 };
+
+	if (ptr)
+		xfer_info = (ep_xfer_info_t *) ptr;
+
+	if (!xfer_info->ep) {
+		DWC_ERROR("xfer_info->ep = %p\n", xfer_info->ep);
+		return;
+	}
+
+	ep_num = xfer_info->ep->num;
+	DWC_WARN("%s: timeout on endpoit %d\n", __func__, ep_num);
+	/* Put the sate to 2 as it was time outed */
+	xfer_info->state = 2;
+
+	dctl.d32 = DWC_READ_REG32(&xfer_info->core_if->
+		dev_if->dev_global_regs->dctl);
+	gintsts.d32 = DWC_READ_REG32(&xfer_info->core_if->
+		core_global_regs->gintsts);
+	gintmsk.d32 = DWC_READ_REG32(&xfer_info->core_if->
+		core_global_regs->gintmsk);
+
+	if (!gintmsk.b.goutnakeff) {
+		/* Unmask it */
+		gintmsk.b.goutnakeff = 1;
+		DWC_WRITE_REG32(&xfer_info->core_if->
+			core_global_regs->gintmsk, gintmsk.d32);
+
+	}
+
+	if (!gintsts.b.goutnakeff) {
+		dctl.b.sgoutnak = 1;
+	}
+	DWC_WRITE_REG32(&xfer_info->core_if->dev_if->
+		dev_global_regs->dctl, dctl.d32);
+
+}
+
+void set_pid_isoc(dwc_hc_t * hc)
+{
+	/* Set up the initial PID for the transfer. */
+	if (hc->speed == DWC_OTG_EP_SPEED_HIGH) {
+		if (hc->ep_is_in) {
+			if (hc->multi_count == 1) {
+				hc->data_pid_start = DWC_OTG_HC_PID_DATA0;
+			} else if (hc->multi_count == 2) {
+				hc->data_pid_start = DWC_OTG_HC_PID_DATA1;
+			} else {
+				hc->data_pid_start = DWC_OTG_HC_PID_DATA2;
+			}
+		} else {
+			if (hc->multi_count == 1) {
+				hc->data_pid_start = DWC_OTG_HC_PID_DATA0;
+			} else {
+				hc->data_pid_start = DWC_OTG_HC_PID_MDATA;
+			}
+		}
+	} else {
+		hc->data_pid_start = DWC_OTG_HC_PID_DATA0;
+	}
+}
+
+/**
+ * This function does the setup for a data transfer for a host channel and
+ * starts the transfer. May be called in either Slave mode or DMA mode. In
+ * Slave mode, the caller must ensure that there is sufficient space in the
+ * request queue and Tx Data FIFO.
+ *
+ * For an OUT transfer in Slave mode, it loads a data packet into the
+ * appropriate FIFO. If necessary, additional data packets will be loaded in
+ * the Host ISR.
+ *
+ * For an IN transfer in Slave mode, a data packet is requested. The data
+ * packets are unloaded from the Rx FIFO in the Host ISR. If necessary,
+ * additional data packets are requested in the Host ISR.
+ *
+ * For a PING transfer in Slave mode, the Do Ping bit is set in the HCTSIZ
+ * register along with a packet count of 1 and the channel is enabled. This
+ * causes a single PING transaction to occur. Other fields in HCTSIZ are
+ * simply set to 0 since no data transfer occurs in this case.
+ *
+ * For a PING transfer in DMA mode, the HCTSIZ register is initialized with
+ * all the information required to perform the subsequent data transfer. In
+ * addition, the Do Ping bit is set in the HCTSIZ register. In this case, the
+ * controller performs the entire PING protocol, then starts the data
+ * transfer.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param hc Information needed to initialize the host channel. The xfer_len
+ * value may be reduced to accommodate the max widths of the XferSize and
+ * PktCnt fields in the HCTSIZn register. The multi_count value may be changed
+ * to reflect the final xfer_len value.
+ */
+void dwc_otg_hc_start_transfer(dwc_otg_core_if_t * core_if, dwc_hc_t * hc)
+{
+	hcchar_data_t hcchar;
+	hctsiz_data_t hctsiz;
+	uint16_t num_packets;
+	uint32_t max_hc_xfer_size = core_if->core_params->max_transfer_size;
+	uint16_t max_hc_pkt_count = core_if->core_params->max_packet_count;
+	dwc_otg_hc_regs_t *hc_regs = core_if->host_if->hc_regs[hc->hc_num];
+
+	hctsiz.d32 = 0;
+
+	if (hc->do_ping) {
+		if (!core_if->dma_enable) {
+			dwc_otg_hc_do_ping(core_if, hc);
+			hc->xfer_started = 1;
+			return;
+		} else {
+			hctsiz.b.dopng = 1;
+		}
+	}
+
+	if (hc->do_split) {
+		num_packets = 1;
+
+		if (hc->complete_split && !hc->ep_is_in) {
+			/* For CSPLIT OUT Transfer, set the size to 0 so the
+			 * core doesn't expect any data written to the FIFO */
+			hc->xfer_len = 0;
+		} else if (hc->ep_is_in || (hc->xfer_len > hc->max_packet)) {
+			hc->xfer_len = hc->max_packet;
+		} else if (!hc->ep_is_in && (hc->xfer_len > 188)) {
+			hc->xfer_len = 188;
+		}
+
+		hctsiz.b.xfersize = hc->xfer_len;
+	} else {
+		/*
+		 * Ensure that the transfer length and packet count will fit
+		 * in the widths allocated for them in the HCTSIZn register.
+		 */
+		if (hc->ep_type == DWC_OTG_EP_TYPE_INTR ||
+		    hc->ep_type == DWC_OTG_EP_TYPE_ISOC) {
+			/*
+			 * Make sure the transfer size is no larger than one
+			 * (micro)frame's worth of data. (A check was done
+			 * when the periodic transfer was accepted to ensure
+			 * that a (micro)frame's worth of data can be
+			 * programmed into a channel.)
+			 */
+			uint32_t max_periodic_len =
+			    hc->multi_count * hc->max_packet;
+			if (hc->xfer_len > max_periodic_len) {
+				hc->xfer_len = max_periodic_len;
+			} else {
+			}
+		} else if (hc->xfer_len > max_hc_xfer_size) {
+			/* Make sure that xfer_len is a multiple of max packet size. */
+			hc->xfer_len = max_hc_xfer_size - hc->max_packet + 1;
+		}
+
+		if (hc->xfer_len > 0) {
+			num_packets =
+			    (hc->xfer_len + hc->max_packet -
+			     1) / hc->max_packet;
+			if (num_packets > max_hc_pkt_count) {
+				num_packets = max_hc_pkt_count;
+				hc->xfer_len = num_packets * hc->max_packet;
+			}
+		} else {
+			/* Need 1 packet for transfer length of 0. */
+			num_packets = 1;
+		}
+
+		if (hc->ep_is_in) {
+			/* Always program an integral # of max packets for IN transfers. */
+			hc->xfer_len = num_packets * hc->max_packet;
+		}
+
+		if (hc->ep_type == DWC_OTG_EP_TYPE_INTR ||
+		    hc->ep_type == DWC_OTG_EP_TYPE_ISOC) {
+			/*
+			 * Make sure that the multi_count field matches the
+			 * actual transfer length.
+			 */
+			hc->multi_count = num_packets;
+		}
+
+		if (hc->ep_type == DWC_OTG_EP_TYPE_ISOC)
+			set_pid_isoc(hc);
+
+		hctsiz.b.xfersize = hc->xfer_len;
+	}
+
+	hc->start_pkt_count = num_packets;
+	hctsiz.b.pktcnt = num_packets;
+	hctsiz.b.pid = hc->data_pid_start;
+	DWC_WRITE_REG32(&hc_regs->hctsiz, hctsiz.d32);
+
+	DWC_DEBUGPL(DBG_HCDV, "%s: Channel %d\n", __func__, hc->hc_num);
+	DWC_DEBUGPL(DBG_HCDV, "	 Xfer Size: %d\n", hctsiz.b.xfersize);
+	DWC_DEBUGPL(DBG_HCDV, "	 Num Pkts: %d\n", hctsiz.b.pktcnt);
+	DWC_DEBUGPL(DBG_HCDV, "	 Start PID: %d\n", hctsiz.b.pid);
+
+	if (core_if->dma_enable) {
+		dwc_dma_t dma_addr;
+		if (hc->align_buff) {
+			dma_addr = hc->align_buff;
+		} else {
+			dma_addr = ((unsigned long)hc->xfer_buff & 0xffffffff);
+		}
+		DWC_WRITE_REG32(&hc_regs->hcdma, dma_addr);
+	}
+
+	/* Start the split */
+	if (hc->do_split) {
+		hcsplt_data_t hcsplt;
+		hcsplt.d32 = DWC_READ_REG32(&hc_regs->hcsplt);
+		hcsplt.b.spltena = 1;
+		DWC_WRITE_REG32(&hc_regs->hcsplt, hcsplt.d32);
+	}
+
+	hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+	hcchar.b.multicnt = hc->multi_count;
+	hc_set_even_odd_frame(core_if, hc, &hcchar);
+#ifdef DEBUG
+	core_if->start_hcchar_val[hc->hc_num] = hcchar.d32;
+	if (hcchar.b.chdis) {
+		DWC_WARN("%s: chdis set, channel %d, hcchar 0x%08x\n",
+			 __func__, hc->hc_num, hcchar.d32);
+	}
+#endif
+
+	/* Set host channel enable after all other setup is complete. */
+	hcchar.b.chen = 1;
+	hcchar.b.chdis = 0;
+	DWC_WRITE_REG32(&hc_regs->hcchar, hcchar.d32);
+
+	hc->xfer_started = 1;
+	hc->requests++;
+
+	if (!core_if->dma_enable && !hc->ep_is_in && hc->xfer_len > 0) {
+		/* Load OUT packet into the appropriate Tx FIFO. */
+		dwc_otg_hc_write_packet(core_if, hc);
+	}
+#ifdef DEBUG
+	if (hc->ep_type != DWC_OTG_EP_TYPE_INTR) {
+		core_if->hc_xfer_info[hc->hc_num].core_if = core_if;
+		core_if->hc_xfer_info[hc->hc_num].hc = hc;
+
+		/* Start a timer for this transfer. */
+		DWC_TIMER_SCHEDULE(core_if->hc_xfer_timer[hc->hc_num], 10000);
+	}
+#endif
+}
+
+/**
+ * This function does the setup for a data transfer for a host channel
+ * and starts the transfer in Descriptor DMA mode.
+ *
+ * Initializes HCTSIZ register. For a PING transfer the Do Ping bit is set.
+ * Sets PID and NTD values. For periodic transfers
+ * initializes SCHED_INFO field with micro-frame bitmap.
+ *
+ * Initializes HCDMA register with descriptor list address and CTD value
+ * then starts the transfer via enabling the channel.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param hc Information needed to initialize the host channel.
+ */
+void dwc_otg_hc_start_transfer_ddma(dwc_otg_core_if_t * core_if, dwc_hc_t * hc)
+{
+	dwc_otg_hc_regs_t *hc_regs = core_if->host_if->hc_regs[hc->hc_num];
+	hcchar_data_t hcchar;
+	hctsiz_data_t hctsiz;
+	hcdma_data_t hcdma;
+
+	hctsiz.d32 = 0;
+
+	if (hc->do_ping)
+		hctsiz.b_ddma.dopng = 1;
+
+	if (hc->ep_type == DWC_OTG_EP_TYPE_ISOC)
+		set_pid_isoc(hc);
+
+	/* Packet Count and Xfer Size are not used in Descriptor DMA mode */
+	hctsiz.b_ddma.pid = hc->data_pid_start;
+	hctsiz.b_ddma.ntd = hc->ntd - 1;	/* 0 - 1 descriptor, 1 - 2 descriptors, etc. */
+	hctsiz.b_ddma.schinfo = hc->schinfo;	/* Non-zero only for high-speed interrupt endpoints */
+
+	DWC_DEBUGPL(DBG_HCDV, "%s: Channel %d\n", __func__, hc->hc_num);
+	DWC_DEBUGPL(DBG_HCDV, "	 Start PID: %d\n", hctsiz.b.pid);
+	DWC_DEBUGPL(DBG_HCDV, "	 NTD: %d\n", hctsiz.b_ddma.ntd);
+
+	DWC_WRITE_REG32(&hc_regs->hctsiz, hctsiz.d32);
+
+	hcdma.d32 = 0;
+	hcdma.b.dma_addr = ((uint32_t) hc->desc_list_addr) >> 11;
+
+	/* Always start from first descriptor. */
+	hcdma.b.ctd = 0;
+	DWC_WRITE_REG32(&hc_regs->hcdma, hcdma.d32);
+
+	hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+	hcchar.b.multicnt = hc->multi_count;
+
+#ifdef DEBUG
+	core_if->start_hcchar_val[hc->hc_num] = hcchar.d32;
+	if (hcchar.b.chdis) {
+		DWC_WARN("%s: chdis set, channel %d, hcchar 0x%08x\n",
+			 __func__, hc->hc_num, hcchar.d32);
+	}
+#endif
+
+	/* Set host channel enable after all other setup is complete. */
+	hcchar.b.chen = 1;
+	hcchar.b.chdis = 0;
+
+	DWC_WRITE_REG32(&hc_regs->hcchar, hcchar.d32);
+
+	hc->xfer_started = 1;
+	hc->requests++;
+
+#ifdef DEBUG
+	if ((hc->ep_type != DWC_OTG_EP_TYPE_INTR)
+	    && (hc->ep_type != DWC_OTG_EP_TYPE_ISOC)) {
+		core_if->hc_xfer_info[hc->hc_num].core_if = core_if;
+		core_if->hc_xfer_info[hc->hc_num].hc = hc;
+		/* Start a timer for this transfer. */
+		DWC_TIMER_SCHEDULE(core_if->hc_xfer_timer[hc->hc_num], 10000);
+	}
+#endif
+
+}
+
+/**
+ * This function continues a data transfer that was started by previous call
+ * to <code>dwc_otg_hc_start_transfer</code>. The caller must ensure there is
+ * sufficient space in the request queue and Tx Data FIFO. This function
+ * should only be called in Slave mode. In DMA mode, the controller acts
+ * autonomously to complete transfers programmed to a host channel.
+ *
+ * For an OUT transfer, a new data packet is loaded into the appropriate FIFO
+ * if there is any data remaining to be queued. For an IN transfer, another
+ * data packet is always requested. For the SETUP phase of a control transfer,
+ * this function does nothing.
+ *
+ * @return 1 if a new request is queued, 0 if no more requests are required
+ * for this transfer.
+ */
+int dwc_otg_hc_continue_transfer(dwc_otg_core_if_t * core_if, dwc_hc_t * hc)
+{
+	DWC_DEBUGPL(DBG_HCDV, "%s: Channel %d\n", __func__, hc->hc_num);
+
+	if (hc->do_split) {
+		/* SPLITs always queue just once per channel */
+		return 0;
+	} else if (hc->data_pid_start == DWC_OTG_HC_PID_SETUP) {
+		/* SETUPs are queued only once since they can't be NAKed. */
+		return 0;
+	} else if (hc->ep_is_in) {
+		/*
+		 * Always queue another request for other IN transfers. If
+		 * back-to-back INs are issued and NAKs are received for both,
+		 * the driver may still be processing the first NAK when the
+		 * second NAK is received. When the interrupt handler clears
+		 * the NAK interrupt for the first NAK, the second NAK will
+		 * not be seen. So we can't depend on the NAK interrupt
+		 * handler to requeue a NAKed request. Instead, IN requests
+		 * are issued each time this function is called. When the
+		 * transfer completes, the extra requests for the channel will
+		 * be flushed.
+		 */
+		hcchar_data_t hcchar;
+		dwc_otg_hc_regs_t *hc_regs =
+		    core_if->host_if->hc_regs[hc->hc_num];
+
+		hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+		hc_set_even_odd_frame(core_if, hc, &hcchar);
+		hcchar.b.chen = 1;
+		hcchar.b.chdis = 0;
+		DWC_DEBUGPL(DBG_HCDV, "	 IN xfer: hcchar = 0x%08x\n",
+			    hcchar.d32);
+		DWC_WRITE_REG32(&hc_regs->hcchar, hcchar.d32);
+		hc->requests++;
+		return 1;
+	} else {
+		/* OUT transfers. */
+		if (hc->xfer_count < hc->xfer_len) {
+			if (hc->ep_type == DWC_OTG_EP_TYPE_INTR ||
+			    hc->ep_type == DWC_OTG_EP_TYPE_ISOC) {
+				hcchar_data_t hcchar;
+				dwc_otg_hc_regs_t *hc_regs;
+				hc_regs = core_if->host_if->hc_regs[hc->hc_num];
+				hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+				hc_set_even_odd_frame(core_if, hc, &hcchar);
+			}
+
+			/* Load OUT packet into the appropriate Tx FIFO. */
+			dwc_otg_hc_write_packet(core_if, hc);
+			hc->requests++;
+			return 1;
+		} else {
+			return 0;
+		}
+	}
+}
+
+/**
+ * Starts a PING transfer. This function should only be called in Slave mode.
+ * The Do Ping bit is set in the HCTSIZ register, then the channel is enabled.
+ */
+void dwc_otg_hc_do_ping(dwc_otg_core_if_t * core_if, dwc_hc_t * hc)
+{
+	hcchar_data_t hcchar;
+	hctsiz_data_t hctsiz;
+	dwc_otg_hc_regs_t *hc_regs = core_if->host_if->hc_regs[hc->hc_num];
+
+	DWC_DEBUGPL(DBG_HCDV, "%s: Channel %d\n", __func__, hc->hc_num);
+
+	hctsiz.d32 = 0;
+	hctsiz.b.dopng = 1;
+	hctsiz.b.pktcnt = 1;
+	DWC_WRITE_REG32(&hc_regs->hctsiz, hctsiz.d32);
+
+	hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+	hcchar.b.chen = 1;
+	hcchar.b.chdis = 0;
+	DWC_WRITE_REG32(&hc_regs->hcchar, hcchar.d32);
+}
+
+/*
+ * This function writes a packet into the Tx FIFO associated with the Host
+ * Channel. For a channel associated with a non-periodic EP, the non-periodic
+ * Tx FIFO is written. For a channel associated with a periodic EP, the
+ * periodic Tx FIFO is written. This function should only be called in Slave
+ * mode.
+ *
+ * Upon return the xfer_buff and xfer_count fields in _hc are incremented by
+ * then number of bytes written to the Tx FIFO.
+ */
+void dwc_otg_hc_write_packet(dwc_otg_core_if_t * core_if, dwc_hc_t * hc)
+{
+	uint32_t i;
+	uint32_t remaining_count;
+	uint32_t byte_count;
+	uint32_t dword_count;
+
+	uint32_t *data_buff = (uint32_t *) (hc->xfer_buff);
+	uint32_t *data_fifo = core_if->data_fifo[hc->hc_num];
+
+	remaining_count = hc->xfer_len - hc->xfer_count;
+	if (remaining_count > hc->max_packet) {
+		byte_count = hc->max_packet;
+	} else {
+		byte_count = remaining_count;
+	}
+
+	dword_count = (byte_count + 3) / 4;
+
+	if ((((unsigned long)data_buff) & 0x3) == 0) {
+		/* xfer_buff is DWORD aligned. */
+		for (i = 0; i < dword_count; i++, data_buff++) {
+			DWC_WRITE_REG32(data_fifo, *data_buff);
+		}
+	} else {
+		/* xfer_buff is not DWORD aligned. */
+		for (i = 0; i < dword_count; i++, data_buff++) {
+			uint32_t data;
+			data =
+			    (data_buff[0] | data_buff[1] << 8 | data_buff[2] <<
+			     16 | data_buff[3] << 24);
+			DWC_WRITE_REG32(data_fifo, data);
+		}
+	}
+
+	hc->xfer_count += byte_count;
+	hc->xfer_buff += byte_count;
+}
+
+/**
+ * Gets the current USB frame number. This is the frame number from the last
+ * SOF packet.
+ */
+uint32_t dwc_otg_get_frame_number(dwc_otg_core_if_t * core_if)
+{
+	dsts_data_t dsts;
+	dsts.d32 = DWC_READ_REG32(&core_if->dev_if->dev_global_regs->dsts);
+
+	/* read current frame/microframe number from DSTS register */
+	return dsts.b.soffn;
+}
+
+/**
+ * Calculates and gets the frame Interval value of HFIR register according PHY
+ * type and speed.The application can modify a value of HFIR register only after
+ * the Port Enable bit of the Host Port Control and Status register
+ * (HPRT.PrtEnaPort) has been set.
+*/
+
+uint32_t calc_frame_interval(dwc_otg_core_if_t * core_if)
+{
+	gusbcfg_data_t usbcfg;
+	hwcfg2_data_t hwcfg2;
+	hprt0_data_t hprt0;
+	int clock = 60;		// default value
+	usbcfg.d32 = DWC_READ_REG32(&core_if->core_global_regs->gusbcfg);
+	hwcfg2.d32 = DWC_READ_REG32(&core_if->core_global_regs->ghwcfg2);
+	hprt0.d32 = DWC_READ_REG32(core_if->host_if->hprt0);
+	if (!usbcfg.b.physel && usbcfg.b.ulpi_utmi_sel && !usbcfg.b.phyif)
+		clock = 60;
+	if (usbcfg.b.physel && hwcfg2.b.fs_phy_type == 3)
+		clock = 48;
+	if (!usbcfg.b.phylpwrclksel && !usbcfg.b.physel &&
+	    !usbcfg.b.ulpi_utmi_sel && usbcfg.b.phyif)
+		clock = 30;
+	if (!usbcfg.b.phylpwrclksel && !usbcfg.b.physel &&
+	    !usbcfg.b.ulpi_utmi_sel && !usbcfg.b.phyif)
+		clock = 60;
+	if (usbcfg.b.phylpwrclksel && !usbcfg.b.physel &&
+	    !usbcfg.b.ulpi_utmi_sel && usbcfg.b.phyif)
+		clock = 48;
+	if (usbcfg.b.physel && !usbcfg.b.phyif && hwcfg2.b.fs_phy_type == 2)
+		clock = 48;
+	if (usbcfg.b.physel && hwcfg2.b.fs_phy_type == 1)
+		clock = 48;
+	if (hprt0.b.prtspd == 0)
+		/* High speed case */
+		return 125 * clock;
+	else
+		/* FS/LS case */
+		return 1000 * clock;
+}
+
+/**
+ * This function reads a setup packet from the Rx FIFO into the destination
+ * buffer. This function is called from the Rx Status Queue Level (RxStsQLvl)
+ * Interrupt routine when a SETUP packet has been received in Slave mode.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param dest Destination buffer for packet data.
+ */
+void dwc_otg_read_setup_packet(dwc_otg_core_if_t * core_if, uint32_t * dest)
+{
+	/* Get the 8 bytes of a setup transaction data */
+
+	/* Pop 2 DWORDS off the receive data FIFO into memory */
+	dest[0] = DWC_READ_REG32(core_if->data_fifo[0]);
+	dest[1] = DWC_READ_REG32(core_if->data_fifo[0]);
+}
+
+/**
+ * This function enables EP0 OUT to receive SETUP packets and configures EP0
+ * IN for transmitting packets. It is normally called when the
+ * "Enumeration Done" interrupt occurs.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param ep The EP0 data.
+ */
+void dwc_otg_ep0_activate(dwc_otg_core_if_t * core_if, dwc_ep_t * ep)
+{
+	dwc_otg_dev_if_t *dev_if = core_if->dev_if;
+	dsts_data_t dsts;
+	depctl_data_t diepctl;
+	depctl_data_t doepctl;
+	dctl_data_t dctl = {.d32 = 0 };
+
+	/* Read the Device Status and Endpoint 0 Control registers */
+	dsts.d32 = DWC_READ_REG32(&dev_if->dev_global_regs->dsts);
+	diepctl.d32 = DWC_READ_REG32(&dev_if->in_ep_regs[0]->diepctl);
+	doepctl.d32 = DWC_READ_REG32(&dev_if->out_ep_regs[0]->doepctl);
+
+	/* Set the MPS of the IN EP based on the enumeration speed */
+	switch (dsts.b.enumspd) {
+	case DWC_DSTS_ENUMSPD_HS_PHY_30MHZ_OR_60MHZ:
+	case DWC_DSTS_ENUMSPD_FS_PHY_30MHZ_OR_60MHZ:
+	case DWC_DSTS_ENUMSPD_FS_PHY_48MHZ:
+		diepctl.b.mps = DWC_DEP0CTL_MPS_64;
+		break;
+	case DWC_DSTS_ENUMSPD_LS_PHY_6MHZ:
+		diepctl.b.mps = DWC_DEP0CTL_MPS_8;
+		break;
+	}
+
+	DWC_WRITE_REG32(&dev_if->in_ep_regs[0]->diepctl, diepctl.d32);
+
+	/* Enable OUT EP for receive */
+	doepctl.b.epena = 1;
+	DWC_WRITE_REG32(&dev_if->out_ep_regs[0]->doepctl, doepctl.d32);
+
+#ifdef VERBOSE
+	DWC_DEBUGPL(DBG_PCDV, "doepctl0=%0x\n",
+		    DWC_READ_REG32(&dev_if->out_ep_regs[0]->doepctl));
+	DWC_DEBUGPL(DBG_PCDV, "diepctl0=%0x\n",
+		    DWC_READ_REG32(&dev_if->in_ep_regs[0]->diepctl));
+#endif
+	dctl.b.cgnpinnak = 1;
+
+	DWC_MODIFY_REG32(&dev_if->dev_global_regs->dctl, dctl.d32, dctl.d32);
+	DWC_DEBUGPL(DBG_PCDV, "dctl=%0x\n",
+		    DWC_READ_REG32(&dev_if->dev_global_regs->dctl));
+
+}
+
+/**
+ * This function activates an EP.  The Device EP control register for
+ * the EP is configured as defined in the ep structure. Note: This
+ * function is not used for EP0.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param ep The EP to activate.
+ */
+void dwc_otg_ep_activate(dwc_otg_core_if_t * core_if, dwc_ep_t * ep)
+{
+	dwc_otg_dev_if_t *dev_if = core_if->dev_if;
+	depctl_data_t depctl;
+	volatile uint32_t *addr;
+	daint_data_t daintmsk = {.d32 = 0 };
+	dcfg_data_t dcfg;
+	uint8_t i;
+
+	DWC_DEBUGPL(DBG_PCDV, "%s() EP%d-%s\n", __func__, ep->num,
+		    (ep->is_in ? "IN" : "OUT"));
+
+#ifdef DWC_UTE_PER_IO
+	ep->xiso_frame_num = 0xFFFFFFFF;
+	ep->xiso_active_xfers = 0;
+	ep->xiso_queued_xfers = 0;
+#endif
+	/* Read DEPCTLn register */
+	if (ep->is_in == 1) {
+		addr = &dev_if->in_ep_regs[ep->num]->diepctl;
+		daintmsk.ep.in = 1 << ep->num;
+	} else {
+		addr = &dev_if->out_ep_regs[ep->num]->doepctl;
+		daintmsk.ep.out = 1 << ep->num;
+	}
+
+	/* If the EP is already active don't change the EP Control
+	 * register. */
+	depctl.d32 = DWC_READ_REG32(addr);
+	if (!depctl.b.usbactep) {
+		depctl.b.mps = ep->maxpacket;
+		depctl.b.eptype = ep->type;
+		depctl.b.txfnum = ep->tx_fifo_num;
+
+		if (ep->type == DWC_OTG_EP_TYPE_ISOC) {
+			depctl.b.setd0pid = 1;	// ???
+		} else {
+			depctl.b.setd0pid = 1;
+		}
+		depctl.b.usbactep = 1;
+
+		/* Update nextep_seq array and EPMSCNT in DCFG*/
+		if (!(depctl.b.eptype & 1) && (ep->is_in == 1)) {	// NP IN EP
+			for (i = 0; i <= core_if->dev_if->num_in_eps; i++) {
+				if (core_if->nextep_seq[i] == core_if->first_in_nextep_seq)
+				break;
+			}
+			core_if->nextep_seq[i] = ep->num;
+			core_if->nextep_seq[ep->num] = core_if->first_in_nextep_seq;
+			depctl.b.nextep = core_if->nextep_seq[ep->num];
+			dcfg.d32 = DWC_READ_REG32(&dev_if->dev_global_regs->dcfg);
+			dcfg.b.epmscnt++;
+			DWC_WRITE_REG32(&dev_if->dev_global_regs->dcfg, dcfg.d32);
+
+			DWC_DEBUGPL(DBG_PCDV,"%s first_in_nextep_seq= %2d; nextep_seq[]:\n",
+				__func__, core_if->first_in_nextep_seq);
+			for (i=0; i <= core_if->dev_if->num_in_eps; i++) {
+				DWC_DEBUGPL(DBG_PCDV, "%2d\n", core_if->nextep_seq[i]);
+			}
+
+		}
+
+
+		DWC_WRITE_REG32(addr, depctl.d32);
+		DWC_DEBUGPL(DBG_PCDV, "DEPCTL=%08x\n", DWC_READ_REG32(addr));
+	}
+
+	/* Enable the Interrupt for this EP */
+	if (core_if->multiproc_int_enable) {
+		if (ep->is_in == 1) {
+			diepmsk_data_t diepmsk = {.d32 = 0 };
+			diepmsk.b.xfercompl = 1;
+			diepmsk.b.timeout = 1;
+			diepmsk.b.epdisabled = 1;
+			diepmsk.b.ahberr = 1;
+			diepmsk.b.intknepmis = 1;
+			if (!core_if->en_multiple_tx_fifo && core_if->dma_enable)
+				diepmsk.b.intknepmis = 0;
+			diepmsk.b.txfifoundrn = 1;	//?????
+			if (ep->type == DWC_OTG_EP_TYPE_ISOC) {
+				diepmsk.b.nak = 1;
+			}
+
+
+
+/*
+			if (core_if->dma_desc_enable) {
+				diepmsk.b.bna = 1;
+			}
+*/
+/*
+			if (core_if->dma_enable) {
+				doepmsk.b.nak = 1;
+			}
+*/
+			DWC_WRITE_REG32(&dev_if->dev_global_regs->
+					diepeachintmsk[ep->num], diepmsk.d32);
+
+		} else {
+			doepmsk_data_t doepmsk = {.d32 = 0 };
+			doepmsk.b.xfercompl = 1;
+			doepmsk.b.ahberr = 1;
+			doepmsk.b.epdisabled = 1;
+			if (ep->type == DWC_OTG_EP_TYPE_ISOC)
+				doepmsk.b.outtknepdis = 1;
+
+/*
+
+			if (core_if->dma_desc_enable) {
+				doepmsk.b.bna = 1;
+			}
+*/
+/*
+			doepmsk.b.babble = 1;
+			doepmsk.b.nyet = 1;
+			doepmsk.b.nak = 1;
+*/
+			DWC_WRITE_REG32(&dev_if->dev_global_regs->
+					doepeachintmsk[ep->num], doepmsk.d32);
+		}
+		DWC_MODIFY_REG32(&dev_if->dev_global_regs->deachintmsk,
+				 0, daintmsk.d32);
+	} else {
+		if (ep->type == DWC_OTG_EP_TYPE_ISOC) {
+			if (ep->is_in)
+			{
+				diepmsk_data_t diepmsk = {.d32 = 0 };
+				diepmsk.b.nak = 1;
+				DWC_MODIFY_REG32(&dev_if->dev_global_regs->diepmsk, 0, diepmsk.d32);
+			} else {
+				doepmsk_data_t doepmsk = {.d32 = 0 };
+				doepmsk.b.outtknepdis = 1;
+				DWC_MODIFY_REG32(&dev_if->dev_global_regs->doepmsk, 0, doepmsk.d32);
+			}
+		}
+		DWC_MODIFY_REG32(&dev_if->dev_global_regs->daintmsk,
+				 0, daintmsk.d32);
+	}
+
+	DWC_DEBUGPL(DBG_PCDV, "DAINTMSK=%0x\n",
+		    DWC_READ_REG32(&dev_if->dev_global_regs->daintmsk));
+
+	ep->stall_clear_flag = 0;
+
+	return;
+}
+
+/**
+ * This function deactivates an EP. This is done by clearing the USB Active
+ * EP bit in the Device EP control register. Note: This function is not used
+ * for EP0. EP0 cannot be deactivated.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param ep The EP to deactivate.
+ */
+void dwc_otg_ep_deactivate(dwc_otg_core_if_t * core_if, dwc_ep_t * ep)
+{
+	depctl_data_t depctl = {.d32 = 0 };
+	volatile uint32_t *addr;
+	daint_data_t daintmsk = {.d32 = 0 };
+	dcfg_data_t dcfg;
+	uint8_t i = 0;
+
+#ifdef DWC_UTE_PER_IO
+	ep->xiso_frame_num = 0xFFFFFFFF;
+	ep->xiso_active_xfers = 0;
+	ep->xiso_queued_xfers = 0;
+#endif
+
+	/* Read DEPCTLn register */
+	if (ep->is_in == 1) {
+		addr = &core_if->dev_if->in_ep_regs[ep->num]->diepctl;
+		daintmsk.ep.in = 1 << ep->num;
+	} else {
+		addr = &core_if->dev_if->out_ep_regs[ep->num]->doepctl;
+		daintmsk.ep.out = 1 << ep->num;
+	}
+
+	depctl.d32 = DWC_READ_REG32(addr);
+
+	depctl.b.usbactep = 0;
+
+	/* Update nextep_seq array and EPMSCNT in DCFG*/
+	if (!(depctl.b.eptype & 1) && ep->is_in == 1) {	// NP EP IN
+		for (i = 0; i <= core_if->dev_if->num_in_eps; i++) {
+			if (core_if->nextep_seq[i] == ep->num)
+			break;
+		}
+		core_if->nextep_seq[i] = core_if->nextep_seq[ep->num];
+		if (core_if->first_in_nextep_seq == ep->num)
+			core_if->first_in_nextep_seq = i;
+		core_if->nextep_seq[ep->num] = 0xff;
+		depctl.b.nextep = 0;
+		dcfg.d32 = DWC_READ_REG32(&core_if->dev_if->dev_global_regs->dcfg);
+		dcfg.b.epmscnt--;
+		DWC_WRITE_REG32(&core_if->dev_if->dev_global_regs->dcfg, dcfg.d32);
+
+			DWC_DEBUGPL(DBG_PCDV,"%s first_in_nextep_seq= %2d; nextep_seq[]:\n",
+				__func__, core_if->first_in_nextep_seq);
+			for (i=0; i <= core_if->dev_if->num_in_eps; i++) {
+				DWC_DEBUGPL(DBG_PCDV, "%2d\n", core_if->nextep_seq[i]);
+			}
+	}
+
+	if (ep->is_in == 1)
+		depctl.b.txfnum = 0;
+
+	if (core_if->dma_desc_enable)
+		depctl.b.epdis = 1;
+
+	DWC_WRITE_REG32(addr, depctl.d32);
+	depctl.d32 = DWC_READ_REG32(addr);
+	if (core_if->dma_enable && ep->type == DWC_OTG_EP_TYPE_ISOC && depctl.b.epena)
+	{
+		depctl_data_t depctl = {.d32 = 0};
+		if (ep->is_in)
+		{
+			diepint_data_t diepint = {.d32 = 0};
+
+			depctl.b.snak = 1;
+			DWC_WRITE_REG32(&core_if->dev_if->in_ep_regs[ep->num]->diepctl, depctl.d32);
+			do
+			{
+				dwc_udelay(10);
+				diepint.d32 = DWC_READ_REG32(&core_if->dev_if->
+											in_ep_regs[ep->num]->diepint);
+			} while (!diepint.b.inepnakeff);
+			diepint.b.inepnakeff = 1;
+			DWC_WRITE_REG32(&core_if->dev_if->
+							in_ep_regs[ep->num]->diepint, diepint.d32);
+			depctl.d32 = 0;
+			depctl.b.epdis = 1;
+			DWC_WRITE_REG32(&core_if->dev_if->in_ep_regs[ep->num]->diepctl, depctl.d32);
+			do
+			{
+				dwc_udelay(10);
+				diepint.d32 = DWC_READ_REG32(&core_if->dev_if->
+											in_ep_regs[ep->num]->diepint);
+			} while (!diepint.b.epdisabled);
+			diepint.b.epdisabled = 1;
+			DWC_WRITE_REG32(&core_if->dev_if->in_ep_regs[ep->num]->diepint, diepint.d32);
+		} else {
+			dctl_data_t dctl = {.d32 = 0};
+			gintmsk_data_t gintsts = {.d32 = 0};
+			doepint_data_t doepint = {.d32 = 0};
+			dctl.b.sgoutnak = 1;
+			DWC_MODIFY_REG32(&core_if->dev_if->dev_global_regs->dctl, 0, dctl.d32);
+			do
+			{
+				dwc_udelay(10);
+				gintsts.d32 = DWC_READ_REG32(&core_if->core_global_regs->gintsts);
+			} while (!gintsts.b.goutnakeff);
+			gintsts.d32 = 0;
+			gintsts.b.goutnakeff = 1;
+			DWC_WRITE_REG32(&core_if->core_global_regs->gintsts, gintsts.d32);
+
+			depctl.d32 = 0;
+			depctl.b.epdis = 1;
+			depctl.b.snak = 1;
+			DWC_WRITE_REG32(&core_if->dev_if->out_ep_regs[ep->num]->doepctl, depctl.d32);
+			do
+			{
+				dwc_udelay(10);
+				doepint.d32 = DWC_READ_REG32(&core_if->dev_if->
+											out_ep_regs[ep->num]->doepint);
+			} while (!doepint.b.epdisabled);
+
+			doepint.b.epdisabled = 1;
+			DWC_WRITE_REG32(&core_if->dev_if->out_ep_regs[ep->num]->doepint, doepint.d32);
+
+			dctl.d32 = 0;
+			dctl.b.cgoutnak = 1;
+			DWC_MODIFY_REG32(&core_if->dev_if->dev_global_regs->dctl, 0, dctl.d32);
+		}
+	}
+
+	/* Disable the Interrupt for this EP */
+	if (core_if->multiproc_int_enable) {
+		DWC_MODIFY_REG32(&core_if->dev_if->dev_global_regs->deachintmsk,
+				 daintmsk.d32, 0);
+
+		if (ep->is_in == 1) {
+			DWC_WRITE_REG32(&core_if->dev_if->dev_global_regs->
+					diepeachintmsk[ep->num], 0);
+		} else {
+			DWC_WRITE_REG32(&core_if->dev_if->dev_global_regs->
+					doepeachintmsk[ep->num], 0);
+		}
+	} else {
+		DWC_MODIFY_REG32(&core_if->dev_if->dev_global_regs->daintmsk,
+				 daintmsk.d32, 0);
+	}
+
+}
+
+/**
+ * This function initializes dma descriptor chain.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param ep The EP to start the transfer on.
+ */
+static void init_dma_desc_chain(dwc_otg_core_if_t * core_if, dwc_ep_t * ep)
+{
+	dwc_otg_dev_dma_desc_t *dma_desc;
+	uint32_t offset;
+	uint32_t xfer_est;
+	int i;
+	unsigned maxxfer_local, total_len;
+
+	if (!ep->is_in && ep->type == DWC_OTG_EP_TYPE_INTR &&
+					(ep->maxpacket%4)) {
+		maxxfer_local = ep->maxpacket;
+		total_len = ep->xfer_len;
+	} else {
+		maxxfer_local = ep->maxxfer;
+		total_len = ep->total_len;
+	}
+
+	ep->desc_cnt = (total_len / maxxfer_local) +
+            ((total_len % maxxfer_local) ? 1 : 0);
+
+	if (!ep->desc_cnt)
+		ep->desc_cnt = 1;
+
+	if (ep->desc_cnt > MAX_DMA_DESC_CNT)
+		ep->desc_cnt = MAX_DMA_DESC_CNT;
+
+	dma_desc = ep->desc_addr;
+	if (maxxfer_local == ep->maxpacket) {
+		if ((total_len % maxxfer_local) &&
+				(total_len/maxxfer_local < MAX_DMA_DESC_CNT)) {
+			xfer_est = (ep->desc_cnt - 1) * maxxfer_local +
+					(total_len % maxxfer_local);
+		} else
+			xfer_est = ep->desc_cnt * maxxfer_local;
+	}
+	else
+		xfer_est = total_len;
+	offset = 0;
+	for (i = 0; i < ep->desc_cnt; ++i) {
+		/** DMA Descriptor Setup */
+		if (xfer_est > maxxfer_local) {
+			dma_desc->status.b.bs = BS_HOST_BUSY;
+			dma_desc->status.b.l = 0;
+			dma_desc->status.b.ioc = 0;
+			dma_desc->status.b.sp = 0;
+			dma_desc->status.b.bytes = maxxfer_local;
+			dma_desc->buf = ep->dma_addr + offset;
+			dma_desc->status.b.sts = 0;
+			dma_desc->status.b.bs = BS_HOST_READY;
+
+			xfer_est -= maxxfer_local;
+			offset += maxxfer_local;
+		} else {
+			dma_desc->status.b.bs = BS_HOST_BUSY;
+			dma_desc->status.b.l = 1;
+			dma_desc->status.b.ioc = 1;
+			if (ep->is_in) {
+				dma_desc->status.b.sp =
+				    (xfer_est %
+				     ep->maxpacket) ? 1 : ((ep->
+							    sent_zlp) ? 1 : 0);
+				dma_desc->status.b.bytes = xfer_est;
+			} else {
+				if (maxxfer_local == ep->maxpacket)
+					dma_desc->status.b.bytes = xfer_est;
+				else
+					dma_desc->status.b.bytes =
+				    		xfer_est + ((4 - (xfer_est & 0x3)) & 0x3);
+			}
+
+			dma_desc->buf = ep->dma_addr + offset;
+			dma_desc->status.b.sts = 0;
+			dma_desc->status.b.bs = BS_HOST_READY;
+		}
+		dma_desc++;
+	}
+}
+/**
+ * This function is called when to write ISOC data into appropriate dedicated
+ * periodic FIFO.
+ */
+static int32_t write_isoc_tx_fifo(dwc_otg_core_if_t * core_if, dwc_ep_t * dwc_ep)
+{
+	dwc_otg_dev_if_t *dev_if = core_if->dev_if;
+	dwc_otg_dev_in_ep_regs_t *ep_regs;
+	dtxfsts_data_t txstatus = {.d32 = 0 };
+	uint32_t len = 0;
+	int epnum = dwc_ep->num;
+	int dwords;
+
+	DWC_DEBUGPL(DBG_PCD, "Dedicated TxFifo Empty: %d \n", epnum);
+
+	ep_regs = core_if->dev_if->in_ep_regs[epnum];
+
+	len = dwc_ep->xfer_len - dwc_ep->xfer_count;
+
+	if (len > dwc_ep->maxpacket) {
+		len = dwc_ep->maxpacket;
+	}
+
+	dwords = (len + 3) / 4;
+
+	/* While there is space in the queue and space in the FIFO and
+	 * More data to tranfer, Write packets to the Tx FIFO */
+	txstatus.d32 = DWC_READ_REG32(&dev_if->in_ep_regs[epnum]->dtxfsts);
+	DWC_DEBUGPL(DBG_PCDV, "b4 dtxfsts[%d]=0x%08x\n", epnum, txstatus.d32);
+
+	while (txstatus.b.txfspcavail > dwords &&
+	       dwc_ep->xfer_count < dwc_ep->xfer_len &&
+	       dwc_ep->xfer_len != 0) {
+		/* Write the FIFO */
+		dwc_otg_ep_write_packet(core_if, dwc_ep, 0);
+
+		len = dwc_ep->xfer_len - dwc_ep->xfer_count;
+		if (len > dwc_ep->maxpacket) {
+			len = dwc_ep->maxpacket;
+		}
+
+		dwords = (len + 3) / 4;
+		txstatus.d32 =
+		    DWC_READ_REG32(&dev_if->in_ep_regs[epnum]->dtxfsts);
+		DWC_DEBUGPL(DBG_PCDV, "dtxfsts[%d]=0x%08x\n", epnum,
+			    txstatus.d32);
+	}
+
+	DWC_DEBUGPL(DBG_PCDV, "b4 dtxfsts[%d]=0x%08x\n", epnum,
+		    DWC_READ_REG32(&dev_if->in_ep_regs[epnum]->dtxfsts));
+
+	return 1;
+}
+/**
+ * This function does the setup for a data transfer for an EP and
+ * starts the transfer. For an IN transfer, the packets will be
+ * loaded into the appropriate Tx FIFO in the ISR. For OUT transfers,
+ * the packets are unloaded from the Rx FIFO in the ISR.  the ISR.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param ep The EP to start the transfer on.
+ */
+
+void dwc_otg_ep_start_transfer(dwc_otg_core_if_t * core_if, dwc_ep_t * ep)
+{
+	depctl_data_t depctl;
+	deptsiz_data_t deptsiz;
+	gintmsk_data_t intr_mask = {.d32 = 0 };
+
+	DWC_DEBUGPL((DBG_PCDV | DBG_CILV), "%s()\n", __func__);
+	DWC_DEBUGPL(DBG_PCD, "ep%d-%s xfer_len=%d xfer_cnt=%d "
+		    "xfer_buff=%p start_xfer_buff=%p, total_len = %d\n",
+		    ep->num, (ep->is_in ? "IN" : "OUT"), ep->xfer_len,
+		    ep->xfer_count, ep->xfer_buff, ep->start_xfer_buff,
+		    ep->total_len);
+	/* IN endpoint */
+	if (ep->is_in == 1) {
+		dwc_otg_dev_in_ep_regs_t *in_regs =
+		    core_if->dev_if->in_ep_regs[ep->num];
+
+		gnptxsts_data_t gtxstatus;
+
+		gtxstatus.d32 =
+		    DWC_READ_REG32(&core_if->core_global_regs->gnptxsts);
+
+		if (core_if->en_multiple_tx_fifo == 0
+		    && gtxstatus.b.nptxqspcavail == 0
+		    && !core_if->dma_enable) {
+#ifdef DEBUG
+			DWC_PRINTF("TX Queue Full (0x%0x)\n", gtxstatus.d32);
+#endif
+			return;
+		}
+
+		depctl.d32 = DWC_READ_REG32(&(in_regs->diepctl));
+		deptsiz.d32 = DWC_READ_REG32(&(in_regs->dieptsiz));
+
+		if (ep->maxpacket > ep->maxxfer / MAX_PKT_CNT)
+			ep->xfer_len += (ep->maxxfer < (ep->total_len - ep->xfer_len)) ?
+		    		ep->maxxfer : (ep->total_len - ep->xfer_len);
+		else
+			ep->xfer_len += (MAX_PKT_CNT * ep->maxpacket < (ep->total_len - ep->xfer_len)) ?
+				 MAX_PKT_CNT * ep->maxpacket : (ep->total_len - ep->xfer_len);
+
+
+		/* Zero Length Packet? */
+		if ((ep->xfer_len - ep->xfer_count) == 0) {
+			deptsiz.b.xfersize = 0;
+			deptsiz.b.pktcnt = 1;
+		} else {
+			/* Program the transfer size and packet count
+			 *      as follows: xfersize = N * maxpacket +
+			 *      short_packet pktcnt = N + (short_packet
+			 *      exist ? 1 : 0)
+			 */
+			deptsiz.b.xfersize = ep->xfer_len - ep->xfer_count;
+			deptsiz.b.pktcnt =
+			    (ep->xfer_len - ep->xfer_count - 1 +
+			     ep->maxpacket) / ep->maxpacket;
+			if (deptsiz.b.pktcnt > MAX_PKT_CNT) {
+				deptsiz.b.pktcnt = MAX_PKT_CNT;
+				deptsiz.b.xfersize = deptsiz.b.pktcnt * ep->maxpacket;
+			}
+			if (ep->type == DWC_OTG_EP_TYPE_ISOC)
+				deptsiz.b.mc = deptsiz.b.pktcnt;
+		}
+
+		/* Write the DMA register */
+		if (core_if->dma_enable) {
+			if (core_if->dma_desc_enable == 0) {
+				if (ep->type != DWC_OTG_EP_TYPE_ISOC)
+					deptsiz.b.mc = 1;
+				DWC_WRITE_REG32(&in_regs->dieptsiz,
+						deptsiz.d32);
+				DWC_WRITE_REG32(&(in_regs->diepdma),
+						(uint32_t) ep->dma_addr);
+			} else {
+#ifdef DWC_UTE_CFI
+				/* The descriptor chain should be already initialized by now */
+				if (ep->buff_mode != BM_STANDARD) {
+					DWC_WRITE_REG32(&in_regs->diepdma,
+							ep->descs_dma_addr);
+				} else {
+#endif
+					init_dma_desc_chain(core_if, ep);
+				/** DIEPDMAn Register write */
+					DWC_WRITE_REG32(&in_regs->diepdma,
+							ep->dma_desc_addr);
+#ifdef DWC_UTE_CFI
+				}
+#endif
+			}
+		} else {
+			DWC_WRITE_REG32(&in_regs->dieptsiz, deptsiz.d32);
+			if (ep->type != DWC_OTG_EP_TYPE_ISOC) {
+				/**
+				 * Enable the Non-Periodic Tx FIFO empty interrupt,
+				 * or the Tx FIFO epmty interrupt in dedicated Tx FIFO mode,
+				 * the data will be written into the fifo by the ISR.
+				 */
+				if (core_if->en_multiple_tx_fifo == 0) {
+					intr_mask.b.nptxfempty = 1;
+					DWC_MODIFY_REG32
+					    (&core_if->core_global_regs->gintmsk,
+					     intr_mask.d32, intr_mask.d32);
+				} else {
+					/* Enable the Tx FIFO Empty Interrupt for this EP */
+					if (ep->xfer_len > 0) {
+						uint32_t fifoemptymsk = 0;
+						fifoemptymsk = 1 << ep->num;
+						DWC_MODIFY_REG32
+						    (&core_if->dev_if->dev_global_regs->dtknqr4_fifoemptymsk,
+						     0, fifoemptymsk);
+
+					}
+				}
+			}  else {
+					 write_isoc_tx_fifo(core_if, ep);
+			}
+		}
+		if (!core_if->core_params->en_multiple_tx_fifo && core_if->dma_enable)
+			depctl.b.nextep = core_if->nextep_seq[ep->num];
+
+		if (ep->type == DWC_OTG_EP_TYPE_ISOC)
+		{
+			dsts_data_t dsts = {.d32 = 0};
+			if (ep->bInterval == 1) {
+				dsts.d32 =
+					DWC_READ_REG32(&core_if->dev_if->dev_global_regs->dsts);
+				ep->frame_num = dsts.b.soffn + ep->bInterval;
+				if (ep->frame_num > 0x3FFF)
+				{
+					ep->frm_overrun = 1;
+					ep->frame_num &= 0x3FFF;
+				} else
+					ep->frm_overrun = 0;
+				if (ep->frame_num & 0x1) {
+					depctl.b.setd1pid = 1;
+				} else {
+					depctl.b.setd0pid = 1;
+				}
+			}
+		}
+		/* EP enable, IN data in FIFO */
+		depctl.b.cnak = 1;
+		depctl.b.epena = 1;
+		DWC_WRITE_REG32(&in_regs->diepctl, depctl.d32);
+
+	} else {
+		/* OUT endpoint */
+		dwc_otg_dev_out_ep_regs_t *out_regs =
+		    core_if->dev_if->out_ep_regs[ep->num];
+
+		depctl.d32 = DWC_READ_REG32(&(out_regs->doepctl));
+		deptsiz.d32 = DWC_READ_REG32(&(out_regs->doeptsiz));
+
+		if (!core_if->dma_desc_enable) {
+			if (ep->maxpacket > ep->maxxfer / MAX_PKT_CNT)
+				ep->xfer_len += (ep->maxxfer < (ep->total_len - ep->xfer_len)) ?
+                        	ep->maxxfer : (ep->total_len - ep->xfer_len);
+                else
+					ep->xfer_len += (MAX_PKT_CNT * ep->maxpacket < (ep->total_len
+					- ep->xfer_len)) ? MAX_PKT_CNT * ep->maxpacket : (ep->total_len - ep->xfer_len);
+		}
+
+		/* Program the transfer size and packet count as follows:
+		 *
+		 *      pktcnt = N
+		 *      xfersize = N * maxpacket
+		 */
+		if ((ep->xfer_len - ep->xfer_count) == 0) {
+			/* Zero Length Packet */
+			deptsiz.b.xfersize = ep->maxpacket;
+			deptsiz.b.pktcnt = 1;
+		} else {
+			deptsiz.b.pktcnt =
+			    (ep->xfer_len - ep->xfer_count +
+			     (ep->maxpacket - 1)) / ep->maxpacket;
+			if (deptsiz.b.pktcnt > MAX_PKT_CNT) {
+				deptsiz.b.pktcnt = MAX_PKT_CNT;
+			}
+			if (!core_if->dma_desc_enable) {
+				ep->xfer_len =
+			    		deptsiz.b.pktcnt * ep->maxpacket + ep->xfer_count;
+			}
+			deptsiz.b.xfersize = ep->xfer_len - ep->xfer_count;
+		}
+
+		DWC_DEBUGPL(DBG_PCDV, "ep%d xfersize=%d pktcnt=%d\n",
+			    ep->num, deptsiz.b.xfersize, deptsiz.b.pktcnt);
+
+		if (core_if->dma_enable) {
+			if (!core_if->dma_desc_enable) {
+				DWC_WRITE_REG32(&out_regs->doeptsiz,
+						deptsiz.d32);
+
+				DWC_WRITE_REG32(&(out_regs->doepdma),
+						(uint32_t) ep->dma_addr);
+			} else {
+#ifdef DWC_UTE_CFI
+				/* The descriptor chain should be already initialized by now */
+				if (ep->buff_mode != BM_STANDARD) {
+					DWC_WRITE_REG32(&out_regs->doepdma,
+							ep->descs_dma_addr);
+				} else {
+#endif
+					/** This is used for interrupt out transfers*/
+					if (!ep->xfer_len)
+						ep->xfer_len = ep->total_len;
+					init_dma_desc_chain(core_if, ep);
+
+					if (core_if->core_params->dev_out_nak) {
+						if (ep->type == DWC_OTG_EP_TYPE_BULK) {
+							deptsiz.b.pktcnt = (ep->total_len +
+								(ep->maxpacket - 1)) / ep->maxpacket;
+							deptsiz.b.xfersize = ep->total_len;
+							/* Remember initial value of doeptsiz */
+							core_if->start_doeptsiz_val[ep->num] = deptsiz.d32;
+							DWC_WRITE_REG32(&out_regs->doeptsiz,
+								deptsiz.d32);
+						}
+					}
+				/** DOEPDMAn Register write */
+					DWC_WRITE_REG32(&out_regs->doepdma,
+							ep->dma_desc_addr);
+#ifdef DWC_UTE_CFI
+				}
+#endif
+			}
+		} else {
+			DWC_WRITE_REG32(&out_regs->doeptsiz, deptsiz.d32);
+		}
+
+		if (ep->type == DWC_OTG_EP_TYPE_ISOC)
+		{
+			dsts_data_t dsts = {.d32 = 0};
+			if (ep->bInterval == 1) {
+				dsts.d32 =
+					DWC_READ_REG32(&core_if->dev_if->dev_global_regs->dsts);
+				ep->frame_num = dsts.b.soffn + ep->bInterval;
+				if (ep->frame_num > 0x3FFF)
+				{
+					ep->frm_overrun = 1;
+					ep->frame_num &= 0x3FFF;
+				} else
+					ep->frm_overrun = 0;
+
+				if (ep->frame_num & 0x1) {
+					depctl.b.setd1pid = 1;
+				} else {
+					depctl.b.setd0pid = 1;
+				}
+			}
+		}
+
+		/* EP enable */
+		depctl.b.cnak = 1;
+		depctl.b.epena = 1;
+
+		DWC_WRITE_REG32(&out_regs->doepctl, depctl.d32);
+
+		DWC_DEBUGPL(DBG_PCD, "DOEPCTL=%08x DOEPTSIZ=%08x\n",
+			    DWC_READ_REG32(&out_regs->doepctl),
+			    DWC_READ_REG32(&out_regs->doeptsiz));
+		DWC_DEBUGPL(DBG_PCD, "DAINTMSK=%08x GINTMSK=%08x\n",
+			    DWC_READ_REG32(&core_if->dev_if->
+					   dev_global_regs->daintmsk),
+			    DWC_READ_REG32(&core_if->
+					   core_global_regs->gintmsk));
+
+
+		/* Timer is scheduling only for out bulk transfers for
+		 * "Device DDMA OUT NAK Enhancement" feature to inform user
+		 * about received data payload in case of timeout
+		 */
+		if (core_if->core_params->dev_out_nak) {
+			if (ep->type == DWC_OTG_EP_TYPE_BULK) {
+				core_if->ep_xfer_info[ep->num].core_if = core_if;
+				core_if->ep_xfer_info[ep->num].ep = ep;
+				core_if->ep_xfer_info[ep->num].state = 1;
+
+				/* Start a timer for this transfer. */
+				DWC_TIMER_SCHEDULE(core_if->ep_xfer_timer[ep->num], 10000);
+			}
+		}
+	}
+}
+
+/**
+ * This function setup a zero length transfer in Buffer DMA and
+ * Slave modes for usb requests with zero field set
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param ep The EP to start the transfer on.
+ *
+ */
+void dwc_otg_ep_start_zl_transfer(dwc_otg_core_if_t * core_if, dwc_ep_t * ep)
+{
+
+	depctl_data_t depctl;
+	deptsiz_data_t deptsiz;
+	gintmsk_data_t intr_mask = {.d32 = 0 };
+
+	DWC_DEBUGPL((DBG_PCDV | DBG_CILV), "%s()\n", __func__);
+	DWC_PRINTF("zero length transfer is called\n");
+
+	/* IN endpoint */
+	if (ep->is_in == 1) {
+		dwc_otg_dev_in_ep_regs_t *in_regs =
+		    core_if->dev_if->in_ep_regs[ep->num];
+
+		depctl.d32 = DWC_READ_REG32(&(in_regs->diepctl));
+		deptsiz.d32 = DWC_READ_REG32(&(in_regs->dieptsiz));
+
+		deptsiz.b.xfersize = 0;
+		deptsiz.b.pktcnt = 1;
+
+		/* Write the DMA register */
+		if (core_if->dma_enable) {
+			if (core_if->dma_desc_enable == 0) {
+				deptsiz.b.mc = 1;
+				DWC_WRITE_REG32(&in_regs->dieptsiz,
+						deptsiz.d32);
+				DWC_WRITE_REG32(&(in_regs->diepdma),
+						(uint32_t) ep->dma_addr);
+			}
+		} else {
+			DWC_WRITE_REG32(&in_regs->dieptsiz, deptsiz.d32);
+			/**
+			 * Enable the Non-Periodic Tx FIFO empty interrupt,
+			 * or the Tx FIFO epmty interrupt in dedicated Tx FIFO mode,
+			 * the data will be written into the fifo by the ISR.
+			 */
+			if (core_if->en_multiple_tx_fifo == 0) {
+				intr_mask.b.nptxfempty = 1;
+				DWC_MODIFY_REG32(&core_if->
+						 core_global_regs->gintmsk,
+						 intr_mask.d32, intr_mask.d32);
+			} else {
+				/* Enable the Tx FIFO Empty Interrupt for this EP */
+				if (ep->xfer_len > 0) {
+					uint32_t fifoemptymsk = 0;
+					fifoemptymsk = 1 << ep->num;
+					DWC_MODIFY_REG32(&core_if->
+							 dev_if->dev_global_regs->dtknqr4_fifoemptymsk,
+							 0, fifoemptymsk);
+				}
+			}
+		}
+
+		if (!core_if->core_params->en_multiple_tx_fifo && core_if->dma_enable)
+			depctl.b.nextep = core_if->nextep_seq[ep->num];
+		/* EP enable, IN data in FIFO */
+		depctl.b.cnak = 1;
+		depctl.b.epena = 1;
+		DWC_WRITE_REG32(&in_regs->diepctl, depctl.d32);
+
+	} else {
+		/* OUT endpoint */
+		dwc_otg_dev_out_ep_regs_t *out_regs =
+		    core_if->dev_if->out_ep_regs[ep->num];
+
+		depctl.d32 = DWC_READ_REG32(&(out_regs->doepctl));
+		deptsiz.d32 = DWC_READ_REG32(&(out_regs->doeptsiz));
+
+		/* Zero Length Packet */
+		deptsiz.b.xfersize = ep->maxpacket;
+		deptsiz.b.pktcnt = 1;
+
+		if (core_if->dma_enable) {
+			if (!core_if->dma_desc_enable) {
+				DWC_WRITE_REG32(&out_regs->doeptsiz,
+						deptsiz.d32);
+
+				DWC_WRITE_REG32(&(out_regs->doepdma),
+						(uint32_t) ep->dma_addr);
+			}
+		} else {
+			DWC_WRITE_REG32(&out_regs->doeptsiz, deptsiz.d32);
+		}
+
+		/* EP enable */
+		depctl.b.cnak = 1;
+		depctl.b.epena = 1;
+
+		DWC_WRITE_REG32(&out_regs->doepctl, depctl.d32);
+
+	}
+}
+
+/**
+ * This function does the setup for a data transfer for EP0 and starts
+ * the transfer.  For an IN transfer, the packets will be loaded into
+ * the appropriate Tx FIFO in the ISR. For OUT transfers, the packets are
+ * unloaded from the Rx FIFO in the ISR.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param ep The EP0 data.
+ */
+void dwc_otg_ep0_start_transfer(dwc_otg_core_if_t * core_if, dwc_ep_t * ep)
+{
+	depctl_data_t depctl;
+	deptsiz0_data_t deptsiz;
+	gintmsk_data_t intr_mask = {.d32 = 0 };
+	dwc_otg_dev_dma_desc_t *dma_desc;
+
+	DWC_DEBUGPL(DBG_PCD, "ep%d-%s xfer_len=%d xfer_cnt=%d "
+		    "xfer_buff=%p start_xfer_buff=%p \n",
+		    ep->num, (ep->is_in ? "IN" : "OUT"), ep->xfer_len,
+		    ep->xfer_count, ep->xfer_buff, ep->start_xfer_buff);
+
+	ep->total_len = ep->xfer_len;
+
+	/* IN endpoint */
+	if (ep->is_in == 1) {
+		dwc_otg_dev_in_ep_regs_t *in_regs =
+		    core_if->dev_if->in_ep_regs[0];
+
+		gnptxsts_data_t gtxstatus;
+
+		gtxstatus.d32 =
+		    DWC_READ_REG32(&core_if->core_global_regs->gnptxsts);
+
+		if (core_if->en_multiple_tx_fifo == 0
+		    && gtxstatus.b.nptxqspcavail == 0
+		    && !core_if->dma_enable) {
+#ifdef DEBUG
+			deptsiz.d32 = DWC_READ_REG32(&in_regs->dieptsiz);
+			DWC_DEBUGPL(DBG_PCD, "DIEPCTL0=%0x\n",
+				    DWC_READ_REG32(&in_regs->diepctl));
+			DWC_DEBUGPL(DBG_PCD, "DIEPTSIZ0=%0x (sz=%d, pcnt=%d)\n",
+				    deptsiz.d32,
+				    deptsiz.b.xfersize, deptsiz.b.pktcnt);
+			DWC_PRINTF("TX Queue or FIFO Full (0x%0x)\n",
+				   gtxstatus.d32);
+#endif
+			return;
+		}
+
+		depctl.d32 = DWC_READ_REG32(&in_regs->diepctl);
+		deptsiz.d32 = DWC_READ_REG32(&in_regs->dieptsiz);
+
+		/* Zero Length Packet? */
+		if (ep->xfer_len == 0) {
+			deptsiz.b.xfersize = 0;
+			deptsiz.b.pktcnt = 1;
+		} else {
+			/* Program the transfer size and packet count
+			 *      as follows: xfersize = N * maxpacket +
+			 *      short_packet pktcnt = N + (short_packet
+			 *      exist ? 1 : 0)
+			 */
+			if (ep->xfer_len > ep->maxpacket) {
+				ep->xfer_len = ep->maxpacket;
+				deptsiz.b.xfersize = ep->maxpacket;
+			} else {
+				deptsiz.b.xfersize = ep->xfer_len;
+			}
+			deptsiz.b.pktcnt = 1;
+
+		}
+		DWC_DEBUGPL(DBG_PCDV,
+			    "IN len=%d  xfersize=%d pktcnt=%d [%08x]\n",
+			    ep->xfer_len, deptsiz.b.xfersize, deptsiz.b.pktcnt,
+			    deptsiz.d32);
+
+		/* Write the DMA register */
+		if (core_if->dma_enable) {
+			if (core_if->dma_desc_enable == 0) {
+				DWC_WRITE_REG32(&in_regs->dieptsiz,
+						deptsiz.d32);
+
+				DWC_WRITE_REG32(&(in_regs->diepdma),
+						(uint32_t) ep->dma_addr);
+			} else {
+				dma_desc = core_if->dev_if->in_desc_addr;
+
+				/** DMA Descriptor Setup */
+				dma_desc->status.b.bs = BS_HOST_BUSY;
+				dma_desc->status.b.l = 1;
+				dma_desc->status.b.ioc = 1;
+				dma_desc->status.b.sp =
+				    (ep->xfer_len == ep->maxpacket) ? 0 : 1;
+				dma_desc->status.b.bytes = ep->xfer_len;
+				dma_desc->buf = ep->dma_addr;
+				dma_desc->status.b.sts = 0;
+				dma_desc->status.b.bs = BS_HOST_READY;
+
+				/** DIEPDMA0 Register write */
+				DWC_WRITE_REG32(&in_regs->diepdma,
+						core_if->
+						dev_if->dma_in_desc_addr);
+			}
+		} else {
+			DWC_WRITE_REG32(&in_regs->dieptsiz, deptsiz.d32);
+		}
+
+		if (!core_if->core_params->en_multiple_tx_fifo && core_if->dma_enable)
+			depctl.b.nextep = core_if->nextep_seq[ep->num];
+		/* EP enable, IN data in FIFO */
+		depctl.b.cnak = 1;
+		depctl.b.epena = 1;
+		DWC_WRITE_REG32(&in_regs->diepctl, depctl.d32);
+
+		/**
+		 * Enable the Non-Periodic Tx FIFO empty interrupt, the
+		 * data will be written into the fifo by the ISR.
+		 */
+		if (!core_if->dma_enable) {
+			if (core_if->en_multiple_tx_fifo == 0) {
+				intr_mask.b.nptxfempty = 1;
+				DWC_MODIFY_REG32(&core_if->
+						 core_global_regs->gintmsk,
+						 intr_mask.d32, intr_mask.d32);
+			} else {
+				/* Enable the Tx FIFO Empty Interrupt for this EP */
+				if (ep->xfer_len > 0) {
+					uint32_t fifoemptymsk = 0;
+					fifoemptymsk |= 1 << ep->num;
+					DWC_MODIFY_REG32(&core_if->
+							 dev_if->dev_global_regs->dtknqr4_fifoemptymsk,
+							 0, fifoemptymsk);
+				}
+			}
+		}
+	} else {
+		/* OUT endpoint */
+		dwc_otg_dev_out_ep_regs_t *out_regs =
+		    core_if->dev_if->out_ep_regs[0];
+
+		depctl.d32 = DWC_READ_REG32(&out_regs->doepctl);
+		deptsiz.d32 = DWC_READ_REG32(&out_regs->doeptsiz);
+
+		/* Program the transfer size and packet count as follows:
+		 *      xfersize = N * (maxpacket + 4 - (maxpacket % 4))
+		 *      pktcnt = N                                                                                      */
+		/* Zero Length Packet */
+		deptsiz.b.xfersize = ep->maxpacket;
+		deptsiz.b.pktcnt = 1;
+
+		DWC_DEBUGPL(DBG_PCDV, "len=%d  xfersize=%d pktcnt=%d\n",
+			    ep->xfer_len, deptsiz.b.xfersize, deptsiz.b.pktcnt);
+
+		if (core_if->dma_enable) {
+			if (!core_if->dma_desc_enable) {
+				DWC_WRITE_REG32(&out_regs->doeptsiz,
+						deptsiz.d32);
+
+				DWC_WRITE_REG32(&(out_regs->doepdma),
+						(uint32_t) ep->dma_addr);
+			} else {
+				dma_desc = core_if->dev_if->out_desc_addr;
+
+				/** DMA Descriptor Setup */
+				dma_desc->status.b.bs = BS_HOST_BUSY;
+				dma_desc->status.b.l = 1;
+				dma_desc->status.b.ioc = 1;
+				dma_desc->status.b.bytes = ep->maxpacket;
+				dma_desc->buf = ep->dma_addr;
+				dma_desc->status.b.sts = 0;
+				dma_desc->status.b.bs = BS_HOST_READY;
+
+				/** DOEPDMA0 Register write */
+				DWC_WRITE_REG32(&out_regs->doepdma,
+						core_if->
+						dev_if->dma_out_desc_addr);
+			}
+		} else {
+			DWC_WRITE_REG32(&out_regs->doeptsiz, deptsiz.d32);
+		}
+
+		/* EP enable */
+		depctl.b.cnak = 1;
+		depctl.b.epena = 1;
+		DWC_WRITE_REG32(&(out_regs->doepctl), depctl.d32);
+	}
+}
+
+/**
+ * This function continues control IN transfers started by
+ * dwc_otg_ep0_start_transfer, when the transfer does not fit in a
+ * single packet.  NOTE: The DIEPCTL0/DOEPCTL0 registers only have one
+ * bit for the packet count.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param ep The EP0 data.
+ */
+void dwc_otg_ep0_continue_transfer(dwc_otg_core_if_t * core_if, dwc_ep_t * ep)
+{
+	depctl_data_t depctl;
+	deptsiz0_data_t deptsiz;
+	gintmsk_data_t intr_mask = {.d32 = 0 };
+	dwc_otg_dev_dma_desc_t *dma_desc;
+
+	if (ep->is_in == 1) {
+		dwc_otg_dev_in_ep_regs_t *in_regs =
+		    core_if->dev_if->in_ep_regs[0];
+		gnptxsts_data_t tx_status = {.d32 = 0 };
+
+		tx_status.d32 =
+		    DWC_READ_REG32(&core_if->core_global_regs->gnptxsts);
+		/** @todo Should there be check for room in the Tx
+		 * Status Queue.  If not remove the code above this comment. */
+
+		depctl.d32 = DWC_READ_REG32(&in_regs->diepctl);
+		deptsiz.d32 = DWC_READ_REG32(&in_regs->dieptsiz);
+
+		/* Program the transfer size and packet count
+		 *      as follows: xfersize = N * maxpacket +
+		 *      short_packet pktcnt = N + (short_packet
+		 *      exist ? 1 : 0)
+		 */
+
+		if (core_if->dma_desc_enable == 0) {
+			deptsiz.b.xfersize =
+			    (ep->total_len - ep->xfer_count) >
+			    ep->maxpacket ? ep->maxpacket : (ep->total_len -
+							     ep->xfer_count);
+			deptsiz.b.pktcnt = 1;
+			if (core_if->dma_enable == 0) {
+				ep->xfer_len += deptsiz.b.xfersize;
+			} else {
+				ep->xfer_len = deptsiz.b.xfersize;
+			}
+			DWC_WRITE_REG32(&in_regs->dieptsiz, deptsiz.d32);
+		} else {
+			ep->xfer_len =
+			    (ep->total_len - ep->xfer_count) >
+			    ep->maxpacket ? ep->maxpacket : (ep->total_len -
+							     ep->xfer_count);
+
+			dma_desc = core_if->dev_if->in_desc_addr;
+
+			/** DMA Descriptor Setup */
+			dma_desc->status.b.bs = BS_HOST_BUSY;
+			dma_desc->status.b.l = 1;
+			dma_desc->status.b.ioc = 1;
+			dma_desc->status.b.sp =
+			    (ep->xfer_len == ep->maxpacket) ? 0 : 1;
+			dma_desc->status.b.bytes = ep->xfer_len;
+			dma_desc->buf = ep->dma_addr;
+			dma_desc->status.b.sts = 0;
+			dma_desc->status.b.bs = BS_HOST_READY;
+
+			/** DIEPDMA0 Register write */
+			DWC_WRITE_REG32(&in_regs->diepdma,
+					core_if->dev_if->dma_in_desc_addr);
+		}
+
+		DWC_DEBUGPL(DBG_PCDV,
+			    "IN len=%d  xfersize=%d pktcnt=%d [%08x]\n",
+			    ep->xfer_len, deptsiz.b.xfersize, deptsiz.b.pktcnt,
+			    deptsiz.d32);
+
+		/* Write the DMA register */
+		if (core_if->hwcfg2.b.architecture == DWC_INT_DMA_ARCH) {
+			if (core_if->dma_desc_enable == 0)
+				DWC_WRITE_REG32(&(in_regs->diepdma),
+						(uint32_t) ep->dma_addr);
+		}
+		if (!core_if->core_params->en_multiple_tx_fifo && core_if->dma_enable)
+			depctl.b.nextep = core_if->nextep_seq[ep->num];
+		/* EP enable, IN data in FIFO */
+		depctl.b.cnak = 1;
+		depctl.b.epena = 1;
+		DWC_WRITE_REG32(&in_regs->diepctl, depctl.d32);
+
+		/**
+		 * Enable the Non-Periodic Tx FIFO empty interrupt, the
+		 * data will be written into the fifo by the ISR.
+		 */
+		if (!core_if->dma_enable) {
+			if (core_if->en_multiple_tx_fifo == 0) {
+				/* First clear it from GINTSTS */
+				intr_mask.b.nptxfempty = 1;
+				DWC_MODIFY_REG32(&core_if->
+						 core_global_regs->gintmsk,
+						 intr_mask.d32, intr_mask.d32);
+
+			} else {
+				/* Enable the Tx FIFO Empty Interrupt for this EP */
+				if (ep->xfer_len > 0) {
+					uint32_t fifoemptymsk = 0;
+					fifoemptymsk |= 1 << ep->num;
+					DWC_MODIFY_REG32(&core_if->
+							 dev_if->dev_global_regs->dtknqr4_fifoemptymsk,
+							 0, fifoemptymsk);
+				}
+			}
+		}
+	} else {
+		dwc_otg_dev_out_ep_regs_t *out_regs =
+		    core_if->dev_if->out_ep_regs[0];
+
+		depctl.d32 = DWC_READ_REG32(&out_regs->doepctl);
+		deptsiz.d32 = DWC_READ_REG32(&out_regs->doeptsiz);
+
+		/* Program the transfer size and packet count
+		 *      as follows: xfersize = N * maxpacket +
+		 *      short_packet pktcnt = N + (short_packet
+		 *      exist ? 1 : 0)
+		 */
+		deptsiz.b.xfersize = ep->maxpacket;
+		deptsiz.b.pktcnt = 1;
+
+		if (core_if->dma_desc_enable == 0) {
+			DWC_WRITE_REG32(&out_regs->doeptsiz, deptsiz.d32);
+		} else {
+			dma_desc = core_if->dev_if->out_desc_addr;
+
+			/** DMA Descriptor Setup */
+			dma_desc->status.b.bs = BS_HOST_BUSY;
+			dma_desc->status.b.l = 1;
+			dma_desc->status.b.ioc = 1;
+			dma_desc->status.b.bytes = ep->maxpacket;
+			dma_desc->buf = ep->dma_addr;
+			dma_desc->status.b.sts = 0;
+			dma_desc->status.b.bs = BS_HOST_READY;
+
+			/** DOEPDMA0 Register write */
+			DWC_WRITE_REG32(&out_regs->doepdma,
+					core_if->dev_if->dma_out_desc_addr);
+		}
+
+		DWC_DEBUGPL(DBG_PCDV,
+			    "IN len=%d  xfersize=%d pktcnt=%d [%08x]\n",
+			    ep->xfer_len, deptsiz.b.xfersize, deptsiz.b.pktcnt,
+			    deptsiz.d32);
+
+		/* Write the DMA register */
+		if (core_if->hwcfg2.b.architecture == DWC_INT_DMA_ARCH) {
+			if (core_if->dma_desc_enable == 0)
+				DWC_WRITE_REG32(&(out_regs->doepdma),
+						(uint32_t) ep->dma_addr);
+
+		}
+
+		/* EP enable, IN data in FIFO */
+		depctl.b.cnak = 1;
+		depctl.b.epena = 1;
+		DWC_WRITE_REG32(&out_regs->doepctl, depctl.d32);
+
+	}
+}
+
+#ifdef DEBUG
+void dump_msg(const u8 * buf, unsigned int length)
+{
+	unsigned int start, num, i;
+	char line[52], *p;
+
+	if (length >= 512)
+		return;
+	start = 0;
+	while (length > 0) {
+		num = length < 16u ? length : 16u;
+		p = line;
+		for (i = 0; i < num; ++i) {
+			if (i == 8)
+				*p++ = ' ';
+			DWC_SPRINTF(p, " %02x", buf[i]);
+			p += 3;
+		}
+		*p = 0;
+		DWC_PRINTF("%6x: %s\n", start, line);
+		buf += num;
+		start += num;
+		length -= num;
+	}
+}
+#else
+static inline void dump_msg(const u8 * buf, unsigned int length)
+{
+}
+#endif
+
+/**
+ * This function writes a packet into the Tx FIFO associated with the
+ * EP. For non-periodic EPs the non-periodic Tx FIFO is written.  For
+ * periodic EPs the periodic Tx FIFO associated with the EP is written
+ * with all packets for the next micro-frame.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param ep The EP to write packet for.
+ * @param dma Indicates if DMA is being used.
+ */
+void dwc_otg_ep_write_packet(dwc_otg_core_if_t * core_if, dwc_ep_t * ep,
+			     int dma)
+{
+	/**
+	 * The buffer is padded to DWORD on a per packet basis in
+	 * slave/dma mode if the MPS is not DWORD aligned. The last
+	 * packet, if short, is also padded to a multiple of DWORD.
+	 *
+	 * ep->xfer_buff always starts DWORD aligned in memory and is a
+	 * multiple of DWORD in length
+	 *
+	 * ep->xfer_len can be any number of bytes
+	 *
+	 * ep->xfer_count is a multiple of ep->maxpacket until the last
+	 *	packet
+	 *
+	 * FIFO access is DWORD */
+
+	uint32_t i;
+	uint32_t byte_count;
+	uint32_t dword_count;
+	uint32_t *fifo;
+	uint32_t *data_buff = (uint32_t *) ep->xfer_buff;
+
+	DWC_DEBUGPL((DBG_PCDV | DBG_CILV), "%s(%p,%p)\n", __func__, core_if,
+		    ep);
+	if (ep->xfer_count >= ep->xfer_len) {
+		DWC_WARN("%s() No data for EP%d!!!\n", __func__, ep->num);
+		return;
+	}
+
+	/* Find the byte length of the packet either short packet or MPS */
+	if ((ep->xfer_len - ep->xfer_count) < ep->maxpacket) {
+		byte_count = ep->xfer_len - ep->xfer_count;
+	} else {
+		byte_count = ep->maxpacket;
+	}
+
+	/* Find the DWORD length, padded by extra bytes as neccessary if MPS
+	 * is not a multiple of DWORD */
+	dword_count = (byte_count + 3) / 4;
+
+#ifdef VERBOSE
+	dump_msg(ep->xfer_buff, byte_count);
+#endif
+
+	/**@todo NGS Where are the Periodic Tx FIFO addresses
+	 * intialized?	What should this be? */
+
+	fifo = core_if->data_fifo[ep->num];
+
+	DWC_DEBUGPL((DBG_PCDV | DBG_CILV), "fifo=%p buff=%p *p=%08x bc=%d\n",
+		    fifo, data_buff, *data_buff, byte_count);
+
+	if (!dma) {
+		for (i = 0; i < dword_count; i++, data_buff++) {
+			DWC_WRITE_REG32(fifo, *data_buff);
+		}
+	}
+
+	ep->xfer_count += byte_count;
+	ep->xfer_buff += byte_count;
+	ep->dma_addr += byte_count;
+}
+
+/**
+ * Set the EP STALL.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param ep The EP to set the stall on.
+ */
+void dwc_otg_ep_set_stall(dwc_otg_core_if_t * core_if, dwc_ep_t * ep)
+{
+	depctl_data_t depctl;
+	volatile uint32_t *depctl_addr;
+
+	DWC_DEBUGPL(DBG_PCD, "%s ep%d-%s\n", __func__, ep->num,
+		    (ep->is_in ? "IN" : "OUT"));
+
+	if (ep->is_in == 1) {
+		depctl_addr = &(core_if->dev_if->in_ep_regs[ep->num]->diepctl);
+		depctl.d32 = DWC_READ_REG32(depctl_addr);
+
+		/* set the disable and stall bits */
+		if (depctl.b.epena) {
+			depctl.b.epdis = 1;
+		}
+		depctl.b.stall = 1;
+		DWC_WRITE_REG32(depctl_addr, depctl.d32);
+	} else {
+		depctl_addr = &(core_if->dev_if->out_ep_regs[ep->num]->doepctl);
+		depctl.d32 = DWC_READ_REG32(depctl_addr);
+
+		/* set the stall bit */
+		depctl.b.stall = 1;
+		DWC_WRITE_REG32(depctl_addr, depctl.d32);
+	}
+
+	DWC_DEBUGPL(DBG_PCD, "DEPCTL=%0x\n", DWC_READ_REG32(depctl_addr));
+
+	return;
+}
+
+/**
+ * Clear the EP STALL.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param ep The EP to clear stall from.
+ */
+void dwc_otg_ep_clear_stall(dwc_otg_core_if_t * core_if, dwc_ep_t * ep)
+{
+	depctl_data_t depctl;
+	volatile uint32_t *depctl_addr;
+
+	DWC_DEBUGPL(DBG_PCD, "%s ep%d-%s\n", __func__, ep->num,
+		    (ep->is_in ? "IN" : "OUT"));
+
+	if (ep->is_in == 1) {
+		depctl_addr = &(core_if->dev_if->in_ep_regs[ep->num]->diepctl);
+	} else {
+		depctl_addr = &(core_if->dev_if->out_ep_regs[ep->num]->doepctl);
+	}
+
+	depctl.d32 = DWC_READ_REG32(depctl_addr);
+
+	/* clear the stall bits */
+	depctl.b.stall = 0;
+
+	/*
+	 * USB Spec 9.4.5: For endpoints using data toggle, regardless
+	 * of whether an endpoint has the Halt feature set, a
+	 * ClearFeature(ENDPOINT_HALT) request always results in the
+	 * data toggle being reinitialized to DATA0.
+	 */
+	if (ep->type == DWC_OTG_EP_TYPE_INTR ||
+	    ep->type == DWC_OTG_EP_TYPE_BULK) {
+		depctl.b.setd0pid = 1;	/* DATA0 */
+	}
+
+	DWC_WRITE_REG32(depctl_addr, depctl.d32);
+	DWC_DEBUGPL(DBG_PCD, "DEPCTL=%0x\n", DWC_READ_REG32(depctl_addr));
+	return;
+}
+
+/**
+ * This function reads a packet from the Rx FIFO into the destination
+ * buffer. To read SETUP data use dwc_otg_read_setup_packet.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param dest	  Destination buffer for the packet.
+ * @param bytes  Number of bytes to copy to the destination.
+ */
+void dwc_otg_read_packet(dwc_otg_core_if_t * core_if,
+			 uint8_t * dest, uint16_t bytes)
+{
+	int i;
+	int word_count = (bytes + 3) / 4;
+
+	volatile uint32_t *fifo = core_if->data_fifo[0];
+	uint32_t *data_buff = (uint32_t *) dest;
+
+	/**
+	 * @todo Account for the case where _dest is not dword aligned. This
+	 * requires reading data from the FIFO into a uint32_t temp buffer,
+	 * then moving it into the data buffer.
+	 */
+
+	DWC_DEBUGPL((DBG_PCDV | DBG_CILV), "%s(%p,%p,%d)\n", __func__,
+		    core_if, dest, bytes);
+
+	for (i = 0; i < word_count; i++, data_buff++) {
+		*data_buff = DWC_READ_REG32(fifo);
+	}
+
+	return;
+}
+
+/**
+ * This functions reads the device registers and prints them
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ */
+void dwc_otg_dump_dev_registers(dwc_otg_core_if_t * core_if)
+{
+	int i;
+	volatile uint32_t *addr;
+
+	DWC_PRINTF("Device Global Registers\n");
+	addr = &core_if->dev_if->dev_global_regs->dcfg;
+	DWC_PRINTF("DCFG		 @0x%08lX : 0x%08X\n", (unsigned long) addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->dev_if->dev_global_regs->dctl;
+	DWC_PRINTF("DCTL		 @0x%08lX : 0x%08X\n", (unsigned long) addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->dev_if->dev_global_regs->dsts;
+	DWC_PRINTF("DSTS		 @0x%08lX : 0x%08X\n", (unsigned long) addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->dev_if->dev_global_regs->diepmsk;
+	DWC_PRINTF("DIEPMSK	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->dev_if->dev_global_regs->doepmsk;
+	DWC_PRINTF("DOEPMSK	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->dev_if->dev_global_regs->daint;
+	DWC_PRINTF("DAINT	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->dev_if->dev_global_regs->daintmsk;
+	DWC_PRINTF("DAINTMSK	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->dev_if->dev_global_regs->dtknqr1;
+	DWC_PRINTF("DTKNQR1	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	if (core_if->hwcfg2.b.dev_token_q_depth > 6) {
+		addr = &core_if->dev_if->dev_global_regs->dtknqr2;
+		DWC_PRINTF("DTKNQR2	 @0x%08lX : 0x%08X\n",
+			   (unsigned long)addr, DWC_READ_REG32(addr));
+	}
+
+	addr = &core_if->dev_if->dev_global_regs->dvbusdis;
+	DWC_PRINTF("DVBUSID	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+
+	addr = &core_if->dev_if->dev_global_regs->dvbuspulse;
+	DWC_PRINTF("DVBUSPULSE	@0x%08lX : 0x%08X\n",
+		   (unsigned long)addr, DWC_READ_REG32(addr));
+
+	addr = &core_if->dev_if->dev_global_regs->dtknqr3_dthrctl;
+	DWC_PRINTF("DTKNQR3_DTHRCTL	 @0x%08lX : 0x%08X\n",
+		   (unsigned long)addr, DWC_READ_REG32(addr));
+
+	if (core_if->hwcfg2.b.dev_token_q_depth > 22) {
+		addr = &core_if->dev_if->dev_global_regs->dtknqr4_fifoemptymsk;
+		DWC_PRINTF("DTKNQR4	 @0x%08lX : 0x%08X\n",
+			   (unsigned long)addr, DWC_READ_REG32(addr));
+	}
+
+	addr = &core_if->dev_if->dev_global_regs->dtknqr4_fifoemptymsk;
+	DWC_PRINTF("FIFOEMPMSK	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+
+	if (core_if->hwcfg2.b.multi_proc_int) {
+
+		addr = &core_if->dev_if->dev_global_regs->deachint;
+		DWC_PRINTF("DEACHINT	 @0x%08lX : 0x%08X\n",
+			   (unsigned long)addr, DWC_READ_REG32(addr));
+		addr = &core_if->dev_if->dev_global_regs->deachintmsk;
+		DWC_PRINTF("DEACHINTMSK	 @0x%08lX : 0x%08X\n",
+			   (unsigned long)addr, DWC_READ_REG32(addr));
+
+		for (i = 0; i <= core_if->dev_if->num_in_eps; i++) {
+			addr =
+			    &core_if->dev_if->dev_global_regs->
+			    diepeachintmsk[i];
+			DWC_PRINTF("DIEPEACHINTMSK[%d]	 @0x%08lX : 0x%08X\n",
+				   i, (unsigned long)addr,
+				   DWC_READ_REG32(addr));
+		}
+
+		for (i = 0; i <= core_if->dev_if->num_out_eps; i++) {
+			addr =
+			    &core_if->dev_if->dev_global_regs->
+			    doepeachintmsk[i];
+			DWC_PRINTF("DOEPEACHINTMSK[%d]	 @0x%08lX : 0x%08X\n",
+				   i, (unsigned long)addr,
+				   DWC_READ_REG32(addr));
+		}
+	}
+
+	for (i = 0; i <= core_if->dev_if->num_in_eps; i++) {
+		DWC_PRINTF("Device IN EP %d Registers\n", i);
+		addr = &core_if->dev_if->in_ep_regs[i]->diepctl;
+		DWC_PRINTF("DIEPCTL	 @0x%08lX : 0x%08X\n",
+			   (unsigned long)addr, DWC_READ_REG32(addr));
+		addr = &core_if->dev_if->in_ep_regs[i]->diepint;
+		DWC_PRINTF("DIEPINT	 @0x%08lX : 0x%08X\n",
+			   (unsigned long)addr, DWC_READ_REG32(addr));
+		addr = &core_if->dev_if->in_ep_regs[i]->dieptsiz;
+		DWC_PRINTF("DIETSIZ	 @0x%08lX : 0x%08X\n",
+			   (unsigned long)addr, DWC_READ_REG32(addr));
+		addr = &core_if->dev_if->in_ep_regs[i]->diepdma;
+		DWC_PRINTF("DIEPDMA	 @0x%08lX : 0x%08X\n",
+			   (unsigned long)addr, DWC_READ_REG32(addr));
+		addr = &core_if->dev_if->in_ep_regs[i]->dtxfsts;
+		DWC_PRINTF("DTXFSTS	 @0x%08lX : 0x%08X\n",
+			   (unsigned long)addr, DWC_READ_REG32(addr));
+		addr = &core_if->dev_if->in_ep_regs[i]->diepdmab;
+		DWC_PRINTF("DIEPDMAB	 @0x%08lX : 0x%08X\n",
+			   (unsigned long)addr, 0 /*DWC_READ_REG32(addr) */ );
+	}
+
+	for (i = 0; i <= core_if->dev_if->num_out_eps; i++) {
+		DWC_PRINTF("Device OUT EP %d Registers\n", i);
+		addr = &core_if->dev_if->out_ep_regs[i]->doepctl;
+		DWC_PRINTF("DOEPCTL	 @0x%08lX : 0x%08X\n",
+			   (unsigned long)addr, DWC_READ_REG32(addr));
+		addr = &core_if->dev_if->out_ep_regs[i]->doepint;
+		DWC_PRINTF("DOEPINT	 @0x%08lX : 0x%08X\n",
+			   (unsigned long)addr, DWC_READ_REG32(addr));
+		addr = &core_if->dev_if->out_ep_regs[i]->doeptsiz;
+		DWC_PRINTF("DOETSIZ	 @0x%08lX : 0x%08X\n",
+			   (unsigned long)addr, DWC_READ_REG32(addr));
+		addr = &core_if->dev_if->out_ep_regs[i]->doepdma;
+		DWC_PRINTF("DOEPDMA	 @0x%08lX : 0x%08X\n",
+			   (unsigned long)addr, DWC_READ_REG32(addr));
+		if (core_if->dma_enable) {	/* Don't access this register in SLAVE mode */
+			addr = &core_if->dev_if->out_ep_regs[i]->doepdmab;
+			DWC_PRINTF("DOEPDMAB	 @0x%08lX : 0x%08X\n",
+				   (unsigned long)addr, DWC_READ_REG32(addr));
+		}
+
+	}
+}
+
+/**
+ * This functions reads the SPRAM and prints its content
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ */
+void dwc_otg_dump_spram(dwc_otg_core_if_t * core_if)
+{
+	volatile uint8_t *addr, *start_addr, *end_addr;
+
+	DWC_PRINTF("SPRAM Data:\n");
+	start_addr = (void *)core_if->core_global_regs;
+	DWC_PRINTF("Base Address: 0x%8lX\n", (unsigned long)start_addr);
+	start_addr += 0x00028000;
+	end_addr = (void *)core_if->core_global_regs;
+	end_addr += 0x000280e0;
+
+	for (addr = start_addr; addr < end_addr; addr += 16) {
+		DWC_PRINTF
+		    ("0x%8lX:\t%2X %2X %2X %2X %2X %2X %2X %2X %2X %2X %2X %2X %2X %2X %2X %2X\n",
+		     (unsigned long)addr, addr[0], addr[1], addr[2], addr[3],
+		     addr[4], addr[5], addr[6], addr[7], addr[8], addr[9],
+		     addr[10], addr[11], addr[12], addr[13], addr[14], addr[15]
+		    );
+	}
+
+	return;
+}
+
+/**
+ * This function reads the host registers and prints them
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ */
+void dwc_otg_dump_host_registers(dwc_otg_core_if_t * core_if)
+{
+	int i;
+	volatile uint32_t *addr;
+
+	DWC_PRINTF("Host Global Registers\n");
+	addr = &core_if->host_if->host_global_regs->hcfg;
+	DWC_PRINTF("HCFG		 @0x%08lX : 0x%08X\n",
+		   (unsigned long)addr, DWC_READ_REG32(addr));
+	addr = &core_if->host_if->host_global_regs->hfir;
+	DWC_PRINTF("HFIR		 @0x%08lX : 0x%08X\n",
+		   (unsigned long)addr, DWC_READ_REG32(addr));
+	addr = &core_if->host_if->host_global_regs->hfnum;
+	DWC_PRINTF("HFNUM	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->host_if->host_global_regs->hptxsts;
+	DWC_PRINTF("HPTXSTS	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->host_if->host_global_regs->haint;
+	DWC_PRINTF("HAINT	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->host_if->host_global_regs->haintmsk;
+	DWC_PRINTF("HAINTMSK	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	if (core_if->dma_desc_enable) {
+		addr = &core_if->host_if->host_global_regs->hflbaddr;
+		DWC_PRINTF("HFLBADDR	 @0x%08lX : 0x%08X\n",
+			   (unsigned long)addr, DWC_READ_REG32(addr));
+	}
+
+	addr = core_if->host_if->hprt0;
+	DWC_PRINTF("HPRT0	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+
+	for (i = 0; i < core_if->core_params->host_channels; i++) {
+		DWC_PRINTF("Host Channel %d Specific Registers\n", i);
+		addr = &core_if->host_if->hc_regs[i]->hcchar;
+		DWC_PRINTF("HCCHAR	 @0x%08lX : 0x%08X\n",
+			   (unsigned long)addr, DWC_READ_REG32(addr));
+		addr = &core_if->host_if->hc_regs[i]->hcsplt;
+		DWC_PRINTF("HCSPLT	 @0x%08lX : 0x%08X\n",
+			   (unsigned long)addr, DWC_READ_REG32(addr));
+		addr = &core_if->host_if->hc_regs[i]->hcint;
+		DWC_PRINTF("HCINT	 @0x%08lX : 0x%08X\n",
+			   (unsigned long)addr, DWC_READ_REG32(addr));
+		addr = &core_if->host_if->hc_regs[i]->hcintmsk;
+		DWC_PRINTF("HCINTMSK	 @0x%08lX : 0x%08X\n",
+			   (unsigned long)addr, DWC_READ_REG32(addr));
+		addr = &core_if->host_if->hc_regs[i]->hctsiz;
+		DWC_PRINTF("HCTSIZ	 @0x%08lX : 0x%08X\n",
+			   (unsigned long)addr, DWC_READ_REG32(addr));
+		addr = &core_if->host_if->hc_regs[i]->hcdma;
+		DWC_PRINTF("HCDMA	 @0x%08lX : 0x%08X\n",
+			   (unsigned long)addr, DWC_READ_REG32(addr));
+		if (core_if->dma_desc_enable) {
+			addr = &core_if->host_if->hc_regs[i]->hcdmab;
+			DWC_PRINTF("HCDMAB	 @0x%08lX : 0x%08X\n",
+				   (unsigned long)addr, DWC_READ_REG32(addr));
+		}
+
+	}
+	return;
+}
+
+/**
+ * This function reads the core global registers and prints them
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ */
+void dwc_otg_dump_global_registers(dwc_otg_core_if_t * core_if)
+{
+	int i, ep_num;
+	volatile uint32_t *addr;
+	char *txfsiz;
+
+	DWC_PRINTF("Core Global Registers\n");
+	addr = &core_if->core_global_regs->gotgctl;
+	DWC_PRINTF("GOTGCTL	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->core_global_regs->gotgint;
+	DWC_PRINTF("GOTGINT	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->core_global_regs->gahbcfg;
+	DWC_PRINTF("GAHBCFG	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->core_global_regs->gusbcfg;
+	DWC_PRINTF("GUSBCFG	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->core_global_regs->grstctl;
+	DWC_PRINTF("GRSTCTL	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->core_global_regs->gintsts;
+	DWC_PRINTF("GINTSTS	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->core_global_regs->gintmsk;
+	DWC_PRINTF("GINTMSK	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->core_global_regs->grxstsr;
+	DWC_PRINTF("GRXSTSR	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->core_global_regs->grxfsiz;
+	DWC_PRINTF("GRXFSIZ	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->core_global_regs->gnptxfsiz;
+	DWC_PRINTF("GNPTXFSIZ @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->core_global_regs->gnptxsts;
+	DWC_PRINTF("GNPTXSTS	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->core_global_regs->gi2cctl;
+	DWC_PRINTF("GI2CCTL	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->core_global_regs->gpvndctl;
+	DWC_PRINTF("GPVNDCTL	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->core_global_regs->ggpio;
+	DWC_PRINTF("GGPIO	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->core_global_regs->guid;
+	DWC_PRINTF("GUID		 @0x%08lX : 0x%08X\n",
+		   (unsigned long)addr, DWC_READ_REG32(addr));
+	addr = &core_if->core_global_regs->gsnpsid;
+	DWC_PRINTF("GSNPSID	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->core_global_regs->ghwcfg1;
+	DWC_PRINTF("GHWCFG1	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->core_global_regs->ghwcfg2;
+	DWC_PRINTF("GHWCFG2	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->core_global_regs->ghwcfg3;
+	DWC_PRINTF("GHWCFG3	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->core_global_regs->ghwcfg4;
+	DWC_PRINTF("GHWCFG4	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->core_global_regs->glpmcfg;
+	DWC_PRINTF("GLPMCFG	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->core_global_regs->gpwrdn;
+	DWC_PRINTF("GPWRDN	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->core_global_regs->gdfifocfg;
+	DWC_PRINTF("GDFIFOCFG	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+	addr = &core_if->core_global_regs->adpctl;
+	DWC_PRINTF("ADPCTL	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   dwc_otg_adp_read_reg(core_if));
+	addr = &core_if->core_global_regs->hptxfsiz;
+	DWC_PRINTF("HPTXFSIZ	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+
+	if (core_if->en_multiple_tx_fifo == 0) {
+		ep_num = core_if->hwcfg4.b.num_dev_perio_in_ep;
+		txfsiz = "DPTXFSIZ";
+	} else {
+		ep_num = core_if->hwcfg4.b.num_in_eps;
+		txfsiz = "DIENPTXF";
+	}
+	for (i = 0; i < ep_num; i++) {
+		addr = &core_if->core_global_regs->dtxfsiz[i];
+		DWC_PRINTF("%s[%d] @0x%08lX : 0x%08X\n", txfsiz, i + 1,
+			   (unsigned long)addr, DWC_READ_REG32(addr));
+	}
+	addr = core_if->pcgcctl;
+	DWC_PRINTF("PCGCCTL	 @0x%08lX : 0x%08X\n", (unsigned long)addr,
+		   DWC_READ_REG32(addr));
+}
+
+/**
+ * Flush a Tx FIFO.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param num Tx FIFO to flush.
+ */
+void dwc_otg_flush_tx_fifo(dwc_otg_core_if_t * core_if, const int num)
+{
+	dwc_otg_core_global_regs_t *global_regs = core_if->core_global_regs;
+	volatile grstctl_t greset = {.d32 = 0 };
+	int count = 0;
+
+	DWC_DEBUGPL((DBG_CIL | DBG_PCDV), "Flush Tx FIFO %d\n", num);
+
+	greset.b.txfflsh = 1;
+	greset.b.txfnum = num;
+	DWC_WRITE_REG32(&global_regs->grstctl, greset.d32);
+
+	do {
+		greset.d32 = DWC_READ_REG32(&global_regs->grstctl);
+		if (++count > 10000) {
+			DWC_WARN("%s() HANG! GRSTCTL=%0x GNPTXSTS=0x%08x\n",
+				 __func__, greset.d32,
+				 DWC_READ_REG32(&global_regs->gnptxsts));
+			break;
+		}
+		dwc_udelay(1);
+	} while (greset.b.txfflsh == 1);
+
+	/* Wait for 3 PHY Clocks */
+	dwc_udelay(1);
+}
+
+/**
+ * Flush Rx FIFO.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ */
+void dwc_otg_flush_rx_fifo(dwc_otg_core_if_t * core_if)
+{
+	dwc_otg_core_global_regs_t *global_regs = core_if->core_global_regs;
+	volatile grstctl_t greset = {.d32 = 0 };
+	int count = 0;
+
+	DWC_DEBUGPL((DBG_CIL | DBG_PCDV), "%s\n", __func__);
+	/*
+	 *
+	 */
+	greset.b.rxfflsh = 1;
+	DWC_WRITE_REG32(&global_regs->grstctl, greset.d32);
+
+	do {
+		greset.d32 = DWC_READ_REG32(&global_regs->grstctl);
+		if (++count > 10000) {
+			DWC_WARN("%s() HANG! GRSTCTL=%0x\n", __func__,
+				 greset.d32);
+			break;
+		}
+		dwc_udelay(1);
+	} while (greset.b.rxfflsh == 1);
+
+	/* Wait for 3 PHY Clocks */
+	dwc_udelay(1);
+}
+
+/**
+ * Do core a soft reset of the core.  Be careful with this because it
+ * resets all the internal state machines of the core.
+ */
+void dwc_otg_core_reset(dwc_otg_core_if_t * core_if)
+{
+	dwc_otg_core_global_regs_t *global_regs = core_if->core_global_regs;
+	volatile grstctl_t greset = {.d32 = 0 };
+	int count = 0;
+
+	DWC_DEBUGPL(DBG_CILV, "%s\n", __func__);
+	/* Wait for AHB master IDLE state. */
+	do {
+		dwc_udelay(10);
+		greset.d32 = DWC_READ_REG32(&global_regs->grstctl);
+		if (++count > 100000) {
+			DWC_WARN("%s() HANG! AHB Idle GRSTCTL=%0x\n", __func__,
+				 greset.d32);
+			return;
+		}
+	}
+	while (greset.b.ahbidle == 0);
+
+	/* Core Soft Reset */
+	count = 0;
+	greset.b.csftrst = 1;
+	DWC_WRITE_REG32(&global_regs->grstctl, greset.d32);
+	do {
+		greset.d32 = DWC_READ_REG32(&global_regs->grstctl);
+		if (++count > 10000) {
+			DWC_WARN("%s() HANG! Soft Reset GRSTCTL=%0x\n",
+				 __func__, greset.d32);
+			break;
+		}
+		dwc_udelay(1);
+	}
+	while (greset.b.csftrst == 1);
+
+	/* Wait for 3 PHY Clocks */
+	dwc_mdelay(100);
+}
+
+uint8_t dwc_otg_is_device_mode(dwc_otg_core_if_t * _core_if)
+{
+	return (dwc_otg_mode(_core_if) != DWC_HOST_MODE);
+}
+
+uint8_t dwc_otg_is_host_mode(dwc_otg_core_if_t * _core_if)
+{
+	return (dwc_otg_mode(_core_if) == DWC_HOST_MODE);
+}
+
+/**
+ * Register HCD callbacks. The callbacks are used to start and stop
+ * the HCD for interrupt processing.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param cb the HCD callback structure.
+ * @param p pointer to be passed to callback function (usb_hcd*).
+ */
+void dwc_otg_cil_register_hcd_callbacks(dwc_otg_core_if_t * core_if,
+					dwc_otg_cil_callbacks_t * cb, void *p)
+{
+	core_if->hcd_cb = cb;
+	cb->p = p;
+}
+
+/**
+ * Register PCD callbacks. The callbacks are used to start and stop
+ * the PCD for interrupt processing.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param cb the PCD callback structure.
+ * @param p pointer to be passed to callback function (pcd*).
+ */
+void dwc_otg_cil_register_pcd_callbacks(dwc_otg_core_if_t * core_if,
+					dwc_otg_cil_callbacks_t * cb, void *p)
+{
+	core_if->pcd_cb = cb;
+	cb->p = p;
+}
+
+#ifdef DWC_EN_ISOC
+
+/**
+ * This function writes isoc data per 1 (micro)frame into tx fifo
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param ep The EP to start the transfer on.
+ *
+ */
+void write_isoc_frame_data(dwc_otg_core_if_t * core_if, dwc_ep_t * ep)
+{
+	dwc_otg_dev_in_ep_regs_t *ep_regs;
+	dtxfsts_data_t txstatus = {.d32 = 0 };
+	uint32_t len = 0;
+	uint32_t dwords;
+
+	ep->xfer_len = ep->data_per_frame;
+	ep->xfer_count = 0;
+
+	ep_regs = core_if->dev_if->in_ep_regs[ep->num];
+
+	len = ep->xfer_len - ep->xfer_count;
+
+	if (len > ep->maxpacket) {
+		len = ep->maxpacket;
+	}
+
+	dwords = (len + 3) / 4;
+
+	/* While there is space in the queue and space in the FIFO and
+	 * More data to tranfer, Write packets to the Tx FIFO */
+	txstatus.d32 =
+	    DWC_READ_REG32(&core_if->dev_if->in_ep_regs[ep->num]->dtxfsts);
+	DWC_DEBUGPL(DBG_PCDV, "b4 dtxfsts[%d]=0x%08x\n", ep->num, txstatus.d32);
+
+	while (txstatus.b.txfspcavail > dwords &&
+	       ep->xfer_count < ep->xfer_len && ep->xfer_len != 0) {
+		/* Write the FIFO */
+		dwc_otg_ep_write_packet(core_if, ep, 0);
+
+		len = ep->xfer_len - ep->xfer_count;
+		if (len > ep->maxpacket) {
+			len = ep->maxpacket;
+		}
+
+		dwords = (len + 3) / 4;
+		txstatus.d32 =
+		    DWC_READ_REG32(&core_if->dev_if->
+				   in_ep_regs[ep->num]->dtxfsts);
+		DWC_DEBUGPL(DBG_PCDV, "dtxfsts[%d]=0x%08x\n", ep->num,
+			    txstatus.d32);
+	}
+}
+
+/**
+ * This function initializes a descriptor chain for Isochronous transfer
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param ep The EP to start the transfer on.
+ *
+ */
+void dwc_otg_iso_ep_start_frm_transfer(dwc_otg_core_if_t * core_if,
+				       dwc_ep_t * ep)
+{
+	deptsiz_data_t deptsiz = {.d32 = 0 };
+	depctl_data_t depctl = {.d32 = 0 };
+	dsts_data_t dsts = {.d32 = 0 };
+	volatile uint32_t *addr;
+
+	if (ep->is_in) {
+		addr = &core_if->dev_if->in_ep_regs[ep->num]->diepctl;
+	} else {
+		addr = &core_if->dev_if->out_ep_regs[ep->num]->doepctl;
+	}
+
+	ep->xfer_len = ep->data_per_frame;
+	ep->xfer_count = 0;
+	ep->xfer_buff = ep->cur_pkt_addr;
+	ep->dma_addr = ep->cur_pkt_dma_addr;
+
+	if (ep->is_in) {
+		/* Program the transfer size and packet count
+		 *      as follows: xfersize = N * maxpacket +
+		 *      short_packet pktcnt = N + (short_packet
+		 *      exist ? 1 : 0)
+		 */
+		deptsiz.b.xfersize = ep->xfer_len;
+		deptsiz.b.pktcnt =
+		    (ep->xfer_len - 1 + ep->maxpacket) / ep->maxpacket;
+		deptsiz.b.mc = deptsiz.b.pktcnt;
+		DWC_WRITE_REG32(&core_if->dev_if->in_ep_regs[ep->num]->dieptsiz,
+				deptsiz.d32);
+
+		/* Write the DMA register */
+		if (core_if->dma_enable) {
+			DWC_WRITE_REG32(&
+					(core_if->dev_if->in_ep_regs[ep->num]->
+					 diepdma), (uint32_t) ep->dma_addr);
+		}
+	} else {
+		deptsiz.b.pktcnt =
+		    (ep->xfer_len + (ep->maxpacket - 1)) / ep->maxpacket;
+		deptsiz.b.xfersize = deptsiz.b.pktcnt * ep->maxpacket;
+
+		DWC_WRITE_REG32(&core_if->dev_if->out_ep_regs[ep->num]->
+				doeptsiz, deptsiz.d32);
+
+		if (core_if->dma_enable) {
+			DWC_WRITE_REG32(&
+					(core_if->dev_if->out_ep_regs[ep->num]->
+					 doepdma), (uint32_t) ep->dma_addr);
+		}
+	}
+
+	/** Enable endpoint, clear nak  */
+
+	depctl.d32 = 0;
+	if (ep->bInterval == 1) {
+		dsts.d32 =
+		    DWC_READ_REG32(&core_if->dev_if->dev_global_regs->dsts);
+		ep->next_frame = dsts.b.soffn + ep->bInterval;
+
+		if (ep->next_frame & 0x1) {
+			depctl.b.setd1pid = 1;
+		} else {
+			depctl.b.setd0pid = 1;
+		}
+	} else {
+		ep->next_frame += ep->bInterval;
+
+		if (ep->next_frame & 0x1) {
+			depctl.b.setd1pid = 1;
+		} else {
+			depctl.b.setd0pid = 1;
+		}
+	}
+	depctl.b.epena = 1;
+	depctl.b.cnak = 1;
+
+	DWC_MODIFY_REG32(addr, 0, depctl.d32);
+	depctl.d32 = DWC_READ_REG32(addr);
+
+	if (ep->is_in && core_if->dma_enable == 0) {
+		write_isoc_frame_data(core_if, ep);
+	}
+
+}
+#endif /* DWC_EN_ISOC */
+
+static void dwc_otg_set_uninitialized(int32_t * p, int size)
+{
+	int i;
+	for (i = 0; i < size; i++) {
+		p[i] = -1;
+	}
+}
+
+static int dwc_otg_param_initialized(int32_t val)
+{
+	return val != -1;
+}
+
+static int dwc_otg_setup_params(dwc_otg_core_if_t * core_if)
+{
+	int i;
+	core_if->core_params = DWC_ALLOC(sizeof(*core_if->core_params));
+	if (!core_if->core_params) {
+		return -DWC_E_NO_MEMORY;
+	}
+	dwc_otg_set_uninitialized((int32_t *) core_if->core_params,
+				  sizeof(*core_if->core_params) /
+				  sizeof(int32_t));
+	DWC_PRINTF("Setting default values for core params\n");
+	dwc_otg_set_param_otg_cap(core_if, dwc_param_otg_cap_default);
+	dwc_otg_set_param_dma_enable(core_if, dwc_param_dma_enable_default);
+	dwc_otg_set_param_dma_desc_enable(core_if,
+					  dwc_param_dma_desc_enable_default);
+	dwc_otg_set_param_opt(core_if, dwc_param_opt_default);
+	dwc_otg_set_param_dma_burst_size(core_if,
+					 dwc_param_dma_burst_size_default);
+	dwc_otg_set_param_host_support_fs_ls_low_power(core_if,
+						       dwc_param_host_support_fs_ls_low_power_default);
+	dwc_otg_set_param_enable_dynamic_fifo(core_if,
+					      dwc_param_enable_dynamic_fifo_default);
+	dwc_otg_set_param_data_fifo_size(core_if,
+					 dwc_param_data_fifo_size_default);
+	dwc_otg_set_param_dev_rx_fifo_size(core_if,
+					   dwc_param_dev_rx_fifo_size_default);
+	dwc_otg_set_param_dev_nperio_tx_fifo_size(core_if,
+						  dwc_param_dev_nperio_tx_fifo_size_default);
+	dwc_otg_set_param_host_rx_fifo_size(core_if,
+					    dwc_param_host_rx_fifo_size_default);
+	dwc_otg_set_param_host_nperio_tx_fifo_size(core_if,
+						   dwc_param_host_nperio_tx_fifo_size_default);
+	dwc_otg_set_param_host_perio_tx_fifo_size(core_if,
+						  dwc_param_host_perio_tx_fifo_size_default);
+	dwc_otg_set_param_max_transfer_size(core_if,
+					    dwc_param_max_transfer_size_default);
+	dwc_otg_set_param_max_packet_count(core_if,
+					   dwc_param_max_packet_count_default);
+	dwc_otg_set_param_host_channels(core_if,
+					dwc_param_host_channels_default);
+	dwc_otg_set_param_dev_endpoints(core_if,
+					dwc_param_dev_endpoints_default);
+	dwc_otg_set_param_phy_type(core_if, dwc_param_phy_type_default);
+	dwc_otg_set_param_speed(core_if, dwc_param_speed_default);
+	dwc_otg_set_param_host_ls_low_power_phy_clk(core_if,
+						    dwc_param_host_ls_low_power_phy_clk_default);
+	dwc_otg_set_param_phy_ulpi_ddr(core_if, dwc_param_phy_ulpi_ddr_default);
+	dwc_otg_set_param_phy_ulpi_ext_vbus(core_if,
+					    dwc_param_phy_ulpi_ext_vbus_default);
+	dwc_otg_set_param_phy_utmi_width(core_if,
+					 dwc_param_phy_utmi_width_default);
+	dwc_otg_set_param_ts_dline(core_if, dwc_param_ts_dline_default);
+	dwc_otg_set_param_i2c_enable(core_if, dwc_param_i2c_enable_default);
+	dwc_otg_set_param_ulpi_fs_ls(core_if, dwc_param_ulpi_fs_ls_default);
+	dwc_otg_set_param_en_multiple_tx_fifo(core_if,
+					      dwc_param_en_multiple_tx_fifo_default);
+	for (i = 0; i < 15; i++) {
+		dwc_otg_set_param_dev_perio_tx_fifo_size(core_if,
+							 dwc_param_dev_perio_tx_fifo_size_default,
+							 i);
+	}
+
+	for (i = 0; i < 15; i++) {
+		dwc_otg_set_param_dev_tx_fifo_size(core_if,
+						   dwc_param_dev_tx_fifo_size_default,
+						   i);
+	}
+	dwc_otg_set_param_thr_ctl(core_if, dwc_param_thr_ctl_default);
+	dwc_otg_set_param_mpi_enable(core_if, dwc_param_mpi_enable_default);
+	dwc_otg_set_param_pti_enable(core_if, dwc_param_pti_enable_default);
+	dwc_otg_set_param_lpm_enable(core_if, dwc_param_lpm_enable_default);
+	dwc_otg_set_param_ic_usb_cap(core_if, dwc_param_ic_usb_cap_default);
+	dwc_otg_set_param_tx_thr_length(core_if,
+					dwc_param_tx_thr_length_default);
+	dwc_otg_set_param_rx_thr_length(core_if,
+					dwc_param_rx_thr_length_default);
+	dwc_otg_set_param_ahb_thr_ratio(core_if,
+					dwc_param_ahb_thr_ratio_default);
+	dwc_otg_set_param_power_down(core_if, dwc_param_power_down_default);
+	dwc_otg_set_param_reload_ctl(core_if, dwc_param_reload_ctl_default);
+	dwc_otg_set_param_dev_out_nak(core_if, dwc_param_dev_out_nak_default);
+	dwc_otg_set_param_cont_on_bna(core_if, dwc_param_cont_on_bna_default);
+	dwc_otg_set_param_ahb_single(core_if, dwc_param_ahb_single_default);
+	dwc_otg_set_param_otg_ver(core_if, dwc_param_otg_ver_default);
+	dwc_otg_set_param_adp_enable(core_if, dwc_param_adp_enable_default);
+	return 0;
+}
+
+uint8_t dwc_otg_is_dma_enable(dwc_otg_core_if_t * core_if)
+{
+	return core_if->dma_enable;
+}
+
+/* Checks if the parameter is outside of its valid range of values */
+#define DWC_OTG_PARAM_TEST(_param_, _low_, _high_) \
+		(((_param_) < (_low_)) || \
+		((_param_) > (_high_)))
+
+/* Parameter access functions */
+int dwc_otg_set_param_otg_cap(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	int valid;
+	int retval = 0;
+	if (DWC_OTG_PARAM_TEST(val, 0, 2)) {
+		DWC_WARN("Wrong value for otg_cap parameter\n");
+		DWC_WARN("otg_cap parameter must be 0,1 or 2\n");
+		retval = -DWC_E_INVALID;
+		goto out;
+	}
+
+	valid = 1;
+	switch (val) {
+	case DWC_OTG_CAP_PARAM_HNP_SRP_CAPABLE:
+		if (core_if->hwcfg2.b.op_mode !=
+		    DWC_HWCFG2_OP_MODE_HNP_SRP_CAPABLE_OTG)
+			valid = 0;
+		break;
+	case DWC_OTG_CAP_PARAM_SRP_ONLY_CAPABLE:
+		if ((core_if->hwcfg2.b.op_mode !=
+		     DWC_HWCFG2_OP_MODE_HNP_SRP_CAPABLE_OTG)
+		    && (core_if->hwcfg2.b.op_mode !=
+			DWC_HWCFG2_OP_MODE_SRP_ONLY_CAPABLE_OTG)
+		    && (core_if->hwcfg2.b.op_mode !=
+			DWC_HWCFG2_OP_MODE_SRP_CAPABLE_DEVICE)
+		    && (core_if->hwcfg2.b.op_mode !=
+			DWC_HWCFG2_OP_MODE_SRP_CAPABLE_HOST)) {
+			valid = 0;
+		}
+		break;
+	case DWC_OTG_CAP_PARAM_NO_HNP_SRP_CAPABLE:
+		/* always valid */
+		break;
+	}
+	if (!valid) {
+		if (dwc_otg_param_initialized(core_if->core_params->otg_cap)) {
+			DWC_ERROR
+			    ("%d invalid for otg_cap paremter. Check HW configuration.\n",
+			     val);
+		}
+		val =
+		    (((core_if->hwcfg2.b.op_mode ==
+		       DWC_HWCFG2_OP_MODE_HNP_SRP_CAPABLE_OTG)
+		      || (core_if->hwcfg2.b.op_mode ==
+			  DWC_HWCFG2_OP_MODE_SRP_ONLY_CAPABLE_OTG)
+		      || (core_if->hwcfg2.b.op_mode ==
+			  DWC_HWCFG2_OP_MODE_SRP_CAPABLE_DEVICE)
+		      || (core_if->hwcfg2.b.op_mode ==
+			  DWC_HWCFG2_OP_MODE_SRP_CAPABLE_HOST)) ?
+		     DWC_OTG_CAP_PARAM_SRP_ONLY_CAPABLE :
+		     DWC_OTG_CAP_PARAM_NO_HNP_SRP_CAPABLE);
+		retval = -DWC_E_INVALID;
+	}
+
+	core_if->core_params->otg_cap = val;
+out:
+	return retval;
+}
+
+int32_t dwc_otg_get_param_otg_cap(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->otg_cap;
+}
+
+int dwc_otg_set_param_opt(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	if (DWC_OTG_PARAM_TEST(val, 0, 1)) {
+		DWC_WARN("Wrong value for opt parameter\n");
+		return -DWC_E_INVALID;
+	}
+	core_if->core_params->opt = val;
+	return 0;
+}
+
+int32_t dwc_otg_get_param_opt(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->opt;
+}
+
+int dwc_otg_set_param_dma_enable(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	int retval = 0;
+	if (DWC_OTG_PARAM_TEST(val, 0, 1)) {
+		DWC_WARN("Wrong value for dma enable\n");
+		return -DWC_E_INVALID;
+	}
+
+	if ((val == 1) && (core_if->hwcfg2.b.architecture == 0)) {
+		if (dwc_otg_param_initialized(core_if->core_params->dma_enable)) {
+			DWC_ERROR
+			    ("%d invalid for dma_enable paremter. Check HW configuration.\n",
+			     val);
+		}
+		val = 0;
+		retval = -DWC_E_INVALID;
+	}
+
+	core_if->core_params->dma_enable = val;
+	if (val == 0) {
+		dwc_otg_set_param_dma_desc_enable(core_if, 0);
+	}
+	return retval;
+}
+
+int32_t dwc_otg_get_param_dma_enable(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->dma_enable;
+}
+
+int dwc_otg_set_param_dma_desc_enable(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	int retval = 0;
+	if (DWC_OTG_PARAM_TEST(val, 0, 1)) {
+		DWC_WARN("Wrong value for dma_enable\n");
+		DWC_WARN("dma_desc_enable must be 0 or 1\n");
+		return -DWC_E_INVALID;
+	}
+
+	if ((val == 1)
+	    && ((dwc_otg_get_param_dma_enable(core_if) == 0)
+		|| (core_if->hwcfg4.b.desc_dma == 0))) {
+		if (dwc_otg_param_initialized
+		    (core_if->core_params->dma_desc_enable)) {
+			DWC_ERROR
+			    ("%d invalid for dma_desc_enable paremter. Check HW configuration.\n",
+			     val);
+		}
+		val = 0;
+		retval = -DWC_E_INVALID;
+	}
+	core_if->core_params->dma_desc_enable = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_dma_desc_enable(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->dma_desc_enable;
+}
+
+int dwc_otg_set_param_host_support_fs_ls_low_power(dwc_otg_core_if_t * core_if,
+						   int32_t val)
+{
+	if (DWC_OTG_PARAM_TEST(val, 0, 1)) {
+		DWC_WARN("Wrong value for host_support_fs_low_power\n");
+		DWC_WARN("host_support_fs_low_power must be 0 or 1\n");
+		return -DWC_E_INVALID;
+	}
+	core_if->core_params->host_support_fs_ls_low_power = val;
+	return 0;
+}
+
+int32_t dwc_otg_get_param_host_support_fs_ls_low_power(dwc_otg_core_if_t *
+						       core_if)
+{
+	return core_if->core_params->host_support_fs_ls_low_power;
+}
+
+int dwc_otg_set_param_enable_dynamic_fifo(dwc_otg_core_if_t * core_if,
+					  int32_t val)
+{
+	int retval = 0;
+	if (DWC_OTG_PARAM_TEST(val, 0, 1)) {
+		DWC_WARN("Wrong value for enable_dynamic_fifo\n");
+		DWC_WARN("enable_dynamic_fifo must be 0 or 1\n");
+		return -DWC_E_INVALID;
+	}
+
+	if ((val == 1) && (core_if->hwcfg2.b.dynamic_fifo == 0)) {
+		if (dwc_otg_param_initialized
+		    (core_if->core_params->enable_dynamic_fifo)) {
+			DWC_ERROR
+			    ("%d invalid for enable_dynamic_fifo paremter. Check HW configuration.\n",
+			     val);
+		}
+		val = 0;
+		retval = -DWC_E_INVALID;
+	}
+	core_if->core_params->enable_dynamic_fifo = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_enable_dynamic_fifo(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->enable_dynamic_fifo;
+}
+
+int dwc_otg_set_param_data_fifo_size(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	int retval = 0;
+	if (DWC_OTG_PARAM_TEST(val, 32, 32768)) {
+		DWC_WARN("Wrong value for data_fifo_size\n");
+		DWC_WARN("data_fifo_size must be 32-32768\n");
+		return -DWC_E_INVALID;
+	}
+
+	if (val > core_if->hwcfg3.b.dfifo_depth) {
+		if (dwc_otg_param_initialized
+		    (core_if->core_params->data_fifo_size)) {
+			DWC_ERROR
+			    ("%d invalid for data_fifo_size parameter. Check HW configuration.\n",
+			     val);
+		}
+		val = core_if->hwcfg3.b.dfifo_depth;
+		retval = -DWC_E_INVALID;
+	}
+
+	core_if->core_params->data_fifo_size = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_data_fifo_size(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->data_fifo_size;
+}
+
+int dwc_otg_set_param_dev_rx_fifo_size(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	int retval = 0;
+	if (DWC_OTG_PARAM_TEST(val, 16, 32768)) {
+		DWC_WARN("Wrong value for dev_rx_fifo_size\n");
+		DWC_WARN("dev_rx_fifo_size must be 16-32768\n");
+		return -DWC_E_INVALID;
+	}
+
+	if (val > DWC_READ_REG32(&core_if->core_global_regs->grxfsiz)) {
+		if (dwc_otg_param_initialized(core_if->core_params->dev_rx_fifo_size)) {
+		DWC_WARN("%d invalid for dev_rx_fifo_size parameter\n", val);
+		}
+		val = DWC_READ_REG32(&core_if->core_global_regs->grxfsiz);
+		retval = -DWC_E_INVALID;
+	}
+
+	core_if->core_params->dev_rx_fifo_size = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_dev_rx_fifo_size(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->dev_rx_fifo_size;
+}
+
+int dwc_otg_set_param_dev_nperio_tx_fifo_size(dwc_otg_core_if_t * core_if,
+					      int32_t val)
+{
+	int retval = 0;
+
+	if (DWC_OTG_PARAM_TEST(val, 16, 32768)) {
+		DWC_WARN("Wrong value for dev_nperio_tx_fifo\n");
+		DWC_WARN("dev_nperio_tx_fifo must be 16-32768\n");
+		return -DWC_E_INVALID;
+	}
+
+	if (val > (DWC_READ_REG32(&core_if->core_global_regs->gnptxfsiz) >> 16)) {
+		if (dwc_otg_param_initialized
+		    (core_if->core_params->dev_nperio_tx_fifo_size)) {
+			DWC_ERROR
+			    ("%d invalid for dev_nperio_tx_fifo_size. Check HW configuration.\n",
+			     val);
+		}
+		val =
+		    (DWC_READ_REG32(&core_if->core_global_regs->gnptxfsiz) >>
+		     16);
+		retval = -DWC_E_INVALID;
+	}
+
+	core_if->core_params->dev_nperio_tx_fifo_size = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_dev_nperio_tx_fifo_size(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->dev_nperio_tx_fifo_size;
+}
+
+int dwc_otg_set_param_host_rx_fifo_size(dwc_otg_core_if_t * core_if,
+					int32_t val)
+{
+	int retval = 0;
+
+	if (DWC_OTG_PARAM_TEST(val, 16, 32768)) {
+		DWC_WARN("Wrong value for host_rx_fifo_size\n");
+		DWC_WARN("host_rx_fifo_size must be 16-32768\n");
+		return -DWC_E_INVALID;
+	}
+
+	if (val > DWC_READ_REG32(&core_if->core_global_regs->grxfsiz)) {
+		if (dwc_otg_param_initialized
+		    (core_if->core_params->host_rx_fifo_size)) {
+			DWC_ERROR
+			    ("%d invalid for host_rx_fifo_size. Check HW configuration.\n",
+			     val);
+		}
+		val = DWC_READ_REG32(&core_if->core_global_regs->grxfsiz);
+		retval = -DWC_E_INVALID;
+	}
+
+	core_if->core_params->host_rx_fifo_size = val;
+	return retval;
+
+}
+
+int32_t dwc_otg_get_param_host_rx_fifo_size(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->host_rx_fifo_size;
+}
+
+int dwc_otg_set_param_host_nperio_tx_fifo_size(dwc_otg_core_if_t * core_if,
+					       int32_t val)
+{
+	int retval = 0;
+
+	if (DWC_OTG_PARAM_TEST(val, 16, 32768)) {
+		DWC_WARN("Wrong value for host_nperio_tx_fifo_size\n");
+		DWC_WARN("host_nperio_tx_fifo_size must be 16-32768\n");
+		return -DWC_E_INVALID;
+	}
+
+	if (val > (DWC_READ_REG32(&core_if->core_global_regs->gnptxfsiz) >> 16)) {
+		if (dwc_otg_param_initialized
+		    (core_if->core_params->host_nperio_tx_fifo_size)) {
+			DWC_ERROR
+			    ("%d invalid for host_nperio_tx_fifo_size. Check HW configuration.\n",
+			     val);
+		}
+		val =
+		    (DWC_READ_REG32(&core_if->core_global_regs->gnptxfsiz) >>
+		     16);
+		retval = -DWC_E_INVALID;
+	}
+
+	core_if->core_params->host_nperio_tx_fifo_size = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_host_nperio_tx_fifo_size(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->host_nperio_tx_fifo_size;
+}
+
+int dwc_otg_set_param_host_perio_tx_fifo_size(dwc_otg_core_if_t * core_if,
+					      int32_t val)
+{
+	int retval = 0;
+	if (DWC_OTG_PARAM_TEST(val, 16, 32768)) {
+		DWC_WARN("Wrong value for host_perio_tx_fifo_size\n");
+		DWC_WARN("host_perio_tx_fifo_size must be 16-32768\n");
+		return -DWC_E_INVALID;
+	}
+
+	if (val >
+		((core_if->hptxfsiz.d32)>> 16)) {
+		if (dwc_otg_param_initialized
+		    (core_if->core_params->host_perio_tx_fifo_size)) {
+			DWC_ERROR
+			    ("%d invalid for host_perio_tx_fifo_size. Check HW configuration.\n",
+			     val);
+		}
+		val = (core_if->hptxfsiz.d32) >> 16;
+		retval = -DWC_E_INVALID;
+	}
+
+	core_if->core_params->host_perio_tx_fifo_size = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_host_perio_tx_fifo_size(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->host_perio_tx_fifo_size;
+}
+
+int dwc_otg_set_param_max_transfer_size(dwc_otg_core_if_t * core_if,
+					int32_t val)
+{
+	int retval = 0;
+
+	if (DWC_OTG_PARAM_TEST(val, 2047, 524288)) {
+		DWC_WARN("Wrong value for max_transfer_size\n");
+		DWC_WARN("max_transfer_size must be 2047-524288\n");
+		return -DWC_E_INVALID;
+	}
+
+	if (val >= (1 << (core_if->hwcfg3.b.xfer_size_cntr_width + 11))) {
+		if (dwc_otg_param_initialized
+		    (core_if->core_params->max_transfer_size)) {
+			DWC_ERROR
+			    ("%d invalid for max_transfer_size. Check HW configuration.\n",
+			     val);
+		}
+		val =
+		    ((1 << (core_if->hwcfg3.b.packet_size_cntr_width + 11)) -
+		     1);
+		retval = -DWC_E_INVALID;
+	}
+
+	core_if->core_params->max_transfer_size = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_max_transfer_size(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->max_transfer_size;
+}
+
+int dwc_otg_set_param_max_packet_count(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	int retval = 0;
+
+	if (DWC_OTG_PARAM_TEST(val, 15, 511)) {
+		DWC_WARN("Wrong value for max_packet_count\n");
+		DWC_WARN("max_packet_count must be 15-511\n");
+		return -DWC_E_INVALID;
+	}
+
+	if (val > (1 << (core_if->hwcfg3.b.packet_size_cntr_width + 4))) {
+		if (dwc_otg_param_initialized
+		    (core_if->core_params->max_packet_count)) {
+			DWC_ERROR
+			    ("%d invalid for max_packet_count. Check HW configuration.\n",
+			     val);
+		}
+		val =
+		    ((1 << (core_if->hwcfg3.b.packet_size_cntr_width + 4)) - 1);
+		retval = -DWC_E_INVALID;
+	}
+
+	core_if->core_params->max_packet_count = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_max_packet_count(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->max_packet_count;
+}
+
+int dwc_otg_set_param_host_channels(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	int retval = 0;
+
+	if (DWC_OTG_PARAM_TEST(val, 1, 16)) {
+		DWC_WARN("Wrong value for host_channels\n");
+		DWC_WARN("host_channels must be 1-16\n");
+		return -DWC_E_INVALID;
+	}
+
+	if (val > (core_if->hwcfg2.b.num_host_chan + 1)) {
+		if (dwc_otg_param_initialized
+		    (core_if->core_params->host_channels)) {
+			DWC_ERROR
+			    ("%d invalid for host_channels. Check HW configurations.\n",
+			     val);
+		}
+		val = (core_if->hwcfg2.b.num_host_chan + 1);
+		retval = -DWC_E_INVALID;
+	}
+
+	core_if->core_params->host_channels = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_host_channels(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->host_channels;
+}
+
+int dwc_otg_set_param_dev_endpoints(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	int retval = 0;
+
+	if (DWC_OTG_PARAM_TEST(val, 1, 15)) {
+		DWC_WARN("Wrong value for dev_endpoints\n");
+		DWC_WARN("dev_endpoints must be 1-15\n");
+		return -DWC_E_INVALID;
+	}
+
+	if (val > (core_if->hwcfg2.b.num_dev_ep)) {
+		if (dwc_otg_param_initialized
+		    (core_if->core_params->dev_endpoints)) {
+			DWC_ERROR
+			    ("%d invalid for dev_endpoints. Check HW configurations.\n",
+			     val);
+		}
+		val = core_if->hwcfg2.b.num_dev_ep;
+		retval = -DWC_E_INVALID;
+	}
+
+	core_if->core_params->dev_endpoints = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_dev_endpoints(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->dev_endpoints;
+}
+
+int dwc_otg_set_param_phy_type(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	int retval = 0;
+	int valid = 0;
+
+	if (DWC_OTG_PARAM_TEST(val, 0, 2)) {
+		DWC_WARN("Wrong value for phy_type\n");
+		DWC_WARN("phy_type must be 0,1 or 2\n");
+		return -DWC_E_INVALID;
+	}
+#ifndef NO_FS_PHY_HW_CHECKS
+	if ((val == DWC_PHY_TYPE_PARAM_UTMI) &&
+	    ((core_if->hwcfg2.b.hs_phy_type == 1) ||
+	     (core_if->hwcfg2.b.hs_phy_type == 3))) {
+		valid = 1;
+	} else if ((val == DWC_PHY_TYPE_PARAM_ULPI) &&
+		   ((core_if->hwcfg2.b.hs_phy_type == 2) ||
+		    (core_if->hwcfg2.b.hs_phy_type == 3))) {
+		valid = 1;
+	} else if ((val == DWC_PHY_TYPE_PARAM_FS) &&
+		   (core_if->hwcfg2.b.fs_phy_type == 1)) {
+		valid = 1;
+	}
+	if (!valid) {
+		if (dwc_otg_param_initialized(core_if->core_params->phy_type)) {
+			DWC_ERROR
+			    ("%d invalid for phy_type. Check HW configurations.\n",
+			     val);
+		}
+		if (core_if->hwcfg2.b.hs_phy_type) {
+			if ((core_if->hwcfg2.b.hs_phy_type == 3) ||
+			    (core_if->hwcfg2.b.hs_phy_type == 1)) {
+				val = DWC_PHY_TYPE_PARAM_UTMI;
+			} else {
+				val = DWC_PHY_TYPE_PARAM_ULPI;
+			}
+		}
+		retval = -DWC_E_INVALID;
+	}
+#endif
+	core_if->core_params->phy_type = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_phy_type(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->phy_type;
+}
+
+int dwc_otg_set_param_speed(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	int retval = 0;
+	if (DWC_OTG_PARAM_TEST(val, 0, 1)) {
+		DWC_WARN("Wrong value for speed parameter\n");
+		DWC_WARN("max_speed parameter must be 0 or 1\n");
+		return -DWC_E_INVALID;
+	}
+	if ((val == 0)
+	    && dwc_otg_get_param_phy_type(core_if) == DWC_PHY_TYPE_PARAM_FS) {
+		if (dwc_otg_param_initialized(core_if->core_params->speed)) {
+			DWC_ERROR
+			    ("%d invalid for speed paremter. Check HW configuration.\n",
+			     val);
+		}
+		val =
+		    (dwc_otg_get_param_phy_type(core_if) ==
+		     DWC_PHY_TYPE_PARAM_FS ? 1 : 0);
+		retval = -DWC_E_INVALID;
+	}
+	core_if->core_params->speed = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_speed(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->speed;
+}
+
+int dwc_otg_set_param_host_ls_low_power_phy_clk(dwc_otg_core_if_t * core_if,
+						int32_t val)
+{
+	int retval = 0;
+
+	if (DWC_OTG_PARAM_TEST(val, 0, 1)) {
+		DWC_WARN
+		    ("Wrong value for host_ls_low_power_phy_clk parameter\n");
+		DWC_WARN("host_ls_low_power_phy_clk must be 0 or 1\n");
+		return -DWC_E_INVALID;
+	}
+
+	if ((val == DWC_HOST_LS_LOW_POWER_PHY_CLK_PARAM_48MHZ)
+	    && (dwc_otg_get_param_phy_type(core_if) == DWC_PHY_TYPE_PARAM_FS)) {
+		if (dwc_otg_param_initialized
+		    (core_if->core_params->host_ls_low_power_phy_clk)) {
+			DWC_ERROR
+			    ("%d invalid for host_ls_low_power_phy_clk. Check HW configuration.\n",
+			     val);
+		}
+		val =
+		    (dwc_otg_get_param_phy_type(core_if) ==
+		     DWC_PHY_TYPE_PARAM_FS) ?
+		    DWC_HOST_LS_LOW_POWER_PHY_CLK_PARAM_6MHZ :
+		    DWC_HOST_LS_LOW_POWER_PHY_CLK_PARAM_48MHZ;
+		retval = -DWC_E_INVALID;
+	}
+
+	core_if->core_params->host_ls_low_power_phy_clk = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_host_ls_low_power_phy_clk(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->host_ls_low_power_phy_clk;
+}
+
+int dwc_otg_set_param_phy_ulpi_ddr(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	if (DWC_OTG_PARAM_TEST(val, 0, 1)) {
+		DWC_WARN("Wrong value for phy_ulpi_ddr\n");
+		DWC_WARN("phy_upli_ddr must be 0 or 1\n");
+		return -DWC_E_INVALID;
+	}
+
+	core_if->core_params->phy_ulpi_ddr = val;
+	return 0;
+}
+
+int32_t dwc_otg_get_param_phy_ulpi_ddr(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->phy_ulpi_ddr;
+}
+
+int dwc_otg_set_param_phy_ulpi_ext_vbus(dwc_otg_core_if_t * core_if,
+					int32_t val)
+{
+	if (DWC_OTG_PARAM_TEST(val, 0, 1)) {
+		DWC_WARN("Wrong valaue for phy_ulpi_ext_vbus\n");
+		DWC_WARN("phy_ulpi_ext_vbus must be 0 or 1\n");
+		return -DWC_E_INVALID;
+	}
+
+	core_if->core_params->phy_ulpi_ext_vbus = val;
+	return 0;
+}
+
+int32_t dwc_otg_get_param_phy_ulpi_ext_vbus(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->phy_ulpi_ext_vbus;
+}
+
+int dwc_otg_set_param_phy_utmi_width(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	if (DWC_OTG_PARAM_TEST(val, 8, 8) && DWC_OTG_PARAM_TEST(val, 16, 16)) {
+		DWC_WARN("Wrong valaue for phy_utmi_width\n");
+		DWC_WARN("phy_utmi_width must be 8 or 16\n");
+		return -DWC_E_INVALID;
+	}
+
+	core_if->core_params->phy_utmi_width = val;
+	return 0;
+}
+
+int32_t dwc_otg_get_param_phy_utmi_width(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->phy_utmi_width;
+}
+
+int dwc_otg_set_param_ulpi_fs_ls(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	if (DWC_OTG_PARAM_TEST(val, 0, 1)) {
+		DWC_WARN("Wrong valaue for ulpi_fs_ls\n");
+		DWC_WARN("ulpi_fs_ls must be 0 or 1\n");
+		return -DWC_E_INVALID;
+	}
+
+	core_if->core_params->ulpi_fs_ls = val;
+	return 0;
+}
+
+int32_t dwc_otg_get_param_ulpi_fs_ls(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->ulpi_fs_ls;
+}
+
+int dwc_otg_set_param_ts_dline(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	if (DWC_OTG_PARAM_TEST(val, 0, 1)) {
+		DWC_WARN("Wrong valaue for ts_dline\n");
+		DWC_WARN("ts_dline must be 0 or 1\n");
+		return -DWC_E_INVALID;
+	}
+
+	core_if->core_params->ts_dline = val;
+	return 0;
+}
+
+int32_t dwc_otg_get_param_ts_dline(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->ts_dline;
+}
+
+int dwc_otg_set_param_i2c_enable(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	int retval = 0;
+	if (DWC_OTG_PARAM_TEST(val, 0, 1)) {
+		DWC_WARN("Wrong valaue for i2c_enable\n");
+		DWC_WARN("i2c_enable must be 0 or 1\n");
+		return -DWC_E_INVALID;
+	}
+#ifndef NO_FS_PHY_HW_CHECK
+	if (val == 1 && core_if->hwcfg3.b.i2c == 0) {
+		if (dwc_otg_param_initialized(core_if->core_params->i2c_enable)) {
+			DWC_ERROR
+			    ("%d invalid for i2c_enable. Check HW configuration.\n",
+			     val);
+		}
+		val = 0;
+		retval = -DWC_E_INVALID;
+	}
+#endif
+
+	core_if->core_params->i2c_enable = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_i2c_enable(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->i2c_enable;
+}
+
+int dwc_otg_set_param_dev_perio_tx_fifo_size(dwc_otg_core_if_t * core_if,
+					     int32_t val, int fifo_num)
+{
+	int retval = 0;
+
+	if (DWC_OTG_PARAM_TEST(val, 4, 768)) {
+		DWC_WARN("Wrong value for dev_perio_tx_fifo_size\n");
+		DWC_WARN("dev_perio_tx_fifo_size must be 4-768\n");
+		return -DWC_E_INVALID;
+	}
+
+	if (val >
+	    (DWC_READ_REG32(&core_if->core_global_regs->dtxfsiz[fifo_num]))) {
+		if (dwc_otg_param_initialized
+		    (core_if->core_params->dev_perio_tx_fifo_size[fifo_num])) {
+			DWC_ERROR
+			    ("`%d' invalid for parameter `dev_perio_fifo_size_%d'. Check HW configuration.\n",
+			     val, fifo_num);
+		}
+		val = (DWC_READ_REG32(&core_if->core_global_regs->dtxfsiz[fifo_num]));
+		retval = -DWC_E_INVALID;
+	}
+
+	core_if->core_params->dev_perio_tx_fifo_size[fifo_num] = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_dev_perio_tx_fifo_size(dwc_otg_core_if_t * core_if,
+						 int fifo_num)
+{
+	return core_if->core_params->dev_perio_tx_fifo_size[fifo_num];
+}
+
+int dwc_otg_set_param_en_multiple_tx_fifo(dwc_otg_core_if_t * core_if,
+					  int32_t val)
+{
+	int retval = 0;
+	if (DWC_OTG_PARAM_TEST(val, 0, 1)) {
+		DWC_WARN("Wrong valaue for en_multiple_tx_fifo,\n");
+		DWC_WARN("en_multiple_tx_fifo must be 0 or 1\n");
+		return -DWC_E_INVALID;
+	}
+
+	if (val == 1 && core_if->hwcfg4.b.ded_fifo_en == 0) {
+		if (dwc_otg_param_initialized
+		    (core_if->core_params->en_multiple_tx_fifo)) {
+			DWC_ERROR
+			    ("%d invalid for parameter en_multiple_tx_fifo. Check HW configuration.\n",
+			     val);
+		}
+		val = 0;
+		retval = -DWC_E_INVALID;
+	}
+
+	core_if->core_params->en_multiple_tx_fifo = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_en_multiple_tx_fifo(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->en_multiple_tx_fifo;
+}
+
+int dwc_otg_set_param_dev_tx_fifo_size(dwc_otg_core_if_t * core_if, int32_t val,
+				       int fifo_num)
+{
+	int retval = 0;
+
+	if (DWC_OTG_PARAM_TEST(val, 4, 768)) {
+		DWC_WARN("Wrong value for dev_tx_fifo_size\n");
+		DWC_WARN("dev_tx_fifo_size must be 4-768\n");
+		return -DWC_E_INVALID;
+	}
+
+	if (val >
+	    (DWC_READ_REG32(&core_if->core_global_regs->dtxfsiz[fifo_num]))) {
+		if (dwc_otg_param_initialized
+		    (core_if->core_params->dev_tx_fifo_size[fifo_num])) {
+			DWC_ERROR
+			    ("`%d' invalid for parameter `dev_tx_fifo_size_%d'. Check HW configuration.\n",
+			     val, fifo_num);
+		}
+		val = (DWC_READ_REG32(&core_if->core_global_regs->dtxfsiz[fifo_num]));
+		retval = -DWC_E_INVALID;
+	}
+
+	core_if->core_params->dev_tx_fifo_size[fifo_num] = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_dev_tx_fifo_size(dwc_otg_core_if_t * core_if,
+					   int fifo_num)
+{
+	return core_if->core_params->dev_tx_fifo_size[fifo_num];
+}
+
+int dwc_otg_set_param_thr_ctl(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	int retval = 0;
+
+	if (DWC_OTG_PARAM_TEST(val, 0, 7)) {
+		DWC_WARN("Wrong value for thr_ctl\n");
+		DWC_WARN("thr_ctl must be 0-7\n");
+		return -DWC_E_INVALID;
+	}
+
+	if ((val != 0) &&
+	    (!dwc_otg_get_param_dma_enable(core_if) ||
+	     !core_if->hwcfg4.b.ded_fifo_en)) {
+		if (dwc_otg_param_initialized(core_if->core_params->thr_ctl)) {
+			DWC_ERROR
+			    ("%d invalid for parameter thr_ctl. Check HW configuration.\n",
+			     val);
+		}
+		val = 0;
+		retval = -DWC_E_INVALID;
+	}
+
+	core_if->core_params->thr_ctl = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_thr_ctl(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->thr_ctl;
+}
+
+int dwc_otg_set_param_lpm_enable(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	int retval = 0;
+
+	if (DWC_OTG_PARAM_TEST(val, 0, 1)) {
+		DWC_WARN("Wrong value for lpm_enable\n");
+		DWC_WARN("lpm_enable must be 0 or 1\n");
+		return -DWC_E_INVALID;
+	}
+
+	if (val && !core_if->hwcfg3.b.otg_lpm_en) {
+		if (dwc_otg_param_initialized(core_if->core_params->lpm_enable)) {
+			DWC_ERROR
+			    ("%d invalid for parameter lpm_enable. Check HW configuration.\n",
+			     val);
+		}
+		val = 0;
+		retval = -DWC_E_INVALID;
+	}
+
+	core_if->core_params->lpm_enable = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_lpm_enable(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->lpm_enable;
+}
+
+int dwc_otg_set_param_tx_thr_length(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	if (DWC_OTG_PARAM_TEST(val, 8, 128)) {
+		DWC_WARN("Wrong valaue for tx_thr_length\n");
+		DWC_WARN("tx_thr_length must be 8 - 128\n");
+		return -DWC_E_INVALID;
+	}
+
+	core_if->core_params->tx_thr_length = val;
+	return 0;
+}
+
+int32_t dwc_otg_get_param_tx_thr_length(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->tx_thr_length;
+}
+
+int dwc_otg_set_param_rx_thr_length(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	if (DWC_OTG_PARAM_TEST(val, 8, 128)) {
+		DWC_WARN("Wrong valaue for rx_thr_length\n");
+		DWC_WARN("rx_thr_length must be 8 - 128\n");
+		return -DWC_E_INVALID;
+	}
+
+	core_if->core_params->rx_thr_length = val;
+	return 0;
+}
+
+int32_t dwc_otg_get_param_rx_thr_length(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->rx_thr_length;
+}
+
+int dwc_otg_set_param_dma_burst_size(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	if (DWC_OTG_PARAM_TEST(val, 1, 1) &&
+	    DWC_OTG_PARAM_TEST(val, 4, 4) &&
+	    DWC_OTG_PARAM_TEST(val, 8, 8) &&
+	    DWC_OTG_PARAM_TEST(val, 16, 16) &&
+	    DWC_OTG_PARAM_TEST(val, 32, 32) &&
+	    DWC_OTG_PARAM_TEST(val, 64, 64) &&
+	    DWC_OTG_PARAM_TEST(val, 128, 128) &&
+	    DWC_OTG_PARAM_TEST(val, 256, 256)) {
+		DWC_WARN("`%d' invalid for parameter `dma_burst_size'\n", val);
+		return -DWC_E_INVALID;
+	}
+	core_if->core_params->dma_burst_size = val;
+	return 0;
+}
+
+int32_t dwc_otg_get_param_dma_burst_size(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->dma_burst_size;
+}
+
+int dwc_otg_set_param_pti_enable(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	int retval = 0;
+	if (DWC_OTG_PARAM_TEST(val, 0, 1)) {
+		DWC_WARN("`%d' invalid for parameter `pti_enable'\n", val);
+		return -DWC_E_INVALID;
+	}
+	if (val && (core_if->snpsid < OTG_CORE_REV_2_72a)) {
+		if (dwc_otg_param_initialized(core_if->core_params->pti_enable)) {
+			DWC_ERROR
+			    ("%d invalid for parameter pti_enable. Check HW configuration.\n",
+			     val);
+		}
+		retval = -DWC_E_INVALID;
+		val = 0;
+	}
+	core_if->core_params->pti_enable = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_pti_enable(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->pti_enable;
+}
+
+int dwc_otg_set_param_mpi_enable(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	int retval = 0;
+	if (DWC_OTG_PARAM_TEST(val, 0, 1)) {
+		DWC_WARN("`%d' invalid for parameter `mpi_enable'\n", val);
+		return -DWC_E_INVALID;
+	}
+	if (val && (core_if->hwcfg2.b.multi_proc_int == 0)) {
+		if (dwc_otg_param_initialized(core_if->core_params->mpi_enable)) {
+			DWC_ERROR
+			    ("%d invalid for parameter mpi_enable. Check HW configuration.\n",
+			     val);
+		}
+		retval = -DWC_E_INVALID;
+		val = 0;
+	}
+	core_if->core_params->mpi_enable = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_mpi_enable(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->mpi_enable;
+}
+
+int dwc_otg_set_param_adp_enable(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	int retval = 0;
+	if (DWC_OTG_PARAM_TEST(val, 0, 1)) {
+		DWC_WARN("`%d' invalid for parameter `adp_enable'\n", val);
+		return -DWC_E_INVALID;
+	}
+	if (val && (core_if->hwcfg3.b.adp_supp == 0)) {
+		if (dwc_otg_param_initialized
+		    (core_if->core_params->adp_supp_enable)) {
+			DWC_ERROR
+			    ("%d invalid for parameter adp_enable. Check HW configuration.\n",
+			     val);
+		}
+		retval = -DWC_E_INVALID;
+		val = 0;
+	}
+	core_if->core_params->adp_supp_enable = val;
+	/*Set OTG version 2.0 in case of enabling ADP*/
+	if (val)
+		dwc_otg_set_param_otg_ver(core_if, 1);
+
+	return retval;
+}
+
+int32_t dwc_otg_get_param_adp_enable(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->adp_supp_enable;
+}
+
+int dwc_otg_set_param_ic_usb_cap(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	int retval = 0;
+	if (DWC_OTG_PARAM_TEST(val, 0, 1)) {
+		DWC_WARN("`%d' invalid for parameter `ic_usb_cap'\n", val);
+		DWC_WARN("ic_usb_cap must be 0 or 1\n");
+		return -DWC_E_INVALID;
+	}
+
+	if (val && (core_if->hwcfg2.b.otg_enable_ic_usb == 0)) {
+		if (dwc_otg_param_initialized(core_if->core_params->ic_usb_cap)) {
+			DWC_ERROR
+			    ("%d invalid for parameter ic_usb_cap. Check HW configuration.\n",
+			     val);
+		}
+		retval = -DWC_E_INVALID;
+		val = 0;
+	}
+	core_if->core_params->ic_usb_cap = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_ic_usb_cap(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->ic_usb_cap;
+}
+
+int dwc_otg_set_param_ahb_thr_ratio(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	int retval = 0;
+	int valid = 1;
+
+	if (DWC_OTG_PARAM_TEST(val, 0, 3)) {
+		DWC_WARN("`%d' invalid for parameter `ahb_thr_ratio'\n", val);
+		DWC_WARN("ahb_thr_ratio must be 0 - 3\n");
+		return -DWC_E_INVALID;
+	}
+
+	if (val
+	    && (core_if->snpsid < OTG_CORE_REV_2_81a
+		|| !dwc_otg_get_param_thr_ctl(core_if))) {
+		valid = 0;
+	} else if (val
+		   && ((dwc_otg_get_param_tx_thr_length(core_if) / (1 << val)) <
+		       4)) {
+		valid = 0;
+	}
+	if (valid == 0) {
+		if (dwc_otg_param_initialized
+		    (core_if->core_params->ahb_thr_ratio)) {
+			DWC_ERROR
+			    ("%d invalid for parameter ahb_thr_ratio. Check HW configuration.\n",
+			     val);
+		}
+		retval = -DWC_E_INVALID;
+		val = 0;
+	}
+
+	core_if->core_params->ahb_thr_ratio = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_ahb_thr_ratio(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->ahb_thr_ratio;
+}
+
+int dwc_otg_set_param_power_down(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	int retval = 0;
+	int valid = 1;
+
+	if (DWC_OTG_PARAM_TEST(val, 0, 2)) {
+		DWC_WARN("`%d' invalid for parameter `power_down'\n", val);
+		DWC_WARN("power_down must be 0 - 2\n");
+		return -DWC_E_INVALID;
+	}
+
+	if ((val == 2) && (core_if->snpsid < OTG_CORE_REV_2_91a)) {
+		valid = 0;
+	}
+	if (valid == 0) {
+		if (dwc_otg_param_initialized(core_if->core_params->power_down)) {
+			DWC_ERROR
+			    ("%d invalid for parameter power_down. Check HW configuration.\n",
+			     val);
+		}
+		retval = -DWC_E_INVALID;
+		val = 0;
+	}
+	core_if->core_params->power_down = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_power_down(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->power_down;
+}
+
+int dwc_otg_set_param_reload_ctl(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	int retval = 0;
+	int valid = 1;
+
+	if (DWC_OTG_PARAM_TEST(val, 0, 1)) {
+		DWC_WARN("`%d' invalid for parameter `reload_ctl'\n", val);
+		DWC_WARN("reload_ctl must be 0 or 1\n");
+		return -DWC_E_INVALID;
+	}
+
+	if ((val == 1) && (core_if->snpsid < OTG_CORE_REV_2_92a)) {
+		valid = 0;
+	}
+	if (valid == 0) {
+		if (dwc_otg_param_initialized(core_if->core_params->reload_ctl)) {
+			DWC_ERROR("%d invalid for parameter reload_ctl."
+				  "Check HW configuration.\n", val);
+		}
+		retval = -DWC_E_INVALID;
+		val = 0;
+	}
+	core_if->core_params->reload_ctl = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_reload_ctl(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->reload_ctl;
+}
+
+int dwc_otg_set_param_dev_out_nak(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	int retval = 0;
+	int valid = 1;
+
+	if (DWC_OTG_PARAM_TEST(val, 0, 1)) {
+		DWC_WARN("`%d' invalid for parameter `dev_out_nak'\n", val);
+		DWC_WARN("dev_out_nak must be 0 or 1\n");
+		return -DWC_E_INVALID;
+	}
+
+	if ((val == 1) && ((core_if->snpsid < OTG_CORE_REV_2_93a) ||
+		!(core_if->core_params->dma_desc_enable))) {
+		valid = 0;
+	}
+	if (valid == 0) {
+		if (dwc_otg_param_initialized(core_if->core_params->dev_out_nak)) {
+			DWC_ERROR("%d invalid for parameter dev_out_nak."
+				"Check HW configuration.\n", val);
+		}
+		retval = -DWC_E_INVALID;
+		val = 0;
+	}
+	core_if->core_params->dev_out_nak = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_dev_out_nak(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->dev_out_nak;
+}
+
+int dwc_otg_set_param_cont_on_bna(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	int retval = 0;
+	int valid = 1;
+
+	if (DWC_OTG_PARAM_TEST(val, 0, 1)) {
+		DWC_WARN("`%d' invalid for parameter `cont_on_bna'\n", val);
+		DWC_WARN("cont_on_bna must be 0 or 1\n");
+		return -DWC_E_INVALID;
+	}
+
+	if ((val == 1) && ((core_if->snpsid < OTG_CORE_REV_2_94a) ||
+		!(core_if->core_params->dma_desc_enable))) {
+			valid = 0;
+	}
+	if (valid == 0) {
+		if (dwc_otg_param_initialized(core_if->core_params->cont_on_bna)) {
+			DWC_ERROR("%d invalid for parameter cont_on_bna."
+				"Check HW configuration.\n", val);
+		}
+		retval = -DWC_E_INVALID;
+		val = 0;
+	}
+	core_if->core_params->cont_on_bna = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_cont_on_bna(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->cont_on_bna;
+}
+
+int dwc_otg_set_param_ahb_single(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	int retval = 0;
+	int valid = 1;
+
+	if (DWC_OTG_PARAM_TEST(val, 0, 1)) {
+		DWC_WARN("`%d' invalid for parameter `ahb_single'\n", val);
+		DWC_WARN("ahb_single must be 0 or 1\n");
+		return -DWC_E_INVALID;
+	}
+
+	if ((val == 1) && (core_if->snpsid < OTG_CORE_REV_2_94a)) {
+			valid = 0;
+	}
+	if (valid == 0) {
+		if (dwc_otg_param_initialized(core_if->core_params->ahb_single)) {
+			DWC_ERROR("%d invalid for parameter ahb_single."
+				"Check HW configuration.\n", val);
+		}
+		retval = -DWC_E_INVALID;
+		val = 0;
+	}
+	core_if->core_params->ahb_single = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_ahb_single(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->ahb_single;
+}
+
+int dwc_otg_set_param_otg_ver(dwc_otg_core_if_t * core_if, int32_t val)
+{
+	int retval = 0;
+
+	if (DWC_OTG_PARAM_TEST(val, 0, 1)) {
+		DWC_WARN("`%d' invalid for parameter `otg_ver'\n", val);
+		DWC_WARN
+		    ("otg_ver must be 0(for OTG 1.3 support) or 1(for OTG 2.0 support)\n");
+		return -DWC_E_INVALID;
+	}
+
+	core_if->core_params->otg_ver = val;
+	return retval;
+}
+
+int32_t dwc_otg_get_param_otg_ver(dwc_otg_core_if_t * core_if)
+{
+	return core_if->core_params->otg_ver;
+}
+
+uint32_t dwc_otg_get_hnpstatus(dwc_otg_core_if_t * core_if)
+{
+	gotgctl_data_t otgctl;
+	otgctl.d32 = DWC_READ_REG32(&core_if->core_global_regs->gotgctl);
+	return otgctl.b.hstnegscs;
+}
+
+uint32_t dwc_otg_get_srpstatus(dwc_otg_core_if_t * core_if)
+{
+	gotgctl_data_t otgctl;
+	otgctl.d32 = DWC_READ_REG32(&core_if->core_global_regs->gotgctl);
+	return otgctl.b.sesreqscs;
+}
+
+void dwc_otg_set_hnpreq(dwc_otg_core_if_t * core_if, uint32_t val)
+{
+	if(core_if->otg_ver == 0) {
+		gotgctl_data_t otgctl;
+		otgctl.d32 = DWC_READ_REG32(&core_if->core_global_regs->gotgctl);
+		otgctl.b.hnpreq = val;
+		DWC_WRITE_REG32(&core_if->core_global_regs->gotgctl, otgctl.d32);
+	} else {
+		core_if->otg_sts = val;
+	}
+}
+
+uint32_t dwc_otg_get_gsnpsid(dwc_otg_core_if_t * core_if)
+{
+	return core_if->snpsid;
+}
+
+uint32_t dwc_otg_get_mode(dwc_otg_core_if_t * core_if)
+{
+	gintsts_data_t gintsts;
+	gintsts.d32 = DWC_READ_REG32(&core_if->core_global_regs->gintsts);
+	return gintsts.b.curmode;
+}
+
+uint32_t dwc_otg_get_hnpcapable(dwc_otg_core_if_t * core_if)
+{
+	gusbcfg_data_t usbcfg;
+	usbcfg.d32 = DWC_READ_REG32(&core_if->core_global_regs->gusbcfg);
+	return usbcfg.b.hnpcap;
+}
+
+void dwc_otg_set_hnpcapable(dwc_otg_core_if_t * core_if, uint32_t val)
+{
+	gusbcfg_data_t usbcfg;
+	usbcfg.d32 = DWC_READ_REG32(&core_if->core_global_regs->gusbcfg);
+	usbcfg.b.hnpcap = val;
+	DWC_WRITE_REG32(&core_if->core_global_regs->gusbcfg, usbcfg.d32);
+}
+
+uint32_t dwc_otg_get_srpcapable(dwc_otg_core_if_t * core_if)
+{
+	gusbcfg_data_t usbcfg;
+	usbcfg.d32 = DWC_READ_REG32(&core_if->core_global_regs->gusbcfg);
+	return usbcfg.b.srpcap;
+}
+
+void dwc_otg_set_srpcapable(dwc_otg_core_if_t * core_if, uint32_t val)
+{
+	gusbcfg_data_t usbcfg;
+	usbcfg.d32 = DWC_READ_REG32(&core_if->core_global_regs->gusbcfg);
+	usbcfg.b.srpcap = val;
+	DWC_WRITE_REG32(&core_if->core_global_regs->gusbcfg, usbcfg.d32);
+}
+
+uint32_t dwc_otg_get_devspeed(dwc_otg_core_if_t * core_if)
+{
+	dcfg_data_t dcfg;
+	dcfg.d32 = DWC_READ_REG32(&core_if->dev_if->dev_global_regs->dcfg);
+	return dcfg.b.devspd;
+}
+
+void dwc_otg_set_devspeed(dwc_otg_core_if_t * core_if, uint32_t val)
+{
+	dcfg_data_t dcfg;
+	dcfg.d32 = DWC_READ_REG32(&core_if->dev_if->dev_global_regs->dcfg);
+	dcfg.b.devspd = val;
+	DWC_WRITE_REG32(&core_if->dev_if->dev_global_regs->dcfg, dcfg.d32);
+}
+
+uint32_t dwc_otg_get_busconnected(dwc_otg_core_if_t * core_if)
+{
+	hprt0_data_t hprt0;
+	hprt0.d32 = DWC_READ_REG32(core_if->host_if->hprt0);
+	return hprt0.b.prtconnsts;
+}
+
+uint32_t dwc_otg_get_enumspeed(dwc_otg_core_if_t * core_if)
+{
+	dsts_data_t dsts;
+	dsts.d32 = DWC_READ_REG32(&core_if->dev_if->dev_global_regs->dsts);
+	return dsts.b.enumspd;
+}
+
+uint32_t dwc_otg_get_prtpower(dwc_otg_core_if_t * core_if)
+{
+	hprt0_data_t hprt0;
+	hprt0.d32 = DWC_READ_REG32(core_if->host_if->hprt0);
+	return hprt0.b.prtpwr;
+
+}
+
+uint32_t dwc_otg_get_core_state(dwc_otg_core_if_t * core_if)
+{
+	return core_if->hibernation_suspend;
+}
+
+void dwc_otg_set_prtpower(dwc_otg_core_if_t * core_if, uint32_t val)
+{
+	hprt0_data_t hprt0;
+	hprt0.d32 = dwc_otg_read_hprt0(core_if);
+	hprt0.b.prtpwr = val;
+	DWC_WRITE_REG32(core_if->host_if->hprt0, hprt0.d32);
+}
+
+uint32_t dwc_otg_get_prtsuspend(dwc_otg_core_if_t * core_if)
+{
+	hprt0_data_t hprt0;
+	hprt0.d32 = DWC_READ_REG32(core_if->host_if->hprt0);
+	return hprt0.b.prtsusp;
+
+}
+
+void dwc_otg_set_prtsuspend(dwc_otg_core_if_t * core_if, uint32_t val)
+{
+	hprt0_data_t hprt0;
+	hprt0.d32 = dwc_otg_read_hprt0(core_if);
+	hprt0.b.prtsusp = val;
+	DWC_WRITE_REG32(core_if->host_if->hprt0, hprt0.d32);
+}
+
+uint32_t dwc_otg_get_fr_interval(dwc_otg_core_if_t * core_if)
+{
+	hfir_data_t hfir;
+	hfir.d32 = DWC_READ_REG32(&core_if->host_if->host_global_regs->hfir);
+	return hfir.b.frint;
+
+}
+
+void dwc_otg_set_fr_interval(dwc_otg_core_if_t * core_if, uint32_t val)
+{
+	hfir_data_t hfir;
+	uint32_t fram_int;
+	fram_int = calc_frame_interval(core_if);
+	hfir.d32 = DWC_READ_REG32(&core_if->host_if->host_global_regs->hfir);
+	if (!core_if->core_params->reload_ctl) {
+		DWC_WARN("\nCannot reload HFIR register.HFIR.HFIRRldCtrl bit is"
+			 "not set to 1.\nShould load driver with reload_ctl=1"
+			 " module parameter\n");
+		return;
+	}
+	switch (fram_int) {
+	case 3750:
+		if ((val < 3350) || (val > 4150)) {
+			DWC_WARN("HFIR interval for HS core and 30 MHz"
+				 "clock freq should be from 3350 to 4150\n");
+			return;
+		}
+		break;
+	case 30000:
+		if ((val < 26820) || (val > 33180)) {
+			DWC_WARN("HFIR interval for FS/LS core and 30 MHz"
+				 "clock freq should be from 26820 to 33180\n");
+			return;
+		}
+		break;
+	case 6000:
+		if ((val < 5360) || (val > 6640)) {
+			DWC_WARN("HFIR interval for HS core and 48 MHz"
+				 "clock freq should be from 5360 to 6640\n");
+			return;
+		}
+		break;
+	case 48000:
+		if ((val < 42912) || (val > 53088)) {
+			DWC_WARN("HFIR interval for FS/LS core and 48 MHz"
+				 "clock freq should be from 42912 to 53088\n");
+			return;
+		}
+		break;
+	case 7500:
+		if ((val < 6700) || (val > 8300)) {
+			DWC_WARN("HFIR interval for HS core and 60 MHz"
+				 "clock freq should be from 6700 to 8300\n");
+			return;
+		}
+		break;
+	case 60000:
+		if ((val < 53640) || (val > 65536)) {
+			DWC_WARN("HFIR interval for FS/LS core and 60 MHz"
+				 "clock freq should be from 53640 to 65536\n");
+			return;
+		}
+		break;
+	default:
+		DWC_WARN("Unknown frame interval\n");
+		return;
+		break;
+
+	}
+	hfir.b.frint = val;
+	DWC_WRITE_REG32(&core_if->host_if->host_global_regs->hfir, hfir.d32);
+}
+
+uint32_t dwc_otg_get_mode_ch_tim(dwc_otg_core_if_t * core_if)
+{
+	hcfg_data_t hcfg;
+	hcfg.d32 = DWC_READ_REG32(&core_if->host_if->host_global_regs->hcfg);
+	return hcfg.b.modechtimen;
+
+}
+
+void dwc_otg_set_mode_ch_tim(dwc_otg_core_if_t * core_if, uint32_t val)
+{
+	hcfg_data_t hcfg;
+	hcfg.d32 = DWC_READ_REG32(&core_if->host_if->host_global_regs->hcfg);
+	hcfg.b.modechtimen = val;
+	DWC_WRITE_REG32(&core_if->host_if->host_global_regs->hcfg, hcfg.d32);
+}
+
+void dwc_otg_set_prtresume(dwc_otg_core_if_t * core_if, uint32_t val)
+{
+	hprt0_data_t hprt0;
+	hprt0.d32 = dwc_otg_read_hprt0(core_if);
+	hprt0.b.prtres = val;
+	DWC_WRITE_REG32(core_if->host_if->hprt0, hprt0.d32);
+}
+
+uint32_t dwc_otg_get_remotewakesig(dwc_otg_core_if_t * core_if)
+{
+	dctl_data_t dctl;
+	dctl.d32 = DWC_READ_REG32(&core_if->dev_if->dev_global_regs->dctl);
+	return dctl.b.rmtwkupsig;
+}
+
+uint32_t dwc_otg_get_lpm_portsleepstatus(dwc_otg_core_if_t * core_if)
+{
+	glpmcfg_data_t lpmcfg;
+	lpmcfg.d32 = DWC_READ_REG32(&core_if->core_global_regs->glpmcfg);
+
+	DWC_ASSERT(!
+		   ((core_if->lx_state == DWC_OTG_L1) ^ lpmcfg.b.prt_sleep_sts),
+		   "lx_state = %d, lmpcfg.prt_sleep_sts = %d\n",
+		   core_if->lx_state, lpmcfg.b.prt_sleep_sts);
+
+	return lpmcfg.b.prt_sleep_sts;
+}
+
+uint32_t dwc_otg_get_lpm_remotewakeenabled(dwc_otg_core_if_t * core_if)
+{
+	glpmcfg_data_t lpmcfg;
+	lpmcfg.d32 = DWC_READ_REG32(&core_if->core_global_regs->glpmcfg);
+	return lpmcfg.b.rem_wkup_en;
+}
+
+uint32_t dwc_otg_get_lpmresponse(dwc_otg_core_if_t * core_if)
+{
+	glpmcfg_data_t lpmcfg;
+	lpmcfg.d32 = DWC_READ_REG32(&core_if->core_global_regs->glpmcfg);
+	return lpmcfg.b.appl_resp;
+}
+
+void dwc_otg_set_lpmresponse(dwc_otg_core_if_t * core_if, uint32_t val)
+{
+	glpmcfg_data_t lpmcfg;
+	lpmcfg.d32 = DWC_READ_REG32(&core_if->core_global_regs->glpmcfg);
+	lpmcfg.b.appl_resp = val;
+	DWC_WRITE_REG32(&core_if->core_global_regs->glpmcfg, lpmcfg.d32);
+}
+
+uint32_t dwc_otg_get_hsic_connect(dwc_otg_core_if_t * core_if)
+{
+	glpmcfg_data_t lpmcfg;
+	lpmcfg.d32 = DWC_READ_REG32(&core_if->core_global_regs->glpmcfg);
+	return lpmcfg.b.hsic_connect;
+}
+
+void dwc_otg_set_hsic_connect(dwc_otg_core_if_t * core_if, uint32_t val)
+{
+	glpmcfg_data_t lpmcfg;
+	lpmcfg.d32 = DWC_READ_REG32(&core_if->core_global_regs->glpmcfg);
+	lpmcfg.b.hsic_connect = val;
+	DWC_WRITE_REG32(&core_if->core_global_regs->glpmcfg, lpmcfg.d32);
+}
+
+uint32_t dwc_otg_get_inv_sel_hsic(dwc_otg_core_if_t * core_if)
+{
+	glpmcfg_data_t lpmcfg;
+	lpmcfg.d32 = DWC_READ_REG32(&core_if->core_global_regs->glpmcfg);
+	return lpmcfg.b.inv_sel_hsic;
+
+}
+
+void dwc_otg_set_inv_sel_hsic(dwc_otg_core_if_t * core_if, uint32_t val)
+{
+	glpmcfg_data_t lpmcfg;
+	lpmcfg.d32 = DWC_READ_REG32(&core_if->core_global_regs->glpmcfg);
+	lpmcfg.b.inv_sel_hsic = val;
+	DWC_WRITE_REG32(&core_if->core_global_regs->glpmcfg, lpmcfg.d32);
+}
+
+uint32_t dwc_otg_get_gotgctl(dwc_otg_core_if_t * core_if)
+{
+	return DWC_READ_REG32(&core_if->core_global_regs->gotgctl);
+}
+
+void dwc_otg_set_gotgctl(dwc_otg_core_if_t * core_if, uint32_t val)
+{
+	DWC_WRITE_REG32(&core_if->core_global_regs->gotgctl, val);
+}
+
+uint32_t dwc_otg_get_gusbcfg(dwc_otg_core_if_t * core_if)
+{
+	return DWC_READ_REG32(&core_if->core_global_regs->gusbcfg);
+}
+
+void dwc_otg_set_gusbcfg(dwc_otg_core_if_t * core_if, uint32_t val)
+{
+	DWC_WRITE_REG32(&core_if->core_global_regs->gusbcfg, val);
+}
+
+uint32_t dwc_otg_get_grxfsiz(dwc_otg_core_if_t * core_if)
+{
+	return DWC_READ_REG32(&core_if->core_global_regs->grxfsiz);
+}
+
+void dwc_otg_set_grxfsiz(dwc_otg_core_if_t * core_if, uint32_t val)
+{
+	DWC_WRITE_REG32(&core_if->core_global_regs->grxfsiz, val);
+}
+
+uint32_t dwc_otg_get_gnptxfsiz(dwc_otg_core_if_t * core_if)
+{
+	return DWC_READ_REG32(&core_if->core_global_regs->gnptxfsiz);
+}
+
+void dwc_otg_set_gnptxfsiz(dwc_otg_core_if_t * core_if, uint32_t val)
+{
+	DWC_WRITE_REG32(&core_if->core_global_regs->gnptxfsiz, val);
+}
+
+uint32_t dwc_otg_get_gpvndctl(dwc_otg_core_if_t * core_if)
+{
+	return DWC_READ_REG32(&core_if->core_global_regs->gpvndctl);
+}
+
+void dwc_otg_set_gpvndctl(dwc_otg_core_if_t * core_if, uint32_t val)
+{
+	DWC_WRITE_REG32(&core_if->core_global_regs->gpvndctl, val);
+}
+
+uint32_t dwc_otg_get_ggpio(dwc_otg_core_if_t * core_if)
+{
+	return DWC_READ_REG32(&core_if->core_global_regs->ggpio);
+}
+
+void dwc_otg_set_ggpio(dwc_otg_core_if_t * core_if, uint32_t val)
+{
+	DWC_WRITE_REG32(&core_if->core_global_regs->ggpio, val);
+}
+
+uint32_t dwc_otg_get_hprt0(dwc_otg_core_if_t * core_if)
+{
+	return DWC_READ_REG32(core_if->host_if->hprt0);
+
+}
+
+void dwc_otg_set_hprt0(dwc_otg_core_if_t * core_if, uint32_t val)
+{
+	DWC_WRITE_REG32(core_if->host_if->hprt0, val);
+}
+
+uint32_t dwc_otg_get_guid(dwc_otg_core_if_t * core_if)
+{
+	return DWC_READ_REG32(&core_if->core_global_regs->guid);
+}
+
+void dwc_otg_set_guid(dwc_otg_core_if_t * core_if, uint32_t val)
+{
+	DWC_WRITE_REG32(&core_if->core_global_regs->guid, val);
+}
+
+uint32_t dwc_otg_get_hptxfsiz(dwc_otg_core_if_t * core_if)
+{
+	return DWC_READ_REG32(&core_if->core_global_regs->hptxfsiz);
+}
+
+uint16_t dwc_otg_get_otg_version(dwc_otg_core_if_t * core_if)
+{
+	return ((core_if->otg_ver == 1) ? (uint16_t)0x0200 : (uint16_t)0x0103);
+}
+
+/**
+ * Start the SRP timer to detect when the SRP does not complete within
+ * 6 seconds.
+ *
+ * @param core_if the pointer to core_if strucure.
+ */
+void dwc_otg_pcd_start_srp_timer(dwc_otg_core_if_t * core_if)
+{
+	core_if->srp_timer_started = 1;
+	DWC_TIMER_SCHEDULE(core_if->srp_timer, 6000 /* 6 secs */ );
+}
+
+void dwc_otg_initiate_srp(dwc_otg_core_if_t * core_if)
+{
+	uint32_t *addr = (uint32_t *) & (core_if->core_global_regs->gotgctl);
+	gotgctl_data_t mem;
+	gotgctl_data_t val;
+
+	val.d32 = DWC_READ_REG32(addr);
+	if (val.b.sesreq) {
+		DWC_ERROR("Session Request Already active!\n");
+		return;
+	}
+
+	DWC_INFO("Session Request Initated\n");	//NOTICE
+	mem.d32 = DWC_READ_REG32(addr);
+	mem.b.sesreq = 1;
+	DWC_WRITE_REG32(addr, mem.d32);
+
+	/* Start the SRP timer */
+	dwc_otg_pcd_start_srp_timer(core_if);
+	return;
+}
diff --git a/drivers/usb/dwc_otg/dwc_otg_cil.h b/drivers/usb/dwc_otg/dwc_otg_cil.h
new file mode 100644
index 0000000..b6edcb7
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_otg_cil.h
@@ -0,0 +1,1456 @@
+/* ==========================================================================
+ * $File: //dwh/usb_iip/dev/software/otg/linux/drivers/dwc_otg_cil.h $
+ * $Revision: #122 $
+ * $Date: 2011/10/24 $
+ * $Change: 1871160 $
+ *
+ * Synopsys HS OTG Linux Software Driver and documentation (hereinafter,
+ * "Software") is an Unsupported proprietary work of Synopsys, Inc. unless
+ * otherwise expressly agreed to in writing between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product under
+ * any End User Software License Agreement or Agreement for Licensed Product
+ * with Synopsys or any supplement thereto. You are permitted to use and
+ * redistribute this Software in source and binary forms, with or without
+ * modification, provided that redistributions of source code must retain this
+ * notice. You may not view, use, disclose, copy or distribute this file or
+ * any information contained herein except pursuant to this license grant from
+ * Synopsys. If you do not agree with this notice, including the disclaimer
+ * below, then you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS" BASIS
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE HEREBY DISCLAIMED. IN NO EVENT SHALL SYNOPSYS BE LIABLE FOR ANY DIRECT,
+ * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================== */
+
+#if !defined(__DWC_CIL_H__)
+#define __DWC_CIL_H__
+
+#include "dwc_list.h"
+#include "dwc_otg_dbg.h"
+#include "dwc_otg_regs.h"
+
+#include "dwc_otg_core_if.h"
+#include "dwc_otg_adp.h"
+
+/**
+ * @file
+ * This file contains the interface to the Core Interface Layer.
+ */
+
+#ifdef DWC_UTE_CFI
+
+#define MAX_DMA_DESCS_PER_EP	256
+
+/**
+ * Enumeration for the data buffer mode
+ */
+typedef enum _data_buffer_mode {
+	BM_STANDARD = 0,	/* data buffer is in normal mode */
+	BM_SG = 1,		/* data buffer uses the scatter/gather mode */
+	BM_CONCAT = 2,		/* data buffer uses the concatenation mode */
+	BM_CIRCULAR = 3,	/* data buffer uses the circular DMA mode */
+	BM_ALIGN = 4		/* data buffer is in buffer alignment mode */
+} data_buffer_mode_e;
+#endif //DWC_UTE_CFI
+
+/** Macros defined for DWC OTG HW Release version */
+
+#define OTG_CORE_REV_2_60a	0x4F54260A
+#define OTG_CORE_REV_2_71a	0x4F54271A
+#define OTG_CORE_REV_2_72a	0x4F54272A
+#define OTG_CORE_REV_2_80a	0x4F54280A
+#define OTG_CORE_REV_2_81a	0x4F54281A
+#define OTG_CORE_REV_2_90a	0x4F54290A
+#define OTG_CORE_REV_2_91a	0x4F54291A
+#define OTG_CORE_REV_2_92a	0x4F54292A
+#define OTG_CORE_REV_2_93a	0x4F54293A
+#define OTG_CORE_REV_2_94a	0x4F54294A
+
+/**
+ * Information for each ISOC packet.
+ */
+typedef struct iso_pkt_info {
+	uint32_t offset;
+	uint32_t length;
+	int32_t status;
+} iso_pkt_info_t;
+
+/**
+ * The <code>dwc_ep</code> structure represents the state of a single
+ * endpoint when acting in device mode. It contains the data items
+ * needed for an endpoint to be activated and transfer packets.
+ */
+typedef struct dwc_ep {
+	/** EP number used for register address lookup */
+	uint8_t num;
+	/** EP direction 0 = OUT */
+	unsigned is_in:1;
+	/** EP active. */
+	unsigned active:1;
+
+	/**
+	 * Periodic Tx FIFO # for IN EPs For INTR EP set to 0 to use non-periodic
+	 * Tx FIFO. If dedicated Tx FIFOs are enabled Tx FIFO # FOR IN EPs*/
+	unsigned tx_fifo_num:4;
+	/** EP type: 0 - Control, 1 - ISOC,	 2 - BULK,	3 - INTR */
+	unsigned type:2;
+#define DWC_OTG_EP_TYPE_CONTROL	   0
+#define DWC_OTG_EP_TYPE_ISOC	   1
+#define DWC_OTG_EP_TYPE_BULK	   2
+#define DWC_OTG_EP_TYPE_INTR	   3
+
+	/** DATA start PID for INTR and BULK EP */
+	unsigned data_pid_start:1;
+	/** Frame (even/odd) for ISOC EP */
+	unsigned even_odd_frame:1;
+	/** Max Packet bytes */
+	unsigned maxpacket:11;
+
+	/** Max Transfer size */
+	uint32_t maxxfer;
+
+	/** @name Transfer state */
+	/** @{ */
+
+	/**
+	 * Pointer to the beginning of the transfer buffer -- do not modify
+	 * during transfer.
+	 */
+
+	dwc_dma_t dma_addr;
+
+	dwc_dma_t dma_desc_addr;
+	dwc_otg_dev_dma_desc_t *desc_addr;
+
+	uint8_t *start_xfer_buff;
+	/** pointer to the transfer buffer */
+	uint8_t *xfer_buff;
+	/** Number of bytes to transfer */
+	unsigned xfer_len:19;
+	/** Number of bytes transferred. */
+	unsigned xfer_count:19;
+	/** Sent ZLP */
+	unsigned sent_zlp:1;
+	/** Total len for control transfer */
+	unsigned total_len:19;
+
+	/** stall clear flag */
+	unsigned stall_clear_flag:1;
+
+#ifdef DWC_UTE_CFI
+	/* The buffer mode */
+	data_buffer_mode_e buff_mode;
+
+	/* The chain of DMA descriptors.
+	 * MAX_DMA_DESCS_PER_EP will be allocated for each active EP.
+	 */
+	dwc_otg_dma_desc_t *descs;
+
+	/* The DMA address of the descriptors chain start */
+	dma_addr_t descs_dma_addr;
+	/** This variable stores the length of the last enqueued request */
+	uint32_t cfi_req_len;
+#endif				//DWC_UTE_CFI
+
+/** Max DMA Descriptor count for any EP */
+#define MAX_DMA_DESC_CNT 256
+	/** Allocated DMA Desc count */
+	uint32_t desc_cnt;
+
+	/** bInterval */
+	uint32_t bInterval;
+	/** Next frame num to setup next ISOC transfer */
+	uint32_t frame_num;
+	/** Indicates SOF number overrun in DSTS */
+	uint8_t frm_overrun;
+
+#ifdef DWC_UTE_PER_IO
+	/** Next frame num for which will be setup DMA Desc */
+	uint32_t xiso_frame_num;
+	/** bInterval */
+	uint32_t xiso_bInterval;
+	/** Count of currently active transfers - shall be either 0 or 1 */
+	int xiso_active_xfers;
+	int xiso_queued_xfers;
+#endif
+#ifdef DWC_EN_ISOC
+	/**
+	 * Variables specific for ISOC EPs
+	 *
+	 */
+	/** DMA addresses of ISOC buffers */
+	dwc_dma_t dma_addr0;
+	dwc_dma_t dma_addr1;
+
+	dwc_dma_t iso_dma_desc_addr;
+	dwc_otg_dev_dma_desc_t *iso_desc_addr;
+
+	/** pointer to the transfer buffers */
+	uint8_t *xfer_buff0;
+	uint8_t *xfer_buff1;
+
+	/** number of ISOC Buffer is processing */
+	uint32_t proc_buf_num;
+	/** Interval of ISOC Buffer processing */
+	uint32_t buf_proc_intrvl;
+	/** Data size for regular frame */
+	uint32_t data_per_frame;
+
+	/* todo - pattern data support is to be implemented in the future */
+	/** Data size for pattern frame */
+	uint32_t data_pattern_frame;
+	/** Frame number of pattern data */
+	uint32_t sync_frame;
+
+	/** bInterval */
+	//uint32_t bInterval;	// Makarand
+
+	/** ISO Packet number per frame */
+	uint32_t pkt_per_frm;
+	/** Next frame num for which will be setup DMA Desc */
+	uint32_t next_frame;
+	/** Number of packets per buffer processing */
+	uint32_t pkt_cnt;
+	/** Info for all isoc packets */
+	iso_pkt_info_t *pkt_info;
+	/** current pkt number */
+	uint32_t cur_pkt;
+	/** current pkt number */
+	uint8_t *cur_pkt_addr;
+	/** current pkt number */
+	uint32_t cur_pkt_dma_addr;
+#endif				/* DWC_EN_ISOC */
+
+/** @} */
+} dwc_ep_t;
+
+/*
+ * Reasons for halting a host channel.
+ */
+typedef enum dwc_otg_halt_status {
+	DWC_OTG_HC_XFER_NO_HALT_STATUS,
+	DWC_OTG_HC_XFER_COMPLETE,
+	DWC_OTG_HC_XFER_URB_COMPLETE,
+	DWC_OTG_HC_XFER_ACK,
+	DWC_OTG_HC_XFER_NAK,
+	DWC_OTG_HC_XFER_NYET,
+	DWC_OTG_HC_XFER_STALL,
+	DWC_OTG_HC_XFER_XACT_ERR,
+	DWC_OTG_HC_XFER_FRAME_OVERRUN,
+	DWC_OTG_HC_XFER_BABBLE_ERR,
+	DWC_OTG_HC_XFER_DATA_TOGGLE_ERR,
+	DWC_OTG_HC_XFER_AHB_ERR,
+	DWC_OTG_HC_XFER_PERIODIC_INCOMPLETE,
+	DWC_OTG_HC_XFER_URB_DEQUEUE
+} dwc_otg_halt_status_e;
+
+/**
+ * Host channel descriptor. This structure represents the state of a single
+ * host channel when acting in host mode. It contains the data items needed to
+ * transfer packets to an endpoint via a host channel.
+ */
+typedef struct dwc_hc {
+	/** Host channel number used for register address lookup */
+	uint8_t hc_num;
+
+	/** Device to access */
+	unsigned dev_addr:7;
+
+	/** EP to access */
+	unsigned ep_num:4;
+
+	/** EP direction. 0: OUT, 1: IN */
+	unsigned ep_is_in:1;
+
+	/**
+	 * EP speed.
+	 * One of the following values:
+	 *	- DWC_OTG_EP_SPEED_LOW
+	 *	- DWC_OTG_EP_SPEED_FULL
+	 *	- DWC_OTG_EP_SPEED_HIGH
+	 */
+	unsigned speed:2;
+#define DWC_OTG_EP_SPEED_LOW	0
+#define DWC_OTG_EP_SPEED_FULL	1
+#define DWC_OTG_EP_SPEED_HIGH	2
+
+	/**
+	 * Endpoint type.
+	 * One of the following values:
+	 *	- DWC_OTG_EP_TYPE_CONTROL: 0
+	 *	- DWC_OTG_EP_TYPE_ISOC: 1
+	 *	- DWC_OTG_EP_TYPE_BULK: 2
+	 *	- DWC_OTG_EP_TYPE_INTR: 3
+	 */
+	unsigned ep_type:2;
+
+	/** Max packet size in bytes */
+	unsigned max_packet:11;
+
+	/**
+	 * PID for initial transaction.
+	 * 0: DATA0,<br>
+	 * 1: DATA2,<br>
+	 * 2: DATA1,<br>
+	 * 3: MDATA (non-Control EP),
+	 *	  SETUP (Control EP)
+	 */
+	unsigned data_pid_start:2;
+#define DWC_OTG_HC_PID_DATA0 0
+#define DWC_OTG_HC_PID_DATA2 1
+#define DWC_OTG_HC_PID_DATA1 2
+#define DWC_OTG_HC_PID_MDATA 3
+#define DWC_OTG_HC_PID_SETUP 3
+
+	/** Number of periodic transactions per (micro)frame */
+	unsigned multi_count:2;
+
+	/** @name Transfer State */
+	/** @{ */
+
+	/** Pointer to the current transfer buffer position. */
+	uint8_t *xfer_buff;
+	/**
+	 * In Buffer DMA mode this buffer will be used
+	 * if xfer_buff is not DWORD aligned.
+	 */
+	dwc_dma_t align_buff;
+	/** Total number of bytes to transfer. */
+	uint32_t xfer_len;
+	/** Number of bytes transferred so far. */
+	uint32_t xfer_count;
+	/** Packet count at start of transfer.*/
+	uint16_t start_pkt_count;
+
+	/**
+	 * Flag to indicate whether the transfer has been started. Set to 1 if
+	 * it has been started, 0 otherwise.
+	 */
+	uint8_t xfer_started;
+
+	/**
+	 * Set to 1 to indicate that a PING request should be issued on this
+	 * channel. If 0, process normally.
+	 */
+	uint8_t do_ping;
+
+	/**
+	 * Set to 1 to indicate that the error count for this transaction is
+	 * non-zero. Set to 0 if the error count is 0.
+	 */
+	uint8_t error_state;
+
+	/**
+	 * Set to 1 to indicate that this channel should be halted the next
+	 * time a request is queued for the channel. This is necessary in
+	 * slave mode if no request queue space is available when an attempt
+	 * is made to halt the channel.
+	 */
+	uint8_t halt_on_queue;
+
+	/**
+	 * Set to 1 if the host channel has been halted, but the core is not
+	 * finished flushing queued requests. Otherwise 0.
+	 */
+	uint8_t halt_pending;
+
+	/**
+	 * Reason for halting the host channel.
+	 */
+	dwc_otg_halt_status_e halt_status;
+
+	/*
+	 * Split settings for the host channel
+	 */
+	uint8_t do_split;		   /**< Enable split for the channel */
+	uint8_t complete_split;	   /**< Enable complete split */
+	uint8_t hub_addr;		   /**< Address of high speed hub */
+
+	uint8_t port_addr;		   /**< Port of the low/full speed device */
+	/** Split transaction position
+	 * One of the following values:
+	 *	  - DWC_HCSPLIT_XACTPOS_MID
+	 *	  - DWC_HCSPLIT_XACTPOS_BEGIN
+	 *	  - DWC_HCSPLIT_XACTPOS_END
+	 *	  - DWC_HCSPLIT_XACTPOS_ALL */
+	uint8_t xact_pos;
+
+	/** Set when the host channel does a short read. */
+	uint8_t short_read;
+
+	/**
+	 * Number of requests issued for this channel since it was assigned to
+	 * the current transfer (not counting PINGs).
+	 */
+	uint8_t requests;
+
+	/**
+	 * Queue Head for the transfer being processed by this channel.
+	 */
+	struct dwc_otg_qh *qh;
+
+	/** @} */
+
+	/** Entry in list of host channels. */
+	 DWC_CIRCLEQ_ENTRY(dwc_hc) hc_list_entry;
+
+	/** @name Descriptor DMA support */
+	/** @{ */
+
+	/** Number of Transfer Descriptors */
+	uint16_t ntd;
+
+	/** Descriptor List DMA address */
+	dwc_dma_t desc_list_addr;
+
+	/** Scheduling micro-frame bitmap. */
+	uint8_t schinfo;
+
+	/** @} */
+} dwc_hc_t;
+
+/**
+ * The following parameters may be specified when starting the module. These
+ * parameters define how the DWC_otg controller should be configured.
+ */
+typedef struct dwc_otg_core_params {
+	int32_t opt;
+
+	/**
+	 * Specifies the OTG capabilities. The driver will automatically
+	 * detect the value for this parameter if none is specified.
+	 * 0 - HNP and SRP capable (default)
+	 * 1 - SRP Only capable
+	 * 2 - No HNP/SRP capable
+	 */
+	int32_t otg_cap;
+
+	/**
+	 * Specifies whether to use slave or DMA mode for accessing the data
+	 * FIFOs. The driver will automatically detect the value for this
+	 * parameter if none is specified.
+	 * 0 - Slave
+	 * 1 - DMA (default, if available)
+	 */
+	int32_t dma_enable;
+
+	/**
+	 * When DMA mode is enabled specifies whether to use address DMA or DMA
+	 * Descriptor mode for accessing the data FIFOs in device mode. The driver
+	 * will automatically detect the value for this if none is specified.
+	 * 0 - address DMA
+	 * 1 - DMA Descriptor(default, if available)
+	 */
+	int32_t dma_desc_enable;
+	/** The DMA Burst size (applicable only for External DMA
+	 * Mode). 1, 4, 8 16, 32, 64, 128, 256 (default 32)
+	 */
+	int32_t dma_burst_size;	/* Translate this to GAHBCFG values */
+
+	/**
+	 * Specifies the maximum speed of operation in host and device mode.
+	 * The actual speed depends on the speed of the attached device and
+	 * the value of phy_type. The actual speed depends on the speed of the
+	 * attached device.
+	 * 0 - High Speed (default)
+	 * 1 - Full Speed
+	 */
+	int32_t speed;
+	/** Specifies whether low power mode is supported when attached
+	 *	to a Full Speed or Low Speed device in host mode.
+	 * 0 - Don't support low power mode (default)
+	 * 1 - Support low power mode
+	 */
+	int32_t host_support_fs_ls_low_power;
+
+	/** Specifies the PHY clock rate in low power mode when connected to a
+	 * Low Speed device in host mode. This parameter is applicable only if
+	 * HOST_SUPPORT_FS_LS_LOW_POWER is enabled. If PHY_TYPE is set to FS
+	 * then defaults to 6 MHZ otherwise 48 MHZ.
+	 *
+	 * 0 - 48 MHz
+	 * 1 - 6 MHz
+	 */
+	int32_t host_ls_low_power_phy_clk;
+
+	/**
+	 * 0 - Use cC FIFO size parameters
+	 * 1 - Allow dynamic FIFO sizing (default)
+	 */
+	int32_t enable_dynamic_fifo;
+
+	/** Total number of 4-byte words in the data FIFO memory. This
+	 * memory includes the Rx FIFO, non-periodic Tx FIFO, and periodic
+	 * Tx FIFOs.
+	 * 32 to 32768 (default 8192)
+	 * Note: The total FIFO memory depth in the FPGA configuration is 8192.
+	 */
+	int32_t data_fifo_size;
+
+	/** Number of 4-byte words in the Rx FIFO in device mode when dynamic
+	 * FIFO sizing is enabled.
+	 * 16 to 32768 (default 1064)
+	 */
+	int32_t dev_rx_fifo_size;
+
+	/** Number of 4-byte words in the non-periodic Tx FIFO in device mode
+	 * when dynamic FIFO sizing is enabled.
+	 * 16 to 32768 (default 1024)
+	 */
+	int32_t dev_nperio_tx_fifo_size;
+
+	/** Number of 4-byte words in each of the periodic Tx FIFOs in device
+	 * mode when dynamic FIFO sizing is enabled.
+	 * 4 to 768 (default 256)
+	 */
+	uint32_t dev_perio_tx_fifo_size[MAX_PERIO_FIFOS];
+
+	/** Number of 4-byte words in the Rx FIFO in host mode when dynamic
+	 * FIFO sizing is enabled.
+	 * 16 to 32768 (default 1024)
+	 */
+	int32_t host_rx_fifo_size;
+
+	/** Number of 4-byte words in the non-periodic Tx FIFO in host mode
+	 * when Dynamic FIFO sizing is enabled in the core.
+	 * 16 to 32768 (default 1024)
+	 */
+	int32_t host_nperio_tx_fifo_size;
+
+	/** Number of 4-byte words in the host periodic Tx FIFO when dynamic
+	 * FIFO sizing is enabled.
+	 * 16 to 32768 (default 1024)
+	 */
+	int32_t host_perio_tx_fifo_size;
+
+	/** The maximum transfer size supported in bytes.
+	 * 2047 to 65,535  (default 65,535)
+	 */
+	int32_t max_transfer_size;
+
+	/** The maximum number of packets in a transfer.
+	 * 15 to 511  (default 511)
+	 */
+	int32_t max_packet_count;
+
+	/** The number of host channel registers to use.
+	 * 1 to 16 (default 12)
+	 * Note: The FPGA configuration supports a maximum of 12 host channels.
+	 */
+	int32_t host_channels;
+
+	/** The number of endpoints in addition to EP0 available for device
+	 * mode operations.
+	 * 1 to 15 (default 6 IN and OUT)
+	 * Note: The FPGA configuration supports a maximum of 6 IN and OUT
+	 * endpoints in addition to EP0.
+	 */
+	int32_t dev_endpoints;
+
+		/**
+		 * Specifies the type of PHY interface to use. By default, the driver
+		 * will automatically detect the phy_type.
+		 *
+		 * 0 - Full Speed PHY
+		 * 1 - UTMI+ (default)
+		 * 2 - ULPI
+		 */
+	int32_t phy_type;
+
+	/**
+	 * Specifies the UTMI+ Data Width. This parameter is
+	 * applicable for a PHY_TYPE of UTMI+ or ULPI. (For a ULPI
+	 * PHY_TYPE, this parameter indicates the data width between
+	 * the MAC and the ULPI Wrapper.) Also, this parameter is
+	 * applicable only if the OTG_HSPHY_WIDTH cC parameter was set
+	 * to "8 and 16 bits", meaning that the core has been
+	 * configured to work at either data path width.
+	 *
+	 * 8 or 16 bits (default 16)
+	 */
+	int32_t phy_utmi_width;
+
+	/**
+	 * Specifies whether the ULPI operates at double or single
+	 * data rate. This parameter is only applicable if PHY_TYPE is
+	 * ULPI.
+	 *
+	 * 0 - single data rate ULPI interface with 8 bit wide data
+	 * bus (default)
+	 * 1 - double data rate ULPI interface with 4 bit wide data
+	 * bus
+	 */
+	int32_t phy_ulpi_ddr;
+
+	/**
+	 * Specifies whether to use the internal or external supply to
+	 * drive the vbus with a ULPI phy.
+	 */
+	int32_t phy_ulpi_ext_vbus;
+
+	/**
+	 * Specifies whether to use the I2Cinterface for full speed PHY. This
+	 * parameter is only applicable if PHY_TYPE is FS.
+	 * 0 - No (default)
+	 * 1 - Yes
+	 */
+	int32_t i2c_enable;
+
+	int32_t ulpi_fs_ls;
+
+	int32_t ts_dline;
+
+	/**
+	 * Specifies whether dedicated transmit FIFOs are
+	 * enabled for non periodic IN endpoints in device mode
+	 * 0 - No
+	 * 1 - Yes
+	 */
+	int32_t en_multiple_tx_fifo;
+
+	/** Number of 4-byte words in each of the Tx FIFOs in device
+	 * mode when dynamic FIFO sizing is enabled.
+	 * 4 to 768 (default 256)
+	 */
+	uint32_t dev_tx_fifo_size[MAX_TX_FIFOS];
+
+	/** Thresholding enable flag-
+	 * bit 0 - enable non-ISO Tx thresholding
+	 * bit 1 - enable ISO Tx thresholding
+	 * bit 2 - enable Rx thresholding
+	 */
+	uint32_t thr_ctl;
+
+	/** Thresholding length for Tx
+	 *	FIFOs in 32 bit DWORDs
+	 */
+	uint32_t tx_thr_length;
+
+	/** Thresholding length for Rx
+	 *	FIFOs in 32 bit DWORDs
+	 */
+	uint32_t rx_thr_length;
+
+	/**
+	 * Specifies whether LPM (Link Power Management) support is enabled
+	 */
+	int32_t lpm_enable;
+
+	/** Per Transfer Interrupt
+	 *	mode enable flag
+	 * 1 - Enabled
+	 * 0 - Disabled
+	 */
+	int32_t pti_enable;
+
+	/** Multi Processor Interrupt
+	 *	mode enable flag
+	 * 1 - Enabled
+	 * 0 - Disabled
+	 */
+	int32_t mpi_enable;
+
+	/** IS_USB Capability
+	 * 1 - Enabled
+	 * 0 - Disabled
+	 */
+	int32_t ic_usb_cap;
+
+	/** AHB Threshold Ratio
+	 * 2'b00 AHB Threshold = 	MAC Threshold
+	 * 2'b01 AHB Threshold = 1/2 	MAC Threshold
+	 * 2'b10 AHB Threshold = 1/4	MAC Threshold
+	 * 2'b11 AHB Threshold = 1/8	MAC Threshold
+	 */
+	int32_t ahb_thr_ratio;
+
+	/** ADP Support
+	 * 1 - Enabled
+	 * 0 - Disabled
+	 */
+	int32_t adp_supp_enable;
+
+	/** HFIR Reload Control
+	 * 0 - The HFIR cannot be reloaded dynamically.
+	 * 1 - Allow dynamic reloading of the HFIR register during runtime.
+	 */
+	int32_t reload_ctl;
+
+	/** DCFG: Enable device Out NAK
+	 * 0 - The core does not set NAK after Bulk Out transfer complete.
+	 * 1 - The core sets NAK after Bulk OUT transfer complete.
+	 */
+	int32_t dev_out_nak;
+
+	/** DCFG: Enable Continue on BNA
+	 * After receiving BNA interrupt the core disables the endpoint,when the
+	 * endpoint is re-enabled by the application the core starts processing
+	 * 0 - from the DOEPDMA descriptor
+	 * 1 - from the descriptor which received the BNA.
+	 */
+	int32_t cont_on_bna;
+
+	/** GAHBCFG: AHB Single Support
+	 * This bit when programmed supports SINGLE transfers for remainder
+	 * data in a transfer for DMA mode of operation.
+	 * 0 - in this case the remainder data will be sent using INCR burst size.
+	 * 1 - in this case the remainder data will be sent using SINGLE burst size.
+	 */
+	int32_t ahb_single;
+
+	/** Core Power down mode
+	 * 0 - No Power Down is enabled
+	 * 1 - Reserved
+	 * 2 - Complete Power Down (Hibernation)
+	 */
+	int32_t power_down;
+
+	/** OTG revision supported
+	 * 0 - OTG 1.3 revision
+	 * 1 - OTG 2.0 revision
+	 */
+	int32_t otg_ver;
+
+} dwc_otg_core_params_t;
+
+#ifdef DEBUG
+struct dwc_otg_core_if;
+typedef struct hc_xfer_info {
+	struct dwc_otg_core_if *core_if;
+	dwc_hc_t *hc;
+} hc_xfer_info_t;
+#endif
+
+typedef struct ep_xfer_info {
+	struct dwc_otg_core_if *core_if;
+	dwc_ep_t *ep;
+	uint8_t state;
+} ep_xfer_info_t;
+/*
+ * Device States
+ */
+typedef enum dwc_otg_lx_state {
+	/** On state */
+	DWC_OTG_L0,
+	/** LPM sleep state*/
+	DWC_OTG_L1,
+	/** USB suspend state*/
+	DWC_OTG_L2,
+	/** Off state*/
+	DWC_OTG_L3
+} dwc_otg_lx_state_e;
+
+struct dwc_otg_global_regs_backup {
+	uint32_t gotgctl_local;
+	uint32_t gintmsk_local;
+	uint32_t gahbcfg_local;
+	uint32_t gusbcfg_local;
+	uint32_t grxfsiz_local;
+	uint32_t gnptxfsiz_local;
+#ifdef CONFIG_USB_DWC_OTG_LPM
+	uint32_t glpmcfg_local;
+#endif
+	uint32_t gi2cctl_local;
+	uint32_t hptxfsiz_local;
+	uint32_t pcgcctl_local;
+	uint32_t gdfifocfg_local;
+	uint32_t dtxfsiz_local[MAX_EPS_CHANNELS];
+	uint32_t gpwrdn_local;
+};
+
+struct dwc_otg_host_regs_backup {
+	uint32_t hcfg_local;
+	uint32_t haintmsk_local;
+	uint32_t hcintmsk_local[MAX_EPS_CHANNELS];
+	uint32_t hprt0_local;
+	uint32_t hfir_local;
+};
+
+struct dwc_otg_dev_regs_backup {
+	uint32_t dcfg;
+	uint32_t dctl;
+	uint32_t daintmsk;
+	uint32_t diepmsk;
+	uint32_t doepmsk;
+	uint32_t diepctl[MAX_EPS_CHANNELS];
+	uint32_t dieptsiz[MAX_EPS_CHANNELS];
+	uint32_t diepdma[MAX_EPS_CHANNELS];
+};
+/**
+ * The <code>dwc_otg_core_if</code> structure contains information needed to manage
+ * the DWC_otg controller acting in either host or device mode. It
+ * represents the programming view of the controller as a whole.
+ */
+struct dwc_otg_core_if {
+	/** Parameters that define how the core should be configured.*/
+	dwc_otg_core_params_t *core_params;
+
+	/** Core Global registers starting at offset 000h. */
+	dwc_otg_core_global_regs_t *core_global_regs;
+
+	/** Device-specific information */
+	dwc_otg_dev_if_t *dev_if;
+	/** Host-specific information */
+	dwc_otg_host_if_t *host_if;
+
+	/** Value from SNPSID register */
+	uint32_t snpsid;
+
+	/*
+	 * Set to 1 if the core PHY interface bits in USBCFG have been
+	 * initialized.
+	 */
+	uint8_t phy_init_done;
+
+	/*
+	 * SRP Success flag, set by srp success interrupt in FS I2C mode
+	 */
+	uint8_t srp_success;
+	uint8_t srp_timer_started;
+	/** Timer for SRP. If it expires before SRP is successful
+	 * clear the SRP. */
+	dwc_timer_t *srp_timer;
+
+#ifdef DWC_DEV_SRPCAP
+	/* This timer is needed to power on the hibernated host core if SRP is not
+	 * initiated on connected SRP capable device for limited period of time
+	 */
+	uint8_t pwron_timer_started;
+	dwc_timer_t *pwron_timer;
+#endif
+	/* Common configuration information */
+	/** Power and Clock Gating Control Register */
+	volatile uint32_t *pcgcctl;
+#define DWC_OTG_PCGCCTL_OFFSET 0xE00
+
+	/** Push/pop addresses for endpoints or host channels.*/
+	uint32_t *data_fifo[MAX_EPS_CHANNELS];
+#define DWC_OTG_DATA_FIFO_OFFSET 0x1000
+#define DWC_OTG_DATA_FIFO_SIZE 0x1000
+
+	/** Total RAM for FIFOs (Bytes) */
+	uint16_t total_fifo_size;
+	/** Size of Rx FIFO (Bytes) */
+	uint16_t rx_fifo_size;
+	/** Size of Non-periodic Tx FIFO (Bytes) */
+	uint16_t nperio_tx_fifo_size;
+
+	/** 1 if DMA is enabled, 0 otherwise. */
+	uint8_t dma_enable;
+
+	/** 1 if DMA descriptor is enabled, 0 otherwise. */
+	uint8_t dma_desc_enable;
+
+	/** 1 if PTI Enhancement mode is enabled, 0 otherwise. */
+	uint8_t pti_enh_enable;
+
+	/** 1 if MPI Enhancement mode is enabled, 0 otherwise. */
+	uint8_t multiproc_int_enable;
+
+	/** 1 if dedicated Tx FIFOs are enabled, 0 otherwise. */
+	uint8_t en_multiple_tx_fifo;
+
+	/** Set to 1 if multiple packets of a high-bandwidth transfer is in
+	 * process of being queued */
+	uint8_t queuing_high_bandwidth;
+
+	/** Hardware Configuration -- stored here for convenience.*/
+	hwcfg1_data_t hwcfg1;
+	hwcfg2_data_t hwcfg2;
+	hwcfg3_data_t hwcfg3;
+	hwcfg4_data_t hwcfg4;
+	fifosize_data_t hptxfsiz;
+
+	/** Host and Device Configuration -- stored here for convenience.*/
+	hcfg_data_t hcfg;
+	dcfg_data_t dcfg;
+
+	/** The operational State, during transations
+	 * (a_host>>a_peripherial and b_device=>b_host) this may not
+	 * match the core but allows the software to determine
+	 * transitions.
+	 */
+	uint8_t op_state;
+
+	/**
+	 * Set to 1 if the HCD needs to be restarted on a session request
+	 * interrupt. This is required if no connector ID status change has
+	 * occurred since the HCD was last disconnected.
+	 */
+	uint8_t restart_hcd_on_session_req;
+
+	/** HCD callbacks */
+	/** A-Device is a_host */
+#define A_HOST		(1)
+	/** A-Device is a_suspend */
+#define A_SUSPEND	(2)
+	/** A-Device is a_peripherial */
+#define A_PERIPHERAL	(3)
+	/** B-Device is operating as a Peripheral. */
+#define B_PERIPHERAL	(4)
+	/** B-Device is operating as a Host. */
+#define B_HOST		(5)
+
+	/** HCD callbacks */
+	struct dwc_otg_cil_callbacks *hcd_cb;
+	/** PCD callbacks */
+	struct dwc_otg_cil_callbacks *pcd_cb;
+
+	/** Device mode Periodic Tx FIFO Mask */
+	uint32_t p_tx_msk;
+	/** Device mode Periodic Tx FIFO Mask */
+	uint32_t tx_msk;
+
+	/** Workqueue object used for handling several interrupts */
+	dwc_workq_t *wq_otg;
+
+	/** Timer object used for handling "Wakeup Detected" Interrupt */
+	dwc_timer_t *wkp_timer;
+	/** This arrays used for debug purposes for DEV OUT NAK enhancement */
+	uint32_t start_doeptsiz_val[MAX_EPS_CHANNELS];
+	ep_xfer_info_t ep_xfer_info[MAX_EPS_CHANNELS];
+	dwc_timer_t *ep_xfer_timer[MAX_EPS_CHANNELS];
+#ifdef DEBUG
+	uint32_t start_hcchar_val[MAX_EPS_CHANNELS];
+
+	hc_xfer_info_t hc_xfer_info[MAX_EPS_CHANNELS];
+	dwc_timer_t *hc_xfer_timer[MAX_EPS_CHANNELS];
+
+	uint32_t hfnum_7_samples;
+	uint64_t hfnum_7_frrem_accum;
+	uint32_t hfnum_0_samples;
+	uint64_t hfnum_0_frrem_accum;
+	uint32_t hfnum_other_samples;
+	uint64_t hfnum_other_frrem_accum;
+#endif
+
+#ifdef DWC_UTE_CFI
+	uint16_t pwron_rxfsiz;
+	uint16_t pwron_gnptxfsiz;
+	uint16_t pwron_txfsiz[15];
+
+	uint16_t init_rxfsiz;
+	uint16_t init_gnptxfsiz;
+	uint16_t init_txfsiz[15];
+#endif
+
+	/** Lx state of device */
+	dwc_otg_lx_state_e lx_state;
+
+	/** Saved Core Global registers */
+	struct dwc_otg_global_regs_backup *gr_backup;
+	/** Saved Host registers */
+	struct dwc_otg_host_regs_backup *hr_backup;
+	/** Saved Device registers */
+	struct dwc_otg_dev_regs_backup *dr_backup;
+
+	/** Power Down Enable */
+	uint32_t power_down;
+
+	/** ADP support Enable */
+	uint32_t adp_enable;
+
+	/** ADP structure object */
+	dwc_otg_adp_t adp;
+
+	/** hibernation/suspend flag */
+	int hibernation_suspend;
+
+	/** OTG revision supported */
+	uint32_t otg_ver;
+
+	/** OTG status flag used for HNP polling */
+	uint8_t otg_sts;
+
+	/** Pointer to either hcd->lock or pcd->lock */
+	dwc_spinlock_t *lock;
+
+	/** Start predict NextEP based on Learning Queue if equal 1,
+	 * also used as counter of disabled NP IN EP's */
+	uint8_t start_predict;
+
+	/** NextEp sequence, including EP0: nextep_seq[] = EP if non-periodic and
+	 * active, 0xff otherwise */
+	uint8_t nextep_seq[MAX_EPS_CHANNELS];
+
+	/** Index of fisrt EP in nextep_seq array which should be re-enabled **/
+	uint8_t first_in_nextep_seq;
+
+	/** Frame number while entering to ISR - needed for ISOCs **/
+	uint32_t frame_num;
+
+};
+
+#ifdef DEBUG
+/*
+ * This function is called when transfer is timed out.
+ */
+extern void hc_xfer_timeout(void *ptr);
+#endif
+
+/*
+ * This function is called when transfer is timed out on endpoint.
+ */
+extern void ep_xfer_timeout(void *ptr);
+
+/*
+ * The following functions are functions for works
+ * using during handling some interrupts
+ */
+extern void w_conn_id_status_change(void *p);
+
+extern void w_wakeup_detected(void *p);
+
+/** Saves global register values into system memory. */
+extern int dwc_otg_save_global_regs(dwc_otg_core_if_t * core_if);
+/** Saves device register values into system memory. */
+extern int dwc_otg_save_dev_regs(dwc_otg_core_if_t * core_if);
+/** Saves host register values into system memory. */
+extern int dwc_otg_save_host_regs(dwc_otg_core_if_t * core_if);
+/** Restore global register values. */
+extern int dwc_otg_restore_global_regs(dwc_otg_core_if_t * core_if);
+/** Restore host register values. */
+extern int dwc_otg_restore_host_regs(dwc_otg_core_if_t * core_if, int reset);
+/** Restore device register values. */
+extern int dwc_otg_restore_dev_regs(dwc_otg_core_if_t * core_if,
+				    int rem_wakeup);
+extern int restore_lpm_i2c_regs(dwc_otg_core_if_t * core_if);
+extern int restore_essential_regs(dwc_otg_core_if_t * core_if, int rmode,
+				  int is_host);
+
+extern int dwc_otg_host_hibernation_restore(dwc_otg_core_if_t * core_if,
+					    int restore_mode, int reset);
+extern int dwc_otg_device_hibernation_restore(dwc_otg_core_if_t * core_if,
+					      int rem_wakeup, int reset);
+
+/*
+ * The following functions support initialization of the CIL driver component
+ * and the DWC_otg controller.
+ */
+extern void dwc_otg_core_host_init(dwc_otg_core_if_t * _core_if);
+extern void dwc_otg_core_dev_init(dwc_otg_core_if_t * _core_if);
+
+/** @name Device CIL Functions
+ * The following functions support managing the DWC_otg controller in device
+ * mode.
+ */
+/**@{*/
+extern void dwc_otg_wakeup(dwc_otg_core_if_t * _core_if);
+extern void dwc_otg_read_setup_packet(dwc_otg_core_if_t * _core_if,
+				      uint32_t * _dest);
+extern uint32_t dwc_otg_get_frame_number(dwc_otg_core_if_t * _core_if);
+extern void dwc_otg_ep0_activate(dwc_otg_core_if_t * _core_if, dwc_ep_t * _ep);
+extern void dwc_otg_ep_activate(dwc_otg_core_if_t * _core_if, dwc_ep_t * _ep);
+extern void dwc_otg_ep_deactivate(dwc_otg_core_if_t * _core_if, dwc_ep_t * _ep);
+extern void dwc_otg_ep_start_transfer(dwc_otg_core_if_t * _core_if,
+				      dwc_ep_t * _ep);
+extern void dwc_otg_ep_start_zl_transfer(dwc_otg_core_if_t * _core_if,
+					 dwc_ep_t * _ep);
+extern void dwc_otg_ep0_start_transfer(dwc_otg_core_if_t * _core_if,
+				       dwc_ep_t * _ep);
+extern void dwc_otg_ep0_continue_transfer(dwc_otg_core_if_t * _core_if,
+					  dwc_ep_t * _ep);
+extern void dwc_otg_ep_write_packet(dwc_otg_core_if_t * _core_if,
+				    dwc_ep_t * _ep, int _dma);
+extern void dwc_otg_ep_set_stall(dwc_otg_core_if_t * _core_if, dwc_ep_t * _ep);
+extern void dwc_otg_ep_clear_stall(dwc_otg_core_if_t * _core_if,
+				   dwc_ep_t * _ep);
+extern void dwc_otg_enable_device_interrupts(dwc_otg_core_if_t * _core_if);
+
+#ifdef DWC_EN_ISOC
+extern void dwc_otg_iso_ep_start_frm_transfer(dwc_otg_core_if_t * core_if,
+					      dwc_ep_t * ep);
+extern void dwc_otg_iso_ep_start_buf_transfer(dwc_otg_core_if_t * core_if,
+					      dwc_ep_t * ep);
+#endif /* DWC_EN_ISOC */
+/**@}*/
+
+/** @name Host CIL Functions
+ * The following functions support managing the DWC_otg controller in host
+ * mode.
+ */
+/**@{*/
+extern void dwc_otg_hc_init(dwc_otg_core_if_t * _core_if, dwc_hc_t * _hc);
+extern void dwc_otg_hc_halt(dwc_otg_core_if_t * _core_if,
+			    dwc_hc_t * _hc, dwc_otg_halt_status_e _halt_status);
+extern void dwc_otg_hc_cleanup(dwc_otg_core_if_t * _core_if, dwc_hc_t * _hc);
+extern void dwc_otg_hc_start_transfer(dwc_otg_core_if_t * _core_if,
+				      dwc_hc_t * _hc);
+extern int dwc_otg_hc_continue_transfer(dwc_otg_core_if_t * _core_if,
+					dwc_hc_t * _hc);
+extern void dwc_otg_hc_do_ping(dwc_otg_core_if_t * _core_if, dwc_hc_t * _hc);
+extern void dwc_otg_hc_write_packet(dwc_otg_core_if_t * _core_if,
+				    dwc_hc_t * _hc);
+extern void dwc_otg_enable_host_interrupts(dwc_otg_core_if_t * _core_if);
+extern void dwc_otg_disable_host_interrupts(dwc_otg_core_if_t * _core_if);
+
+extern void dwc_otg_hc_start_transfer_ddma(dwc_otg_core_if_t * core_if,
+					   dwc_hc_t * hc);
+
+extern uint32_t calc_frame_interval(dwc_otg_core_if_t * core_if);
+
+/* Macro used to clear one channel interrupt */
+#define clear_hc_int(_hc_regs_, _intr_) \
+do { \
+	hcint_data_t hcint_clear = {.d32 = 0}; \
+	hcint_clear.b._intr_ = 1; \
+	DWC_WRITE_REG32(&(_hc_regs_)->hcint, hcint_clear.d32); \
+} while (0)
+
+/*
+ * Macro used to disable one channel interrupt. Channel interrupts are
+ * disabled when the channel is halted or released by the interrupt handler.
+ * There is no need to handle further interrupts of that type until the
+ * channel is re-assigned. In fact, subsequent handling may cause crashes
+ * because the channel structures are cleaned up when the channel is released.
+ */
+#define disable_hc_int(_hc_regs_, _intr_) \
+do { \
+	hcintmsk_data_t hcintmsk = {.d32 = 0}; \
+	hcintmsk.b._intr_ = 1; \
+	DWC_MODIFY_REG32(&(_hc_regs_)->hcintmsk, hcintmsk.d32, 0); \
+} while (0)
+
+/**
+ * This function Reads HPRT0 in preparation to modify. It keeps the
+ * WC bits 0 so that if they are read as 1, they won't clear when you
+ * write it back
+ */
+static inline uint32_t dwc_otg_read_hprt0(dwc_otg_core_if_t * _core_if)
+{
+	hprt0_data_t hprt0;
+	hprt0.d32 = DWC_READ_REG32(_core_if->host_if->hprt0);
+	hprt0.b.prtena = 0;
+	hprt0.b.prtconndet = 0;
+	hprt0.b.prtenchng = 0;
+	hprt0.b.prtovrcurrchng = 0;
+	return hprt0.d32;
+}
+
+/**@}*/
+
+/** @name Common CIL Functions
+ * The following functions support managing the DWC_otg controller in either
+ * device or host mode.
+ */
+/**@{*/
+
+extern void dwc_otg_read_packet(dwc_otg_core_if_t * core_if,
+				uint8_t * dest, uint16_t bytes);
+
+extern void dwc_otg_flush_tx_fifo(dwc_otg_core_if_t * _core_if, const int _num);
+extern void dwc_otg_flush_rx_fifo(dwc_otg_core_if_t * _core_if);
+extern void dwc_otg_core_reset(dwc_otg_core_if_t * _core_if);
+
+/**
+ * This function returns the Core Interrupt register.
+ */
+static inline uint32_t dwc_otg_read_core_intr(dwc_otg_core_if_t * core_if)
+{
+	return (DWC_READ_REG32(&core_if->core_global_regs->gintsts) &
+		DWC_READ_REG32(&core_if->core_global_regs->gintmsk));
+}
+
+/**
+ * This function returns the OTG Interrupt register.
+ */
+static inline uint32_t dwc_otg_read_otg_intr(dwc_otg_core_if_t * core_if)
+{
+	return (DWC_READ_REG32(&core_if->core_global_regs->gotgint));
+}
+
+/**
+ * This function reads the Device All Endpoints Interrupt register and
+ * returns the IN endpoint interrupt bits.
+ */
+static inline uint32_t dwc_otg_read_dev_all_in_ep_intr(dwc_otg_core_if_t *
+						       core_if)
+{
+
+	uint32_t v;
+
+	if (core_if->multiproc_int_enable) {
+		v = DWC_READ_REG32(&core_if->dev_if->
+				   dev_global_regs->deachint) &
+		    DWC_READ_REG32(&core_if->
+				   dev_if->dev_global_regs->deachintmsk);
+	} else {
+		v = DWC_READ_REG32(&core_if->dev_if->dev_global_regs->daint) &
+		    DWC_READ_REG32(&core_if->dev_if->dev_global_regs->daintmsk);
+	}
+	return (v & 0xffff);
+}
+
+/**
+ * This function reads the Device All Endpoints Interrupt register and
+ * returns the OUT endpoint interrupt bits.
+ */
+static inline uint32_t dwc_otg_read_dev_all_out_ep_intr(dwc_otg_core_if_t *
+							core_if)
+{
+	uint32_t v;
+
+	if (core_if->multiproc_int_enable) {
+		v = DWC_READ_REG32(&core_if->dev_if->
+				   dev_global_regs->deachint) &
+		    DWC_READ_REG32(&core_if->
+				   dev_if->dev_global_regs->deachintmsk);
+	} else {
+		v = DWC_READ_REG32(&core_if->dev_if->dev_global_regs->daint) &
+		    DWC_READ_REG32(&core_if->dev_if->dev_global_regs->daintmsk);
+	}
+
+	return ((v & 0xffff0000) >> 16);
+}
+
+/**
+ * This function returns the Device IN EP Interrupt register
+ */
+static inline uint32_t dwc_otg_read_dev_in_ep_intr(dwc_otg_core_if_t * core_if,
+						   dwc_ep_t * ep)
+{
+	dwc_otg_dev_if_t *dev_if = core_if->dev_if;
+	uint32_t v, msk, emp;
+
+	if (core_if->multiproc_int_enable) {
+		msk =
+		    DWC_READ_REG32(&dev_if->
+				   dev_global_regs->diepeachintmsk[ep->num]);
+		emp =
+		    DWC_READ_REG32(&dev_if->
+				   dev_global_regs->dtknqr4_fifoemptymsk);
+		msk |= ((emp >> ep->num) & 0x1) << 7;
+		v = DWC_READ_REG32(&dev_if->in_ep_regs[ep->num]->diepint) & msk;
+	} else {
+		msk = DWC_READ_REG32(&dev_if->dev_global_regs->diepmsk);
+		emp =
+		    DWC_READ_REG32(&dev_if->
+				   dev_global_regs->dtknqr4_fifoemptymsk);
+		msk |= ((emp >> ep->num) & 0x1) << 7;
+		v = DWC_READ_REG32(&dev_if->in_ep_regs[ep->num]->diepint) & msk;
+	}
+
+	return v;
+}
+
+/**
+ * This function returns the Device OUT EP Interrupt register
+ */
+static inline uint32_t dwc_otg_read_dev_out_ep_intr(dwc_otg_core_if_t *
+						    _core_if, dwc_ep_t * _ep)
+{
+	dwc_otg_dev_if_t *dev_if = _core_if->dev_if;
+	uint32_t v;
+	doepmsk_data_t msk = {.d32 = 0 };
+
+	if (_core_if->multiproc_int_enable) {
+		msk.d32 =
+		    DWC_READ_REG32(&dev_if->
+				   dev_global_regs->doepeachintmsk[_ep->num]);
+		if (_core_if->pti_enh_enable) {
+			msk.b.pktdrpsts = 1;
+		}
+		v = DWC_READ_REG32(&dev_if->
+				   out_ep_regs[_ep->num]->doepint) & msk.d32;
+	} else {
+		msk.d32 = DWC_READ_REG32(&dev_if->dev_global_regs->doepmsk);
+		if (_core_if->pti_enh_enable) {
+			msk.b.pktdrpsts = 1;
+		}
+		v = DWC_READ_REG32(&dev_if->
+				   out_ep_regs[_ep->num]->doepint) & msk.d32;
+	}
+	return v;
+}
+
+/**
+ * This function returns the Host All Channel Interrupt register
+ */
+static inline uint32_t dwc_otg_read_host_all_channels_intr(dwc_otg_core_if_t *
+							   _core_if)
+{
+	return (DWC_READ_REG32(&_core_if->host_if->host_global_regs->haint));
+}
+
+static inline uint32_t dwc_otg_read_host_channel_intr(dwc_otg_core_if_t *
+						      _core_if, dwc_hc_t * _hc)
+{
+	return (DWC_READ_REG32
+		(&_core_if->host_if->hc_regs[_hc->hc_num]->hcint));
+}
+
+/**
+ * This function returns the mode of the operation, host or device.
+ *
+ * @return 0 - Device Mode, 1 - Host Mode
+ */
+static inline uint32_t dwc_otg_mode(dwc_otg_core_if_t * _core_if)
+{
+	return (DWC_READ_REG32(&_core_if->core_global_regs->gintsts) & 0x1);
+}
+
+/**@}*/
+
+/**
+ * DWC_otg CIL callback structure. This structure allows the HCD and
+ * PCD to register functions used for starting and stopping the PCD
+ * and HCD for role change on for a DRD.
+ */
+typedef struct dwc_otg_cil_callbacks {
+	/** Start function for role change */
+	int (*start) (void *_p);
+	/** Stop Function for role change */
+	int (*stop) (void *_p);
+	/** Disconnect Function for role change */
+	int (*disconnect) (void *_p);
+	/** Resume/Remote wakeup Function */
+	int (*resume_wakeup) (void *_p);
+	/** Suspend function */
+	int (*suspend) (void *_p);
+	/** Session Start (SRP) */
+	int (*session_start) (void *_p);
+#ifdef CONFIG_USB_DWC_OTG_LPM
+	/** Sleep (switch to L0 state) */
+	int (*sleep) (void *_p);
+#endif
+	/** Pointer passed to start() and stop() */
+	void *p;
+} dwc_otg_cil_callbacks_t;
+
+extern void dwc_otg_cil_register_pcd_callbacks(dwc_otg_core_if_t * _core_if,
+					       dwc_otg_cil_callbacks_t * _cb,
+					       void *_p);
+extern void dwc_otg_cil_register_hcd_callbacks(dwc_otg_core_if_t * _core_if,
+					       dwc_otg_cil_callbacks_t * _cb,
+					       void *_p);
+
+void dwc_otg_initiate_srp(dwc_otg_core_if_t * core_if);
+
+//////////////////////////////////////////////////////////////////////
+/** Start the HCD.  Helper function for using the HCD callbacks.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ */
+static inline void cil_hcd_start(dwc_otg_core_if_t * core_if)
+{
+	if (core_if->hcd_cb && core_if->hcd_cb->start) {
+		core_if->hcd_cb->start(core_if->hcd_cb->p);
+	}
+}
+
+/** Stop the HCD.  Helper function for using the HCD callbacks.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ */
+static inline void cil_hcd_stop(dwc_otg_core_if_t * core_if)
+{
+	if (core_if->hcd_cb && core_if->hcd_cb->stop) {
+		core_if->hcd_cb->stop(core_if->hcd_cb->p);
+	}
+}
+
+/** Disconnect the HCD.  Helper function for using the HCD callbacks.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ */
+static inline void cil_hcd_disconnect(dwc_otg_core_if_t * core_if)
+{
+	if (core_if->hcd_cb && core_if->hcd_cb->disconnect) {
+		core_if->hcd_cb->disconnect(core_if->hcd_cb->p);
+	}
+}
+
+/** Inform the HCD the a New Session has begun.  Helper function for
+ * using the HCD callbacks.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ */
+static inline void cil_hcd_session_start(dwc_otg_core_if_t * core_if)
+{
+	if (core_if->hcd_cb && core_if->hcd_cb->session_start) {
+		core_if->hcd_cb->session_start(core_if->hcd_cb->p);
+	}
+}
+
+#ifdef CONFIG_USB_DWC_OTG_LPM
+/**
+ * Inform the HCD about LPM sleep.
+ * Helper function for using the HCD callbacks.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ */
+static inline void cil_hcd_sleep(dwc_otg_core_if_t * core_if)
+{
+	if (core_if->hcd_cb && core_if->hcd_cb->sleep) {
+		core_if->hcd_cb->sleep(core_if->hcd_cb->p);
+	}
+}
+#endif
+
+/** Resume the HCD.  Helper function for using the HCD callbacks.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ */
+static inline void cil_hcd_resume(dwc_otg_core_if_t * core_if)
+{
+	if (core_if->hcd_cb && core_if->hcd_cb->resume_wakeup) {
+		core_if->hcd_cb->resume_wakeup(core_if->hcd_cb->p);
+	}
+}
+
+/** Start the PCD.  Helper function for using the PCD callbacks.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ */
+static inline void cil_pcd_start(dwc_otg_core_if_t * core_if)
+{
+	if (core_if->pcd_cb && core_if->pcd_cb->start) {
+		core_if->pcd_cb->start(core_if->pcd_cb->p);
+	}
+}
+
+/** Stop the PCD.  Helper function for using the PCD callbacks.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ */
+static inline void cil_pcd_stop(dwc_otg_core_if_t * core_if)
+{
+	if (core_if->pcd_cb && core_if->pcd_cb->stop) {
+		core_if->pcd_cb->stop(core_if->pcd_cb->p);
+	}
+}
+
+/** Suspend the PCD.  Helper function for using the PCD callbacks.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ */
+static inline void cil_pcd_suspend(dwc_otg_core_if_t * core_if)
+{
+	if (core_if->pcd_cb && core_if->pcd_cb->suspend) {
+		core_if->pcd_cb->suspend(core_if->pcd_cb->p);
+	}
+}
+
+/** Resume the PCD.  Helper function for using the PCD callbacks.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ */
+static inline void cil_pcd_resume(dwc_otg_core_if_t * core_if)
+{
+	if (core_if->pcd_cb && core_if->pcd_cb->resume_wakeup) {
+		core_if->pcd_cb->resume_wakeup(core_if->pcd_cb->p);
+	}
+}
+
+//////////////////////////////////////////////////////////////////////
+
+#endif
diff --git a/drivers/usb/dwc_otg/dwc_otg_cil_intr.c b/drivers/usb/dwc_otg/dwc_otg_cil_intr.c
new file mode 100644
index 0000000..c47f176
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_otg_cil_intr.c
@@ -0,0 +1,1432 @@
+/* ==========================================================================
+ * $File: //dwh/usb_iip/dev/software/otg/linux/drivers/dwc_otg_cil_intr.c $
+ * $Revision: #31 $
+ * $Date: 2011/10/24 $
+ * $Change: 1871286 $
+ *
+ * Synopsys HS OTG Linux Software Driver and documentation (hereinafter,
+ * "Software") is an Unsupported proprietary work of Synopsys, Inc. unless
+ * otherwise expressly agreed to in writing between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product under
+ * any End User Software License Agreement or Agreement for Licensed Product
+ * with Synopsys or any supplement thereto. You are permitted to use and
+ * redistribute this Software in source and binary forms, with or without
+ * modification, provided that redistributions of source code must retain this
+ * notice. You may not view, use, disclose, copy or distribute this file or
+ * any information contained herein except pursuant to this license grant from
+ * Synopsys. If you do not agree with this notice, including the disclaimer
+ * below, then you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS" BASIS
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE HEREBY DISCLAIMED. IN NO EVENT SHALL SYNOPSYS BE LIABLE FOR ANY DIRECT,
+ * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================== */
+
+/** @file
+ *
+ * The Core Interface Layer provides basic services for accessing and
+ * managing the DWC_otg hardware. These services are used by both the
+ * Host Controller Driver and the Peripheral Controller Driver.
+ *
+ * This file contains the Common Interrupt handlers.
+ */
+#include "dwc_os.h"
+#include "dwc_otg_regs.h"
+#include "dwc_otg_cil.h"
+#include "dwc_otg_driver.h"
+#include "dwc_otg_pcd.h"
+#include "dwc_otg_hcd.h"
+
+#ifdef DEBUG
+inline const char *op_state_str(dwc_otg_core_if_t * core_if)
+{
+	return (core_if->op_state == A_HOST ? "a_host" :
+		(core_if->op_state == A_SUSPEND ? "a_suspend" :
+		 (core_if->op_state == A_PERIPHERAL ? "a_peripheral" :
+		  (core_if->op_state == B_PERIPHERAL ? "b_peripheral" :
+		   (core_if->op_state == B_HOST ? "b_host" : "unknown")))));
+}
+#endif
+
+/** This function will log a debug message
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ */
+int32_t dwc_otg_handle_mode_mismatch_intr(dwc_otg_core_if_t * core_if)
+{
+	gintsts_data_t gintsts;
+	DWC_WARN("Mode Mismatch Interrupt: currently in %s mode\n",
+		 dwc_otg_mode(core_if) ? "Host" : "Device");
+
+	/* Clear interrupt */
+	gintsts.d32 = 0;
+	gintsts.b.modemismatch = 1;
+	DWC_WRITE_REG32(&core_if->core_global_regs->gintsts, gintsts.d32);
+	return 1;
+}
+
+/**
+ * This function handles the OTG Interrupts. It reads the OTG
+ * Interrupt Register (GOTGINT) to determine what interrupt has
+ * occurred.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ */
+int32_t dwc_otg_handle_otg_intr(dwc_otg_core_if_t * core_if)
+{
+	dwc_otg_core_global_regs_t *global_regs = core_if->core_global_regs;
+	gotgint_data_t gotgint;
+	gotgctl_data_t gotgctl;
+	gintmsk_data_t gintmsk;
+	gpwrdn_data_t gpwrdn;
+
+	gotgint.d32 = DWC_READ_REG32(&global_regs->gotgint);
+	gotgctl.d32 = DWC_READ_REG32(&global_regs->gotgctl);
+	DWC_DEBUGPL(DBG_CIL, "++OTG Interrupt gotgint=%0x [%s]\n", gotgint.d32,
+		    op_state_str(core_if));
+
+	if (gotgint.b.sesenddet) {
+		DWC_DEBUGPL(DBG_ANY, " ++OTG Interrupt: "
+			    "Session End Detected++ (%s)\n",
+			    op_state_str(core_if));
+		gotgctl.d32 = DWC_READ_REG32(&global_regs->gotgctl);
+
+		if (core_if->op_state == B_HOST) {
+			cil_pcd_start(core_if);
+			core_if->op_state = B_PERIPHERAL;
+		} else {
+			/* If not B_HOST and Device HNP still set. HNP
+			 * Did not succeed!*/
+			if (gotgctl.b.devhnpen) {
+				DWC_DEBUGPL(DBG_ANY, "Session End Detected\n");
+				__DWC_ERROR("Device Not Connected/Responding!\n");
+			}
+
+			/* If Session End Detected the B-Cable has
+			 * been disconnected. */
+			/* Reset PCD and Gadget driver to a
+			 * clean state. */
+			core_if->lx_state = DWC_OTG_L0;
+			DWC_SPINUNLOCK(core_if->lock);
+			cil_pcd_stop(core_if);
+			DWC_SPINLOCK(core_if->lock);
+
+			if (core_if->adp_enable) {
+				if (core_if->power_down == 2) {
+					gpwrdn.d32 = 0;
+					gpwrdn.b.pwrdnswtch = 1;
+					DWC_MODIFY_REG32(&core_if->
+							 core_global_regs->
+							 gpwrdn, gpwrdn.d32, 0);
+				}
+
+				gpwrdn.d32 = 0;
+				gpwrdn.b.pmuintsel = 1;
+				gpwrdn.b.pmuactv = 1;
+				DWC_MODIFY_REG32(&core_if->core_global_regs->
+						 gpwrdn, 0, gpwrdn.d32);
+
+				dwc_otg_adp_sense_start(core_if);
+			}
+		}
+
+#ifndef CONFIG_MACH_M822XX
+		/* Base value of GOTGCTL, VBUS checking enabled */
+		gotgctl.d32 = 0;
+#else  /* CONFIG_MACH_M822XX */
+		/* Base value of GOTGCTL, VBUS checking disabled */
+		gotgctl.d32 = ((1<<3) | (1<<2));
+#endif	/* CONFIG_MACH_M822XX */
+
+		gotgctl.b.devhnpen = 1;
+		DWC_MODIFY_REG32(&global_regs->gotgctl, gotgctl.d32, 0);
+	}
+	if (gotgint.b.sesreqsucstschng) {
+		DWC_DEBUGPL(DBG_ANY, " ++OTG Interrupt: "
+			    "Session Reqeust Success Status Change++\n");
+		gotgctl.d32 = DWC_READ_REG32(&global_regs->gotgctl);
+		if (gotgctl.b.sesreqscs) {
+
+			if ((core_if->core_params->phy_type ==
+			     DWC_PHY_TYPE_PARAM_FS) && (core_if->core_params->i2c_enable))
+			{
+				core_if->srp_success = 1;
+			} else {
+				DWC_SPINUNLOCK(core_if->lock);
+				cil_pcd_resume(core_if);
+				DWC_SPINLOCK(core_if->lock);
+
+				/* Clear Session Request */
+#ifndef CONFIG_MACH_M822XX
+				/* Base value of GOTGCTL, VBUS checking enabled */
+				gotgctl.d32 = 0;
+#else  /* CONFIG_MACH_M822XX */
+				/* Base value of GOTGCTL, VBUS checking disabled */
+				gotgctl.d32 = ((1<<3) | (1<<2));
+#endif	/* CONFIG_MACH_M822XX */
+
+				gotgctl.b.sesreq = 1;
+				DWC_MODIFY_REG32(&global_regs->gotgctl,
+						 gotgctl.d32, 0);
+			}
+		}
+	}
+	if (gotgint.b.hstnegsucstschng) {
+		/* Print statements during the HNP interrupt handling
+		 * can cause it to fail.*/
+		gotgctl.d32 = DWC_READ_REG32(&global_regs->gotgctl);
+		if (gotgctl.b.hstnegscs) {
+			if (dwc_otg_is_host_mode(core_if)) {
+				core_if->op_state = B_HOST;
+				/*
+				 * Need to disable SOF interrupt immediately.
+				 * When switching from device to host, the PCD
+				 * interrupt handler won't handle the
+				 * interrupt if host mode is already set. The
+				 * HCD interrupt handler won't get called if
+				 * the HCD state is HALT. This means that the
+				 * interrupt does not get handled and Linux
+				 * complains loudly.
+				 */
+				gintmsk.d32 = 0;
+				gintmsk.b.sofintr = 1;
+				DWC_MODIFY_REG32(&global_regs->gintmsk,
+						 gintmsk.d32, 0);
+				/* Call callback function with spin lock released */
+				DWC_SPINUNLOCK(core_if->lock);
+				cil_pcd_stop(core_if);
+				/*
+				 * Initialize the Core for Host mode.
+				 */
+				cil_hcd_start(core_if);
+				DWC_SPINLOCK(core_if->lock);
+				core_if->op_state = B_HOST;
+			}
+		} else {
+
+#ifndef CONFIG_MACH_M822XX
+			/* Base value of GOTGCTL, VBUS checking enabled */
+			gotgctl.d32 = 0;
+#else  /* CONFIG_MACH_M822XX */
+			/* Base value of GOTGCTL, VBUS checking disabled */
+			gotgctl.d32 = ((1<<3) | (1<<2));
+#endif	/* CONFIG_MACH_M822XX */
+
+			gotgctl.b.hnpreq = 1;
+			gotgctl.b.devhnpen = 1;
+			DWC_MODIFY_REG32(&global_regs->gotgctl, gotgctl.d32, 0);
+			DWC_DEBUGPL(DBG_ANY, "HNP Failed\n");
+			__DWC_ERROR("Device Not Connected/Responding\n");
+		}
+	}
+	if (gotgint.b.hstnegdet) {
+		/* The disconnect interrupt is set at the same time as
+		 * Host Negotiation Detected.  During the mode
+		 * switch all interrupts are cleared so the disconnect
+		 * interrupt handler will not get executed.
+		 */
+		DWC_DEBUGPL(DBG_ANY, " ++OTG Interrupt: "
+			    "Host Negotiation Detected++ (%s)\n",
+			    (dwc_otg_is_host_mode(core_if) ? "Host" :
+			     "Device"));
+		if (dwc_otg_is_device_mode(core_if)) {
+			DWC_DEBUGPL(DBG_ANY, "a_suspend->a_peripheral (%d)\n",
+				    core_if->op_state);
+			DWC_SPINUNLOCK(core_if->lock);
+			cil_hcd_disconnect(core_if);
+			cil_pcd_start(core_if);
+			DWC_SPINLOCK(core_if->lock);
+			core_if->op_state = A_PERIPHERAL;
+		} else {
+			/*
+			 * Need to disable SOF interrupt immediately. When
+			 * switching from device to host, the PCD interrupt
+			 * handler won't handle the interrupt if host mode is
+			 * already set. The HCD interrupt handler won't get
+			 * called if the HCD state is HALT. This means that
+			 * the interrupt does not get handled and Linux
+			 * complains loudly.
+			 */
+			gintmsk.d32 = 0;
+			gintmsk.b.sofintr = 1;
+			DWC_MODIFY_REG32(&global_regs->gintmsk, gintmsk.d32, 0);
+			DWC_SPINUNLOCK(core_if->lock);
+			cil_pcd_stop(core_if);
+			cil_hcd_start(core_if);
+			DWC_SPINLOCK(core_if->lock);
+			core_if->op_state = A_HOST;
+		}
+	}
+	if (gotgint.b.adevtoutchng) {
+		DWC_DEBUGPL(DBG_ANY, " ++OTG Interrupt: "
+			    "A-Device Timeout Change++\n");
+	}
+	if (gotgint.b.debdone) {
+		DWC_DEBUGPL(DBG_ANY, " ++OTG Interrupt: " "Debounce Done++\n");
+	}
+
+	/* Clear GOTGINT */
+	DWC_WRITE_REG32(&core_if->core_global_regs->gotgint, gotgint.d32);
+
+	return 1;
+}
+
+void w_conn_id_status_change(void *p)
+{
+	dwc_otg_core_if_t *core_if = p;
+	uint32_t count = 0;
+#ifndef CONFIG_MACH_M822XX
+	/* Base value of GOTGCTL, VBUS checking enabled */
+	gotgctl_data_t gotgctl = {.d32 = 0 };
+#else  /* CONFIG_MACH_M822XX */
+	/* Base value of GOTGCTL, VBUS checking disabled */
+	gotgctl_data_t gotgctl = {.d32 = ((1<<3) | (1<<2)) };
+#endif	/* CONFIG_MACH_M822XX */
+
+	gotgctl.d32 = DWC_READ_REG32(&core_if->core_global_regs->gotgctl);
+	DWC_DEBUGPL(DBG_CIL, "gotgctl=%0x\n", gotgctl.d32);
+	DWC_DEBUGPL(DBG_CIL, "gotgctl.b.conidsts=%d\n", gotgctl.b.conidsts);
+
+	/* B-Device connector (Device Mode) */
+	if (gotgctl.b.conidsts) {
+		/* Wait for switch to device mode. */
+		while (!dwc_otg_is_device_mode(core_if)) {
+			DWC_PRINTF("Waiting for Peripheral Mode, Mode=%s\n",
+				   (dwc_otg_is_host_mode(core_if) ? "Host" :
+				    "Peripheral"));
+			dwc_mdelay(100);
+			if (++count > 10000)
+				break;
+		}
+		DWC_ASSERT(++count < 10000,
+			   "Connection id status change timed out");
+		core_if->op_state = B_PERIPHERAL;
+		dwc_otg_core_init(core_if);
+		dwc_otg_enable_global_interrupts(core_if);
+		cil_pcd_start(core_if);
+	} else {
+		/* A-Device connector (Host Mode) */
+		while (!dwc_otg_is_host_mode(core_if)) {
+			DWC_PRINTF("Waiting for Host Mode, Mode=%s\n",
+				   (dwc_otg_is_host_mode(core_if) ? "Host" :
+				    "Peripheral"));
+			dwc_mdelay(100);
+			if (++count > 10000)
+				break;
+		}
+		DWC_ASSERT(++count < 10000,
+			   "Connection id status change timed out");
+		core_if->op_state = A_HOST;
+		/*
+		 * Initialize the Core for Host mode.
+		 */
+		dwc_otg_core_init(core_if);
+		dwc_otg_enable_global_interrupts(core_if);
+		cil_hcd_start(core_if);
+	}
+}
+
+/**
+ * This function handles the Connector ID Status Change Interrupt.  It
+ * reads the OTG Interrupt Register (GOTCTL) to determine whether this
+ * is a Device to Host Mode transition or a Host Mode to Device
+ * Transition. 
+ *
+ * This only occurs when the cable is connected/removed from the PHY
+ * connector.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ */
+int32_t dwc_otg_handle_conn_id_status_change_intr(dwc_otg_core_if_t * core_if)
+{
+
+	/*
+	 * Need to disable SOF interrupt immediately. If switching from device
+	 * to host, the PCD interrupt handler won't handle the interrupt if
+	 * host mode is already set. The HCD interrupt handler won't get
+	 * called if the HCD state is HALT. This means that the interrupt does
+	 * not get handled and Linux complains loudly.
+	 */
+	gintmsk_data_t gintmsk = {.d32 = 0 };
+	gintsts_data_t gintsts = {.d32 = 0 };
+
+	gintmsk.b.sofintr = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gintmsk, gintmsk.d32, 0);
+
+	DWC_DEBUGPL(DBG_CIL,
+		    " ++Connector ID Status Change Interrupt++  (%s)\n",
+		    (dwc_otg_is_host_mode(core_if) ? "Host" : "Device"));
+
+	if (core_if->lock)
+		DWC_SPINUNLOCK(core_if->lock);
+
+	/*
+	 * Need to schedule a work, as there are possible DELAY function calls
+	 * Release lock before scheduling workq as it holds spinlock during scheduling
+	 */
+
+	DWC_WORKQ_SCHEDULE(core_if->wq_otg, w_conn_id_status_change,
+			   core_if, "connection id status change");
+
+	if (core_if->lock)
+		DWC_SPINLOCK(core_if->lock);
+
+	/* Set flag and clear interrupt */
+	gintsts.b.conidstschng = 1;
+	DWC_WRITE_REG32(&core_if->core_global_regs->gintsts, gintsts.d32);
+
+	return 1;
+}
+
+/**
+ * This interrupt indicates that a device is initiating the Session
+ * Request Protocol to request the host to turn on bus power so a new
+ * session can begin. The handler responds by turning on bus power. If
+ * the DWC_otg controller is in low power mode, the handler brings the
+ * controller out of low power mode before turning on bus power.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ */
+int32_t dwc_otg_handle_session_req_intr(dwc_otg_core_if_t * core_if)
+{
+	gintsts_data_t gintsts;
+
+#ifndef DWC_HOST_ONLY
+	DWC_DEBUGPL(DBG_ANY, "++Session Request Interrupt++\n");
+
+	if (dwc_otg_is_device_mode(core_if)) {
+		DWC_PRINTF("SRP: Device mode\n");
+	} else {
+		hprt0_data_t hprt0;
+		DWC_PRINTF("SRP: Host mode\n");
+
+		/* Turn on the port power bit. */
+		hprt0.d32 = dwc_otg_read_hprt0(core_if);
+		hprt0.b.prtpwr = 1;
+		DWC_WRITE_REG32(core_if->host_if->hprt0, hprt0.d32);
+
+		/* Start the Connection timer. So a message can be displayed
+		 * if connect does not occur within 10 seconds. */
+		cil_hcd_session_start(core_if);
+	}
+#endif
+
+	/* Clear interrupt */
+	gintsts.d32 = 0;
+	gintsts.b.sessreqintr = 1;
+	DWC_WRITE_REG32(&core_if->core_global_regs->gintsts, gintsts.d32);
+
+	return 1;
+}
+
+void w_wakeup_detected(void *p)
+{
+	dwc_otg_core_if_t *core_if = (dwc_otg_core_if_t *) p;
+	/*
+	 * Clear the Resume after 70ms. (Need 20 ms minimum. Use 70 ms
+	 * so that OPT tests pass with all PHYs).
+	 */
+	hprt0_data_t hprt0 = {.d32 = 0 };
+
+#ifndef CONFIG_MACH_M822XX
+	pcgcctl_data_t pcgcctl = {.d32 = 0 };
+	/* Restart the Phy Clock */
+	pcgcctl.b.stoppclk = 1;
+	DWC_MODIFY_REG32(core_if->pcgcctl, pcgcctl.d32, 0);
+	dwc_udelay(10);
+#endif	/* CONFIG_MACH_M822XX */
+
+	hprt0.d32 = dwc_otg_read_hprt0(core_if);
+	DWC_DEBUGPL(DBG_ANY, "Resume: HPRT0=%0x\n", hprt0.d32);
+	/* dwc_mdelay(70); */
+	hprt0.b.prtres = 0;	/* Resume */
+	DWC_WRITE_REG32(core_if->host_if->hprt0, hprt0.d32);
+	DWC_DEBUGPL(DBG_ANY, "Clear Resume: HPRT0=%0x\n",
+		    DWC_READ_REG32(core_if->host_if->hprt0));
+
+	cil_hcd_resume(core_if);
+
+	/** Change to L0 state*/
+	core_if->lx_state = DWC_OTG_L0;
+}
+
+/**
+ * This interrupt indicates that the DWC_otg controller has detected a
+ * resume or remote wakeup sequence. If the DWC_otg controller is in
+ * low power mode, the handler must brings the controller out of low
+ * power mode. The controller automatically begins resume
+ * signaling. The handler schedules a time to stop resume signaling.
+ */
+int32_t dwc_otg_handle_wakeup_detected_intr(dwc_otg_core_if_t * core_if)
+{
+	gintsts_data_t gintsts;
+
+	DWC_DEBUGPL(DBG_ANY,
+		    "++Resume and Remote Wakeup Detected Interrupt++\n");
+
+	DWC_PRINTF("%s lxstate = %d\n", __func__, core_if->lx_state);
+
+	if (dwc_otg_is_device_mode(core_if)) {
+		dctl_data_t dctl = {.d32 = 0 };
+		DWC_DEBUGPL(DBG_PCD, "DSTS=0x%0x\n",
+			    DWC_READ_REG32(&core_if->dev_if->
+					   dev_global_regs->dsts));
+		if (core_if->lx_state == DWC_OTG_L2) {
+#ifdef PARTIAL_POWER_DOWN
+			if (core_if->hwcfg4.b.power_optimiz) {
+				pcgcctl_data_t power = {.d32 = 0 };
+
+				power.d32 = DWC_READ_REG32(core_if->pcgcctl);
+				DWC_DEBUGPL(DBG_CIL, "PCGCCTL=%0x\n",
+					    power.d32);
+
+				power.b.stoppclk = 0;
+				DWC_WRITE_REG32(core_if->pcgcctl, power.d32);
+
+				power.b.pwrclmp = 0;
+				DWC_WRITE_REG32(core_if->pcgcctl, power.d32);
+
+				power.b.rstpdwnmodule = 0;
+				DWC_WRITE_REG32(core_if->pcgcctl, power.d32);
+			}
+#endif
+			/* Clear the Remote Wakeup Signaling */
+			dctl.b.rmtwkupsig = 1;
+			DWC_MODIFY_REG32(&core_if->dev_if->
+					 dev_global_regs->dctl, dctl.d32, 0);
+
+			DWC_SPINUNLOCK(core_if->lock);
+			if (core_if->pcd_cb && core_if->pcd_cb->resume_wakeup) {
+				core_if->pcd_cb->resume_wakeup(core_if->pcd_cb->p);
+			}
+			DWC_SPINLOCK(core_if->lock);
+		} else {
+			glpmcfg_data_t lpmcfg;
+			lpmcfg.d32 =
+			    DWC_READ_REG32(&core_if->core_global_regs->glpmcfg);
+			lpmcfg.b.hird_thres &= (~(1 << 4));
+			DWC_WRITE_REG32(&core_if->core_global_regs->glpmcfg,
+					lpmcfg.d32);
+		}
+		/** Change to L0 state*/
+		core_if->lx_state = DWC_OTG_L0;
+	} else {
+		if (core_if->lx_state != DWC_OTG_L1) {
+			pcgcctl_data_t pcgcctl = {.d32 = 0 };
+
+			/* Restart the Phy Clock */
+			pcgcctl.b.stoppclk = 1;
+			DWC_MODIFY_REG32(core_if->pcgcctl, pcgcctl.d32, 0);
+			DWC_TIMER_SCHEDULE(core_if->wkp_timer, 71);
+		} else {
+			/** Change to L0 state*/
+			core_if->lx_state = DWC_OTG_L0;
+		}
+	}
+
+	/* Clear interrupt */
+	gintsts.d32 = 0;
+	gintsts.b.wkupintr = 1;
+	DWC_WRITE_REG32(&core_if->core_global_regs->gintsts, gintsts.d32);
+
+	return 1;
+}
+
+/**
+ * This interrupt indicates that the Wakeup Logic has detected a
+ * Device disconnect.
+ */
+static int32_t dwc_otg_handle_pwrdn_disconnect_intr(dwc_otg_core_if_t *core_if)
+{
+	gpwrdn_data_t gpwrdn = { .d32 = 0 };
+	gpwrdn_data_t gpwrdn_temp = { .d32 = 0 };
+	gpwrdn_temp.d32 = DWC_READ_REG32(&core_if->core_global_regs->gpwrdn);
+
+	DWC_PRINTF("%s called\n", __FUNCTION__);
+
+	if (!core_if->hibernation_suspend) {
+		DWC_PRINTF("Already exited from Hibernation\n");
+		return 1;
+	}
+
+	/* Switch on the voltage to the core */
+	gpwrdn.b.pwrdnswtch = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+	dwc_udelay(10);
+
+	/* Reset the core */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.pwrdnrstn = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+	dwc_udelay(10);
+
+	/* Disable power clamps*/
+	gpwrdn.d32 = 0;
+	gpwrdn.b.pwrdnclmp = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+
+	/* Remove reset the core signal */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.pwrdnrstn = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, 0, gpwrdn.d32);
+	dwc_udelay(10);
+
+	/* Disable PMU interrupt */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.pmuintsel = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+
+	core_if->hibernation_suspend = 0;
+
+	/* Disable PMU */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.pmuactv = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+	dwc_udelay(10);
+
+	if (gpwrdn_temp.b.idsts) {
+		core_if->op_state = B_PERIPHERAL;
+		dwc_otg_core_init(core_if);
+		dwc_otg_enable_global_interrupts(core_if);
+		cil_pcd_start(core_if);
+	} else {
+		core_if->op_state = A_HOST;
+		dwc_otg_core_init(core_if);
+		dwc_otg_enable_global_interrupts(core_if);
+		cil_hcd_start(core_if);
+	}
+
+	return 1;
+}
+
+/**
+ * This interrupt indicates that the Wakeup Logic has detected a
+ * remote wakeup sequence.
+ */
+static int32_t dwc_otg_handle_pwrdn_wakeup_detected_intr(dwc_otg_core_if_t * core_if)
+{
+	gpwrdn_data_t gpwrdn = {.d32 = 0 };
+	DWC_DEBUGPL(DBG_ANY,
+		    "++Powerdown Remote Wakeup Detected Interrupt++\n");
+
+	if (!core_if->hibernation_suspend) {
+		DWC_PRINTF("Already exited from Hibernation\n");
+		return 1;
+	}
+
+	gpwrdn.d32 = DWC_READ_REG32(&core_if->core_global_regs->gpwrdn);
+	if (gpwrdn.b.idsts) {	// Device Mode
+		if ((core_if->power_down == 2)
+		    && (core_if->hibernation_suspend == 1)) {
+			dwc_otg_device_hibernation_restore(core_if, 0, 0);
+		}
+	} else {
+		if ((core_if->power_down == 2)
+		    && (core_if->hibernation_suspend == 1)) {
+			dwc_otg_host_hibernation_restore(core_if, 1, 0);
+		}
+	}
+	return 1;
+}
+
+static int32_t dwc_otg_handle_pwrdn_idsts_change(dwc_otg_device_t *otg_dev)
+{
+	gpwrdn_data_t gpwrdn = {.d32 = 0 };
+	gpwrdn_data_t gpwrdn_temp = {.d32 = 0 };
+	dwc_otg_core_if_t *core_if = otg_dev->core_if;
+
+	DWC_DEBUGPL(DBG_ANY, "%s called\n", __FUNCTION__);
+	gpwrdn_temp.d32 = DWC_READ_REG32(&core_if->core_global_regs->gpwrdn);
+	if (core_if->power_down == 2)
+	{		
+		if (!core_if->hibernation_suspend) {
+			DWC_PRINTF("Already exited from Hibernation\n");
+			return 1;
+		}
+		DWC_DEBUGPL(DBG_ANY, "Exit from hibernation on ID sts change\n");
+		/* Switch on the voltage to the core */
+		gpwrdn.b.pwrdnswtch = 1;
+		DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+		dwc_udelay(10);
+
+		/* Reset the core */
+		gpwrdn.d32 = 0;
+		gpwrdn.b.pwrdnrstn = 1;
+		DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+		dwc_udelay(10);
+
+		/* Disable power clamps */
+		gpwrdn.d32 = 0;
+		gpwrdn.b.pwrdnclmp = 1;
+		DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+
+		/* Remove reset the core signal */
+		gpwrdn.d32 = 0;
+		gpwrdn.b.pwrdnrstn = 1;
+		DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, 0, gpwrdn.d32);
+		dwc_udelay(10);
+
+		/* Disable PMU interrupt */
+		gpwrdn.d32 = 0;
+		gpwrdn.b.pmuintsel = 1;
+		DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+
+		/*Indicates that we are exiting from hibernation */
+		core_if->hibernation_suspend = 0;
+
+		/* Disable PMU */
+		gpwrdn.d32 = 0;
+		gpwrdn.b.pmuactv = 1;
+		DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+		dwc_udelay(10);
+
+		gpwrdn.d32 = core_if->gr_backup->gpwrdn_local;
+		if (gpwrdn.b.dis_vbus == 1) {
+			gpwrdn.d32 = 0;
+			gpwrdn.b.dis_vbus = 1;
+			DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+		}
+
+		if (gpwrdn_temp.b.idsts) {
+			core_if->op_state = B_PERIPHERAL;
+			dwc_otg_core_init(core_if);
+			dwc_otg_enable_global_interrupts(core_if);
+			cil_pcd_start(core_if);
+		} else {
+			core_if->op_state = A_HOST;
+			dwc_otg_core_init(core_if);
+			dwc_otg_enable_global_interrupts(core_if);
+			cil_hcd_start(core_if);
+		}
+	}
+
+	if (core_if->adp_enable)
+	{
+		uint8_t is_host = 0;
+		DWC_SPINUNLOCK(core_if->lock);
+		/* Change the core_if's lock to hcd/pcd lock depend on mode? */
+#ifndef DWC_HOST_ONLY		
+		if (gpwrdn_temp.b.idsts)
+			core_if->lock = otg_dev->pcd->lock;
+#endif
+#ifndef DWC_DEVICE_ONLY
+		if (!gpwrdn_temp.b.idsts) {
+				core_if->lock = otg_dev->hcd->lock;	
+				is_host = 1;
+		}
+#endif
+		DWC_PRINTF("RESTART ADP\n");
+		if (core_if->adp.probe_enabled)		
+			dwc_otg_adp_probe_stop(core_if);
+		if (core_if->adp.sense_enabled)		
+			dwc_otg_adp_sense_stop(core_if);
+		if (core_if->adp.sense_timer_started)		
+			DWC_TIMER_CANCEL(core_if->adp.sense_timer);
+		if (core_if->adp.vbuson_timer_started)		
+			DWC_TIMER_CANCEL(core_if->adp.vbuson_timer);
+		core_if->adp.probe_timer_values[0] = -1;
+		core_if->adp.probe_timer_values[1] = -1;
+		core_if->adp.sense_timer_started = 0;
+		core_if->adp.vbuson_timer_started = 0;
+		core_if->adp.probe_counter = 0;
+		core_if->adp.gpwrdn = 0;
+		
+		/* Disable PMU and restart ADP */
+		gpwrdn_temp.d32 = 0;
+		gpwrdn_temp.b.pmuactv = 1;
+		gpwrdn_temp.b.pmuintsel = 1;
+		DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+		DWC_PRINTF("Check point 1\n");
+		dwc_mdelay(110);
+		dwc_otg_adp_start(core_if, is_host);
+		DWC_SPINLOCK(core_if->lock);
+	}
+	
+
+	return 1;
+}
+
+static int32_t dwc_otg_handle_pwrdn_session_change(dwc_otg_core_if_t * core_if)
+{
+	gpwrdn_data_t gpwrdn = {.d32 = 0 };
+	int32_t otg_cap_param = core_if->core_params->otg_cap;
+	DWC_DEBUGPL(DBG_ANY, "%s called\n", __FUNCTION__);
+
+	gpwrdn.d32 = DWC_READ_REG32(&core_if->core_global_regs->gpwrdn);
+	if (core_if->power_down == 2) {
+		if (!core_if->hibernation_suspend) {
+			DWC_PRINTF("Already exited from Hibernation\n");
+			return 1;
+		}
+
+		if ((otg_cap_param != DWC_OTG_CAP_PARAM_HNP_SRP_CAPABLE ||
+			 otg_cap_param != DWC_OTG_CAP_PARAM_SRP_ONLY_CAPABLE) &&
+			gpwrdn.b.bsessvld == 0) {
+			/* Save gpwrdn register for further usage if stschng interrupt */
+			core_if->gr_backup->gpwrdn_local =
+				DWC_READ_REG32(&core_if->core_global_regs->gpwrdn);
+			/*Exit from ISR and wait for stschng interrupt with bsessvld = 1 */
+			return 1;
+		}
+
+		/* Switch on the voltage to the core */
+		gpwrdn.d32 = 0;
+		gpwrdn.b.pwrdnswtch = 1;
+		DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+		dwc_udelay(10);
+
+		/* Reset the core */
+		gpwrdn.d32 = 0;
+		gpwrdn.b.pwrdnrstn = 1;
+		DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+		dwc_udelay(10);
+
+		/* Disable power clamps */
+		gpwrdn.d32 = 0;
+		gpwrdn.b.pwrdnclmp = 1;
+		DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+
+		/* Remove reset the core signal */
+		gpwrdn.d32 = 0;
+		gpwrdn.b.pwrdnrstn = 1;
+		DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, 0, gpwrdn.d32);
+		dwc_udelay(10);
+
+		/* Disable PMU interrupt */
+		gpwrdn.d32 = 0;
+		gpwrdn.b.pmuintsel = 1;
+		DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+		dwc_udelay(10);
+
+		/*Indicates that we are exiting from hibernation */
+		core_if->hibernation_suspend = 0;
+
+		/* Disable PMU */
+		gpwrdn.d32 = 0;
+		gpwrdn.b.pmuactv = 1;
+		DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+		dwc_udelay(10);
+
+		core_if->op_state = B_PERIPHERAL;
+		dwc_otg_core_init(core_if);
+		dwc_otg_enable_global_interrupts(core_if);
+		cil_pcd_start(core_if);
+
+		if (otg_cap_param == DWC_OTG_CAP_PARAM_HNP_SRP_CAPABLE ||
+			otg_cap_param == DWC_OTG_CAP_PARAM_SRP_ONLY_CAPABLE) {
+			/*
+			 * Initiate SRP after initial ADP probe.
+			 */
+			dwc_otg_initiate_srp(core_if);	
+		}
+	}
+
+	return 1;
+}
+/**
+ * This interrupt indicates that the Wakeup Logic has detected a
+ * status change either on IDDIG or BSessVld.
+ */
+static uint32_t dwc_otg_handle_pwrdn_stschng_intr(dwc_otg_device_t *otg_dev)
+{
+	int retval;
+	gpwrdn_data_t gpwrdn = {.d32 = 0 };
+	gpwrdn_data_t gpwrdn_temp = {.d32 = 0 };
+	dwc_otg_core_if_t *core_if = otg_dev->core_if;
+
+	DWC_PRINTF("%s called\n", __FUNCTION__);
+	
+	if (core_if->power_down == 2) {
+		if (core_if->hibernation_suspend <= 0) {
+			DWC_PRINTF("Already exited from Hibernation\n");
+			return 1;
+		} else
+			gpwrdn_temp.d32 = core_if->gr_backup->gpwrdn_local;
+
+	} else {
+		gpwrdn_temp.d32 = core_if->adp.gpwrdn;
+	}
+
+	gpwrdn.d32 = DWC_READ_REG32(&core_if->core_global_regs->gpwrdn);
+	
+	if (gpwrdn.b.idsts ^ gpwrdn_temp.b.idsts) {
+		retval = dwc_otg_handle_pwrdn_idsts_change(otg_dev);
+	} else if (gpwrdn.b.bsessvld ^ gpwrdn_temp.b.bsessvld) {
+		retval = dwc_otg_handle_pwrdn_session_change(core_if);
+	}
+
+	return retval;
+}
+
+/**
+ * This interrupt indicates that the Wakeup Logic has detected a
+ * SRP.
+ */
+static int32_t dwc_otg_handle_pwrdn_srp_intr(dwc_otg_core_if_t * core_if)
+{
+	gpwrdn_data_t gpwrdn = {.d32 = 0 };
+
+	DWC_PRINTF("%s called\n", __FUNCTION__);
+
+	if (!core_if->hibernation_suspend) {
+		DWC_PRINTF("Already exited from Hibernation\n");
+		return 1;
+	}
+#ifdef DWC_DEV_SRPCAP
+	if (core_if->pwron_timer_started) {
+		core_if->pwron_timer_started = 0;
+		DWC_TIMER_CANCEL(core_if->pwron_timer);
+	}
+#endif
+
+	/* Switch on the voltage to the core */
+	gpwrdn.b.pwrdnswtch = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+	dwc_udelay(10);
+
+	/* Reset the core */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.pwrdnrstn = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+	dwc_udelay(10);
+
+	/* Disable power clamps */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.pwrdnclmp = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+
+	/* Remove reset the core signal */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.pwrdnrstn = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, 0, gpwrdn.d32);
+	dwc_udelay(10);
+
+	/* Disable PMU interrupt */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.pmuintsel = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+
+	/* Indicates that we are exiting from hibernation */
+	core_if->hibernation_suspend = 0;
+
+	/* Disable PMU */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.pmuactv = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+	dwc_udelay(10);
+
+	/* Programm Disable VBUS to 0 */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.dis_vbus = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+
+	/*Initialize the core as Host */
+	core_if->op_state = A_HOST;
+	dwc_otg_core_init(core_if);
+	dwc_otg_enable_global_interrupts(core_if);
+	cil_hcd_start(core_if);
+
+	return 1;
+}
+
+/** This interrupt indicates that restore command after Hibernation
+ * was completed by the core. */
+int32_t dwc_otg_handle_restore_done_intr(dwc_otg_core_if_t * core_if)
+{
+	pcgcctl_data_t pcgcctl;
+	DWC_DEBUGPL(DBG_ANY, "++Restore Done Interrupt++\n");
+
+	//TODO De-assert restore signal. 8.a
+	pcgcctl.d32 = DWC_READ_REG32(core_if->pcgcctl);
+	if (pcgcctl.b.restoremode == 1) {
+		gintmsk_data_t gintmsk = {.d32 = 0 };
+		/*
+		 * If restore mode is Remote Wakeup,
+		 * unmask Remote Wakeup interrupt.
+		 */
+		gintmsk.b.wkupintr = 1;
+		DWC_MODIFY_REG32(&core_if->core_global_regs->gintmsk,
+				 0, gintmsk.d32);
+	}
+
+	return 1;
+}
+
+/**
+ * This interrupt indicates that a device has been disconnected from
+ * the root port.
+ */
+int32_t dwc_otg_handle_disconnect_intr(dwc_otg_core_if_t * core_if)
+{
+	gintsts_data_t gintsts;
+
+	DWC_DEBUGPL(DBG_ANY, "++Disconnect Detected Interrupt++ (%s) %s\n",
+		    (dwc_otg_is_host_mode(core_if) ? "Host" : "Device"),
+		    op_state_str(core_if));
+
+/** @todo Consolidate this if statement. */
+#ifndef DWC_HOST_ONLY
+	if (core_if->op_state == B_HOST) {
+		/* If in device mode Disconnect and stop the HCD, then
+		 * start the PCD. */
+		DWC_SPINUNLOCK(core_if->lock);
+		cil_hcd_disconnect(core_if);
+		cil_pcd_start(core_if);
+		DWC_SPINLOCK(core_if->lock);
+		core_if->op_state = B_PERIPHERAL;
+	} else if (dwc_otg_is_device_mode(core_if)) {
+#ifndef CONFIG_MACH_M822XX
+		/* Base value of GOTGCTL, VBUS checking enabled */
+		gotgctl_data_t gotgctl = {.d32 = 0 };
+#else  /* CONFIG_MACH_M822XX */
+		/* Base value of GOTGCTL, VBUS checking disabled */
+		gotgctl_data_t gotgctl = {.d32 = ((1<<3) | (1<<2)) };
+#endif	/* CONFIG_MACH_M822XX */
+		gotgctl.d32 =
+		    DWC_READ_REG32(&core_if->core_global_regs->gotgctl);
+		if (gotgctl.b.hstsethnpen == 1) {
+			/* Do nothing, if HNP in process the OTG
+			 * interrupt "Host Negotiation Detected"
+			 * interrupt will do the mode switch.
+			 */
+		} else if (gotgctl.b.devhnpen == 0) {
+			/* If in device mode Disconnect and stop the HCD, then
+			 * start the PCD. */
+			DWC_SPINUNLOCK(core_if->lock);
+			cil_hcd_disconnect(core_if);
+			cil_pcd_start(core_if);
+			DWC_SPINLOCK(core_if->lock);
+			core_if->op_state = B_PERIPHERAL;
+		} else {
+			DWC_DEBUGPL(DBG_ANY, "!a_peripheral && !devhnpen\n");
+		}
+	} else {
+		if (core_if->op_state == A_HOST) {
+			/* A-Cable still connected but device disconnected. */
+			cil_hcd_disconnect(core_if);
+			if (core_if->adp_enable) {
+				gpwrdn_data_t gpwrdn = { .d32 = 0 };
+				cil_hcd_stop(core_if);
+				/* Enable Power Down Logic */
+				gpwrdn.b.pmuintsel = 1;
+				gpwrdn.b.pmuactv = 1;
+				DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, 0, gpwrdn.d32);
+				dwc_otg_adp_probe_start(core_if);
+
+				/* Power off the core */
+				if (core_if->power_down == 2) {
+					gpwrdn.d32 = 0;
+					gpwrdn.b.pwrdnswtch = 1;
+					DWC_MODIFY_REG32(&core_if->
+							 core_global_regs->
+							 gpwrdn, gpwrdn.d32, 0);
+				}
+			}
+		}
+	}
+#endif
+	/* Change to L3(OFF) state */
+	core_if->lx_state = DWC_OTG_L3;
+
+	gintsts.d32 = 0;
+	gintsts.b.disconnect = 1;
+	DWC_WRITE_REG32(&core_if->core_global_regs->gintsts, gintsts.d32);
+	return 1;
+}
+
+/**
+ * This interrupt indicates that SUSPEND state has been detected on
+ * the USB.
+ *
+ * For HNP the USB Suspend interrupt signals the change from
+ * "a_peripheral" to "a_host".
+ *
+ * When power management is enabled the core will be put in low power
+ * mode.
+ */
+int32_t dwc_otg_handle_usb_suspend_intr(dwc_otg_core_if_t * core_if)
+{
+	dsts_data_t dsts;
+	gintsts_data_t gintsts;
+	dcfg_data_t dcfg;
+
+	DWC_DEBUGPL(DBG_ANY, "USB SUSPEND\n");
+
+	if (dwc_otg_is_device_mode(core_if)) {
+		/* Check the Device status register to determine if the Suspend
+		 * state is active. */
+		dsts.d32 =
+		    DWC_READ_REG32(&core_if->dev_if->dev_global_regs->dsts);
+		DWC_DEBUGPL(DBG_PCD, "DSTS=0x%0x\n", dsts.d32);
+		DWC_DEBUGPL(DBG_PCD, "DSTS.Suspend Status=%d "
+			    "HWCFG4.power Optimize=%d\n",
+			    dsts.b.suspsts, core_if->hwcfg4.b.power_optimiz);
+
+#ifdef PARTIAL_POWER_DOWN
+/** @todo Add a module parameter for power management. */
+
+		if (dsts.b.suspsts && core_if->hwcfg4.b.power_optimiz) {
+			pcgcctl_data_t power = {.d32 = 0 };
+			DWC_DEBUGPL(DBG_CIL, "suspend\n");
+
+			power.b.pwrclmp = 1;
+			DWC_WRITE_REG32(core_if->pcgcctl, power.d32);
+
+			power.b.rstpdwnmodule = 1;
+			DWC_MODIFY_REG32(core_if->pcgcctl, 0, power.d32);
+
+			power.b.stoppclk = 1;
+			DWC_MODIFY_REG32(core_if->pcgcctl, 0, power.d32);
+
+		} else {
+			DWC_DEBUGPL(DBG_ANY, "disconnect?\n");
+		}
+#endif
+		/* PCD callback for suspend. Release the lock inside of callback function */
+		cil_pcd_suspend(core_if);
+		if (core_if->power_down == 2)
+		{
+			dcfg.d32 = DWC_READ_REG32(&core_if->dev_if->dev_global_regs->dcfg);
+			DWC_DEBUGPL(DBG_ANY,"lx_state = %08x\n",core_if->lx_state);
+			DWC_DEBUGPL(DBG_ANY," device address = %08d\n",dcfg.b.devaddr);
+
+			if (core_if->lx_state != DWC_OTG_L3 && dcfg.b.devaddr) {
+				pcgcctl_data_t pcgcctl = {.d32 = 0 };
+				gpwrdn_data_t gpwrdn = {.d32 = 0 };
+				gusbcfg_data_t gusbcfg = {.d32 = 0 };
+
+				/* Change to L2(suspend) state */
+				core_if->lx_state = DWC_OTG_L2;
+
+				/* Clear interrupt in gintsts */
+				gintsts.d32 = 0;
+				gintsts.b.usbsuspend = 1;
+				DWC_WRITE_REG32(&core_if->core_global_regs->
+						gintsts, gintsts.d32);
+				DWC_PRINTF("Start of hibernation completed\n");
+				dwc_otg_save_global_regs(core_if);
+				dwc_otg_save_dev_regs(core_if);
+
+				gusbcfg.d32 =
+				    DWC_READ_REG32(&core_if->core_global_regs->
+						   gusbcfg);
+				if (gusbcfg.b.ulpi_utmi_sel == 1) {
+					/* ULPI interface */
+					/* Suspend the Phy Clock */
+					pcgcctl.d32 = 0;
+					pcgcctl.b.stoppclk = 1;
+					DWC_MODIFY_REG32(core_if->pcgcctl, 0,
+							 pcgcctl.d32);
+					dwc_udelay(10);
+					gpwrdn.b.pmuactv = 1;
+					DWC_MODIFY_REG32(&core_if->
+							 core_global_regs->
+							 gpwrdn, 0, gpwrdn.d32);
+				} else {
+					/* UTMI+ Interface */
+					gpwrdn.b.pmuactv = 1;
+					DWC_MODIFY_REG32(&core_if->
+							 core_global_regs->
+							 gpwrdn, 0, gpwrdn.d32);
+					dwc_udelay(10);
+					pcgcctl.b.stoppclk = 1;
+					DWC_MODIFY_REG32(core_if->pcgcctl, 0,
+							 pcgcctl.d32);
+					dwc_udelay(10);
+				}
+
+				/* Set flag to indicate that we are in hibernation */
+				core_if->hibernation_suspend = 1;
+				/* Enable interrupts from wake up logic */
+				gpwrdn.d32 = 0;
+				gpwrdn.b.pmuintsel = 1;
+				DWC_MODIFY_REG32(&core_if->core_global_regs->
+						 gpwrdn, 0, gpwrdn.d32);
+				dwc_udelay(10);
+
+				/* Unmask device mode interrupts in GPWRDN */
+				gpwrdn.d32 = 0;
+				gpwrdn.b.rst_det_msk = 1;
+				gpwrdn.b.lnstchng_msk = 1;
+				gpwrdn.b.sts_chngint_msk = 1;
+				DWC_MODIFY_REG32(&core_if->core_global_regs->
+						 gpwrdn, 0, gpwrdn.d32);
+				dwc_udelay(10);
+
+				/* Enable Power Down Clamp */
+				gpwrdn.d32 = 0;
+				gpwrdn.b.pwrdnclmp = 1;
+				DWC_MODIFY_REG32(&core_if->core_global_regs->
+						 gpwrdn, 0, gpwrdn.d32);
+				dwc_udelay(10);
+
+				/* Switch off VDD */
+				gpwrdn.d32 = 0;
+				gpwrdn.b.pwrdnswtch = 1;
+				DWC_MODIFY_REG32(&core_if->core_global_regs->
+						 gpwrdn, 0, gpwrdn.d32);
+
+				/* Save gpwrdn register for further usage if stschng interrupt */
+				core_if->gr_backup->gpwrdn_local =
+							DWC_READ_REG32(&core_if->core_global_regs->gpwrdn);
+				DWC_PRINTF("Hibernation completed\n");
+
+				return 1;
+			}
+		}
+	} else {
+		if (core_if->op_state == A_PERIPHERAL) {
+			DWC_DEBUGPL(DBG_ANY, "a_peripheral->a_host\n");
+			/* Clear the a_peripheral flag, back to a_host. */
+			DWC_SPINUNLOCK(core_if->lock);
+			cil_pcd_stop(core_if);
+			cil_hcd_start(core_if);
+			DWC_SPINLOCK(core_if->lock);
+			core_if->op_state = A_HOST;
+		}
+	}
+
+	/* Change to L2(suspend) state */
+	core_if->lx_state = DWC_OTG_L2;
+
+	/* Clear interrupt */
+	gintsts.d32 = 0;
+	gintsts.b.usbsuspend = 1;
+	DWC_WRITE_REG32(&core_if->core_global_regs->gintsts, gintsts.d32);
+
+	return 1;
+}
+
+#ifdef CONFIG_USB_DWC_OTG_LPM
+/**
+ * This function hadles LPM transaction received interrupt.
+ */
+static int32_t dwc_otg_handle_lpm_intr(dwc_otg_core_if_t * core_if)
+{
+	glpmcfg_data_t lpmcfg;
+	gintsts_data_t gintsts;
+
+	if (!core_if->core_params->lpm_enable) {
+		DWC_PRINTF("Unexpected LPM interrupt\n");
+	}
+
+	lpmcfg.d32 = DWC_READ_REG32(&core_if->core_global_regs->glpmcfg);
+	DWC_PRINTF("LPM config register = 0x%08x\n", lpmcfg.d32);
+
+	if (dwc_otg_is_host_mode(core_if)) {
+		cil_hcd_sleep(core_if);
+	} else {
+		lpmcfg.b.hird_thres |= (1 << 4);
+		DWC_WRITE_REG32(&core_if->core_global_regs->glpmcfg,
+				lpmcfg.d32);
+	}
+
+	/* Examine prt_sleep_sts after TL1TokenTetry period max (10 us) */
+	dwc_udelay(10);
+	lpmcfg.d32 = DWC_READ_REG32(&core_if->core_global_regs->glpmcfg);
+	if (lpmcfg.b.prt_sleep_sts) {
+		/* Save the current state */
+		core_if->lx_state = DWC_OTG_L1;
+	}
+
+	/* Clear interrupt  */
+	gintsts.d32 = 0;
+	gintsts.b.lpmtranrcvd = 1;
+	DWC_WRITE_REG32(&core_if->core_global_regs->gintsts, gintsts.d32);
+	return 1;
+}
+#endif /* CONFIG_USB_DWC_OTG_LPM */
+
+/**
+ * This function returns the Core Interrupt register.
+ */
+static inline uint32_t dwc_otg_read_common_intr(dwc_otg_core_if_t * core_if)
+{
+	gahbcfg_data_t gahbcfg = {.d32 = 0 };
+	gintsts_data_t gintsts;
+	gintmsk_data_t gintmsk;
+	gintmsk_data_t gintmsk_common = {.d32 = 0 };
+	gintmsk_common.b.wkupintr = 1;
+	gintmsk_common.b.sessreqintr = 1;
+	gintmsk_common.b.conidstschng = 1;
+	gintmsk_common.b.otgintr = 1;
+	gintmsk_common.b.modemismatch = 1;
+	gintmsk_common.b.disconnect = 1;
+	gintmsk_common.b.usbsuspend = 1;
+#ifdef CONFIG_USB_DWC_OTG_LPM
+	gintmsk_common.b.lpmtranrcvd = 1;
+#endif
+	gintmsk_common.b.restoredone = 1;
+	/** @todo: The port interrupt occurs while in device
+         * mode. Added code to CIL to clear the interrupt for now!
+         */
+	gintmsk_common.b.portintr = 1;
+
+	gintsts.d32 = DWC_READ_REG32(&core_if->core_global_regs->gintsts);
+	gintmsk.d32 = DWC_READ_REG32(&core_if->core_global_regs->gintmsk);
+	gahbcfg.d32 = DWC_READ_REG32(&core_if->core_global_regs->gahbcfg);
+
+#ifdef DEBUG
+	/* if any common interrupts set */
+	if (gintsts.d32 & gintmsk_common.d32) {
+		DWC_DEBUGPL(DBG_ANY, "gintsts=%08x  gintmsk=%08x\n",
+			    gintsts.d32, gintmsk.d32);
+	}
+#endif
+	if (gahbcfg.b.glblintrmsk)	
+		return ((gintsts.d32 & gintmsk.d32) & gintmsk_common.d32);
+	else
+		return 0;
+
+}
+
+/* MACRO for clearing interupt bits in GPWRDN register */
+#define CLEAR_GPWRDN_INTR(__core_if,__intr) \
+do { \
+		gpwrdn_data_t gpwrdn = {.d32=0}; \
+		gpwrdn.b.__intr = 1; \
+		DWC_MODIFY_REG32(&__core_if->core_global_regs->gpwrdn, \
+		0, gpwrdn.d32); \
+} while (0)
+
+/**
+ * Common interrupt handler.
+ *
+ * The common interrupts are those that occur in both Host and Device mode.
+ * This handler handles the following interrupts:
+ * - Mode Mismatch Interrupt
+ * - Disconnect Interrupt
+ * - OTG Interrupt
+ * - Connector ID Status Change Interrupt
+ * - Session Request Interrupt.
+ * - Resume / Remote Wakeup Detected Interrupt.
+ * - LPM Transaction Received Interrupt
+ * - ADP Transaction Received Interrupt
+ *
+ */
+int32_t dwc_otg_handle_common_intr(void *dev)
+{
+	int retval = 0;
+	gintsts_data_t gintsts;
+	gpwrdn_data_t gpwrdn = {.d32 = 0 };
+	dwc_otg_device_t *otg_dev = dev;
+	dwc_otg_core_if_t *core_if = otg_dev->core_if;
+	gpwrdn.d32 = DWC_READ_REG32(&core_if->core_global_regs->gpwrdn);
+	if (dwc_otg_is_device_mode(core_if))
+		core_if->frame_num = dwc_otg_get_frame_number(core_if);
+		
+	if (core_if->lock)
+		DWC_SPINLOCK(core_if->lock);
+
+	if (core_if->hibernation_suspend <= 0) {
+		gintsts.d32 = dwc_otg_read_common_intr(core_if);
+
+		if (gintsts.b.modemismatch) {
+			retval |= dwc_otg_handle_mode_mismatch_intr(core_if);
+		}
+		if (gintsts.b.otgintr) {
+			retval |= dwc_otg_handle_otg_intr(core_if);
+		}
+		if (gintsts.b.conidstschng) {
+			retval |= dwc_otg_handle_conn_id_status_change_intr(core_if);
+		}
+		if (gintsts.b.disconnect) {
+			retval |= dwc_otg_handle_disconnect_intr(core_if);
+		}
+		if (gintsts.b.sessreqintr) {
+			retval |= dwc_otg_handle_session_req_intr(core_if);
+		}
+		if (gintsts.b.wkupintr) {
+			retval |= dwc_otg_handle_wakeup_detected_intr(core_if);
+		}
+		if (gintsts.b.usbsuspend) {
+			retval |= dwc_otg_handle_usb_suspend_intr(core_if);
+		}
+#ifdef CONFIG_USB_DWC_OTG_LPM
+		if (gintsts.b.lpmtranrcvd) {
+			retval |= dwc_otg_handle_lpm_intr(core_if);
+		}
+#endif
+		if (gintsts.b.restoredone) {
+			gintsts.d32 = 0;
+	                if (core_if->power_down == 2)
+				core_if->hibernation_suspend = -1;
+			gintsts.b.restoredone = 1;
+			DWC_WRITE_REG32(&core_if->core_global_regs->gintsts,gintsts.d32);
+			DWC_PRINTF(" --Restore done interrupt received-- \n");
+			retval |= 1;
+		}
+		if (gintsts.b.portintr && dwc_otg_is_device_mode(core_if)) {
+			/* The port interrupt occurs while in device mode with HPRT0
+			 * Port Enable/Disable.
+			 */
+			gintsts.d32 = 0;
+			gintsts.b.portintr = 1;
+			DWC_WRITE_REG32(&core_if->core_global_regs->gintsts,gintsts.d32);
+			retval |= 1;
+
+		}
+	} else {
+		DWC_DEBUGPL(DBG_ANY, "gpwrdn=%08x\n", gpwrdn.d32);
+
+		if (gpwrdn.b.disconn_det && gpwrdn.b.disconn_det_msk) {
+			CLEAR_GPWRDN_INTR(core_if, disconn_det);
+			if (gpwrdn.b.linestate == 0) {
+				dwc_otg_handle_pwrdn_disconnect_intr(core_if);
+			} else {
+				DWC_PRINTF("Disconnect detected while linestate is not 0\n");
+			}
+
+			retval |= 1;
+		}
+	 	if (gpwrdn.b.lnstschng && gpwrdn.b.lnstchng_msk) {
+			CLEAR_GPWRDN_INTR(core_if, lnstschng);
+			/* remote wakeup from hibernation */
+			if (gpwrdn.b.linestate == 2 || gpwrdn.b.linestate == 1) {
+				dwc_otg_handle_pwrdn_wakeup_detected_intr(core_if);
+			} else {
+				DWC_PRINTF("gpwrdn.linestate = %d\n", gpwrdn.b.linestate);
+			}
+			retval |= 1;
+	 	}
+		if (gpwrdn.b.rst_det && gpwrdn.b.rst_det_msk) {
+			CLEAR_GPWRDN_INTR(core_if, rst_det);
+			if (gpwrdn.b.linestate == 0) {
+				DWC_PRINTF("Reset detected\n");
+				retval |= dwc_otg_device_hibernation_restore(core_if, 0, 1);
+			}
+		}
+		if (gpwrdn.b.srp_det && gpwrdn.b.srp_det_msk) {
+			CLEAR_GPWRDN_INTR(core_if, srp_det);
+			dwc_otg_handle_pwrdn_srp_intr(core_if);
+			retval |= 1;
+		}
+	}
+	/* Handle ADP interrupt here */
+	if (gpwrdn.b.adp_int) {
+		DWC_PRINTF("ADP interrupt\n");
+		CLEAR_GPWRDN_INTR(core_if, adp_int);
+		dwc_otg_adp_handle_intr(core_if);
+		retval |= 1;
+	}
+	if (gpwrdn.b.sts_chngint && gpwrdn.b.sts_chngint_msk) {
+		DWC_PRINTF("STS CHNG interrupt asserted\n");
+		CLEAR_GPWRDN_INTR(core_if, sts_chngint);
+		dwc_otg_handle_pwrdn_stschng_intr(otg_dev);
+
+		retval |= 1;
+	}
+	if (core_if->lock)
+		DWC_SPINUNLOCK(core_if->lock);
+
+	return retval;
+}
diff --git a/drivers/usb/dwc_otg/dwc_otg_core_if.h b/drivers/usb/dwc_otg/dwc_otg_core_if.h
new file mode 100644
index 0000000..1092a47
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_otg_core_if.h
@@ -0,0 +1,707 @@
+/* ==========================================================================
+ * $File: //dwh/usb_iip/dev/software/otg/linux/drivers/dwc_otg_core_if.h $
+ * $Revision: #12 $
+ * $Date: 2011/10/24 $
+ * $Change: 1871159 $
+ *
+ * Synopsys HS OTG Linux Software Driver and documentation (hereinafter,
+ * "Software") is an Unsupported proprietary work of Synopsys, Inc. unless
+ * otherwise expressly agreed to in writing between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product under
+ * any End User Software License Agreement or Agreement for Licensed Product
+ * with Synopsys or any supplement thereto. You are permitted to use and
+ * redistribute this Software in source and binary forms, with or without
+ * modification, provided that redistributions of source code must retain this
+ * notice. You may not view, use, disclose, copy or distribute this file or
+ * any information contained herein except pursuant to this license grant from
+ * Synopsys. If you do not agree with this notice, including the disclaimer
+ * below, then you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS" BASIS
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE HEREBY DISCLAIMED. IN NO EVENT SHALL SYNOPSYS BE LIABLE FOR ANY DIRECT,
+ * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================== */
+#if !defined(__DWC_CORE_IF_H__)
+#define __DWC_CORE_IF_H__
+
+#include "dwc_os.h"
+
+/** @file
+ * This file defines DWC_OTG Core API
+ */
+
+struct dwc_otg_core_if;
+typedef struct dwc_otg_core_if dwc_otg_core_if_t;
+
+/** Maximum number of Periodic FIFOs */
+#define MAX_PERIO_FIFOS 15
+/** Maximum number of Periodic FIFOs */
+#define MAX_TX_FIFOS 15
+
+/** Maximum number of Endpoints/HostChannels */
+#define MAX_EPS_CHANNELS 16
+
+extern dwc_otg_core_if_t *dwc_otg_cil_init(const uint32_t * _reg_base_addr);
+extern void dwc_otg_core_init(dwc_otg_core_if_t * _core_if);
+extern void dwc_otg_cil_remove(dwc_otg_core_if_t * _core_if);
+
+extern void dwc_otg_enable_global_interrupts(dwc_otg_core_if_t * _core_if);
+extern void dwc_otg_disable_global_interrupts(dwc_otg_core_if_t * _core_if);
+
+extern uint8_t dwc_otg_is_device_mode(dwc_otg_core_if_t * _core_if);
+extern uint8_t dwc_otg_is_host_mode(dwc_otg_core_if_t * _core_if);
+
+extern uint8_t dwc_otg_is_dma_enable(dwc_otg_core_if_t * core_if);
+
+/** This function should be called on every hardware interrupt. */
+extern int32_t dwc_otg_handle_common_intr(void *otg_dev);
+
+/** @name OTG Core Parameters */
+/** @{ */
+
+/**
+ * Specifies the OTG capabilities. The driver will automatically
+ * detect the value for this parameter if none is specified.
+ * 0 - HNP and SRP capable (default)
+ * 1 - SRP Only capable
+ * 2 - No HNP/SRP capable
+ */
+extern int dwc_otg_set_param_otg_cap(dwc_otg_core_if_t * core_if, int32_t val);
+extern int32_t dwc_otg_get_param_otg_cap(dwc_otg_core_if_t * core_if);
+#define DWC_OTG_CAP_PARAM_HNP_SRP_CAPABLE 0
+#define DWC_OTG_CAP_PARAM_SRP_ONLY_CAPABLE 1
+#define DWC_OTG_CAP_PARAM_NO_HNP_SRP_CAPABLE 2
+#define dwc_param_otg_cap_default DWC_OTG_CAP_PARAM_HNP_SRP_CAPABLE
+
+extern int dwc_otg_set_param_opt(dwc_otg_core_if_t * core_if, int32_t val);
+extern int32_t dwc_otg_get_param_opt(dwc_otg_core_if_t * core_if);
+#define dwc_param_opt_default 1
+
+/**
+ * Specifies whether to use slave or DMA mode for accessing the data
+ * FIFOs. The driver will automatically detect the value for this
+ * parameter if none is specified.
+ * 0 - Slave
+ * 1 - DMA (default, if available)
+ */
+extern int dwc_otg_set_param_dma_enable(dwc_otg_core_if_t * core_if,
+					int32_t val);
+extern int32_t dwc_otg_get_param_dma_enable(dwc_otg_core_if_t * core_if);
+#define dwc_param_dma_enable_default 1
+
+/**
+ * When DMA mode is enabled specifies whether to use
+ * address DMA or DMA Descritor mode for accessing the data
+ * FIFOs in device mode. The driver will automatically detect
+ * the value for this parameter if none is specified.
+ * 0 - address DMA
+ * 1 - DMA Descriptor(default, if available)
+ */
+extern int dwc_otg_set_param_dma_desc_enable(dwc_otg_core_if_t * core_if,
+					     int32_t val);
+extern int32_t dwc_otg_get_param_dma_desc_enable(dwc_otg_core_if_t * core_if);
+//#define dwc_param_dma_desc_enable_default 1
+#define dwc_param_dma_desc_enable_default 0 // TEST
+
+/** The DMA Burst size (applicable only for External DMA
+ * Mode). 1, 4, 8 16, 32, 64, 128, 256 (default 32)
+ */
+extern int dwc_otg_set_param_dma_burst_size(dwc_otg_core_if_t * core_if,
+					    int32_t val);
+extern int32_t dwc_otg_get_param_dma_burst_size(dwc_otg_core_if_t * core_if);
+#define dwc_param_dma_burst_size_default 32
+
+/**
+ * Specifies the maximum speed of operation in host and device mode.
+ * The actual speed depends on the speed of the attached device and
+ * the value of phy_type. The actual speed depends on the speed of the
+ * attached device.
+ * 0 - High Speed (default)
+ * 1 - Full Speed
+ */
+extern int dwc_otg_set_param_speed(dwc_otg_core_if_t * core_if, int32_t val);
+extern int32_t dwc_otg_get_param_speed(dwc_otg_core_if_t * core_if);
+#define dwc_param_speed_default 0
+#define DWC_SPEED_PARAM_HIGH 0
+#define DWC_SPEED_PARAM_FULL 1
+
+/** Specifies whether low power mode is supported when attached
+ *	to a Full Speed or Low Speed device in host mode.
+ * 0 - Don't support low power mode (default)
+ * 1 - Support low power mode
+ */
+extern int dwc_otg_set_param_host_support_fs_ls_low_power(dwc_otg_core_if_t *
+							  core_if, int32_t val);
+extern int32_t dwc_otg_get_param_host_support_fs_ls_low_power(dwc_otg_core_if_t
+							      * core_if);
+#define dwc_param_host_support_fs_ls_low_power_default 0
+
+/** Specifies the PHY clock rate in low power mode when connected to a
+ * Low Speed device in host mode. This parameter is applicable only if
+ * HOST_SUPPORT_FS_LS_LOW_POWER is enabled. If PHY_TYPE is set to FS
+ * then defaults to 6 MHZ otherwise 48 MHZ.
+ *
+ * 0 - 48 MHz
+ * 1 - 6 MHz
+ */
+extern int dwc_otg_set_param_host_ls_low_power_phy_clk(dwc_otg_core_if_t *
+						       core_if, int32_t val);
+extern int32_t dwc_otg_get_param_host_ls_low_power_phy_clk(dwc_otg_core_if_t *
+							   core_if);
+#define dwc_param_host_ls_low_power_phy_clk_default 0
+#define DWC_HOST_LS_LOW_POWER_PHY_CLK_PARAM_48MHZ 0
+#define DWC_HOST_LS_LOW_POWER_PHY_CLK_PARAM_6MHZ 1
+
+/**
+ * 0 - Use cC FIFO size parameters
+ * 1 - Allow dynamic FIFO sizing (default)
+ */
+extern int dwc_otg_set_param_enable_dynamic_fifo(dwc_otg_core_if_t * core_if,
+						 int32_t val);
+extern int32_t dwc_otg_get_param_enable_dynamic_fifo(dwc_otg_core_if_t *
+						     core_if);
+#define dwc_param_enable_dynamic_fifo_default 1
+
+/** Total number of 4-byte words in the data FIFO memory. This
+ * memory includes the Rx FIFO, non-periodic Tx FIFO, and periodic
+ * Tx FIFOs.
+ * 32 to 32768 (default 8192)
+ * Note: The total FIFO memory depth in the FPGA configuration is 8192.
+ */
+extern int dwc_otg_set_param_data_fifo_size(dwc_otg_core_if_t * core_if,
+					    int32_t val);
+extern int32_t dwc_otg_get_param_data_fifo_size(dwc_otg_core_if_t * core_if);
+#define dwc_param_data_fifo_size_default 8192
+
+/** Number of 4-byte words in the Rx FIFO in device mode when dynamic
+ * FIFO sizing is enabled.
+ * 16 to 32768 (default 1064)
+ */
+extern int dwc_otg_set_param_dev_rx_fifo_size(dwc_otg_core_if_t * core_if,
+					      int32_t val);
+extern int32_t dwc_otg_get_param_dev_rx_fifo_size(dwc_otg_core_if_t * core_if);
+#define dwc_param_dev_rx_fifo_size_default 1064
+
+/** Number of 4-byte words in the non-periodic Tx FIFO in device mode
+ * when dynamic FIFO sizing is enabled.
+ * 16 to 32768 (default 1024)
+ */
+extern int dwc_otg_set_param_dev_nperio_tx_fifo_size(dwc_otg_core_if_t *
+						     core_if, int32_t val);
+extern int32_t dwc_otg_get_param_dev_nperio_tx_fifo_size(dwc_otg_core_if_t *
+							 core_if);
+#define dwc_param_dev_nperio_tx_fifo_size_default 1024
+
+/** Number of 4-byte words in each of the periodic Tx FIFOs in device
+ * mode when dynamic FIFO sizing is enabled.
+ * 4 to 768 (default 256)
+ */
+extern int dwc_otg_set_param_dev_perio_tx_fifo_size(dwc_otg_core_if_t * core_if,
+						    int32_t val, int fifo_num);
+extern int32_t dwc_otg_get_param_dev_perio_tx_fifo_size(dwc_otg_core_if_t *
+							core_if, int fifo_num);
+#define dwc_param_dev_perio_tx_fifo_size_default 256
+
+/** Number of 4-byte words in the Rx FIFO in host mode when dynamic
+ * FIFO sizing is enabled.
+ * 16 to 32768 (default 1024)
+ */
+extern int dwc_otg_set_param_host_rx_fifo_size(dwc_otg_core_if_t * core_if,
+					       int32_t val);
+extern int32_t dwc_otg_get_param_host_rx_fifo_size(dwc_otg_core_if_t * core_if);
+//#define dwc_param_host_rx_fifo_size_default 1024
+#define dwc_param_host_rx_fifo_size_default 776 // Makarand: USB2 DEBUG
+
+
+/** Number of 4-byte words in the non-periodic Tx FIFO in host mode
+ * when Dynamic FIFO sizing is enabled in the core.
+ * 16 to 32768 (default 1024)
+ */
+extern int dwc_otg_set_param_host_nperio_tx_fifo_size(dwc_otg_core_if_t *
+						      core_if, int32_t val);
+extern int32_t dwc_otg_get_param_host_nperio_tx_fifo_size(dwc_otg_core_if_t *
+							  core_if);
+//#define dwc_param_host_nperio_tx_fifo_size_default 1024
+#define dwc_param_host_nperio_tx_fifo_size_default 512 // Makarand: USB2 DEBUG
+
+/** Number of 4-byte words in the host periodic Tx FIFO when dynamic
+ * FIFO sizing is enabled.
+ * 16 to 32768 (default 1024)
+ */
+extern int dwc_otg_set_param_host_perio_tx_fifo_size(dwc_otg_core_if_t *
+						     core_if, int32_t val);
+extern int32_t dwc_otg_get_param_host_perio_tx_fifo_size(dwc_otg_core_if_t *
+							 core_if);
+//#define dwc_param_host_perio_tx_fifo_size_default 1024
+#define dwc_param_host_perio_tx_fifo_size_default 768 // Makarand: USB2 DEBUG
+
+/** The maximum transfer size supported in bytes.
+ * 2047 to 65,535  (default 65,535)
+ */
+extern int dwc_otg_set_param_max_transfer_size(dwc_otg_core_if_t * core_if,
+					       int32_t val);
+extern int32_t dwc_otg_get_param_max_transfer_size(dwc_otg_core_if_t * core_if);
+#define dwc_param_max_transfer_size_default 65535
+
+/** The maximum number of packets in a transfer.
+ * 15 to 511  (default 511)
+ */
+extern int dwc_otg_set_param_max_packet_count(dwc_otg_core_if_t * core_if,
+					      int32_t val);
+extern int32_t dwc_otg_get_param_max_packet_count(dwc_otg_core_if_t * core_if);
+#define dwc_param_max_packet_count_default 511
+
+/** The number of host channel registers to use.
+ * 1 to 16 (default 12)
+ * Note: The FPGA configuration supports a maximum of 12 host channels.
+ */
+extern int dwc_otg_set_param_host_channels(dwc_otg_core_if_t * core_if,
+					   int32_t val);
+extern int32_t dwc_otg_get_param_host_channels(dwc_otg_core_if_t * core_if);
+//#define dwc_param_host_channels_default 12
+//#define dwc_param_host_channels_default 13 // original
+#define dwc_param_host_channels_default 14 // Makarand: USB2 DEBUG
+
+/** The number of endpoints in addition to EP0 available for device
+ * mode operations.
+ * 1 to 15 (default 6 IN and OUT)
+ * Note: The FPGA configuration supports a maximum of 6 IN and OUT
+ * endpoints in addition to EP0.
+ */
+extern int dwc_otg_set_param_dev_endpoints(dwc_otg_core_if_t * core_if,
+					   int32_t val);
+extern int32_t dwc_otg_get_param_dev_endpoints(dwc_otg_core_if_t * core_if);
+//#define dwc_param_dev_endpoints_default 6
+#define dwc_param_dev_endpoints_default 7 // Makarand: USB2 DEBUG
+
+/**
+ * Specifies the type of PHY interface to use. By default, the driver
+ * will automatically detect the phy_type.
+ *
+ * 0 - Full Speed PHY
+ * 1 - UTMI+ (default)
+ * 2 - ULPI
+ */
+extern int dwc_otg_set_param_phy_type(dwc_otg_core_if_t * core_if, int32_t val);
+extern int32_t dwc_otg_get_param_phy_type(dwc_otg_core_if_t * core_if);
+#define DWC_PHY_TYPE_PARAM_FS 0
+#define DWC_PHY_TYPE_PARAM_UTMI 1
+#define DWC_PHY_TYPE_PARAM_ULPI 2
+#define dwc_param_phy_type_default DWC_PHY_TYPE_PARAM_UTMI
+
+/**
+ * Specifies the UTMI+ Data Width. This parameter is
+ * applicable for a PHY_TYPE of UTMI+ or ULPI. (For a ULPI
+ * PHY_TYPE, this parameter indicates the data width between
+ * the MAC and the ULPI Wrapper.) Also, this parameter is
+ * applicable only if the OTG_HSPHY_WIDTH cC parameter was set
+ * to "8 and 16 bits", meaning that the core has been
+ * configured to work at either data path width.
+ *
+ * 8 or 16 bits (default 16)
+ */
+extern int dwc_otg_set_param_phy_utmi_width(dwc_otg_core_if_t * core_if,
+					    int32_t val);
+extern int32_t dwc_otg_get_param_phy_utmi_width(dwc_otg_core_if_t * core_if);
+#define dwc_param_phy_utmi_width_default 16
+
+/**
+ * Specifies whether the ULPI operates at double or single
+ * data rate. This parameter is only applicable if PHY_TYPE is
+ * ULPI.
+ *
+ * 0 - single data rate ULPI interface with 8 bit wide data
+ * bus (default)
+ * 1 - double data rate ULPI interface with 4 bit wide data
+ * bus
+ */
+extern int dwc_otg_set_param_phy_ulpi_ddr(dwc_otg_core_if_t * core_if,
+					  int32_t val);
+extern int32_t dwc_otg_get_param_phy_ulpi_ddr(dwc_otg_core_if_t * core_if);
+#define dwc_param_phy_ulpi_ddr_default 0
+
+/**
+ * Specifies whether to use the internal or external supply to
+ * drive the vbus with a ULPI phy.
+ */
+extern int dwc_otg_set_param_phy_ulpi_ext_vbus(dwc_otg_core_if_t * core_if,
+					       int32_t val);
+extern int32_t dwc_otg_get_param_phy_ulpi_ext_vbus(dwc_otg_core_if_t * core_if);
+#define DWC_PHY_ULPI_INTERNAL_VBUS 0
+#define DWC_PHY_ULPI_EXTERNAL_VBUS 1
+#define dwc_param_phy_ulpi_ext_vbus_default DWC_PHY_ULPI_INTERNAL_VBUS
+
+/**
+ * Specifies whether to use the I2Cinterface for full speed PHY. This
+ * parameter is only applicable if PHY_TYPE is FS.
+ * 0 - No (default)
+ * 1 - Yes
+ */
+extern int dwc_otg_set_param_i2c_enable(dwc_otg_core_if_t * core_if,
+					int32_t val);
+extern int32_t dwc_otg_get_param_i2c_enable(dwc_otg_core_if_t * core_if);
+#define dwc_param_i2c_enable_default 0
+
+extern int dwc_otg_set_param_ulpi_fs_ls(dwc_otg_core_if_t * core_if,
+					int32_t val);
+extern int32_t dwc_otg_get_param_ulpi_fs_ls(dwc_otg_core_if_t * core_if);
+#define dwc_param_ulpi_fs_ls_default 0
+
+extern int dwc_otg_set_param_ts_dline(dwc_otg_core_if_t * core_if, int32_t val);
+extern int32_t dwc_otg_get_param_ts_dline(dwc_otg_core_if_t * core_if);
+#define dwc_param_ts_dline_default 0
+
+/**
+ * Specifies whether dedicated transmit FIFOs are
+ * enabled for non periodic IN endpoints in device mode
+ * 0 - No
+ * 1 - Yes
+ */
+extern int dwc_otg_set_param_en_multiple_tx_fifo(dwc_otg_core_if_t * core_if,
+						 int32_t val);
+extern int32_t dwc_otg_get_param_en_multiple_tx_fifo(dwc_otg_core_if_t *
+						     core_if);
+#define dwc_param_en_multiple_tx_fifo_default 1
+
+/** Number of 4-byte words in each of the Tx FIFOs in device
+ * mode when dynamic FIFO sizing is enabled.
+ * 4 to 768 (default 256)
+ */
+extern int dwc_otg_set_param_dev_tx_fifo_size(dwc_otg_core_if_t * core_if,
+					      int fifo_num, int32_t val);
+extern int32_t dwc_otg_get_param_dev_tx_fifo_size(dwc_otg_core_if_t * core_if,
+						  int fifo_num);
+#define dwc_param_dev_tx_fifo_size_default 768
+//#define dwc_param_dev_tx_fifo_size_default 256 //Try this
+
+/** Thresholding enable flag-
+ * bit 0 - enable non-ISO Tx thresholding
+ * bit 1 - enable ISO Tx thresholding
+ * bit 2 - enable Rx thresholding
+ */
+extern int dwc_otg_set_param_thr_ctl(dwc_otg_core_if_t * core_if, int32_t val);
+extern int32_t dwc_otg_get_thr_ctl(dwc_otg_core_if_t * core_if, int fifo_num);
+#define dwc_param_thr_ctl_default 0
+
+/** Thresholding length for Tx
+ * FIFOs in 32 bit DWORDs
+ */
+extern int dwc_otg_set_param_tx_thr_length(dwc_otg_core_if_t * core_if,
+					   int32_t val);
+extern int32_t dwc_otg_get_tx_thr_length(dwc_otg_core_if_t * core_if);
+#define dwc_param_tx_thr_length_default 64
+
+/** Thresholding length for Rx
+ *	FIFOs in 32 bit DWORDs
+ */
+extern int dwc_otg_set_param_rx_thr_length(dwc_otg_core_if_t * core_if,
+					   int32_t val);
+extern int32_t dwc_otg_get_rx_thr_length(dwc_otg_core_if_t * core_if);
+#define dwc_param_rx_thr_length_default 64
+
+/**
+ * Specifies whether LPM (Link Power Management) support is enabled
+ */
+extern int dwc_otg_set_param_lpm_enable(dwc_otg_core_if_t * core_if,
+					int32_t val);
+extern int32_t dwc_otg_get_param_lpm_enable(dwc_otg_core_if_t * core_if);
+#define dwc_param_lpm_enable_default 1
+
+/**
+ * Specifies whether PTI enhancement is enabled
+ */
+extern int dwc_otg_set_param_pti_enable(dwc_otg_core_if_t * core_if,
+					int32_t val);
+extern int32_t dwc_otg_get_param_pti_enable(dwc_otg_core_if_t * core_if);
+#define dwc_param_pti_enable_default 0
+
+/**
+ * Specifies whether MPI enhancement is enabled
+ */
+extern int dwc_otg_set_param_mpi_enable(dwc_otg_core_if_t * core_if,
+					int32_t val);
+extern int32_t dwc_otg_get_param_mpi_enable(dwc_otg_core_if_t * core_if);
+#define dwc_param_mpi_enable_default 0
+
+/**
+ * Specifies whether ADP capability is enabled
+ */
+extern int dwc_otg_set_param_adp_enable(dwc_otg_core_if_t * core_if,
+					int32_t val);
+extern int32_t dwc_otg_get_param_adp_enable(dwc_otg_core_if_t * core_if);
+#define dwc_param_adp_enable_default 0
+
+/**
+ * Specifies whether IC_USB capability is enabled
+ */
+
+extern int dwc_otg_set_param_ic_usb_cap(dwc_otg_core_if_t * core_if,
+					int32_t val);
+extern int32_t dwc_otg_get_param_ic_usb_cap(dwc_otg_core_if_t * core_if);
+#define dwc_param_ic_usb_cap_default 0
+
+extern int dwc_otg_set_param_ahb_thr_ratio(dwc_otg_core_if_t * core_if,
+					   int32_t val);
+extern int32_t dwc_otg_get_param_ahb_thr_ratio(dwc_otg_core_if_t * core_if);
+#define dwc_param_ahb_thr_ratio_default 0
+
+extern int dwc_otg_set_param_power_down(dwc_otg_core_if_t * core_if,
+					int32_t val);
+extern int32_t dwc_otg_get_param_power_down(dwc_otg_core_if_t * core_if);
+#define dwc_param_power_down_default 0
+
+extern int dwc_otg_set_param_reload_ctl(dwc_otg_core_if_t * core_if,
+					int32_t val);
+extern int32_t dwc_otg_get_param_reload_ctl(dwc_otg_core_if_t * core_if);
+#define dwc_param_reload_ctl_default 0
+
+extern int dwc_otg_set_param_dev_out_nak(dwc_otg_core_if_t * core_if,
+										int32_t val);
+extern int32_t dwc_otg_get_param_dev_out_nak(dwc_otg_core_if_t * core_if);
+#define dwc_param_dev_out_nak_default 0
+
+extern int dwc_otg_set_param_cont_on_bna(dwc_otg_core_if_t * core_if,
+										 int32_t val);
+extern int32_t dwc_otg_get_param_cont_on_bna(dwc_otg_core_if_t * core_if);
+#define dwc_param_cont_on_bna_default 0
+
+extern int dwc_otg_set_param_ahb_single(dwc_otg_core_if_t * core_if,
+										 int32_t val);
+extern int32_t dwc_otg_get_param_ahb_single(dwc_otg_core_if_t * core_if);
+#define dwc_param_ahb_single_default 0
+
+extern int dwc_otg_set_param_otg_ver(dwc_otg_core_if_t * core_if, int32_t val);
+extern int32_t dwc_otg_get_param_otg_ver(dwc_otg_core_if_t * core_if);
+#define dwc_param_otg_ver_default 0 // Makarand ??
+
+/** @} */
+
+/** @name Access to registers and bit-fields */
+
+/**
+ * Dump core registers and SPRAM
+ */
+extern void dwc_otg_dump_dev_registers(dwc_otg_core_if_t * _core_if);
+extern void dwc_otg_dump_spram(dwc_otg_core_if_t * _core_if);
+extern void dwc_otg_dump_host_registers(dwc_otg_core_if_t * _core_if);
+extern void dwc_otg_dump_global_registers(dwc_otg_core_if_t * _core_if);
+
+/**
+ * Get host negotiation status.
+ */
+extern uint32_t dwc_otg_get_hnpstatus(dwc_otg_core_if_t * core_if);
+
+/**
+ * Get srp status
+ */
+extern uint32_t dwc_otg_get_srpstatus(dwc_otg_core_if_t * core_if);
+
+/**
+ * Set hnpreq bit in the GOTGCTL register.
+ */
+extern void dwc_otg_set_hnpreq(dwc_otg_core_if_t * core_if, uint32_t val);
+
+/**
+ * Get Content of SNPSID register.
+ */
+extern uint32_t dwc_otg_get_gsnpsid(dwc_otg_core_if_t * core_if);
+
+/**
+ * Get current mode.
+ * Returns 0 if in device mode, and 1 if in host mode.
+ */
+extern uint32_t dwc_otg_get_mode(dwc_otg_core_if_t * core_if);
+
+/**
+ * Get value of hnpcapable field in the GUSBCFG register
+ */
+extern uint32_t dwc_otg_get_hnpcapable(dwc_otg_core_if_t * core_if);
+/**
+ * Set value of hnpcapable field in the GUSBCFG register
+ */
+extern void dwc_otg_set_hnpcapable(dwc_otg_core_if_t * core_if, uint32_t val);
+
+/**
+ * Get value of srpcapable field in the GUSBCFG register
+ */
+extern uint32_t dwc_otg_get_srpcapable(dwc_otg_core_if_t * core_if);
+/**
+ * Set value of srpcapable field in the GUSBCFG register
+ */
+extern void dwc_otg_set_srpcapable(dwc_otg_core_if_t * core_if, uint32_t val);
+
+/**
+ * Get value of devspeed field in the DCFG register
+ */
+extern uint32_t dwc_otg_get_devspeed(dwc_otg_core_if_t * core_if);
+/**
+ * Set value of devspeed field in the DCFG register
+ */
+extern void dwc_otg_set_devspeed(dwc_otg_core_if_t * core_if, uint32_t val);
+
+/**
+ * Get the value of busconnected field from the HPRT0 register
+ */
+extern uint32_t dwc_otg_get_busconnected(dwc_otg_core_if_t * core_if);
+
+/**
+ * Gets the device enumeration Speed.
+ */
+extern uint32_t dwc_otg_get_enumspeed(dwc_otg_core_if_t * core_if);
+
+/**
+ * Get value of prtpwr field from the HPRT0 register
+ */
+extern uint32_t dwc_otg_get_prtpower(dwc_otg_core_if_t * core_if);
+
+/**
+ * Get value of flag indicating core state - hibernated or not
+ */
+extern uint32_t dwc_otg_get_core_state(dwc_otg_core_if_t * core_if);
+
+/**
+ * Set value of prtpwr field from the HPRT0 register
+ */
+extern void dwc_otg_set_prtpower(dwc_otg_core_if_t * core_if, uint32_t val);
+
+/**
+ * Get value of prtsusp field from the HPRT0 regsiter
+ */
+extern uint32_t dwc_otg_get_prtsuspend(dwc_otg_core_if_t * core_if);
+/**
+ * Set value of prtpwr field from the HPRT0 register
+ */
+extern void dwc_otg_set_prtsuspend(dwc_otg_core_if_t * core_if, uint32_t val);
+
+/**
+ * Get value of ModeChTimEn field from the HCFG regsiter
+ */
+extern uint32_t dwc_otg_get_mode_ch_tim(dwc_otg_core_if_t * core_if);
+/**
+ * Set value of ModeChTimEn field from the HCFG regsiter
+ */
+extern void dwc_otg_set_mode_ch_tim(dwc_otg_core_if_t * core_if, uint32_t val);
+
+/**
+ * Get value of Fram Interval field from the HFIR regsiter
+ */
+extern uint32_t dwc_otg_get_fr_interval(dwc_otg_core_if_t * core_if);
+/**
+ * Set value of Frame Interval field from the HFIR regsiter
+ */
+extern void dwc_otg_set_fr_interval(dwc_otg_core_if_t * core_if, uint32_t val);
+
+/**
+ * Set value of prtres field from the HPRT0 register
+ *FIXME Remove?
+ */
+extern void dwc_otg_set_prtresume(dwc_otg_core_if_t * core_if, uint32_t val);
+
+/**
+ * Get value of rmtwkupsig bit in DCTL register
+ */
+extern uint32_t dwc_otg_get_remotewakesig(dwc_otg_core_if_t * core_if);
+
+/**
+ * Get value of prt_sleep_sts field from the GLPMCFG register
+ */
+extern uint32_t dwc_otg_get_lpm_portsleepstatus(dwc_otg_core_if_t * core_if);
+
+/**
+ * Get value of rem_wkup_en field from the GLPMCFG register
+ */
+extern uint32_t dwc_otg_get_lpm_remotewakeenabled(dwc_otg_core_if_t * core_if);
+
+/**
+ * Get value of appl_resp field from the GLPMCFG register
+ */
+extern uint32_t dwc_otg_get_lpmresponse(dwc_otg_core_if_t * core_if);
+/**
+ * Set value of appl_resp field from the GLPMCFG register
+ */
+extern void dwc_otg_set_lpmresponse(dwc_otg_core_if_t * core_if, uint32_t val);
+
+/**
+ * Get value of hsic_connect field from the GLPMCFG register
+ */
+extern uint32_t dwc_otg_get_hsic_connect(dwc_otg_core_if_t * core_if);
+/**
+ * Set value of hsic_connect field from the GLPMCFG register
+ */
+extern void dwc_otg_set_hsic_connect(dwc_otg_core_if_t * core_if, uint32_t val);
+
+/**
+ * Get value of inv_sel_hsic field from the GLPMCFG register.
+ */
+extern uint32_t dwc_otg_get_inv_sel_hsic(dwc_otg_core_if_t * core_if);
+/**
+ * Set value of inv_sel_hsic field from the GLPMFG register.
+ */
+extern void dwc_otg_set_inv_sel_hsic(dwc_otg_core_if_t * core_if, uint32_t val);
+
+/*
+ * Some functions for accessing registers
+ */
+
+/**
+ *  GOTGCTL register
+ */
+extern uint32_t dwc_otg_get_gotgctl(dwc_otg_core_if_t * core_if);
+extern void dwc_otg_set_gotgctl(dwc_otg_core_if_t * core_if, uint32_t val);
+
+/**
+ * GUSBCFG register
+ */
+extern uint32_t dwc_otg_get_gusbcfg(dwc_otg_core_if_t * core_if);
+extern void dwc_otg_set_gusbcfg(dwc_otg_core_if_t * core_if, uint32_t val);
+
+/**
+ * GRXFSIZ register
+ */
+extern uint32_t dwc_otg_get_grxfsiz(dwc_otg_core_if_t * core_if);
+extern void dwc_otg_set_grxfsiz(dwc_otg_core_if_t * core_if, uint32_t val);
+
+/**
+ * GNPTXFSIZ register
+ */
+extern uint32_t dwc_otg_get_gnptxfsiz(dwc_otg_core_if_t * core_if);
+extern void dwc_otg_set_gnptxfsiz(dwc_otg_core_if_t * core_if, uint32_t val);
+
+extern uint32_t dwc_otg_get_gpvndctl(dwc_otg_core_if_t * core_if);
+extern void dwc_otg_set_gpvndctl(dwc_otg_core_if_t * core_if, uint32_t val);
+
+/**
+ * GGPIO register
+ */
+extern uint32_t dwc_otg_get_ggpio(dwc_otg_core_if_t * core_if);
+extern void dwc_otg_set_ggpio(dwc_otg_core_if_t * core_if, uint32_t val);
+
+/**
+ * GUID register
+ */
+extern uint32_t dwc_otg_get_guid(dwc_otg_core_if_t * core_if);
+extern void dwc_otg_set_guid(dwc_otg_core_if_t * core_if, uint32_t val);
+
+/**
+ * HPRT0 register
+ */
+extern uint32_t dwc_otg_get_hprt0(dwc_otg_core_if_t * core_if);
+extern void dwc_otg_set_hprt0(dwc_otg_core_if_t * core_if, uint32_t val);
+
+/**
+ * GHPTXFSIZE
+ */
+extern uint32_t dwc_otg_get_hptxfsiz(dwc_otg_core_if_t * core_if);
+
+/** @} */
+
+#endif				/* __DWC_CORE_IF_H__ */
diff --git a/drivers/usb/dwc_otg/dwc_otg_dbg.h b/drivers/usb/dwc_otg/dwc_otg_dbg.h
new file mode 100644
index 0000000..32c7d10
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_otg_dbg.h
@@ -0,0 +1,113 @@
+/* ==========================================================================
+ *
+ * Synopsys HS OTG Linux Software Driver and documentation (hereinafter,
+ * "Software") is an Unsupported proprietary work of Synopsys, Inc. unless
+ * otherwise expressly agreed to in writing between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product under
+ * any End User Software License Agreement or Agreement for Licensed Product
+ * with Synopsys or any supplement thereto. You are permitted to use and
+ * redistribute this Software in source and binary forms, with or without
+ * modification, provided that redistributions of source code must retain this
+ * notice. You may not view, use, disclose, copy or distribute this file or
+ * any information contained herein except pursuant to this license grant from
+ * Synopsys. If you do not agree with this notice, including the disclaimer
+ * below, then you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS" BASIS
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE HEREBY DISCLAIMED. IN NO EVENT SHALL SYNOPSYS BE LIABLE FOR ANY DIRECT,
+ * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================== */
+
+#ifndef __DWC_OTG_DBG_H__
+#define __DWC_OTG_DBG_H__
+
+/** @file
+ * This file defines debug levels.
+ * Debugging support vanishes in non-debug builds.
+ */
+
+/**
+ * The Debug Level bit-mask variable.
+ */
+extern uint32_t g_dbg_lvl;
+/**
+ * Set the Debug Level variable.
+ */
+static inline uint32_t SET_DEBUG_LEVEL(const uint32_t new)
+{
+	uint32_t old = g_dbg_lvl;
+	g_dbg_lvl = new;
+	return old;
+}
+
+/** When debug level has the DBG_CIL bit set, display CIL Debug messages. */
+#define DBG_CIL		(0x2)
+/** When debug level has the DBG_CILV bit set, display CIL Verbose debug
+ * messages */
+#define DBG_CILV	(0x20)
+/**  When debug level has the DBG_PCD bit set, display PCD (Device) debug
+ *  messages */
+#define DBG_PCD		(0x4)
+/** When debug level has the DBG_PCDV set, display PCD (Device) Verbose debug
+ * messages */
+#define DBG_PCDV	(0x40)
+/** When debug level has the DBG_HCD bit set, display Host debug messages */
+#define DBG_HCD		(0x8)
+/** When debug level has the DBG_HCDV bit set, display Verbose Host debug
+ * messages */
+#define DBG_HCDV	(0x80)
+/** When debug level has the DBG_HCD_URB bit set, display enqueued URBs in host
+ *  mode. */
+#define DBG_HCD_URB	(0x800)
+
+/** When debug level has any bit set, display debug messages */
+#define DBG_ANY		(0xFF)
+
+/** All debug messages off */
+#define DBG_OFF		0
+
+/** Prefix string for DWC_DEBUG print macros. */
+#define USB_DWC "DWC_otg: "
+
+/**
+ * Print a debug message when the Global debug level variable contains
+ * the bit defined in <code>lvl</code>.
+ *
+ * @param[in] lvl - Debug level, use one of the DBG_ constants above.
+ * @param[in] x - like printf
+ *
+ *    Example:<p>
+ * <code>
+ *      DWC_DEBUGPL( DBG_ANY, "%s(%p)\n", __func__, _reg_base_addr);
+ * </code>
+ * <br>
+ * results in:<br>
+ * <code>
+ * usb-DWC_otg: dwc_otg_cil_init(ca867000)
+ * </code>
+ */
+#ifdef DEBUG
+
+# define DWC_DEBUGPL(lvl, x...) do{ if ((lvl)&g_dbg_lvl)__DWC_DEBUG(USB_DWC x ); }while(0)
+# define DWC_DEBUGP(x...)	DWC_DEBUGPL(DBG_ANY, x )
+
+# define CHK_DEBUG_LEVEL(level) ((level) & g_dbg_lvl)
+
+#else
+
+# define DWC_DEBUGPL(lvl, x...) do{}while(0)
+# define DWC_DEBUGP(x...)
+
+# define CHK_DEBUG_LEVEL(level) (0)
+
+#endif /*DEBUG*/
+#endif
diff --git a/drivers/usb/dwc_otg/dwc_otg_driver.c b/drivers/usb/dwc_otg/dwc_otg_driver.c
new file mode 100644
index 0000000..bd504a9
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_otg_driver.c
@@ -0,0 +1,1877 @@
+/* ==========================================================================
+ * $File: //dwh/usb_iip/dev/software/otg/linux/drivers/dwc_otg_driver.c $
+ * $Revision: #91 $
+ * $Date: 2011/10/24 $
+ * $Change: 1871159 $
+ *
+ * Synopsys HS OTG Linux Software Driver and documentation (hereinafter,
+ * "Software") is an Unsupported proprietary work of Synopsys, Inc. unless
+ * otherwise expressly agreed to in writing between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product under
+ * any End User Software License Agreement or Agreement for Licensed Product
+ * with Synopsys or any supplement thereto. You are permitted to use and
+ * redistribute this Software in source and binary forms, with or without
+ * modification, provided that redistributions of source code must retain this
+ * notice. You may not view, use, disclose, copy or distribute this file or
+ * any information contained herein except pursuant to this license grant from
+ * Synopsys. If you do not agree with this notice, including the disclaimer
+ * below, then you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS" BASIS
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE HEREBY DISCLAIMED. IN NO EVENT SHALL SYNOPSYS BE LIABLE FOR ANY DIRECT,
+ * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================== */
+
+/** @file
+ * The dwc_otg_driver module provides the initialization and cleanup entry
+ * points for the DWC_otg driver. This module will be dynamically installed
+ * after Linux is booted using the insmod command. When the module is
+ * installed, the dwc_otg_driver_init function is called. When the module is
+ * removed (using rmmod), the dwc_otg_driver_cleanup function is called.
+ *
+ * This module also defines a data structure for the dwc_otg_driver, which is
+ * used in conjunction with the standard ARM lm_device structure. These
+ * structures allow the OTG driver to comply with the standard Linux driver
+ * model in which devices and drivers are registered with a bus driver. This
+ * has the benefit that Linux can expose attributes of the driver and device
+ * in its special sysfs file system. Users can then read or write files in
+ * this file system to perform diagnostics on the driver components or the
+ * device.
+ */
+
+#include "dwc_otg_os_dep.h"
+#include "dwc_os.h"
+#include "dwc_otg_dbg.h"
+#include "dwc_otg_driver.h"
+#include "dwc_otg_attr.h"
+#include "dwc_otg_core_if.h"
+#include "dwc_otg_pcd_if.h"
+#include "dwc_otg_hcd_if.h"
+
+#include <linux/clk.h>
+
+#ifdef CONFIG_MACH_M822XX
+#include <mach/usb_mmap.h>
+#endif	/* CONFIG_MACH_M822XX */
+
+struct usb_hcd;
+
+extern void dwc_otg_host_port_suspend(struct usb_hcd *hcd);
+extern void dwc_otg_host_port_resume(struct usb_hcd *hcd);
+
+
+#define DWC_DRIVER_VERSION	"2.94a 27-OCT-2011"
+#define DWC_DRIVER_DESC		"HS OTG USB Controller driver"
+
+static const char dwc_driver_name[] = "dwc_otg";
+
+extern int pcd_init(
+#ifdef LM_INTERFACE
+			   struct lm_device *_dev
+#elif  defined(PCI_INTERFACE)
+			   struct pci_dev *_dev
+#else
+				struct platform_device *_dev
+#endif
+    );
+extern int hcd_init(
+#ifdef LM_INTERFACE
+			   struct lm_device *_dev
+#elif  defined(PCI_INTERFACE)
+			   struct pci_dev *_dev
+#else
+				struct platform_device *_dev
+#endif
+    );
+
+extern int pcd_remove(
+#ifdef LM_INTERFACE
+			     struct lm_device *_dev
+#elif  defined(PCI_INTERFACE)
+			     struct pci_dev *_dev
+#else
+				 struct platform_device *_dev
+#endif
+    );
+
+extern void hcd_remove(
+#ifdef LM_INTERFACE
+			      struct lm_device *_dev
+#elif  defined(PCI_INTERFACE)
+			      struct pci_dev *_dev
+#else
+					struct platform_device *_dev
+#endif
+    );
+
+extern void dwc_otg_adp_start(dwc_otg_core_if_t * core_if, uint8_t is_host);
+
+/*-------------------------------------------------------------------------*/
+/* Encapsulate the module parameter settings */
+
+struct dwc_otg_driver_module_params {
+	int32_t opt;
+	int32_t otg_cap;
+	int32_t dma_enable;
+	int32_t dma_desc_enable;
+	int32_t dma_burst_size;
+	int32_t speed;
+	int32_t host_support_fs_ls_low_power;
+	int32_t host_ls_low_power_phy_clk;
+	int32_t enable_dynamic_fifo;
+	int32_t data_fifo_size;
+	int32_t dev_rx_fifo_size;
+	int32_t dev_nperio_tx_fifo_size;
+	uint32_t dev_perio_tx_fifo_size[MAX_PERIO_FIFOS];
+	int32_t host_rx_fifo_size;
+	int32_t host_nperio_tx_fifo_size;
+	int32_t host_perio_tx_fifo_size;
+	int32_t max_transfer_size;
+	int32_t max_packet_count;
+	int32_t host_channels;
+	int32_t dev_endpoints;
+	int32_t phy_type;
+	int32_t phy_utmi_width;
+	int32_t phy_ulpi_ddr;
+	int32_t phy_ulpi_ext_vbus;
+	int32_t i2c_enable;
+	int32_t ulpi_fs_ls;
+	int32_t ts_dline;
+	int32_t en_multiple_tx_fifo;
+	uint32_t dev_tx_fifo_size[MAX_TX_FIFOS];
+	uint32_t thr_ctl;
+	uint32_t tx_thr_length;
+	uint32_t rx_thr_length;
+	int32_t pti_enable;
+	int32_t mpi_enable;
+	int32_t lpm_enable;
+	int32_t ic_usb_cap;
+	int32_t ahb_thr_ratio;
+	int32_t power_down;
+	int32_t reload_ctl;
+	int32_t dev_out_nak;
+	int32_t cont_on_bna;
+	int32_t ahb_single;
+	int32_t otg_ver;
+	int32_t adp_enable;
+};
+
+static struct dwc_otg_driver_module_params dwc_otg_module_params = {
+	.opt = -1,
+	.otg_cap = -1,
+	.dma_enable = -1,
+	.dma_desc_enable = -1,
+	.dma_burst_size = -1,
+	.speed = -1,
+	.host_support_fs_ls_low_power = -1,
+	.host_ls_low_power_phy_clk = -1,
+	.enable_dynamic_fifo = -1,
+	.data_fifo_size = -1,
+	.dev_rx_fifo_size = -1,
+	.dev_nperio_tx_fifo_size = -1,
+	.dev_perio_tx_fifo_size = {
+				   /* dev_perio_tx_fifo_size_1 */
+				   -1,
+				   -1,
+				   -1,
+				   -1,
+				   -1,
+				   -1,
+				   -1,
+				   -1,
+				   -1,
+				   -1,
+				   -1,
+				   -1,
+				   -1,
+				   -1,
+				   -1
+				   /* 15 */
+				   },
+	.host_rx_fifo_size = -1,
+	.host_nperio_tx_fifo_size = -1,
+	.host_perio_tx_fifo_size = -1,
+	.max_transfer_size = -1,
+	.max_packet_count = -1,
+	.host_channels = -1,
+	.dev_endpoints = -1,
+	.phy_type = -1,
+	.phy_utmi_width = -1,
+	.phy_ulpi_ddr = -1,
+	.phy_ulpi_ext_vbus = -1,
+	.i2c_enable = -1,
+	.ulpi_fs_ls = -1,
+	.ts_dline = -1,
+	.en_multiple_tx_fifo = -1,
+	.dev_tx_fifo_size = {
+			     /* dev_tx_fifo_size */
+			     -1,
+			     -1,
+			     -1,
+			     -1,
+			     -1,
+			     -1,
+			     -1,
+			     -1,
+			     -1,
+			     -1,
+			     -1,
+			     -1,
+			     -1,
+			     -1,
+			     -1
+			     /* 15 */
+			     },
+	.thr_ctl = -1,
+	.tx_thr_length = -1,
+	.rx_thr_length = -1,
+	.pti_enable = -1,
+	.mpi_enable = -1,
+	.lpm_enable = -1,
+	.ic_usb_cap = -1,
+	.ahb_thr_ratio = -1,
+	.power_down = -1,
+	.reload_ctl = -1,
+	.dev_out_nak = -1,
+	.cont_on_bna = -1,
+	.ahb_single = -1,
+	.otg_ver = -1,
+	.adp_enable = -1,
+};
+
+static unsigned int READL(unsigned int reg)
+{
+	unsigned int value;
+
+	value = readl(reg);
+
+#ifdef DEBUG
+	printk(KERN_INFO  "Register: 0x%08X, Read:  0x%08X\n", reg, value);
+#endif	/* DEBUG */
+
+	return value;
+}
+
+static void WRITEL(unsigned int value, unsigned int reg)
+{
+#ifdef DEBUG
+	printk(KERN_INFO  "Register: 0x%08X, Write: 0x%08X\n", reg, value);
+#endif	/* DEBUG */
+
+	writel(value, reg);
+}
+
+/**
+ * This function shows the Driver Version.
+ */
+static ssize_t version_show(struct device_driver *dev, char *buf)
+{
+	return snprintf(buf, sizeof(DWC_DRIVER_VERSION) + 2, "%s\n",
+			DWC_DRIVER_VERSION);
+}
+
+static DRIVER_ATTR(version, S_IRUGO, version_show, NULL);
+
+/**
+ * Global Debug Level Mask.
+ */
+uint32_t g_dbg_lvl = 0;		/* OFF */
+
+/**
+ * This function shows the driver Debug Level.
+ */
+static ssize_t dbg_level_show(struct device_driver *drv, char *buf)
+{
+	return sprintf(buf, "0x%0x\n", g_dbg_lvl);
+}
+
+/**
+ * This function stores the driver Debug Level.
+ */
+static ssize_t dbg_level_store(struct device_driver *drv, const char *buf,
+			       size_t count)
+{
+	g_dbg_lvl = simple_strtoul(buf, NULL, 16);
+	return count;
+}
+
+static DRIVER_ATTR(debuglevel, S_IRUGO | S_IWUSR, dbg_level_show,
+		   dbg_level_store);
+
+/**
+ * This function is called during module intialization
+ * to pass module parameters to the DWC_OTG CORE.
+ */
+static int set_parameters(dwc_otg_core_if_t * core_if)
+{
+	int retval = 0;
+	int i;
+
+	if (dwc_otg_module_params.otg_cap != -1) {
+		retval +=
+		    dwc_otg_set_param_otg_cap(core_if,
+					      dwc_otg_module_params.otg_cap);
+	}
+	if (dwc_otg_module_params.dma_enable != -1) {
+		retval +=
+		    dwc_otg_set_param_dma_enable(core_if,
+						 dwc_otg_module_params.
+						 dma_enable);
+	}
+	if (dwc_otg_module_params.dma_desc_enable != -1) {
+		retval +=
+		    dwc_otg_set_param_dma_desc_enable(core_if,
+						      dwc_otg_module_params.
+						      dma_desc_enable);
+	}
+	if (dwc_otg_module_params.opt != -1) {
+		retval +=
+		    dwc_otg_set_param_opt(core_if, dwc_otg_module_params.opt);
+	}
+	if (dwc_otg_module_params.dma_burst_size != -1) {
+		retval +=
+		    dwc_otg_set_param_dma_burst_size(core_if,
+						     dwc_otg_module_params.
+						     dma_burst_size);
+	}
+	if (dwc_otg_module_params.host_support_fs_ls_low_power != -1) {
+		retval +=
+		    dwc_otg_set_param_host_support_fs_ls_low_power(core_if,
+								   dwc_otg_module_params.
+								   host_support_fs_ls_low_power);
+	}
+	if (dwc_otg_module_params.enable_dynamic_fifo != -1) {
+		retval +=
+		    dwc_otg_set_param_enable_dynamic_fifo(core_if,
+							  dwc_otg_module_params.
+							  enable_dynamic_fifo);
+	}
+	if (dwc_otg_module_params.data_fifo_size != -1) {
+		retval +=
+		    dwc_otg_set_param_data_fifo_size(core_if,
+						     dwc_otg_module_params.
+						     data_fifo_size);
+	}
+	if (dwc_otg_module_params.dev_rx_fifo_size != -1) {
+		retval +=
+		    dwc_otg_set_param_dev_rx_fifo_size(core_if,
+						       dwc_otg_module_params.
+						       dev_rx_fifo_size);
+	}
+	if (dwc_otg_module_params.dev_nperio_tx_fifo_size != -1) {
+		retval +=
+		    dwc_otg_set_param_dev_nperio_tx_fifo_size(core_if,
+							      dwc_otg_module_params.
+							      dev_nperio_tx_fifo_size);
+	}
+	if (dwc_otg_module_params.host_rx_fifo_size != -1) {
+		retval +=
+		    dwc_otg_set_param_host_rx_fifo_size(core_if,
+							dwc_otg_module_params.host_rx_fifo_size);
+	}
+	if (dwc_otg_module_params.host_nperio_tx_fifo_size != -1) {
+		retval +=
+		    dwc_otg_set_param_host_nperio_tx_fifo_size(core_if,
+							       dwc_otg_module_params.
+							       host_nperio_tx_fifo_size);
+	}
+	if (dwc_otg_module_params.host_perio_tx_fifo_size != -1) {
+		retval +=
+		    dwc_otg_set_param_host_perio_tx_fifo_size(core_if,
+							      dwc_otg_module_params.
+							      host_perio_tx_fifo_size);
+	}
+	if (dwc_otg_module_params.max_transfer_size != -1) {
+		retval +=
+		    dwc_otg_set_param_max_transfer_size(core_if,
+							dwc_otg_module_params.
+							max_transfer_size);
+	}
+	if (dwc_otg_module_params.max_packet_count != -1) {
+		retval +=
+		    dwc_otg_set_param_max_packet_count(core_if,
+						       dwc_otg_module_params.
+						       max_packet_count);
+	}
+	if (dwc_otg_module_params.host_channels != -1) {
+		retval +=
+		    dwc_otg_set_param_host_channels(core_if,
+						    dwc_otg_module_params.
+						    host_channels);
+	}
+	if (dwc_otg_module_params.dev_endpoints != -1) {
+		retval +=
+		    dwc_otg_set_param_dev_endpoints(core_if,
+						    dwc_otg_module_params.
+						    dev_endpoints);
+	}
+	if (dwc_otg_module_params.phy_type != -1) {
+		retval +=
+		    dwc_otg_set_param_phy_type(core_if,
+					       dwc_otg_module_params.phy_type);
+	}
+	if (dwc_otg_module_params.speed != -1) {
+		retval +=
+		    dwc_otg_set_param_speed(core_if,
+					    dwc_otg_module_params.speed);
+	}
+	if (dwc_otg_module_params.host_ls_low_power_phy_clk != -1) {
+		retval +=
+		    dwc_otg_set_param_host_ls_low_power_phy_clk(core_if,
+								dwc_otg_module_params.
+								host_ls_low_power_phy_clk);
+	}
+	if (dwc_otg_module_params.phy_ulpi_ddr != -1) {
+		retval +=
+		    dwc_otg_set_param_phy_ulpi_ddr(core_if,
+						   dwc_otg_module_params.
+						   phy_ulpi_ddr);
+	}
+	if (dwc_otg_module_params.phy_ulpi_ext_vbus != -1) {
+		retval +=
+		    dwc_otg_set_param_phy_ulpi_ext_vbus(core_if,
+							dwc_otg_module_params.
+							phy_ulpi_ext_vbus);
+	}
+	if (dwc_otg_module_params.phy_utmi_width != -1) {
+		retval +=
+		    dwc_otg_set_param_phy_utmi_width(core_if,
+						     dwc_otg_module_params.
+						     phy_utmi_width);
+	}
+	if (dwc_otg_module_params.ulpi_fs_ls != -1) {
+		retval +=
+		    dwc_otg_set_param_ulpi_fs_ls(core_if,
+						 dwc_otg_module_params.ulpi_fs_ls);
+	}
+	if (dwc_otg_module_params.ts_dline != -1) {
+		retval +=
+		    dwc_otg_set_param_ts_dline(core_if,
+					       dwc_otg_module_params.ts_dline);
+	}
+	if (dwc_otg_module_params.i2c_enable != -1) {
+		retval +=
+		    dwc_otg_set_param_i2c_enable(core_if,
+						 dwc_otg_module_params.
+						 i2c_enable);
+	}
+	if (dwc_otg_module_params.en_multiple_tx_fifo != -1) {
+		retval +=
+		    dwc_otg_set_param_en_multiple_tx_fifo(core_if,
+							  dwc_otg_module_params.
+							  en_multiple_tx_fifo);
+	}
+
+	for (i = 0; i < 7; i++) {
+		if (dwc_otg_module_params.dev_perio_tx_fifo_size[i] != -1) {
+			retval +=
+			    dwc_otg_set_param_dev_perio_tx_fifo_size(core_if,
+								     dwc_otg_module_params.
+								     dev_perio_tx_fifo_size
+								     [i], i);
+		}
+	}
+
+	for (i = 0; i < 7; i++) {
+		if (dwc_otg_module_params.dev_tx_fifo_size[i] != -1) {
+			retval += dwc_otg_set_param_dev_tx_fifo_size(core_if,
+								     dwc_otg_module_params.
+								     dev_tx_fifo_size
+								     [i], i);
+		}
+	}
+	if (dwc_otg_module_params.thr_ctl != -1) {
+		retval +=
+		    dwc_otg_set_param_thr_ctl(core_if,
+					      dwc_otg_module_params.thr_ctl);
+	}
+	if (dwc_otg_module_params.mpi_enable != -1) {
+		retval +=
+		    dwc_otg_set_param_mpi_enable(core_if,
+						 dwc_otg_module_params.
+						 mpi_enable);
+	}
+	if (dwc_otg_module_params.pti_enable != -1) {
+		retval +=
+		    dwc_otg_set_param_pti_enable(core_if,
+						 dwc_otg_module_params.
+						 pti_enable);
+	}
+	if (dwc_otg_module_params.lpm_enable != -1) {
+		retval +=
+		    dwc_otg_set_param_lpm_enable(core_if,
+						 dwc_otg_module_params.
+						 lpm_enable);
+	}
+	if (dwc_otg_module_params.ic_usb_cap != -1) {
+		retval +=
+		    dwc_otg_set_param_ic_usb_cap(core_if,
+						 dwc_otg_module_params.
+						 ic_usb_cap);
+	}
+	if (dwc_otg_module_params.tx_thr_length != -1) {
+		retval +=
+		    dwc_otg_set_param_tx_thr_length(core_if,
+						    dwc_otg_module_params.tx_thr_length);
+	}
+	if (dwc_otg_module_params.rx_thr_length != -1) {
+		retval +=
+		    dwc_otg_set_param_rx_thr_length(core_if,
+						    dwc_otg_module_params.
+						    rx_thr_length);
+	}
+	if (dwc_otg_module_params.ahb_thr_ratio != -1) {
+		retval +=
+		    dwc_otg_set_param_ahb_thr_ratio(core_if,
+						    dwc_otg_module_params.ahb_thr_ratio);
+	}
+	if (dwc_otg_module_params.power_down != -1) {
+		retval +=
+		    dwc_otg_set_param_power_down(core_if,
+						 dwc_otg_module_params.power_down);
+	}
+	if (dwc_otg_module_params.reload_ctl != -1) {
+		retval +=
+		    dwc_otg_set_param_reload_ctl(core_if,
+						 dwc_otg_module_params.reload_ctl);
+	}
+
+	if (dwc_otg_module_params.dev_out_nak != -1) {
+		retval +=
+			dwc_otg_set_param_dev_out_nak(core_if,
+			dwc_otg_module_params.dev_out_nak);
+	}
+
+	if (dwc_otg_module_params.cont_on_bna != -1) {
+		retval +=
+			dwc_otg_set_param_cont_on_bna(core_if,
+			dwc_otg_module_params.cont_on_bna);
+	}
+
+	if (dwc_otg_module_params.ahb_single != -1) {
+		retval +=
+			dwc_otg_set_param_ahb_single(core_if,
+			dwc_otg_module_params.ahb_single);
+	}
+
+	if (dwc_otg_module_params.otg_ver != -1) {
+		retval +=
+		    dwc_otg_set_param_otg_ver(core_if,
+					      dwc_otg_module_params.otg_ver);
+	}
+	if (dwc_otg_module_params.adp_enable != -1) {
+		retval +=
+		    dwc_otg_set_param_adp_enable(core_if,
+						 dwc_otg_module_params.
+						 adp_enable);
+	}
+	return retval;
+}
+
+#ifdef CONFIG_MACH_M822XX
+
+/**
+ * Transced Platform specific initialization for USB HUB on-board
+ */
+static void transcede_usb_hub_init(void)
+{
+	uint32_t data1;
+	uint32_t data2;
+
+	data1 = READL(TRANSCEDE_GPIO_OUTPUT_REG);
+	data2 = READL(TRANSCEDE_GPIO_OE_REG);
+
+	data1 |= 1 << 8;  /* Setup GPIO as initial output value of high (Not reset) */
+	data2 |= 1 << 8;  /* Setup GPIO 8 as output */
+
+	WRITEL(data1, TRANSCEDE_GPIO_OUTPUT_REG);
+	WRITEL(data2, TRANSCEDE_GPIO_OE_REG);
+
+	mdelay(4); /* Hold reset high */
+
+	/* Set hub reset low for 1 millisecond */
+	WRITEL(data1 & ~(1 << 8), TRANSCEDE_GPIO_OUTPUT_REG);
+	mdelay(4);
+
+	/* Release reset */
+	WRITEL(data1, TRANSCEDE_GPIO_OUTPUT_REG);
+	mdelay(4);
+}
+
+/**
+ * Transced Platform specific initialization for USB 2 PHY
+ */
+
+static void transcede_usb_phy_init(void)
+{
+	unsigned long rd_data;
+
+	/* Getting "USB_MISC" clock enable */
+	rd_data = (readl(AXI_CLK_CNTRL_2) | 0x10);
+	writel(rd_data, AXI_CLK_CNTRL_2);
+	mdelay(1);
+
+	/* Getting "USB_MISC_AXI_Reset" Out of reset */
+	rd_data = (readl(AXI_RESET_2) & 0xFFFFFFEF);
+	writel(rd_data, AXI_RESET_2);
+	mdelay(1);
+
+	/* Enabling USB AXI clock */
+	rd_data = (readl(AXI_CLK_CNTRL_2) | 0x8);
+	writel(rd_data, AXI_CLK_CNTRL_2);
+	mdelay(1);
+
+	/* Writing in USB0_PHY_CTRL_REG0 register */
+	writel(0x00000000, USB0_PHY_CTRL_REG0);
+
+	rd_data = 0
+		// | USB_CTRL_TXBITSTUFFEN  // (1 << 0)
+		// | USB_CTRL_TXBITSTUFFENH // (1 << 1)
+		| USB_CTRL_VBUSVLDEXTSEL    // (1 << 2)
+		| USB_CTRL_VBUSVLDEXT       // (1 << 3)
+		// | USB_CTRL_OTGDISABLE    // (1 << 4)
+		// | USB_CTRL_VATESTENB     // (1 << 5)
+		// | USB_CTRL_SIDDQ         // (1 << 6)
+		// | USB_CTRL_LOOPBACKENB   // (1 << 7)
+		// | USB_CTRL_PORTRESET     // (1 << 8)
+		| USB_CTRL_REFCLKDIV(0)     // (x << 16)
+		| USB_CTRL_REFCLKSEL(0)     // (x << 20)
+		// | USB_CTRL_COMMONONN     // (1 << 24)
+		;
+
+	rd_data = READL(USB_PHY_SCALEDOWN_ADDR);
+	//
+	// Clear bits 9, 8, 1 and 0:
+	//   [1:0] ss_scaledown_mode: Controller Scale-down Mode
+	//         USB Scaledown Mode = 0 (Disabled) by default!
+	//   [8]   usb_iddig_sel:
+	//         Select Line to Multiplexer that selects between the
+	//         PHY ID pin's digital output & sideband register value
+	//         usb_iddig_sel = 0, by default
+	//   [9]   usb_iddig_reg
+	//         Sideband Register value to bypass the ID value detected by the PHY
+	//         usb_iddig_reg = 0 (Controller is Host), by default
+	//
+	rd_data &= 0xFFFFFCFC;
+	//
+	// Set bit 8 (usb_iddig_sel) to 1, leave other bits cleared
+	//
+	rd_data |= (1 << 8);
+	/* rd_data = 0x4; */
+	WRITEL(rd_data, USB_PHY_SCALEDOWN_ADDR);
+	mdelay(1);
+
+	/* Put UTMI PHY out of reset here */
+	writel(0xcc, USB_RST_CNTRL);
+	mdelay(2);
+
+	/* Put USB PHY out of reset here */
+	writel(0xee, USB_RST_CNTRL);
+	mdelay(2);
+
+	rd_data = (readl(AXI_RESET_2) & 0xFFFFFFF7);
+	writel(rd_data, AXI_RESET_2);
+	mdelay(1);
+
+	/* Put USB PHY out of reset here */
+	writel(0xee, USB_RST_CNTRL);
+	mdelay(2);
+
+	/* Put UTMI PHY out of reset here */
+	writel(0xcc, USB_RST_CNTRL);
+	mdelay(2);
+
+
+	rd_data = (readl(AXI_RESET_2) & 0xFFFFFFF7);
+	writel(rd_data, AXI_RESET_2);
+	mdelay(1);
+}
+
+/**
+ * Platform specific initialization for Dwc OTG Controller
+ */
+static void transcede_start_dwc_otg(void)
+{
+	/* T2200 EVMs have a hub device with reset on GPIO 8
+	 * Initalize the hub by making GPIO 8 a reset
+	 * setting high, then low, then back to high
+	 * to reset the external hub into a known good state */
+	transcede_usb_hub_init();
+	transcede_usb_phy_init();
+}
+
+
+/**
+ * Platform specific Shutdown for Dwc OTG Controller
+ */
+static void transcede_stop_dwc_otg(void)
+{
+	uint32_t rd_data;
+
+	/* APPLYING THE RESET TO USB2 UTMI */
+	// Set bits 1 and 5 of USB reset control register
+	rd_data  = READL(USB_RST_CNTRL);
+	rd_data |= ((1<<5) | (1<<1));
+	WRITEL(rd_data, USB_RST_CNTRL);
+
+	/* APPLYING THE RESET TO USB2 PHY */
+	// Set bits 0 and 4 or USB reset control register
+	rd_data  = READL(USB_RST_CNTRL);
+	rd_data |= ((1<<4) | (1<<0));
+	WRITEL(rd_data, USB_RST_CNTRL);
+
+	/* APLLYING RESET TO USB2 AXI RESET */
+	// Set bit 4 of AXI reset register that has USB AXI reset
+	rd_data  = READL(USB_AXI_RST_CTRL);
+	rd_data |= (1<<4);
+	WRITEL(rd_data, USB_AXI_RST_CTRL);
+
+	/* Disable the Clock */
+	/* Clear AXI clock control bit 4 (Clear AXI PHY clock) */
+	/* To enable AXI clock to USB block */
+	rd_data = READL(USB_AXI_CLK_CTRL);
+	rd_data &= ~(1<<4);
+	WRITEL(rd_data, USB_AXI_CLK_CTRL);
+}
+
+#endif  /* CONFIG_MACH_M822XX */
+
+
+/**
+ * This function is the top level interrupt handler for the Common
+ * (Device and host modes) interrupts.
+ */
+static irqreturn_t dwc_otg_common_irq(int irq, void *dev)
+{
+	int32_t retval = IRQ_NONE;
+
+	retval = dwc_otg_handle_common_intr(dev);
+	if (retval != 0) {
+		S3C2410X_CLEAR_EINTPEND();
+	}
+	return IRQ_RETVAL(retval);
+}
+
+/**
+ * This function is called when a lm_device is unregistered with the
+ * dwc_otg_driver. This happens, for example, when the rmmod command is
+ * executed. The device may or may not be electrically present. If it is
+ * present, the driver stops device processing. Any resources used on behalf
+ * of this device are freed.
+ *
+ * @param _dev
+ */
+static int dwc_otg_driver_remove(
+#ifdef LM_INTERFACE
+					 struct lm_device *_dev
+#elif defined(PCI_INTERFACE)
+					 struct pci_dev *_dev
+#else
+					 struct platform_device *_dev
+#endif
+    )
+{
+#ifdef LM_INTERFACE
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = pci_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = platform_get_drvdata(_dev);
+#endif
+
+	DWC_DEBUGPL(DBG_ANY, "%s(%p)\n", __func__, _dev);
+
+	if (!otg_dev) {
+		/* Memory allocation for the dwc_otg_device failed. */
+		DWC_DEBUGPL(DBG_ANY, "%s: otg_dev NULL!\n", __func__);
+		return -1;
+	}
+#ifndef DWC_DEVICE_ONLY
+	if (otg_dev->hcd) {
+		hcd_remove(_dev);
+	} else {
+		DWC_DEBUGPL(DBG_ANY, "%s: otg_dev->hcd NULL!\n", __func__);
+		return -1;
+	}
+#endif
+
+#ifndef DWC_HOST_ONLY
+	if (otg_dev->pcd) {
+		pcd_remove(_dev);
+	} else {
+		DWC_DEBUGPL(DBG_ANY, "%s: otg_dev->pcd NULL!\n", __func__);
+		return -1;
+	}
+#endif
+	/*
+	 * Free the IRQ
+	 */
+	if (otg_dev->common_irq_installed) {
+		free_irq(otg_dev->irq, otg_dev);
+	} else {
+		DWC_DEBUGPL(DBG_ANY, "%s: There is no installed irq!\n", __func__);
+		return -1;
+	}
+
+	if (otg_dev->core_if) {
+		dwc_otg_cil_remove(otg_dev->core_if);
+	} else {
+		DWC_DEBUGPL(DBG_ANY, "%s: otg_dev->core_if NULL!\n", __func__);
+		return -1;
+	}
+
+	/*
+	 * Remove the device attributes
+	 */
+#if 1
+	dwc_otg_attr_remove(_dev);
+#else
+	dwc_otg_attr_remove(otg_dev->os_dep.parent);
+#endif
+	/*
+	 * Return the memory.
+	 */
+	if (otg_dev->os_dep.base) {
+		iounmap(otg_dev->os_dep.base);
+	}
+
+	/* Do the Platform specific shutdown */
+#ifdef CONFIG_MACH_M822XX
+    transcede_stop_dwc_otg();
+#endif	/* CONFIG_MACH_M822XX */
+
+	DWC_FREE(otg_dev);
+
+	/*
+	 * Clear the drvdata pointer.
+	 */
+#ifdef LM_INTERFACE
+	lm_set_drvdata(_dev, 0);
+#elif defined(PCI_INTERFACE)
+    release_mem_region(otg_dev->os_dep.rsrc_start, otg_dev->os_dep.rsrc_len);
+	pci_set_drvdata(_dev, 0);
+#else
+	platform_set_drvdata(_dev, 0);
+#endif
+
+	return 0;
+}
+
+/**
+ * This function is called when an lm_device is bound to a
+ * dwc_otg_driver. It creates the driver components required to
+ * control the device (CIL, HCD, and PCD) and it initializes the
+ * device. The driver components are stored in a dwc_otg_device
+ * structure. A reference to the dwc_otg_device is saved in the
+ * lm_device. This allows the driver to access the dwc_otg_device
+ * structure on subsequent calls to driver methods for this device.
+ *
+ * @param _dev Bus device
+ */
+static int dwc_otg_driver_probe(
+#ifdef LM_INTERFACE
+				       struct lm_device *_dev
+#elif defined(PCI_INTERFACE)
+				       struct pci_dev *_dev,
+				       const struct pci_device_id *id
+#else
+						struct platform_device *_dev
+#endif
+    )
+{
+	int retval = 0;
+	dwc_otg_device_t *dwc_otg_device;
+
+	dev_dbg(&_dev->dev, "dwc_otg_driver_probe(%p)\n", _dev);
+#ifdef LM_INTERFACE
+	dev_dbg(&_dev->dev, "start=0x%08x\n", (unsigned)_dev->resource.start);
+#elif defined(PCI_INTERFACE)
+	if (!id) {
+		DWC_ERROR("Invalid pci_device_id %p", id);
+		return -EINVAL;
+	}
+
+	if (!_dev || (pci_enable_device(_dev) < 0)) {
+		DWC_ERROR("Invalid pci_device %p", _dev);
+		return -ENODEV;
+	}
+	dev_dbg(&_dev->dev, "start=0x%08x\n", (unsigned)pci_resource_start(_dev,0));
+	/* other stuff needed as well? */
+#else
+	dev_dbg(&_dev->dev, "start=0x%08x\n", (unsigned)_dev->resource->start);
+#endif
+
+	dwc_otg_device = DWC_ALLOC(sizeof(dwc_otg_device_t));
+
+	if (!dwc_otg_device) {
+		dev_err(&_dev->dev, "kmalloc of dwc_otg_device failed\n");
+		return -ENOMEM;
+	}
+
+	memset(dwc_otg_device, 0, sizeof(*dwc_otg_device));
+	dwc_otg_device->os_dep.reg_offset = 0xFFFFFFFF;
+
+	/* Platform Specific initialization */
+#ifdef CONFIG_MACH_M822XX
+	transcede_start_dwc_otg();
+	/* SET_DEBUG_LEVEL(DBG_ANY); */
+#endif	/* CONFIG_MACH_M822XX */
+
+	dwc_otg_device->os_dep.parent= &_dev->dev;
+
+	/*
+	 * Map the DWC_otg Core memory into virtual address space.
+	 */
+#ifdef LM_INTERFACE
+	dwc_otg_device->os_dep.base = ioremap(_dev->resource.start, SZ_256K);
+
+	if (!dwc_otg_device->os_dep.base) {
+		dev_err(&_dev->dev, "ioremap() failed\n");
+		DWC_FREE(dwc_otg_device);
+		return -ENOMEM;
+	}
+	dev_dbg(&_dev->dev, "base=0x%08x\n",
+		(unsigned)dwc_otg_device->os_dep.base);
+#elif defined(PCI_INTERFACE)
+	_dev->current_state = PCI_D0;
+	_dev->dev.power.power_state = PMSG_ON;
+
+	if (!_dev->irq) {
+		DWC_ERROR("Found HC with no IRQ. Check BIOS/PCI %s setup!",
+			  pci_name(_dev));
+		iounmap(dwc_otg_device->os_dep.base);
+		DWC_FREE(dwc_otg_device);
+		return -ENODEV;
+	}
+
+	dwc_otg_device->os_dep.rsrc_start = pci_resource_start(_dev, 0);
+	dwc_otg_device->os_dep.rsrc_len = pci_resource_len(_dev, 0);
+	DWC_DEBUGPL(DBG_ANY, "PCI resource: start=%08x, len=%08x\n",
+		    (unsigned)dwc_otg_device->os_dep.rsrc_start,
+		    (unsigned)dwc_otg_device->os_dep.rsrc_len);
+	if (!request_mem_region
+	    (dwc_otg_device->os_dep.rsrc_start, dwc_otg_device->os_dep.rsrc_len,
+	     "dwc_otg")) {
+		dev_dbg(&_dev->dev, "error requesting memory\n");
+		iounmap(dwc_otg_device->os_dep.base);
+		DWC_FREE(dwc_otg_device);
+		return -EFAULT;
+	}
+
+	dwc_otg_device->os_dep.base =
+	    ioremap_nocache(dwc_otg_device->os_dep.rsrc_start,
+			    dwc_otg_device->os_dep.rsrc_len);
+	if (dwc_otg_device->os_dep.base == NULL) {
+		dev_dbg(&_dev->dev, "error mapping memory\n");
+		release_mem_region(dwc_otg_device->os_dep.rsrc_start,
+				   dwc_otg_device->os_dep.rsrc_len);
+		iounmap(dwc_otg_device->os_dep.base);
+		DWC_FREE(dwc_otg_device);
+		return -EFAULT;
+	}
+	dev_dbg(&_dev->dev, "base=0x%p (before adjust) \n",
+		dwc_otg_device->os_dep.base);
+	dwc_otg_device->os_dep.base = (char *)dwc_otg_device->os_dep.base;
+	dev_dbg(&_dev->dev, "base=0x%p (after adjust) \n",
+		dwc_otg_device->os_dep.base);
+	dev_dbg(&_dev->dev, "%s: mapped PA 0x%x to VA 0x%p\n", __func__,
+		(unsigned)dwc_otg_device->os_dep.rsrc_start,
+		dwc_otg_device->os_dep.base);
+
+	pci_set_master(_dev);
+	pci_set_drvdata(_dev, dwc_otg_device);
+#else
+	dwc_otg_device->os_dep.base = ioremap(_dev->resource->start, SZ_256K);
+
+	if (!dwc_otg_device->os_dep.base) {
+		dev_err(&_dev->dev, "ioremap() failed\n");
+		DWC_FREE(dwc_otg_device);
+		return -ENOMEM;
+	}
+	dev_dbg(&_dev->dev, "base=0x%08x\n",
+		(unsigned)dwc_otg_device->os_dep.base);
+#endif
+
+	/*
+	 * Retrieve the memory and IRQ resources.
+	 */
+	 dwc_otg_device->irq = platform_get_irq(_dev, 0);
+	if (dwc_otg_device->irq <= 0) {
+		dev_err(&_dev->dev, "no device irq\n");
+		retval = -EINVAL;
+		goto fail;
+	}
+
+	/*
+	 * Initialize driver data to point to the global DWC_otg
+	 * Device structure.
+	 */
+#ifdef LM_INTERFACE
+	lm_set_drvdata(_dev, dwc_otg_device);
+#else
+	platform_set_drvdata(_dev, dwc_otg_device);
+#endif
+	dev_dbg(&_dev->dev, "dwc_otg_device=0x%p\n", dwc_otg_device);
+
+	dwc_otg_device->core_if = dwc_otg_cil_init(dwc_otg_device->os_dep.base);
+	if (!dwc_otg_device->core_if) {
+		dev_err(&_dev->dev, "CIL initialization failed!\n");
+		retval = -ENOMEM;
+		goto fail;
+	}
+
+	/*
+	 * Attempt to ensure this device is really a DWC_otg Controller.
+	 * Read and verify the SNPSID register contents. The value should be
+	 * 0x45F42XXX, which corresponds to "OT2", as in "OTG version 2.XX".
+	 */
+
+	if ((dwc_otg_get_gsnpsid(dwc_otg_device->core_if) & 0xFFFFF000) !=
+	    0x4F542000) {
+		dev_err(&_dev->dev, "Bad value for SNPSID: 0x%08x\n",
+			dwc_otg_get_gsnpsid(dwc_otg_device->core_if));
+		retval = -EINVAL;
+		goto fail;
+	}
+
+	/*
+	 * Validate parameter values.
+	 */
+	if (set_parameters(dwc_otg_device->core_if)) {
+		retval = -EINVAL;
+		goto fail;
+	}
+
+	/*
+	 * Create Device Attributes in sysfs
+	 */
+	//dwc_otg_attr_create(&_dev->dev);
+	dwc_otg_attr_create(_dev);
+
+	/*
+	 * Disable the global interrupt until all the interrupt
+	 * handlers are installed.
+	 */
+	dwc_otg_disable_global_interrupts(dwc_otg_device->core_if);
+
+	/*
+	 * Install the interrupt handler for the common interrupts before
+	 * enabling common interrupts in core_init below.
+	 */
+	DWC_DEBUGPL(DBG_CIL, "registering (common) handler for irq%d\n",
+		    dwc_otg_device->irq);
+	retval = request_irq(dwc_otg_device->irq, dwc_otg_common_irq,
+			     IRQF_SHARED | IRQF_DISABLED | IRQ_LEVEL, "dwc_otg",
+			     dwc_otg_device);
+	if (retval) {
+		DWC_ERROR("request of irq%d failed\n", dwc_otg_device->irq);
+		retval = -EBUSY;
+		goto fail;
+	} else {
+		dwc_otg_device->common_irq_installed = 1;
+	}
+
+#ifdef LM_INTERFACE
+	set_irq_type(_dev->irq, IRQT_LOW);
+#else
+	/* FIXME : Makarand - Need to check about it */
+	//set_irq_type(dwc_otg_device->irq, IRQT_LOW);
+#endif
+	/*
+	 * Initialize the DWC_otg core.
+	 */
+	dwc_otg_core_init(dwc_otg_device->core_if);
+
+#ifndef DWC_HOST_ONLY
+	/*
+	 * Initialize the PCD
+	 */
+	retval = pcd_init(_dev);
+	if (retval != 0) {
+		DWC_ERROR("pcd_init failed\n");
+		dwc_otg_device->pcd = NULL;
+		goto fail;
+	}
+#endif
+#ifndef DWC_DEVICE_ONLY
+	/*
+	 * Initialize the HCD
+	 */
+	retval = hcd_init(_dev);
+	if (retval != 0) {
+		DWC_ERROR("hcd_init failed\n");
+		dwc_otg_device->hcd = NULL;
+		goto fail;
+	}
+#endif
+#ifdef PCI_INTERFACE
+	pci_set_drvdata(_dev, dwc_otg_device);
+	dwc_otg_device->os_dep.pcidev = _dev;
+#endif
+
+	/*
+	 * Enable the global interrupt after all the interrupt
+	 * handlers are installed if there is no ADP support else
+	 * perform initial actions required for Internal ADP logic.
+	 */
+	if (!dwc_otg_get_param_adp_enable(dwc_otg_device->core_if))
+		dwc_otg_enable_global_interrupts(dwc_otg_device->core_if);
+	else
+		dwc_otg_adp_start(dwc_otg_device->core_if,
+							dwc_otg_is_host_mode(dwc_otg_device->core_if));
+
+	return 0;
+
+fail:
+	dwc_otg_driver_remove(_dev);
+	return retval;
+}
+
+/**
+ * Transcede Platform Specific Controller Suspend Routine.
+ * It does a Port suspend, and gate the clock (Power Off L1).
+ * Controller and PHY reset (Power Off L2) functionality
+ * is not supported
+ */
+int transcede_usb2_bus_suspend(struct platform_device * pd, pm_message_t state)
+{
+	int error_status = 0, val = 0;
+	struct usb_hcd *hcd = NULL;
+
+	hcd = (struct usb_hcd *) platform_get_drvdata(pd);
+
+	/* Do the port suspend for USB 2.0 Controller */
+	dwc_otg_host_port_suspend(hcd);
+
+	for (val = 0 ; val < 50 ; val++)
+		udelay(1000);
+
+	/* Disable the Clock */
+
+	/* FIXME: for initial testing, don't mess with the clock, leave alone */
+	/* clk_disable(usb2_clk); */
+
+	return error_status;
+}
+
+/**
+ * Transcede Platform Specific Controller Resume Routiene.
+ * It does a Port resume, and enable the clock (Power Off L1).
+ * Resume from (Power Off L2) is not supported
+ */
+int transcede_usb2_bus_resume(struct platform_device *pd)
+{
+	int error_status = 0;
+	struct usb_hcd *hcd = NULL;
+
+	/* Enable the Clock */
+	/* FIXME: for initial testing, don't mess with the clock, leave alone */
+	/* if (clk_enable(usb2_clk)){ */
+	/* 	pr_err("transcede_usb2_bus_resume_dummy:Unable to enable the usb2 clock \n"); */
+	/* } */
+
+	hcd = (struct usb_hcd *) platform_get_drvdata(pd);
+
+	dwc_otg_host_port_resume(hcd);
+
+	return error_status;
+}
+
+
+/**
+ * This structure defines the methods to be called by a bus driver
+ * during the lifecycle of a device on that bus. Both drivers and
+ * devices are registered with a bus driver. The bus driver matches
+ * devices to drivers based on information in the device and driver
+ * structures.
+ *
+ * The probe function is called when the bus driver matches a device
+ * to this driver. The remove function is called when a device is
+ * unregistered with the bus driver.
+ */
+static struct platform_driver dwc_otg_driver = {
+	.driver = {
+		.name	= "dwc_otg",
+		.owner  = THIS_MODULE,
+	},
+	.probe		= dwc_otg_driver_probe,
+	.remove		= dwc_otg_driver_remove,
+#ifdef CONFIG_MACH_M822XX
+	.suspend = transcede_usb2_bus_suspend,
+	.resume  = transcede_usb2_bus_resume,
+#endif	/* CONFIG_MACH_M822XX */
+};
+
+/**
+ * This function is called when the dwc_otg_driver is installed with the
+ * insmod command. It registers the dwc_otg_driver structure with the
+ * appropriate bus driver. This will cause the dwc_otg_driver_probe function
+ * to be called. In addition, the bus driver will automatically expose
+ * attributes defined for the device and driver in the special sysfs file
+ * system.
+ *
+ * @return
+ */
+static int __init dwc_otg_driver_init(void)
+{
+	int retval = 0;
+	int error;
+
+	printk(KERN_INFO "%s: version %s\n", dwc_driver_name, DWC_DRIVER_VERSION);
+
+	retval = platform_driver_register(&dwc_otg_driver);
+
+	if (retval) {
+		printk(KERN_ERR "%s:%s: platform_driver_register retval=%d\n",
+		       __func__,
+		       dwc_driver_name,
+		       retval);
+
+		return retval;
+	} else {
+		printk(KERN_INFO "%s:%s: platform_driver_register retval=%d\n",
+		       __func__,
+		       dwc_driver_name,
+		       retval
+		      );
+
+		error = driver_create_file(&dwc_otg_driver.driver, &driver_attr_version);
+		printk(KERN_INFO "%s:%s: driver_create_file version error=%d\n",
+		       __func__,
+		       dwc_driver_name,
+		       error
+		      );
+		error = driver_create_file(&dwc_otg_driver.driver, &driver_attr_debuglevel);
+		printk(KERN_INFO "%s:%s: driver_create_file debugLevel error=%d\n",
+		       __func__,
+		       dwc_driver_name,
+		       error
+		      );
+
+		return retval;
+	}
+}
+module_init(dwc_otg_driver_init);
+
+/**
+ * This function is called when the driver is removed from the kernel
+ * with the rmmod command. The driver unregisters itself with its bus
+ * driver.
+ *
+ */
+static void __exit dwc_otg_driver_cleanup(void)
+{
+	printk(KERN_DEBUG "dwc_otg_driver_cleanup()\n");
+
+	driver_remove_file(&dwc_otg_driver.driver, &driver_attr_debuglevel);
+	driver_remove_file(&dwc_otg_driver.driver, &driver_attr_version);
+
+	platform_driver_unregister(&dwc_otg_driver);
+
+	printk(KERN_INFO "%s module removed\n", dwc_driver_name);
+}
+module_exit(dwc_otg_driver_cleanup);
+
+MODULE_DESCRIPTION(DWC_DRIVER_DESC);
+MODULE_AUTHOR("Synopsys Inc.");
+MODULE_LICENSE("GPL");
+
+module_param_named(otg_cap, dwc_otg_module_params.otg_cap, int, 0444);
+MODULE_PARM_DESC(otg_cap, "OTG Capabilities 0=HNP&SRP 1=SRP Only 2=None");
+module_param_named(opt, dwc_otg_module_params.opt, int, 0444);
+MODULE_PARM_DESC(opt, "OPT Mode");
+module_param_named(dma_enable, dwc_otg_module_params.dma_enable, int, 0444);
+MODULE_PARM_DESC(dma_enable, "DMA Mode 0=Slave 1=DMA enabled");
+
+module_param_named(dma_desc_enable, dwc_otg_module_params.dma_desc_enable, int,
+		   0444);
+MODULE_PARM_DESC(dma_desc_enable,
+		 "DMA Desc Mode 0=Address DMA 1=DMA Descriptor enabled");
+
+module_param_named(dma_burst_size, dwc_otg_module_params.dma_burst_size, int,
+		   0444);
+MODULE_PARM_DESC(dma_burst_size,
+		 "DMA Burst Size 1, 4, 8, 16, 32, 64, 128, 256");
+module_param_named(speed, dwc_otg_module_params.speed, int, 0444);
+MODULE_PARM_DESC(speed, "Speed 0=High Speed 1=Full Speed");
+module_param_named(host_support_fs_ls_low_power,
+		   dwc_otg_module_params.host_support_fs_ls_low_power, int,
+		   0444);
+MODULE_PARM_DESC(host_support_fs_ls_low_power,
+		 "Support Low Power w/FS or LS 0=Support 1=Don't Support");
+module_param_named(host_ls_low_power_phy_clk,
+		   dwc_otg_module_params.host_ls_low_power_phy_clk, int, 0444);
+MODULE_PARM_DESC(host_ls_low_power_phy_clk,
+		 "Low Speed Low Power Clock 0=48Mhz 1=6Mhz");
+module_param_named(enable_dynamic_fifo,
+		   dwc_otg_module_params.enable_dynamic_fifo, int, 0444);
+MODULE_PARM_DESC(enable_dynamic_fifo, "0=cC Setting 1=Allow Dynamic Sizing");
+module_param_named(data_fifo_size, dwc_otg_module_params.data_fifo_size, int,
+		   0444);
+MODULE_PARM_DESC(data_fifo_size,
+		 "Total number of words in the data FIFO memory 32-32768");
+module_param_named(dev_rx_fifo_size, dwc_otg_module_params.dev_rx_fifo_size,
+		   int, 0444);
+MODULE_PARM_DESC(dev_rx_fifo_size, "Number of words in the Rx FIFO 16-32768");
+module_param_named(dev_nperio_tx_fifo_size,
+		   dwc_otg_module_params.dev_nperio_tx_fifo_size, int, 0444);
+MODULE_PARM_DESC(dev_nperio_tx_fifo_size,
+		 "Number of words in the non-periodic Tx FIFO 16-32768");
+module_param_named(dev_perio_tx_fifo_size_1,
+		   dwc_otg_module_params.dev_perio_tx_fifo_size[0], int, 0444);
+MODULE_PARM_DESC(dev_perio_tx_fifo_size_1,
+		 "Number of words in the periodic Tx FIFO 4-768");
+module_param_named(dev_perio_tx_fifo_size_2,
+		   dwc_otg_module_params.dev_perio_tx_fifo_size[1], int, 0444);
+MODULE_PARM_DESC(dev_perio_tx_fifo_size_2,
+		 "Number of words in the periodic Tx FIFO 4-768");
+module_param_named(dev_perio_tx_fifo_size_3,
+		   dwc_otg_module_params.dev_perio_tx_fifo_size[2], int, 0444);
+MODULE_PARM_DESC(dev_perio_tx_fifo_size_3,
+		 "Number of words in the periodic Tx FIFO 4-768");
+module_param_named(dev_perio_tx_fifo_size_4,
+		   dwc_otg_module_params.dev_perio_tx_fifo_size[3], int, 0444);
+MODULE_PARM_DESC(dev_perio_tx_fifo_size_4,
+		 "Number of words in the periodic Tx FIFO 4-768");
+module_param_named(dev_perio_tx_fifo_size_5,
+		   dwc_otg_module_params.dev_perio_tx_fifo_size[4], int, 0444);
+MODULE_PARM_DESC(dev_perio_tx_fifo_size_5,
+		 "Number of words in the periodic Tx FIFO 4-768");
+module_param_named(dev_perio_tx_fifo_size_6,
+		   dwc_otg_module_params.dev_perio_tx_fifo_size[5], int, 0444);
+MODULE_PARM_DESC(dev_perio_tx_fifo_size_6,
+		 "Number of words in the periodic Tx FIFO 4-768");
+module_param_named(dev_perio_tx_fifo_size_7,
+		   dwc_otg_module_params.dev_perio_tx_fifo_size[6], int, 0444);
+MODULE_PARM_DESC(dev_perio_tx_fifo_size_7,
+		 "Number of words in the periodic Tx FIFO 4-768");
+module_param_named(dev_perio_tx_fifo_size_8,
+		   dwc_otg_module_params.dev_perio_tx_fifo_size[7], int, 0444);
+MODULE_PARM_DESC(dev_perio_tx_fifo_size_8,
+		 "Number of words in the periodic Tx FIFO 4-768");
+module_param_named(dev_perio_tx_fifo_size_9,
+		   dwc_otg_module_params.dev_perio_tx_fifo_size[8], int, 0444);
+MODULE_PARM_DESC(dev_perio_tx_fifo_size_9,
+		 "Number of words in the periodic Tx FIFO 4-768");
+module_param_named(dev_perio_tx_fifo_size_10,
+		   dwc_otg_module_params.dev_perio_tx_fifo_size[9], int, 0444);
+MODULE_PARM_DESC(dev_perio_tx_fifo_size_10,
+		 "Number of words in the periodic Tx FIFO 4-768");
+module_param_named(dev_perio_tx_fifo_size_11,
+		   dwc_otg_module_params.dev_perio_tx_fifo_size[10], int, 0444);
+MODULE_PARM_DESC(dev_perio_tx_fifo_size_11,
+		 "Number of words in the periodic Tx FIFO 4-768");
+module_param_named(dev_perio_tx_fifo_size_12,
+		   dwc_otg_module_params.dev_perio_tx_fifo_size[11], int, 0444);
+MODULE_PARM_DESC(dev_perio_tx_fifo_size_12,
+		 "Number of words in the periodic Tx FIFO 4-768");
+module_param_named(dev_perio_tx_fifo_size_13,
+		   dwc_otg_module_params.dev_perio_tx_fifo_size[12], int, 0444);
+MODULE_PARM_DESC(dev_perio_tx_fifo_size_13,
+		 "Number of words in the periodic Tx FIFO 4-768");
+module_param_named(dev_perio_tx_fifo_size_14,
+		   dwc_otg_module_params.dev_perio_tx_fifo_size[13], int, 0444);
+MODULE_PARM_DESC(dev_perio_tx_fifo_size_14,
+		 "Number of words in the periodic Tx FIFO 4-768");
+module_param_named(dev_perio_tx_fifo_size_15,
+		   dwc_otg_module_params.dev_perio_tx_fifo_size[14], int, 0444);
+MODULE_PARM_DESC(dev_perio_tx_fifo_size_15,
+		 "Number of words in the periodic Tx FIFO 4-768");
+module_param_named(host_rx_fifo_size, dwc_otg_module_params.host_rx_fifo_size,
+		   int, 0444);
+MODULE_PARM_DESC(host_rx_fifo_size, "Number of words in the Rx FIFO 16-32768");
+module_param_named(host_nperio_tx_fifo_size,
+		   dwc_otg_module_params.host_nperio_tx_fifo_size, int, 0444);
+MODULE_PARM_DESC(host_nperio_tx_fifo_size,
+		 "Number of words in the non-periodic Tx FIFO 16-32768");
+module_param_named(host_perio_tx_fifo_size,
+		   dwc_otg_module_params.host_perio_tx_fifo_size, int, 0444);
+MODULE_PARM_DESC(host_perio_tx_fifo_size,
+		 "Number of words in the host periodic Tx FIFO 16-32768");
+module_param_named(max_transfer_size, dwc_otg_module_params.max_transfer_size,
+		   int, 0444);
+/** @todo Set the max to 512K, modify checks */
+MODULE_PARM_DESC(max_transfer_size,
+		 "The maximum transfer size supported in bytes 2047-65535");
+module_param_named(max_packet_count, dwc_otg_module_params.max_packet_count,
+		   int, 0444);
+MODULE_PARM_DESC(max_packet_count,
+		 "The maximum number of packets in a transfer 15-511");
+module_param_named(host_channels, dwc_otg_module_params.host_channels, int,
+		   0444);
+MODULE_PARM_DESC(host_channels,
+		 "The number of host channel registers to use 1-16");
+module_param_named(dev_endpoints, dwc_otg_module_params.dev_endpoints, int,
+		   0444);
+MODULE_PARM_DESC(dev_endpoints,
+		 "The number of endpoints in addition to EP0 available for device mode 1-15");
+module_param_named(phy_type, dwc_otg_module_params.phy_type, int, 0444);
+MODULE_PARM_DESC(phy_type, "0=Reserved 1=UTMI+ 2=ULPI");
+module_param_named(phy_utmi_width, dwc_otg_module_params.phy_utmi_width, int,
+		   0444);
+MODULE_PARM_DESC(phy_utmi_width, "Specifies the UTMI+ Data Width 8 or 16 bits");
+module_param_named(phy_ulpi_ddr, dwc_otg_module_params.phy_ulpi_ddr, int, 0444);
+MODULE_PARM_DESC(phy_ulpi_ddr,
+		 "ULPI at double or single data rate 0=Single 1=Double");
+module_param_named(phy_ulpi_ext_vbus, dwc_otg_module_params.phy_ulpi_ext_vbus,
+		   int, 0444);
+MODULE_PARM_DESC(phy_ulpi_ext_vbus,
+		 "ULPI PHY using internal or external vbus 0=Internal");
+module_param_named(i2c_enable, dwc_otg_module_params.i2c_enable, int, 0444);
+MODULE_PARM_DESC(i2c_enable, "FS PHY Interface");
+module_param_named(ulpi_fs_ls, dwc_otg_module_params.ulpi_fs_ls, int, 0444);
+MODULE_PARM_DESC(ulpi_fs_ls, "ULPI PHY FS/LS mode only");
+module_param_named(ts_dline, dwc_otg_module_params.ts_dline, int, 0444);
+MODULE_PARM_DESC(ts_dline, "Term select Dline pulsing for all PHYs");
+module_param_named(debug, g_dbg_lvl, int, 0444);
+MODULE_PARM_DESC(debug, "");
+
+module_param_named(en_multiple_tx_fifo,
+		   dwc_otg_module_params.en_multiple_tx_fifo, int, 0444);
+MODULE_PARM_DESC(en_multiple_tx_fifo,
+		 "Dedicated Non Periodic Tx FIFOs 0=disabled 1=enabled");
+module_param_named(dev_tx_fifo_size_1,
+		   dwc_otg_module_params.dev_tx_fifo_size[0], int, 0444);
+MODULE_PARM_DESC(dev_tx_fifo_size_1, "Number of words in the Tx FIFO 4-768");
+module_param_named(dev_tx_fifo_size_2,
+		   dwc_otg_module_params.dev_tx_fifo_size[1], int, 0444);
+MODULE_PARM_DESC(dev_tx_fifo_size_2, "Number of words in the Tx FIFO 4-768");
+module_param_named(dev_tx_fifo_size_3,
+		   dwc_otg_module_params.dev_tx_fifo_size[2], int, 0444);
+MODULE_PARM_DESC(dev_tx_fifo_size_3, "Number of words in the Tx FIFO 4-768");
+module_param_named(dev_tx_fifo_size_4,
+		   dwc_otg_module_params.dev_tx_fifo_size[3], int, 0444);
+MODULE_PARM_DESC(dev_tx_fifo_size_4, "Number of words in the Tx FIFO 4-768");
+module_param_named(dev_tx_fifo_size_5,
+		   dwc_otg_module_params.dev_tx_fifo_size[4], int, 0444);
+MODULE_PARM_DESC(dev_tx_fifo_size_5, "Number of words in the Tx FIFO 4-768");
+module_param_named(dev_tx_fifo_size_6,
+		   dwc_otg_module_params.dev_tx_fifo_size[5], int, 0444);
+MODULE_PARM_DESC(dev_tx_fifo_size_6, "Number of words in the Tx FIFO 4-768");
+module_param_named(dev_tx_fifo_size_7,
+		   dwc_otg_module_params.dev_tx_fifo_size[6], int, 0444);
+MODULE_PARM_DESC(dev_tx_fifo_size_7, "Number of words in the Tx FIFO 4-768");
+module_param_named(dev_tx_fifo_size_8,
+		   dwc_otg_module_params.dev_tx_fifo_size[7], int, 0444);
+MODULE_PARM_DESC(dev_tx_fifo_size_8, "Number of words in the Tx FIFO 4-768");
+module_param_named(dev_tx_fifo_size_9,
+		   dwc_otg_module_params.dev_tx_fifo_size[8], int, 0444);
+MODULE_PARM_DESC(dev_tx_fifo_size_9, "Number of words in the Tx FIFO 4-768");
+module_param_named(dev_tx_fifo_size_10,
+		   dwc_otg_module_params.dev_tx_fifo_size[9], int, 0444);
+MODULE_PARM_DESC(dev_tx_fifo_size_10, "Number of words in the Tx FIFO 4-768");
+module_param_named(dev_tx_fifo_size_11,
+		   dwc_otg_module_params.dev_tx_fifo_size[10], int, 0444);
+MODULE_PARM_DESC(dev_tx_fifo_size_11, "Number of words in the Tx FIFO 4-768");
+module_param_named(dev_tx_fifo_size_12,
+		   dwc_otg_module_params.dev_tx_fifo_size[11], int, 0444);
+MODULE_PARM_DESC(dev_tx_fifo_size_12, "Number of words in the Tx FIFO 4-768");
+module_param_named(dev_tx_fifo_size_13,
+		   dwc_otg_module_params.dev_tx_fifo_size[12], int, 0444);
+MODULE_PARM_DESC(dev_tx_fifo_size_13, "Number of words in the Tx FIFO 4-768");
+module_param_named(dev_tx_fifo_size_14,
+		   dwc_otg_module_params.dev_tx_fifo_size[13], int, 0444);
+MODULE_PARM_DESC(dev_tx_fifo_size_14, "Number of words in the Tx FIFO 4-768");
+module_param_named(dev_tx_fifo_size_15,
+		   dwc_otg_module_params.dev_tx_fifo_size[14], int, 0444);
+MODULE_PARM_DESC(dev_tx_fifo_size_15, "Number of words in the Tx FIFO 4-768");
+
+module_param_named(thr_ctl, dwc_otg_module_params.thr_ctl, int, 0444);
+MODULE_PARM_DESC(thr_ctl,
+		 "Thresholding enable flag bit 0 - non ISO Tx thr., 1 - ISO Tx thr., 2 - Rx thr.- bit 0=disabled 1=enabled");
+module_param_named(tx_thr_length, dwc_otg_module_params.tx_thr_length, int,
+		   0444);
+MODULE_PARM_DESC(tx_thr_length, "Tx Threshold length in 32 bit DWORDs");
+module_param_named(rx_thr_length, dwc_otg_module_params.rx_thr_length, int,
+		   0444);
+MODULE_PARM_DESC(rx_thr_length, "Rx Threshold length in 32 bit DWORDs");
+
+module_param_named(pti_enable, dwc_otg_module_params.pti_enable, int, 0444);
+module_param_named(mpi_enable, dwc_otg_module_params.mpi_enable, int, 0444);
+module_param_named(lpm_enable, dwc_otg_module_params.lpm_enable, int, 0444);
+MODULE_PARM_DESC(lpm_enable, "LPM Enable 0=LPM Disabled 1=LPM Enabled");
+module_param_named(ic_usb_cap, dwc_otg_module_params.ic_usb_cap, int, 0444);
+MODULE_PARM_DESC(ic_usb_cap,
+		 "IC_USB Capability 0=IC_USB Disabled 1=IC_USB Enabled");
+module_param_named(ahb_thr_ratio, dwc_otg_module_params.ahb_thr_ratio, int,
+		   0444);
+MODULE_PARM_DESC(ahb_thr_ratio, "AHB Threshold Ratio");
+module_param_named(power_down, dwc_otg_module_params.power_down, int, 0444);
+MODULE_PARM_DESC(power_down, "Power Down Mode");
+module_param_named(reload_ctl, dwc_otg_module_params.reload_ctl, int, 0444);
+MODULE_PARM_DESC(reload_ctl, "HFIR Reload Control");
+module_param_named(dev_out_nak, dwc_otg_module_params.dev_out_nak, int, 0444);
+MODULE_PARM_DESC(dev_out_nak, "Enable Device OUT NAK");
+module_param_named(cont_on_bna, dwc_otg_module_params.cont_on_bna, int, 0444);
+MODULE_PARM_DESC(cont_on_bna, "Enable Enable Continue on BNA");
+module_param_named(ahb_single, dwc_otg_module_params.ahb_single, int, 0444);
+MODULE_PARM_DESC(ahb_single, "Enable AHB Single Support");
+module_param_named(adp_enable, dwc_otg_module_params.adp_enable, int, 0444);
+MODULE_PARM_DESC(adp_enable, "ADP Enable 0=ADP Disabled 1=ADP Enabled");
+module_param_named(otg_ver, dwc_otg_module_params.otg_ver, int, 0444);
+MODULE_PARM_DESC(otg_ver, "OTG revision supported 0=OTG 1.3 1=OTG 2.0");
+
+/** @page "Module Parameters"
+ *
+ * The following parameters may be specified when starting the module.
+ * These parameters define how the DWC_otg controller should be
+ * configured. Parameter values are passed to the CIL initialization
+ * function dwc_otg_cil_init
+ *
+ * Example: <code>modprobe dwc_otg speed=1 otg_cap=1</code>
+ *
+
+ <table>
+ <tr><td>Parameter Name</td><td>Meaning</td></tr>
+
+ <tr>
+ <td>otg_cap</td>
+ <td>Specifies the OTG capabilities. The driver will automatically detect the
+ value for this parameter if none is specified.
+ - 0: HNP and SRP capable (default, if available)
+ - 1: SRP Only capable
+ - 2: No HNP/SRP capable
+ </td></tr>
+
+ <tr>
+ <td>dma_enable</td>
+ <td>Specifies whether to use slave or DMA mode for accessing the data FIFOs.
+ The driver will automatically detect the value for this parameter if none is
+ specified.
+ - 0: Slave
+ - 1: DMA (default, if available)
+ </td></tr>
+
+ <tr>
+ <td>dma_burst_size</td>
+ <td>The DMA Burst size (applicable only for External DMA Mode).
+ - Values: 1, 4, 8 16, 32, 64, 128, 256 (default 32)
+ </td></tr>
+
+ <tr>
+ <td>speed</td>
+ <td>Specifies the maximum speed of operation in host and device mode. The
+ actual speed depends on the speed of the attached device and the value of
+ phy_type.
+ - 0: High Speed (default)
+ - 1: Full Speed
+ </td></tr>
+
+ <tr>
+ <td>host_support_fs_ls_low_power</td>
+ <td>Specifies whether low power mode is supported when attached to a Full
+ Speed or Low Speed device in host mode.
+ - 0: Don't support low power mode (default)
+ - 1: Support low power mode
+ </td></tr>
+
+ <tr>
+ <td>host_ls_low_power_phy_clk</td>
+ <td>Specifies the PHY clock rate in low power mode when connected to a Low
+ Speed device in host mode. This parameter is applicable only if
+ HOST_SUPPORT_FS_LS_LOW_POWER is enabled.
+ - 0: 48 MHz (default)
+ - 1: 6 MHz
+ </td></tr>
+
+ <tr>
+ <td>enable_dynamic_fifo</td>
+ <td> Specifies whether FIFOs may be resized by the driver software.
+ - 0: Use cC FIFO size parameters
+ - 1: Allow dynamic FIFO sizing (default)
+ </td></tr>
+
+ <tr>
+ <td>data_fifo_size</td>
+ <td>Total number of 4-byte words in the data FIFO memory. This memory
+ includes the Rx FIFO, non-periodic Tx FIFO, and periodic Tx FIFOs.
+ - Values: 32 to 32768 (default 8192)
+
+ Note: The total FIFO memory depth in the FPGA configuration is 8192.
+ </td></tr>
+
+ <tr>
+ <td>dev_rx_fifo_size</td>
+ <td>Number of 4-byte words in the Rx FIFO in device mode when dynamic
+ FIFO sizing is enabled.
+ - Values: 16 to 32768 (default 1064)
+ </td></tr>
+
+ <tr>
+ <td>dev_nperio_tx_fifo_size</td>
+ <td>Number of 4-byte words in the non-periodic Tx FIFO in device mode when
+ dynamic FIFO sizing is enabled.
+ - Values: 16 to 32768 (default 1024)
+ </td></tr>
+
+ <tr>
+ <td>dev_perio_tx_fifo_size_n (n = 1 to 15)</td>
+ <td>Number of 4-byte words in each of the periodic Tx FIFOs in device mode
+ when dynamic FIFO sizing is enabled.
+ - Values: 4 to 768 (default 256)
+ </td></tr>
+
+ <tr>
+ <td>host_rx_fifo_size</td>
+ <td>Number of 4-byte words in the Rx FIFO in host mode when dynamic FIFO
+ sizing is enabled.
+ - Values: 16 to 32768 (default 1024)
+ </td></tr>
+
+ <tr>
+ <td>host_nperio_tx_fifo_size</td>
+ <td>Number of 4-byte words in the non-periodic Tx FIFO in host mode when
+ dynamic FIFO sizing is enabled in the core.
+ - Values: 16 to 32768 (default 1024)
+ </td></tr>
+
+ <tr>
+ <td>host_perio_tx_fifo_size</td>
+ <td>Number of 4-byte words in the host periodic Tx FIFO when dynamic FIFO
+ sizing is enabled.
+ - Values: 16 to 32768 (default 1024)
+ </td></tr>
+
+ <tr>
+ <td>max_transfer_size</td>
+ <td>The maximum transfer size supported in bytes.
+ - Values: 2047 to 65,535 (default 65,535)
+ </td></tr>
+
+ <tr>
+ <td>max_packet_count</td>
+ <td>The maximum number of packets in a transfer.
+ - Values: 15 to 511 (default 511)
+ </td></tr>
+
+ <tr>
+ <td>host_channels</td>
+ <td>The number of host channel registers to use.
+ - Values: 1 to 16 (default 12)
+
+ Note: The FPGA configuration supports a maximum of 12 host channels.
+ </td></tr>
+
+ <tr>
+ <td>dev_endpoints</td>
+ <td>The number of endpoints in addition to EP0 available for device mode
+ operations.
+ - Values: 1 to 15 (default 6 IN and OUT)
+
+ Note: The FPGA configuration supports a maximum of 6 IN and OUT endpoints in
+ addition to EP0.
+ </td></tr>
+
+ <tr>
+ <td>phy_type</td>
+ <td>Specifies the type of PHY interface to use. By default, the driver will
+ automatically detect the phy_type.
+ - 0: Full Speed
+ - 1: UTMI+ (default, if available)
+ - 2: ULPI
+ </td></tr>
+
+ <tr>
+ <td>phy_utmi_width</td>
+ <td>Specifies the UTMI+ Data Width. This parameter is applicable for a
+ phy_type of UTMI+. Also, this parameter is applicable only if the
+ OTG_HSPHY_WIDTH cC parameter was set to "8 and 16 bits", meaning that the
+ core has been configured to work at either data path width.
+ - Values: 8 or 16 bits (default 16)
+ </td></tr>
+
+ <tr>
+ <td>phy_ulpi_ddr</td>
+ <td>Specifies whether the ULPI operates at double or single data rate. This
+ parameter is only applicable if phy_type is ULPI.
+ - 0: single data rate ULPI interface with 8 bit wide data bus (default)
+ - 1: double data rate ULPI interface with 4 bit wide data bus
+ </td></tr>
+
+ <tr>
+ <td>i2c_enable</td>
+ <td>Specifies whether to use the I2C interface for full speed PHY. This
+ parameter is only applicable if PHY_TYPE is FS.
+ - 0: Disabled (default)
+ - 1: Enabled
+ </td></tr>
+
+ <tr>
+ <td>ulpi_fs_ls</td>
+ <td>Specifies whether to use ULPI FS/LS mode only.
+ - 0: Disabled (default)
+ - 1: Enabled
+ </td></tr>
+
+ <tr>
+ <td>ts_dline</td>
+ <td>Specifies whether term select D-Line pulsing for all PHYs is enabled.
+ - 0: Disabled (default)
+ - 1: Enabled
+ </td></tr>
+
+ <tr>
+ <td>en_multiple_tx_fifo</td>
+ <td>Specifies whether dedicatedto tx fifos are enabled for non periodic IN EPs.
+ The driver will automatically detect the value for this parameter if none is
+ specified.
+ - 0: Disabled
+ - 1: Enabled (default, if available)
+ </td></tr>
+
+ <tr>
+ <td>dev_tx_fifo_size_n (n = 1 to 15)</td>
+ <td>Number of 4-byte words in each of the Tx FIFOs in device mode
+ when dynamic FIFO sizing is enabled.
+ - Values: 4 to 768 (default 256)
+ </td></tr>
+
+ <tr>
+ <td>tx_thr_length</td>
+ <td>Transmit Threshold length in 32 bit double words
+ - Values: 8 to 128 (default 64)
+ </td></tr>
+
+ <tr>
+ <td>rx_thr_length</td>
+ <td>Receive Threshold length in 32 bit double words
+ - Values: 8 to 128 (default 64)
+ </td></tr>
+
+<tr>
+ <td>thr_ctl</td>
+ <td>Specifies whether to enable Thresholding for Device mode. Bits 0, 1, 2 of
+ this parmater specifies if thresholding is enabled for non-Iso Tx, Iso Tx and
+ Rx transfers accordingly.
+ The driver will automatically detect the value for this parameter if none is
+ specified.
+ - Values: 0 to 7 (default 0)
+ Bit values indicate:
+ - 0: Thresholding disabled
+ - 1: Thresholding enabled
+ </td></tr>
+
+<tr>
+ <td>dma_desc_enable</td>
+ <td>Specifies whether to enable Descriptor DMA mode.
+ The driver will automatically detect the value for this parameter if none is
+ specified.
+ - 0: Descriptor DMA disabled
+ - 1: Descriptor DMA (default, if available)
+ </td></tr>
+
+<tr>
+ <td>mpi_enable</td>
+ <td>Specifies whether to enable MPI enhancement mode.
+ The driver will automatically detect the value for this parameter if none is
+ specified.
+ - 0: MPI disabled (default)
+ - 1: MPI enable
+ </td></tr>
+
+<tr>
+ <td>pti_enable</td>
+ <td>Specifies whether to enable PTI enhancement support.
+ The driver will automatically detect the value for this parameter if none is
+ specified.
+ - 0: PTI disabled (default)
+ - 1: PTI enable
+ </td></tr>
+
+<tr>
+ <td>lpm_enable</td>
+ <td>Specifies whether to enable LPM support.
+ The driver will automatically detect the value for this parameter if none is
+ specified.
+ - 0: LPM disabled
+ - 1: LPM enable (default, if available)
+ </td></tr>
+
+<tr>
+ <td>ic_usb_cap</td>
+ <td>Specifies whether to enable IC_USB capability.
+ The driver will automatically detect the value for this parameter if none is
+ specified.
+ - 0: IC_USB disabled (default, if available)
+ - 1: IC_USB enable
+ </td></tr>
+
+<tr>
+ <td>ahb_thr_ratio</td>
+ <td>Specifies AHB Threshold ratio.
+ - Values: 0 to 3 (default 0)
+ </td></tr>
+
+<tr>
+ <td>power_down</td>
+ <td>Specifies Power Down(Hibernation) Mode.
+ The driver will automatically detect the value for this parameter if none is
+ specified.
+ - 0: Power Down disabled (default)
+ - 2: Power Down enabled
+ </td></tr>
+
+ <tr>
+ <td>reload_ctl</td>
+ <td>Specifies whether dynamic reloading of the HFIR register is allowed during
+ run time. The driver will automatically detect the value for this parameter if
+ none is specified. In case the HFIR value is reloaded when HFIR.RldCtrl == 1'b0
+ the core might misbehave.
+ - 0: Reload Control disabled (default)
+ - 1: Reload Control enabled
+ </td></tr>
+
+ <tr>
+ <td>dev_out_nak</td>
+ <td>Specifies whether  Device OUT NAK enhancement enabled or no.
+ The driver will automatically detect the value for this parameter if
+ none is specified. This parameter is valid only when OTG_EN_DESC_DMA == 1'b1.
+ - 0: The core does not set NAK after Bulk OUT transfer complete (default)
+ - 1: The core sets NAK after Bulk OUT transfer complete
+ </td></tr>
+
+ <tr>
+ <td>cont_on_bna</td>
+ <td>Specifies whether Enable Continue on BNA enabled or no.
+ After receiving BNA interrupt the core disables the endpoint,when the
+ endpoint is re-enabled by the application the
+ - 0: Core starts processing from the DOEPDMA descriptor (default)
+ - 1: Core starts processing from the descriptor which received the BNA.
+ This parameter is valid only when OTG_EN_DESC_DMA == 1'b1.
+ </td></tr>
+
+ <tr>
+ <td>ahb_single</td>
+ <td>This bit when programmed supports SINGLE transfers for remainder data
+ in a transfer for DMA mode of operation.
+ - 0: The remainder data will be sent using INCR burst size (default)
+ - 1: The remainder data will be sent using SINGLE burst size.
+ </td></tr>
+
+<tr>
+ <td>adp_enable</td>
+ <td>Specifies whether ADP feature is enabled.
+ The driver will automatically detect the value for this parameter if none is
+ specified.
+ - 0: ADP feature disabled (default)
+ - 1: ADP feature enabled
+ </td></tr>
+
+  <tr>
+ <td>otg_ver</td>
+ <td>Specifies whether OTG is performing as USB OTG Revision 2.0 or Revision 1.3
+ USB OTG device.
+ - 0: OTG 2.0 support disabled (default)
+ - 1: OTG 2.0 support enabled
+ </td></tr>
+
+*/
diff --git a/drivers/usb/dwc_otg/dwc_otg_driver.h b/drivers/usb/dwc_otg/dwc_otg_driver.h
new file mode 100644
index 0000000..7e3cf23
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_otg_driver.h
@@ -0,0 +1,88 @@
+/* ==========================================================================
+ * $File: //dwh/usb_iip/dev/software/otg/linux/drivers/dwc_otg_driver.h $
+ * $Revision: #19 $
+ * $Date: 2010/11/15 $
+ * $Change: 1627671 $
+ *
+ * Synopsys HS OTG Linux Software Driver and documentation (hereinafter,
+ * "Software") is an Unsupported proprietary work of Synopsys, Inc. unless
+ * otherwise expressly agreed to in writing between Synopsys and you.
+ * 
+ * The Software IS NOT an item of Licensed Software or Licensed Product under
+ * any End User Software License Agreement or Agreement for Licensed Product
+ * with Synopsys or any supplement thereto. You are permitted to use and
+ * redistribute this Software in source and binary forms, with or without
+ * modification, provided that redistributions of source code must retain this
+ * notice. You may not view, use, disclose, copy or distribute this file or
+ * any information contained herein except pursuant to this license grant from
+ * Synopsys. If you do not agree with this notice, including the disclaimer
+ * below, then you are not authorized to use the Software.
+ * 
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS" BASIS
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE HEREBY DISCLAIMED. IN NO EVENT SHALL SYNOPSYS BE LIABLE FOR ANY DIRECT,
+ * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================== */
+
+#ifndef __DWC_OTG_DRIVER_H__
+#define __DWC_OTG_DRIVER_H__
+
+/** @file
+ * This file contains the interface to the Linux driver.
+ */
+#include "dwc_otg_os_dep.h"
+#include "dwc_otg_core_if.h"
+
+/* Type declarations */
+struct dwc_otg_pcd;
+struct dwc_otg_hcd;
+
+/**
+ * This structure is a wrapper that encapsulates the driver components used to
+ * manage a single DWC_otg controller.
+ */
+typedef struct dwc_otg_device {
+	/** Structure containing OS-dependent stuff. KEEP THIS STRUCT AT THE
+	 * VERY BEGINNING OF THE DEVICE STRUCT. OSes such as FreeBSD and NetBSD
+	 * require this. */
+	struct os_dependent os_dep;
+
+	/** Pointer to the core interface structure. */
+	dwc_otg_core_if_t *core_if;
+
+	/** Pointer to the PCD structure. */
+	struct dwc_otg_pcd *pcd;
+
+	/** Pointer to the HCD structure. */
+	struct dwc_otg_hcd *hcd;
+
+	/** Flag to indicate whether the common IRQ handler is installed. */
+	uint8_t common_irq_installed;
+
+	/* Interrupt request number. */
+	unsigned int irq;
+} dwc_otg_device_t;
+
+/*We must clear S3C24XX_EINTPEND external interrupt register 
+ * because after clearing in this register trigerred IRQ from 
+ * H/W core in kernel interrupt can be occured again before OTG
+ * handlers clear all IRQ sources of Core registers because of
+ * timing latencies and Low Level IRQ Type.
+ */
+#ifdef CONFIG_MACH_IPMATE
+#define  S3C2410X_CLEAR_EINTPEND()   \
+do { \
+	__raw_writel(1UL << 11,S3C24XX_EINTPEND); \
+} while (0)
+#else
+#define  S3C2410X_CLEAR_EINTPEND()   do { } while (0)
+#endif
+
+#endif
diff --git a/drivers/usb/dwc_otg/dwc_otg_hcd.c b/drivers/usb/dwc_otg/dwc_otg_hcd.c
new file mode 100644
index 0000000..ccaa129
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_otg_hcd.c
@@ -0,0 +1,3348 @@
+/* ==========================================================================
+ * $File: //dwh/usb_iip/dev/software/otg/linux/drivers/dwc_otg_hcd.c $
+ * $Revision: #104 $
+ * $Date: 2011/10/24 $
+ * $Change: 1871159 $
+ *
+ * Synopsys HS OTG Linux Software Driver and documentation (hereinafter,
+ * "Software") is an Unsupported proprietary work of Synopsys, Inc. unless
+ * otherwise expressly agreed to in writing between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product under
+ * any End User Software License Agreement or Agreement for Licensed Product
+ * with Synopsys or any supplement thereto. You are permitted to use and
+ * redistribute this Software in source and binary forms, with or without
+ * modification, provided that redistributions of source code must retain this
+ * notice. You may not view, use, disclose, copy or distribute this file or
+ * any information contained herein except pursuant to this license grant from
+ * Synopsys. If you do not agree with this notice, including the disclaimer
+ * below, then you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS" BASIS
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE HEREBY DISCLAIMED. IN NO EVENT SHALL SYNOPSYS BE LIABLE FOR ANY DIRECT,
+ * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================== */
+#ifndef DWC_DEVICE_ONLY
+
+/** @file
+ * This file implements HCD Core. All code in this file is portable and doesn't
+ * use any OS specific functions.
+ * Interface provided by HCD Core is defined in <code><hcd_if.h></code>
+ * header file.
+ */
+
+#include "dwc_otg_hcd.h"
+#include "dwc_otg_regs.h"
+
+dwc_otg_hcd_t *dwc_otg_hcd_alloc_hcd(void)
+{
+	return DWC_ALLOC(sizeof(dwc_otg_hcd_t));
+}
+
+/**
+ * Connection timeout function.  An OTG host is required to display a
+ * message if the device does not connect within 10 seconds.
+ */
+void dwc_otg_hcd_connect_timeout(void *ptr)
+{
+	DWC_DEBUGPL(DBG_HCDV, "%s(%p)\n", __func__, ptr);
+	DWC_PRINTF("Connect Timeout\n");
+	__DWC_ERROR("Device Not Connected/Responding\n");
+}
+
+#ifdef DEBUG
+static void dump_channel_info(dwc_otg_hcd_t * hcd, dwc_otg_qh_t * qh)
+{
+	if (qh->channel != NULL) {
+		dwc_hc_t *hc = qh->channel;
+		dwc_list_link_t *item;
+		dwc_otg_qh_t *qh_item;
+		int num_channels = hcd->core_if->core_params->host_channels;
+		int i;
+
+		dwc_otg_hc_regs_t *hc_regs;
+		hcchar_data_t hcchar;
+		hcsplt_data_t hcsplt;
+		hctsiz_data_t hctsiz;
+		uint32_t hcdma;
+
+		hc_regs = hcd->core_if->host_if->hc_regs[hc->hc_num];
+		hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+		hcsplt.d32 = DWC_READ_REG32(&hc_regs->hcsplt);
+		hctsiz.d32 = DWC_READ_REG32(&hc_regs->hctsiz);
+		hcdma = DWC_READ_REG32(&hc_regs->hcdma);
+
+		DWC_PRINTF("  Assigned to channel %p:\n", hc);
+		DWC_PRINTF("    hcchar 0x%08x, hcsplt 0x%08x\n", hcchar.d32,
+			   hcsplt.d32);
+		DWC_PRINTF("    hctsiz 0x%08x, hcdma 0x%08x\n", hctsiz.d32,
+			   hcdma);
+		DWC_PRINTF("    dev_addr: %d, ep_num: %d, ep_is_in: %d\n",
+			   hc->dev_addr, hc->ep_num, hc->ep_is_in);
+		DWC_PRINTF("    ep_type: %d\n", hc->ep_type);
+		DWC_PRINTF("    max_packet: %d\n", hc->max_packet);
+		DWC_PRINTF("    data_pid_start: %d\n", hc->data_pid_start);
+		DWC_PRINTF("    xfer_started: %d\n", hc->xfer_started);
+		DWC_PRINTF("    halt_status: %d\n", hc->halt_status);
+		DWC_PRINTF("    xfer_buff: %p\n", hc->xfer_buff);
+		DWC_PRINTF("    xfer_len: %d\n", hc->xfer_len);
+		DWC_PRINTF("    qh: %p\n", hc->qh);
+		DWC_PRINTF("  NP inactive sched:\n");
+		DWC_LIST_FOREACH(item, &hcd->non_periodic_sched_inactive) {
+			qh_item =
+			    DWC_LIST_ENTRY(item, dwc_otg_qh_t, qh_list_entry);
+			DWC_PRINTF("    %p\n", qh_item);
+		}
+		DWC_PRINTF("  NP active sched:\n");
+		DWC_LIST_FOREACH(item, &hcd->non_periodic_sched_active) {
+			qh_item =
+			    DWC_LIST_ENTRY(item, dwc_otg_qh_t, qh_list_entry);
+			DWC_PRINTF("    %p\n", qh_item);
+		}
+		DWC_PRINTF("  Channels: \n");
+		for (i = 0; i < num_channels; i++) {
+			dwc_hc_t *hc = hcd->hc_ptr_array[i];
+			DWC_PRINTF("    %2d: %p\n", i, hc);
+		}
+	}
+}
+#endif /* DEBUG */
+
+/**
+ * Work queue function for starting the HCD when A-Cable is connected.
+ * The hcd_start() must be called in a process context.
+ */
+static void hcd_start_func(void *_vp)
+{
+	dwc_otg_hcd_t *hcd = (dwc_otg_hcd_t *) _vp;
+
+	DWC_DEBUGPL(DBG_HCDV, "%s() %p\n", __func__, hcd);
+	if (hcd) {
+		hcd->fops->start(hcd);
+	}
+}
+
+static void del_xfer_timers(dwc_otg_hcd_t * hcd)
+{
+#ifdef DEBUG
+	int i;
+	int num_channels = hcd->core_if->core_params->host_channels;
+	for (i = 0; i < num_channels; i++) {
+		DWC_TIMER_CANCEL(hcd->core_if->hc_xfer_timer[i]);
+	}
+#endif
+}
+
+static void del_timers(dwc_otg_hcd_t * hcd)
+{
+	del_xfer_timers(hcd);
+	DWC_TIMER_CANCEL(hcd->conn_timer);
+}
+
+/**
+ * Processes all the URBs in a single list of QHs. Completes them with
+ * -ETIMEDOUT and frees the QTD.
+ */
+static void kill_urbs_in_qh_list(dwc_otg_hcd_t * hcd, dwc_list_link_t * qh_list)
+{
+	dwc_list_link_t *qh_item;
+	dwc_otg_qh_t *qh;
+	dwc_otg_qtd_t *qtd, *qtd_tmp;
+
+	DWC_LIST_FOREACH(qh_item, qh_list) {
+		qh = DWC_LIST_ENTRY(qh_item, dwc_otg_qh_t, qh_list_entry);
+		DWC_CIRCLEQ_FOREACH_SAFE(qtd, qtd_tmp,
+					 &qh->qtd_list, qtd_list_entry) {
+			qtd = DWC_CIRCLEQ_FIRST(&qh->qtd_list);
+			if (qtd->urb != NULL) {
+				hcd->fops->complete(hcd, qtd->urb->priv,
+						    qtd->urb, -DWC_E_TIMEOUT);
+				dwc_otg_hcd_qtd_remove_and_free(hcd, qtd, qh);
+			}
+
+		}
+	}
+}
+
+/**
+ * Responds with an error status of ETIMEDOUT to all URBs in the non-periodic
+ * and periodic schedules. The QTD associated with each URB is removed from
+ * the schedule and freed. This function may be called when a disconnect is
+ * detected or when the HCD is being stopped.
+ */
+static void kill_all_urbs(dwc_otg_hcd_t * hcd)
+{
+	kill_urbs_in_qh_list(hcd, &hcd->non_periodic_sched_inactive);
+	kill_urbs_in_qh_list(hcd, &hcd->non_periodic_sched_active);
+	kill_urbs_in_qh_list(hcd, &hcd->periodic_sched_inactive);
+	kill_urbs_in_qh_list(hcd, &hcd->periodic_sched_ready);
+	kill_urbs_in_qh_list(hcd, &hcd->periodic_sched_assigned);
+	kill_urbs_in_qh_list(hcd, &hcd->periodic_sched_queued);
+}
+
+/**
+ * Start the connection timer.  An OTG host is required to display a
+ * message if the device does not connect within 10 seconds.  The
+ * timer is deleted if a port connect interrupt occurs before the
+ * timer expires.
+ */
+static void dwc_otg_hcd_start_connect_timer(dwc_otg_hcd_t * hcd)
+{
+	DWC_TIMER_SCHEDULE(hcd->conn_timer, 10000 /* 10 secs */ );
+}
+
+/**
+ * HCD Callback function for disconnect of the HCD.
+ *
+ * @param p void pointer to the <code>struct usb_hcd</code>
+ */
+static int32_t dwc_otg_hcd_session_start_cb(void *p)
+{
+	dwc_otg_hcd_t *dwc_otg_hcd;
+	DWC_DEBUGPL(DBG_HCDV, "%s(%p)\n", __func__, p);
+	dwc_otg_hcd = p;
+	dwc_otg_hcd_start_connect_timer(dwc_otg_hcd);
+	return 1;
+}
+
+/**
+ * HCD Callback function for starting the HCD when A-Cable is
+ * connected.
+ *
+ * @param p void pointer to the <code>struct usb_hcd</code>
+ */
+static int32_t dwc_otg_hcd_start_cb(void *p)
+{
+	dwc_otg_hcd_t *dwc_otg_hcd = p;
+	dwc_otg_core_if_t *core_if;
+	hprt0_data_t hprt0;
+
+	core_if = dwc_otg_hcd->core_if;
+
+	if (core_if->op_state == B_HOST) {
+		/*
+		 * Reset the port.  During a HNP mode switch the reset
+		 * needs to occur within 1ms and have a duration of at
+		 * least 50ms.
+		 */
+		hprt0.d32 = dwc_otg_read_hprt0(core_if);
+		hprt0.b.prtrst = 1;
+		DWC_WRITE_REG32(core_if->host_if->hprt0, hprt0.d32);
+	}
+	DWC_WORKQ_SCHEDULE_DELAYED(core_if->wq_otg,
+				   hcd_start_func, dwc_otg_hcd, 50,
+				   "start hcd");
+
+	return 1;
+}
+
+/**
+ * HCD Callback function for disconnect of the HCD.
+ *
+ * @param p void pointer to the <code>struct usb_hcd</code>
+ */
+static int32_t dwc_otg_hcd_disconnect_cb(void *p)
+{
+	gintsts_data_t intr;
+	dwc_otg_hcd_t *dwc_otg_hcd = p;
+
+	/*
+	 * Set status flags for the hub driver.
+	 */
+	dwc_otg_hcd->flags.b.port_connect_status_change = 1;
+	dwc_otg_hcd->flags.b.port_connect_status = 0;
+
+	/*
+	 * Shutdown any transfers in process by clearing the Tx FIFO Empty
+	 * interrupt mask and status bits and disabling subsequent host
+	 * channel interrupts.
+	 */
+	intr.d32 = 0;
+	intr.b.nptxfempty = 1;
+	intr.b.ptxfempty = 1;
+	intr.b.hcintr = 1;
+	DWC_MODIFY_REG32(&dwc_otg_hcd->core_if->core_global_regs->gintmsk,
+			 intr.d32, 0);
+	DWC_MODIFY_REG32(&dwc_otg_hcd->core_if->core_global_regs->gintsts,
+			 intr.d32, 0);
+
+	del_timers(dwc_otg_hcd);
+
+	/*
+	 * Turn off the vbus power only if the core has transitioned to device
+	 * mode. If still in host mode, need to keep power on to detect a
+	 * reconnection.
+	 */
+	if (dwc_otg_is_device_mode(dwc_otg_hcd->core_if)) {
+		if (dwc_otg_hcd->core_if->op_state != A_SUSPEND) {
+			hprt0_data_t hprt0 = {.d32 = 0 };
+			DWC_PRINTF("Disconnect: PortPower off\n");
+			hprt0.b.prtpwr = 0;
+			DWC_WRITE_REG32(dwc_otg_hcd->core_if->host_if->hprt0,
+					hprt0.d32);
+		}
+
+		dwc_otg_disable_host_interrupts(dwc_otg_hcd->core_if);
+	}
+
+	/* Respond with an error status to all URBs in the schedule. */
+	kill_all_urbs(dwc_otg_hcd);
+
+	if (dwc_otg_is_host_mode(dwc_otg_hcd->core_if)) {
+		/* Clean up any host channels that were in use. */
+		int num_channels;
+		int i;
+		dwc_hc_t *channel;
+		dwc_otg_hc_regs_t *hc_regs;
+		hcchar_data_t hcchar;
+
+		num_channels = dwc_otg_hcd->core_if->core_params->host_channels;
+
+		if (!dwc_otg_hcd->core_if->dma_enable) {
+			/* Flush out any channel requests in slave mode. */
+			for (i = 0; i < num_channels; i++) {
+				channel = dwc_otg_hcd->hc_ptr_array[i];
+				if (DWC_CIRCLEQ_EMPTY_ENTRY
+				    (channel, hc_list_entry)) {
+					hc_regs =
+					    dwc_otg_hcd->core_if->
+					    host_if->hc_regs[i];
+					hcchar.d32 =
+					    DWC_READ_REG32(&hc_regs->hcchar);
+					if (hcchar.b.chen) {
+						hcchar.b.chen = 0;
+						hcchar.b.chdis = 1;
+						hcchar.b.epdir = 0;
+						DWC_WRITE_REG32
+						    (&hc_regs->hcchar,
+						     hcchar.d32);
+					}
+				}
+			}
+		}
+
+		for (i = 0; i < num_channels; i++) {
+			channel = dwc_otg_hcd->hc_ptr_array[i];
+			if (DWC_CIRCLEQ_EMPTY_ENTRY(channel, hc_list_entry)) {
+				hc_regs =
+				    dwc_otg_hcd->core_if->host_if->hc_regs[i];
+				hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+				if (hcchar.b.chen) {
+					/* Halt the channel. */
+					hcchar.b.chdis = 1;
+					DWC_WRITE_REG32(&hc_regs->hcchar,
+							hcchar.d32);
+				}
+
+				dwc_otg_hc_cleanup(dwc_otg_hcd->core_if,
+						   channel);
+				DWC_CIRCLEQ_INSERT_TAIL
+				    (&dwc_otg_hcd->free_hc_list, channel,
+				     hc_list_entry);
+				/*
+				 * Added for Descriptor DMA to prevent channel double cleanup
+				 * in release_channel_ddma(). Which called from ep_disable
+				 * when device disconnect.
+				 */
+				channel->qh = NULL;
+			}
+		}
+	}
+
+	if (dwc_otg_hcd->fops->disconnect) {
+		dwc_otg_hcd->fops->disconnect(dwc_otg_hcd);
+	}
+
+	return 1;
+}
+
+/**
+ * HCD Callback function for stopping the HCD.
+ *
+ * @param p void pointer to the <code>struct usb_hcd</code>
+ */
+static int32_t dwc_otg_hcd_stop_cb(void *p)
+{
+	dwc_otg_hcd_t *dwc_otg_hcd = p;
+
+	DWC_DEBUGPL(DBG_HCDV, "%s(%p)\n", __func__, p);
+	dwc_otg_hcd_stop(dwc_otg_hcd);
+	return 1;
+}
+
+#ifdef CONFIG_USB_DWC_OTG_LPM
+/**
+ * HCD Callback function for sleep of HCD.
+ *
+ * @param p void pointer to the <code>struct usb_hcd</code>
+ */
+static int dwc_otg_hcd_sleep_cb(void *p)
+{
+	dwc_otg_hcd_t *hcd = p;
+
+	dwc_otg_hcd_free_hc_from_lpm(hcd);
+
+	return 0;
+}
+#endif
+
+/**
+ * HCD Callback function for Remote Wakeup.
+ *
+ * @param p void pointer to the <code>struct usb_hcd</code>
+ */
+static int dwc_otg_hcd_rem_wakeup_cb(void *p)
+{
+	dwc_otg_hcd_t *hcd = p;
+
+	if (hcd->core_if->lx_state == DWC_OTG_L2) {
+		hcd->flags.b.port_suspend_change = 1;
+	}
+#ifdef CONFIG_USB_DWC_OTG_LPM
+	else {
+		hcd->flags.b.port_l1_change = 1;
+	}
+#endif
+	return 0;
+}
+
+/**
+ * Halts the DWC_otg host mode operations in a clean manner. USB transfers are
+ * stopped.
+ */
+void dwc_otg_hcd_stop(dwc_otg_hcd_t * hcd)
+{
+	hprt0_data_t hprt0 = {.d32 = 0 };
+
+	DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD STOP\n");
+
+	/*
+	 * The root hub should be disconnected before this function is called.
+	 * The disconnect will clear the QTD lists (via ..._hcd_urb_dequeue)
+	 * and the QH lists (via ..._hcd_endpoint_disable).
+	 */
+
+	/* Turn off all host-specific interrupts. */
+	dwc_otg_disable_host_interrupts(hcd->core_if);
+
+	/* Turn off the vbus power */
+	DWC_PRINTF("PortPower off\n");
+	hprt0.b.prtpwr = 0;
+	DWC_WRITE_REG32(hcd->core_if->host_if->hprt0, hprt0.d32);
+	dwc_mdelay(1);
+}
+
+int dwc_otg_hcd_urb_enqueue(dwc_otg_hcd_t * hcd,
+			    dwc_otg_hcd_urb_t * dwc_otg_urb, void **ep_handle,
+			    int atomic_alloc)
+{
+	dwc_irqflags_t flags;
+	int retval = 0;
+	dwc_otg_qtd_t *qtd;
+	gintmsk_data_t intr_mask = {.d32 = 0 };
+
+	if (!hcd->flags.b.port_connect_status) {
+		/* No longer connected. */
+		DWC_ERROR("Not connected\n");
+		return -DWC_E_NO_DEVICE;
+	}
+
+	qtd = dwc_otg_hcd_qtd_create(dwc_otg_urb, atomic_alloc);
+	if (qtd == NULL) {
+		DWC_ERROR("DWC OTG HCD URB Enqueue failed creating QTD\n");
+		return -DWC_E_NO_MEMORY;
+	}
+
+	retval =
+	    dwc_otg_hcd_qtd_add(qtd, hcd, (dwc_otg_qh_t **) ep_handle, atomic_alloc);
+	if (retval < 0) {
+		DWC_ERROR("DWC OTG HCD URB Enqueue failed adding QTD. "
+			  "Error status %d\n", retval);
+		dwc_otg_hcd_qtd_free(qtd);
+	} else {
+		qtd->qh = *ep_handle;
+	}
+	intr_mask.d32 = DWC_READ_REG32(&hcd->core_if->core_global_regs->gintmsk);
+	if (!intr_mask.b.sofintr && retval == 0) {
+		dwc_otg_transaction_type_e tr_type;
+		if ((qtd->qh->ep_type == UE_BULK)
+		    && !(qtd->urb->flags & URB_GIVEBACK_ASAP)) {
+			/* Do not schedule SG transactions until qtd has URB_GIVEBACK_ASAP set */
+			return 0;
+		}
+		DWC_SPINLOCK_IRQSAVE(hcd->lock, &flags);
+		tr_type = dwc_otg_hcd_select_transactions(hcd);
+		if (tr_type != DWC_OTG_TRANSACTION_NONE) {
+			dwc_otg_hcd_queue_transactions(hcd, tr_type);
+		}
+		DWC_SPINUNLOCK_IRQRESTORE(hcd->lock, flags);
+	}
+
+	return retval;
+}
+
+int dwc_otg_hcd_urb_dequeue(dwc_otg_hcd_t * hcd,
+			    dwc_otg_hcd_urb_t * dwc_otg_urb)
+{
+	dwc_otg_qh_t *qh;
+	dwc_otg_qtd_t *urb_qtd;
+
+	urb_qtd = dwc_otg_urb->qtd;
+	qh = urb_qtd->qh;
+#ifdef DEBUG
+	if (CHK_DEBUG_LEVEL(DBG_HCDV | DBG_HCD_URB)) {
+		if (urb_qtd->in_process) {
+			dump_channel_info(hcd, qh);
+		}
+	}
+#endif
+	if (urb_qtd->in_process && qh->channel) {
+		/* The QTD is in process (it has been assigned to a channel). */
+		if (hcd->flags.b.port_connect_status) {
+			/*
+			 * If still connected (i.e. in host mode), halt the
+			 * channel so it can be used for other transfers. If
+			 * no longer connected, the host registers can't be
+			 * written to halt the channel since the core is in
+			 * device mode.
+			 */
+			dwc_otg_hc_halt(hcd->core_if, qh->channel,
+					DWC_OTG_HC_XFER_URB_DEQUEUE);
+		}
+	}
+
+	/*
+	 * Free the QTD and clean up the associated QH. Leave the QH in the
+	 * schedule if it has any remaining QTDs.
+	 */
+
+	if (!hcd->core_if->dma_desc_enable) {
+		uint8_t b = urb_qtd->in_process;
+		dwc_otg_hcd_qtd_remove_and_free(hcd, urb_qtd, qh);
+		if (b) {
+			dwc_otg_hcd_qh_deactivate(hcd, qh, 0);
+			qh->channel = NULL;
+		} else if (DWC_CIRCLEQ_EMPTY(&qh->qtd_list)) {
+			dwc_otg_hcd_qh_remove(hcd, qh);
+		}
+	} else {
+		dwc_otg_hcd_qtd_remove_and_free(hcd, urb_qtd, qh);
+	}
+	return 0;
+}
+
+int dwc_otg_hcd_endpoint_disable(dwc_otg_hcd_t * hcd, void *ep_handle,
+				 int retry)
+{
+	dwc_otg_qh_t *qh = (dwc_otg_qh_t *) ep_handle;
+	int retval = 0;
+	dwc_irqflags_t flags;
+
+	if (retry < 0) {
+		retval = -DWC_E_INVALID;
+		goto done;
+	}
+
+	if (!qh) {
+		retval = -DWC_E_INVALID;
+		goto done;
+	}
+
+	DWC_SPINLOCK_IRQSAVE(hcd->lock, &flags);
+
+	while (!DWC_CIRCLEQ_EMPTY(&qh->qtd_list) && retry) {
+		DWC_SPINUNLOCK_IRQRESTORE(hcd->lock, flags);
+		retry--;
+		dwc_msleep(5);
+		DWC_SPINLOCK_IRQSAVE(hcd->lock, &flags);
+	}
+
+	dwc_otg_hcd_qh_remove(hcd, qh);
+
+	DWC_SPINUNLOCK_IRQRESTORE(hcd->lock, flags);
+	/*
+	 * Split dwc_otg_hcd_qh_remove_and_free() into qh_remove
+	 * and qh_free to prevent stack dump on DWC_DMA_FREE() with
+	 * irq_disabled (spinlock_irqsave) in dwc_otg_hcd_desc_list_free()
+	 * and dwc_otg_hcd_frame_list_alloc().
+	 */
+	dwc_otg_hcd_qh_free(hcd, qh);
+
+done:
+	return retval;
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,30)
+int dwc_otg_hcd_endpoint_reset(dwc_otg_hcd_t * hcd, void *ep_handle)
+{
+	int retval = 0;
+	dwc_otg_qh_t *qh = (dwc_otg_qh_t *) ep_handle;
+	if (!qh)
+		return -DWC_E_INVALID;
+
+	qh->data_toggle = DWC_OTG_HC_PID_DATA0;
+	return retval;
+}
+#endif
+
+/**
+ * HCD Callback structure for handling mode switching.
+ */
+static dwc_otg_cil_callbacks_t hcd_cil_callbacks = {
+	.start = dwc_otg_hcd_start_cb,
+	.stop = dwc_otg_hcd_stop_cb,
+	.disconnect = dwc_otg_hcd_disconnect_cb,
+	.session_start = dwc_otg_hcd_session_start_cb,
+	.resume_wakeup = dwc_otg_hcd_rem_wakeup_cb,
+#ifdef CONFIG_USB_DWC_OTG_LPM
+	.sleep = dwc_otg_hcd_sleep_cb,
+#endif
+	.p = 0,
+};
+
+/**
+ * Reset tasklet function
+ */
+static void reset_tasklet_func(void *data)
+{
+	dwc_otg_hcd_t *dwc_otg_hcd = (dwc_otg_hcd_t *) data;
+	dwc_otg_core_if_t *core_if = dwc_otg_hcd->core_if;
+	hprt0_data_t hprt0;
+
+	DWC_DEBUGPL(DBG_HCDV, "USB RESET tasklet called\n");
+
+	hprt0.d32 = dwc_otg_read_hprt0(core_if);
+	hprt0.b.prtrst = 1;
+	DWC_WRITE_REG32(core_if->host_if->hprt0, hprt0.d32);
+	dwc_mdelay(60);
+
+	hprt0.b.prtrst = 0;
+	DWC_WRITE_REG32(core_if->host_if->hprt0, hprt0.d32);
+	dwc_otg_hcd->flags.b.port_reset_change = 1;
+}
+
+static void qh_list_free(dwc_otg_hcd_t * hcd, dwc_list_link_t * qh_list)
+{
+	dwc_list_link_t *item;
+	dwc_otg_qh_t *qh;
+	dwc_irqflags_t flags;
+
+	if (!qh_list->next) {
+		/* The list hasn't been initialized yet. */
+		return;
+	}
+	/*
+	 * Hold spinlock here. Not needed in that case if bellow
+	 * function is being called from ISR
+	 */
+	DWC_SPINLOCK_IRQSAVE(hcd->lock, &flags);
+	/* Ensure there are no QTDs or URBs left. */
+	kill_urbs_in_qh_list(hcd, qh_list);
+	DWC_SPINUNLOCK_IRQRESTORE(hcd->lock, flags);
+
+	DWC_LIST_FOREACH(item, qh_list) {
+		qh = DWC_LIST_ENTRY(item, dwc_otg_qh_t, qh_list_entry);
+		dwc_otg_hcd_qh_remove_and_free(hcd, qh);
+	}
+}
+
+/**
+ * Exit from Hibernation if Host did not detect SRP from connected SRP capable
+ * Device during SRP time by host power up.
+ */
+void dwc_otg_hcd_power_up(void *ptr)
+{
+	gpwrdn_data_t gpwrdn = {.d32 = 0 };
+	dwc_otg_core_if_t *core_if = (dwc_otg_core_if_t *) ptr;
+
+	DWC_PRINTF("%s called\n", __FUNCTION__);
+
+	if (!core_if->hibernation_suspend) {
+		DWC_PRINTF("Already exited from Hibernation\n");
+		return;
+	}
+
+	/* Switch on the voltage to the core */
+	gpwrdn.b.pwrdnswtch = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+	dwc_udelay(10);
+
+	/* Reset the core */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.pwrdnrstn = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+	dwc_udelay(10);
+
+	/* Disable power clamps */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.pwrdnclmp = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+
+	/* Remove reset the core signal */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.pwrdnrstn = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, 0, gpwrdn.d32);
+	dwc_udelay(10);
+
+	/* Disable PMU interrupt */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.pmuintsel = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+
+	core_if->hibernation_suspend = 0;
+
+	/* Disable PMU */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.pmuactv = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+	dwc_udelay(10);
+
+	/* Enable VBUS */
+	gpwrdn.d32 = 0;
+	gpwrdn.b.dis_vbus = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, gpwrdn.d32, 0);
+
+	core_if->op_state = A_HOST;
+	dwc_otg_core_init(core_if);
+	dwc_otg_enable_global_interrupts(core_if);
+	cil_hcd_start(core_if);
+}
+
+/**
+ * Frees secondary storage associated with the dwc_otg_hcd structure contained
+ * in the struct usb_hcd field.
+ */
+static void dwc_otg_hcd_free(dwc_otg_hcd_t * dwc_otg_hcd)
+{
+	int i;
+
+	DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD FREE\n");
+
+	del_timers(dwc_otg_hcd);
+
+	/* Free memory for QH/QTD lists */
+	qh_list_free(dwc_otg_hcd, &dwc_otg_hcd->non_periodic_sched_inactive);
+	qh_list_free(dwc_otg_hcd, &dwc_otg_hcd->non_periodic_sched_active);
+	qh_list_free(dwc_otg_hcd, &dwc_otg_hcd->periodic_sched_inactive);
+	qh_list_free(dwc_otg_hcd, &dwc_otg_hcd->periodic_sched_ready);
+	qh_list_free(dwc_otg_hcd, &dwc_otg_hcd->periodic_sched_assigned);
+	qh_list_free(dwc_otg_hcd, &dwc_otg_hcd->periodic_sched_queued);
+
+	/* Free memory for the host channels. */
+	for (i = 0; i < MAX_EPS_CHANNELS; i++) {
+		dwc_hc_t *hc = dwc_otg_hcd->hc_ptr_array[i];
+
+#ifdef DEBUG
+		if (dwc_otg_hcd->core_if->hc_xfer_timer[i]) {
+			DWC_TIMER_FREE(dwc_otg_hcd->core_if->hc_xfer_timer[i]);
+		}
+#endif
+		if (hc != NULL) {
+			DWC_DEBUGPL(DBG_HCDV, "HCD Free channel #%i, hc=%p\n",
+				    i, hc);
+			DWC_FREE(hc);
+		}
+	}
+
+	if (dwc_otg_hcd->core_if->dma_enable) {
+		if (dwc_otg_hcd->status_buf_dma) {
+			DWC_DMA_FREE(DWC_OTG_HCD_STATUS_BUF_SIZE,
+				     dwc_otg_hcd->status_buf,
+				     dwc_otg_hcd->status_buf_dma);
+		}
+	} else if (dwc_otg_hcd->status_buf != NULL) {
+		DWC_FREE(dwc_otg_hcd->status_buf);
+	}
+	DWC_SPINLOCK_FREE(dwc_otg_hcd->lock);
+	/* Set core_if's lock pointer to NULL */
+	dwc_otg_hcd->core_if->lock = NULL;
+
+	DWC_TIMER_FREE(dwc_otg_hcd->conn_timer);
+	DWC_TASK_FREE(dwc_otg_hcd->reset_tasklet);
+
+#ifdef DWC_DEV_SRPCAP
+	if (dwc_otg_hcd->core_if->power_down == 2 &&
+	    dwc_otg_hcd->core_if->pwron_timer) {
+		DWC_TIMER_FREE(dwc_otg_hcd->core_if->pwron_timer);
+	}
+#endif
+	DWC_FREE(dwc_otg_hcd);
+}
+
+int dwc_otg_hcd_init(dwc_otg_hcd_t * hcd, dwc_otg_core_if_t * core_if)
+{
+	int retval = 0;
+	int num_channels;
+	int i;
+	dwc_hc_t *channel;
+
+	hcd->lock = DWC_SPINLOCK_ALLOC();
+	if (!hcd->lock) {
+		DWC_ERROR("Could not allocate lock for pcd");
+		DWC_FREE(hcd);
+		retval = -DWC_E_NO_MEMORY;
+		goto out;
+	}
+	hcd->core_if = core_if;
+
+	/* Register the HCD CIL Callbacks */
+	dwc_otg_cil_register_hcd_callbacks(hcd->core_if,
+					   &hcd_cil_callbacks, hcd);
+
+	/* Initialize the non-periodic schedule. */
+	DWC_LIST_INIT(&hcd->non_periodic_sched_inactive);
+	DWC_LIST_INIT(&hcd->non_periodic_sched_active);
+
+	/* Initialize the periodic schedule. */
+	DWC_LIST_INIT(&hcd->periodic_sched_inactive);
+	DWC_LIST_INIT(&hcd->periodic_sched_ready);
+	DWC_LIST_INIT(&hcd->periodic_sched_assigned);
+	DWC_LIST_INIT(&hcd->periodic_sched_queued);
+
+	/*
+	 * Create a host channel descriptor for each host channel implemented
+	 * in the controller. Initialize the channel descriptor array.
+	 */
+	DWC_CIRCLEQ_INIT(&hcd->free_hc_list);
+	num_channels = hcd->core_if->core_params->host_channels;
+	DWC_MEMSET(hcd->hc_ptr_array, 0, sizeof(hcd->hc_ptr_array));
+	for (i = 0; i < num_channels; i++) {
+		channel = DWC_ALLOC(sizeof(dwc_hc_t));
+		if (channel == NULL) {
+			retval = -DWC_E_NO_MEMORY;
+			DWC_ERROR("%s: host channel allocation failed\n",
+				  __func__);
+			dwc_otg_hcd_free(hcd);
+			goto out;
+		}
+		channel->hc_num = i;
+		hcd->hc_ptr_array[i] = channel;
+#ifdef DEBUG
+		hcd->core_if->hc_xfer_timer[i] =
+		    DWC_TIMER_ALLOC("hc timer", hc_xfer_timeout,
+				    &hcd->core_if->hc_xfer_info[i]);
+#endif
+		DWC_DEBUGPL(DBG_HCDV, "HCD Added channel #%d, hc=%p\n", i,
+			    channel);
+	}
+
+	/* Initialize the Connection timeout timer. */
+	hcd->conn_timer = DWC_TIMER_ALLOC("Connection timer",
+					  dwc_otg_hcd_connect_timeout, 0);
+
+	/* Initialize reset tasklet. */
+	hcd->reset_tasklet = DWC_TASK_ALLOC("reset_tasklet", reset_tasklet_func, hcd);
+#ifdef DWC_DEV_SRPCAP
+	if (hcd->core_if->power_down == 2) {
+		/* Initialize Power on timer for Host power up in case hibernation */
+		hcd->core_if->pwron_timer = DWC_TIMER_ALLOC("PWRON TIMER",
+									dwc_otg_hcd_power_up, core_if);
+	}
+#endif
+
+	/*
+	 * Allocate space for storing data on status transactions. Normally no
+	 * data is sent, but this space acts as a bit bucket. This must be
+	 * done after usb_add_hcd since that function allocates the DMA buffer
+	 * pool.
+	 */
+	if (hcd->core_if->dma_enable) {
+		hcd->status_buf =
+		    DWC_DMA_ALLOC(DWC_OTG_HCD_STATUS_BUF_SIZE,
+				  &hcd->status_buf_dma);
+	} else {
+		hcd->status_buf = DWC_ALLOC(DWC_OTG_HCD_STATUS_BUF_SIZE);
+	}
+	if (!hcd->status_buf) {
+		retval = -DWC_E_NO_MEMORY;
+		DWC_ERROR("%s: status_buf allocation failed\n", __func__);
+		dwc_otg_hcd_free(hcd);
+		goto out;
+	}
+
+	hcd->otg_port = 1;
+	hcd->frame_list = NULL;
+	hcd->frame_list_dma = 0;
+	hcd->periodic_qh_count = 0;
+out:
+	return retval;
+}
+
+void dwc_otg_hcd_remove(dwc_otg_hcd_t * hcd)
+{
+	/* Turn off all host-specific interrupts. */
+	dwc_otg_disable_host_interrupts(hcd->core_if);
+
+	dwc_otg_hcd_free(hcd);
+}
+
+/**
+ * Initializes dynamic portions of the DWC_otg HCD state.
+ */
+static void dwc_otg_hcd_reinit(dwc_otg_hcd_t * hcd)
+{
+	int num_channels;
+	int i;
+	dwc_hc_t *channel;
+	dwc_hc_t *channel_tmp;
+
+	hcd->flags.d32 = 0;
+
+	hcd->non_periodic_qh_ptr = &hcd->non_periodic_sched_active;
+	hcd->non_periodic_channels = 0;
+	hcd->periodic_channels = 0;
+
+	/*
+	 * Put all channels in the free channel list and clean up channel
+	 * states.
+	 */
+	DWC_CIRCLEQ_FOREACH_SAFE(channel, channel_tmp,
+				 &hcd->free_hc_list, hc_list_entry) {
+		DWC_CIRCLEQ_REMOVE(&hcd->free_hc_list, channel, hc_list_entry);
+	}
+
+	num_channels = hcd->core_if->core_params->host_channels;
+	for (i = 0; i < num_channels; i++) {
+		channel = hcd->hc_ptr_array[i];
+		DWC_CIRCLEQ_INSERT_TAIL(&hcd->free_hc_list, channel,
+					hc_list_entry);
+		dwc_otg_hc_cleanup(hcd->core_if, channel);
+	}
+
+	/* Initialize the DWC core for host mode operation. */
+	dwc_otg_core_host_init(hcd->core_if);
+
+	/* Set core_if's lock pointer to the hcd->lock */
+	hcd->core_if->lock = hcd->lock;
+}
+
+/**
+ * Assigns transactions from a QTD to a free host channel and initializes the
+ * host channel to perform the transactions. The host channel is removed from
+ * the free list.
+ *
+ * @param hcd The HCD state structure.
+ * @param qh Transactions from the first QTD for this QH are selected and
+ * assigned to a free host channel.
+ */
+static void assign_and_init_hc(dwc_otg_hcd_t * hcd, dwc_otg_qh_t * qh)
+{
+	dwc_hc_t *hc;
+	dwc_otg_qtd_t *qtd;
+	dwc_otg_hcd_urb_t *urb;
+	void* ptr = NULL;
+
+	DWC_DEBUGPL(DBG_HCDV, "%s(%p,%p)\n", __func__, hcd, qh);
+
+	hc = DWC_CIRCLEQ_FIRST(&hcd->free_hc_list);
+
+	/* Remove the host channel from the free list. */
+	DWC_CIRCLEQ_REMOVE_INIT(&hcd->free_hc_list, hc, hc_list_entry);
+
+	qtd = DWC_CIRCLEQ_FIRST(&qh->qtd_list);
+
+	urb = qtd->urb;
+	qh->channel = hc;
+
+	qtd->in_process = 1;
+
+	/*
+	 * Use usb_pipedevice to determine device address. This address is
+	 * 0 before the SET_ADDRESS command and the correct address afterward.
+	 */
+	hc->dev_addr = dwc_otg_hcd_get_dev_addr(&urb->pipe_info);
+	hc->ep_num = dwc_otg_hcd_get_ep_num(&urb->pipe_info);
+	hc->speed = qh->dev_speed;
+	hc->max_packet = dwc_max_packet(qh->maxp);
+
+	hc->xfer_started = 0;
+	hc->halt_status = DWC_OTG_HC_XFER_NO_HALT_STATUS;
+	hc->error_state = (qtd->error_count > 0);
+	hc->halt_on_queue = 0;
+	hc->halt_pending = 0;
+	hc->requests = 0;
+
+	/*
+	 * The following values may be modified in the transfer type section
+	 * below. The xfer_len value may be reduced when the transfer is
+	 * started to accommodate the max widths of the XferSize and PktCnt
+	 * fields in the HCTSIZn register.
+	 */
+
+	hc->ep_is_in = (dwc_otg_hcd_is_pipe_in(&urb->pipe_info) != 0);
+	if (hc->ep_is_in) {
+		hc->do_ping = 0;
+	} else {
+		hc->do_ping = qh->ping_state;
+	}
+
+	hc->data_pid_start = qh->data_toggle;
+	hc->multi_count = 1;
+
+	if (hcd->core_if->dma_enable) {
+		hc->xfer_buff = (uint8_t *) urb->dma + urb->actual_length;
+
+		/* For non-dword aligned case */
+		if (((unsigned long)hc->xfer_buff & 0x3)
+		    && !hcd->core_if->dma_desc_enable) {
+			ptr = (uint8_t *) urb->buf + urb->actual_length;
+		}
+	} else {
+		hc->xfer_buff = (uint8_t *) urb->buf + urb->actual_length;
+	}
+	hc->xfer_len = urb->length - urb->actual_length;
+	hc->xfer_count = 0;
+
+	/*
+	 * Set the split attributes
+	 */
+	hc->do_split = 0;
+	if (qh->do_split) {
+		uint32_t hub_addr, port_addr;
+		hc->do_split = 1;
+		hc->xact_pos = qtd->isoc_split_pos;
+		hc->complete_split = qtd->complete_split;
+		hcd->fops->hub_info(hcd, urb->priv, &hub_addr, &port_addr);
+		hc->hub_addr = (uint8_t) hub_addr;
+		hc->port_addr = (uint8_t) port_addr;
+	}
+
+	switch (dwc_otg_hcd_get_pipe_type(&urb->pipe_info)) {
+	case UE_CONTROL:
+		hc->ep_type = DWC_OTG_EP_TYPE_CONTROL;
+		switch (qtd->control_phase) {
+		case DWC_OTG_CONTROL_SETUP:
+			DWC_DEBUGPL(DBG_HCDV, "  Control setup transaction\n");
+			hc->do_ping = 0;
+			hc->ep_is_in = 0;
+			hc->data_pid_start = DWC_OTG_HC_PID_SETUP;
+			if (hcd->core_if->dma_enable) {
+				hc->xfer_buff = (uint8_t *) urb->setup_dma;
+			} else {
+				hc->xfer_buff = (uint8_t *) urb->setup_packet;
+			}
+			hc->xfer_len = 8;
+			ptr = NULL;
+			break;
+		case DWC_OTG_CONTROL_DATA:
+			DWC_DEBUGPL(DBG_HCDV, "  Control data transaction\n");
+			hc->data_pid_start = qtd->data_toggle;
+			break;
+		case DWC_OTG_CONTROL_STATUS:
+			/*
+			 * Direction is opposite of data direction or IN if no
+			 * data.
+			 */
+			DWC_DEBUGPL(DBG_HCDV, "  Control status transaction\n");
+			if (urb->length == 0) {
+				hc->ep_is_in = 1;
+			} else {
+				hc->ep_is_in =
+				    dwc_otg_hcd_is_pipe_out(&urb->pipe_info);
+			}
+			if (hc->ep_is_in) {
+				hc->do_ping = 0;
+			}
+
+			hc->data_pid_start = DWC_OTG_HC_PID_DATA1;
+
+			hc->xfer_len = 0;
+			if (hcd->core_if->dma_enable) {
+				hc->xfer_buff = (uint8_t *) hcd->status_buf_dma;
+			} else {
+				hc->xfer_buff = (uint8_t *) hcd->status_buf;
+			}
+			ptr = NULL;
+			break;
+		}
+		break;
+	case UE_BULK:
+		hc->ep_type = DWC_OTG_EP_TYPE_BULK;
+		break;
+	case UE_INTERRUPT:
+		hc->ep_type = DWC_OTG_EP_TYPE_INTR;
+		break;
+	case UE_ISOCHRONOUS:
+		{
+			struct dwc_otg_hcd_iso_packet_desc *frame_desc;
+
+			hc->ep_type = DWC_OTG_EP_TYPE_ISOC;
+
+			if (hcd->core_if->dma_desc_enable)
+				break;
+
+			frame_desc = &urb->iso_descs[qtd->isoc_frame_index];
+
+			frame_desc->status = 0;
+
+			if (hcd->core_if->dma_enable) {
+				hc->xfer_buff = (uint8_t *) urb->dma;
+			} else {
+				hc->xfer_buff = (uint8_t *) urb->buf;
+			}
+			hc->xfer_buff +=
+			    frame_desc->offset + qtd->isoc_split_offset;
+			hc->xfer_len =
+			    frame_desc->length - qtd->isoc_split_offset;
+
+			/* For non-dword aligned buffers */
+			if (((unsigned long)hc->xfer_buff & 0x3)
+			    && hcd->core_if->dma_enable) {
+				ptr =
+				    (uint8_t *) urb->buf + frame_desc->offset +
+				    qtd->isoc_split_offset;
+			} else
+				ptr = NULL;
+
+			if (hc->xact_pos == DWC_HCSPLIT_XACTPOS_ALL) {
+				if (hc->xfer_len <= 188) {
+					hc->xact_pos = DWC_HCSPLIT_XACTPOS_ALL;
+				} else {
+					hc->xact_pos =
+					    DWC_HCSPLIT_XACTPOS_BEGIN;
+				}
+			}
+		}
+		break;
+	}
+	/* non DWORD-aligned buffer case */
+	if (ptr) {
+		uint32_t buf_size;
+		if (hc->ep_type != DWC_OTG_EP_TYPE_ISOC) {
+			buf_size = hcd->core_if->core_params->max_transfer_size;
+		} else {
+			buf_size = 4096;
+		}
+		if (!qh->dw_align_buf) {
+			qh->dw_align_buf = DWC_DMA_ALLOC_ATOMIC(buf_size,
+							 &qh->dw_align_buf_dma);
+			if (!qh->dw_align_buf) {
+				DWC_ERROR
+				    ("%s: Failed to allocate memory to handle "
+				     "non-dword aligned buffer case\n",
+				     __func__);
+				return;
+			}
+		}
+		if (!hc->ep_is_in) {
+			dwc_memcpy(qh->dw_align_buf, ptr, hc->xfer_len);
+		}
+		hc->align_buff = qh->dw_align_buf_dma;
+	} else {
+		hc->align_buff = 0;
+	}
+
+	if (hc->ep_type == DWC_OTG_EP_TYPE_INTR ||
+	    hc->ep_type == DWC_OTG_EP_TYPE_ISOC) {
+		/*
+		 * This value may be modified when the transfer is started to
+		 * reflect the actual transfer length.
+		 */
+		hc->multi_count = dwc_hb_mult(qh->maxp);
+	}
+
+	if (hcd->core_if->dma_desc_enable)
+		hc->desc_list_addr = qh->desc_list_dma;
+
+	dwc_otg_hc_init(hcd->core_if, hc);
+	hc->qh = qh;
+}
+
+/**
+ * This function selects transactions from the HCD transfer schedule and
+ * assigns them to available host channels. It is called from HCD interrupt
+ * handler functions.
+ *
+ * @param hcd The HCD state structure.
+ *
+ * @return The types of new transactions that were assigned to host channels.
+ */
+dwc_otg_transaction_type_e dwc_otg_hcd_select_transactions(dwc_otg_hcd_t * hcd)
+{
+	dwc_list_link_t *qh_ptr;
+	dwc_otg_qh_t *qh;
+	int num_channels;
+	dwc_otg_transaction_type_e ret_val = DWC_OTG_TRANSACTION_NONE;
+
+#ifdef DEBUG_SOF
+	DWC_DEBUGPL(DBG_HCD, "  Select Transactions\n");
+#endif
+
+	/* Process entries in the periodic ready list. */
+	qh_ptr = DWC_LIST_FIRST(&hcd->periodic_sched_ready);
+
+	while (qh_ptr != &hcd->periodic_sched_ready &&
+	       !DWC_CIRCLEQ_EMPTY(&hcd->free_hc_list)) {
+
+		qh = DWC_LIST_ENTRY(qh_ptr, dwc_otg_qh_t, qh_list_entry);
+		assign_and_init_hc(hcd, qh);
+
+		/*
+		 * Move the QH from the periodic ready schedule to the
+		 * periodic assigned schedule.
+		 */
+		qh_ptr = DWC_LIST_NEXT(qh_ptr);
+		DWC_LIST_MOVE_HEAD(&hcd->periodic_sched_assigned,
+				   &qh->qh_list_entry);
+
+		ret_val = DWC_OTG_TRANSACTION_PERIODIC;
+	}
+
+	/*
+	 * Process entries in the inactive portion of the non-periodic
+	 * schedule. Some free host channels may not be used if they are
+	 * reserved for periodic transfers.
+	 */
+	qh_ptr = hcd->non_periodic_sched_inactive.next;
+	num_channels = hcd->core_if->core_params->host_channels;
+	while (qh_ptr != &hcd->non_periodic_sched_inactive &&
+	       (hcd->non_periodic_channels <
+		num_channels - hcd->periodic_channels) &&
+	       !DWC_CIRCLEQ_EMPTY(&hcd->free_hc_list)) {
+
+		qh = DWC_LIST_ENTRY(qh_ptr, dwc_otg_qh_t, qh_list_entry);
+
+		assign_and_init_hc(hcd, qh);
+
+		/*
+		 * Move the QH from the non-periodic inactive schedule to the
+		 * non-periodic active schedule.
+		 */
+		qh_ptr = DWC_LIST_NEXT(qh_ptr);
+		DWC_LIST_MOVE_HEAD(&hcd->non_periodic_sched_active,
+				   &qh->qh_list_entry);
+
+		if (ret_val == DWC_OTG_TRANSACTION_NONE) {
+			ret_val = DWC_OTG_TRANSACTION_NON_PERIODIC;
+		} else {
+			ret_val = DWC_OTG_TRANSACTION_ALL;
+		}
+
+		hcd->non_periodic_channels++;
+	}
+
+	return ret_val;
+}
+
+/**
+ * Attempts to queue a single transaction request for a host channel
+ * associated with either a periodic or non-periodic transfer. This function
+ * assumes that there is space available in the appropriate request queue. For
+ * an OUT transfer or SETUP transaction in Slave mode, it checks whether space
+ * is available in the appropriate Tx FIFO.
+ *
+ * @param hcd The HCD state structure.
+ * @param hc Host channel descriptor associated with either a periodic or
+ * non-periodic transfer.
+ * @param fifo_dwords_avail Number of DWORDs available in the periodic Tx
+ * FIFO for periodic transfers or the non-periodic Tx FIFO for non-periodic
+ * transfers.
+ *
+ * @return 1 if a request is queued and more requests may be needed to
+ * complete the transfer, 0 if no more requests are required for this
+ * transfer, -1 if there is insufficient space in the Tx FIFO.
+ */
+static int queue_transaction(dwc_otg_hcd_t * hcd,
+			     dwc_hc_t * hc, uint16_t fifo_dwords_avail)
+{
+	int retval;
+
+	if (hcd->core_if->dma_enable) {
+		if (hcd->core_if->dma_desc_enable) {
+			if (!hc->xfer_started
+			    || (hc->ep_type == DWC_OTG_EP_TYPE_ISOC)) {
+				dwc_otg_hcd_start_xfer_ddma(hcd, hc->qh);
+				hc->qh->ping_state = 0;
+			}
+		} else if (!hc->xfer_started) {
+			dwc_otg_hc_start_transfer(hcd->core_if, hc);
+			hc->qh->ping_state = 0;
+		}
+		retval = 0;
+	} else if (hc->halt_pending) {
+		/* Don't queue a request if the channel has been halted. */
+		retval = 0;
+	} else if (hc->halt_on_queue) {
+		dwc_otg_hc_halt(hcd->core_if, hc, hc->halt_status);
+		retval = 0;
+	} else if (hc->do_ping) {
+		if (!hc->xfer_started) {
+			dwc_otg_hc_start_transfer(hcd->core_if, hc);
+		}
+		retval = 0;
+	} else if (!hc->ep_is_in || hc->data_pid_start == DWC_OTG_HC_PID_SETUP) {
+		if ((fifo_dwords_avail * 4) >= hc->max_packet) {
+			if (!hc->xfer_started) {
+				dwc_otg_hc_start_transfer(hcd->core_if, hc);
+				retval = 1;
+			} else {
+				retval =
+				    dwc_otg_hc_continue_transfer(hcd->core_if,
+								 hc);
+			}
+		} else {
+			retval = -1;
+		}
+	} else {
+		if (!hc->xfer_started) {
+			dwc_otg_hc_start_transfer(hcd->core_if, hc);
+			retval = 1;
+		} else {
+			retval = dwc_otg_hc_continue_transfer(hcd->core_if, hc);
+		}
+	}
+
+	return retval;
+}
+
+/**
+ * Processes periodic channels for the next frame and queues transactions for
+ * these channels to the DWC_otg controller. After queueing transactions, the
+ * Periodic Tx FIFO Empty interrupt is enabled if there are more transactions
+ * to queue as Periodic Tx FIFO or request queue space becomes available.
+ * Otherwise, the Periodic Tx FIFO Empty interrupt is disabled.
+ */
+static void process_periodic_channels(dwc_otg_hcd_t * hcd)
+{
+	hptxsts_data_t tx_status;
+	dwc_list_link_t *qh_ptr;
+	dwc_otg_qh_t *qh;
+	int status;
+	int no_queue_space = 0;
+	int no_fifo_space = 0;
+
+	dwc_otg_host_global_regs_t *host_regs;
+	host_regs = hcd->core_if->host_if->host_global_regs;
+
+	DWC_DEBUGPL(DBG_HCDV, "Queue periodic transactions\n");
+#ifdef DEBUG
+	tx_status.d32 = DWC_READ_REG32(&host_regs->hptxsts);
+	DWC_DEBUGPL(DBG_HCDV,
+		    "  P Tx Req Queue Space Avail (before queue): %d\n",
+		    tx_status.b.ptxqspcavail);
+	DWC_DEBUGPL(DBG_HCDV, "  P Tx FIFO Space Avail (before queue): %d\n",
+		    tx_status.b.ptxfspcavail);
+#endif
+
+	qh_ptr = hcd->periodic_sched_assigned.next;
+	while (qh_ptr != &hcd->periodic_sched_assigned) {
+		tx_status.d32 = DWC_READ_REG32(&host_regs->hptxsts);
+		if (tx_status.b.ptxqspcavail == 0) {
+			no_queue_space = 1;
+			break;
+		}
+
+		qh = DWC_LIST_ENTRY(qh_ptr, dwc_otg_qh_t, qh_list_entry);
+
+		/*
+		 * Set a flag if we're queuing high-bandwidth in slave mode.
+		 * The flag prevents any halts to get into the request queue in
+		 * the middle of multiple high-bandwidth packets getting queued.
+		 */
+		if (!hcd->core_if->dma_enable && qh->channel->multi_count > 1) {
+			hcd->core_if->queuing_high_bandwidth = 1;
+		}
+		status =
+		    queue_transaction(hcd, qh->channel,
+				      tx_status.b.ptxfspcavail);
+		if (status < 0) {
+			no_fifo_space = 1;
+			break;
+		}
+
+		/*
+		 * In Slave mode, stay on the current transfer until there is
+		 * nothing more to do or the high-bandwidth request count is
+		 * reached. In DMA mode, only need to queue one request. The
+		 * controller automatically handles multiple packets for
+		 * high-bandwidth transfers.
+		 */
+		if (hcd->core_if->dma_enable || status == 0 ||
+		    qh->channel->requests == qh->channel->multi_count) {
+			qh_ptr = qh_ptr->next;
+			/*
+			 * Move the QH from the periodic assigned schedule to
+			 * the periodic queued schedule.
+			 */
+			DWC_LIST_MOVE_HEAD(&hcd->periodic_sched_queued,
+					   &qh->qh_list_entry);
+
+			/* done queuing high bandwidth */
+			hcd->core_if->queuing_high_bandwidth = 0;
+		}
+	}
+
+	if (!hcd->core_if->dma_enable) {
+		dwc_otg_core_global_regs_t *global_regs;
+		gintmsk_data_t intr_mask = {.d32 = 0 };
+
+		global_regs = hcd->core_if->core_global_regs;
+		intr_mask.b.ptxfempty = 1;
+#ifdef DEBUG
+		tx_status.d32 = DWC_READ_REG32(&host_regs->hptxsts);
+		DWC_DEBUGPL(DBG_HCDV,
+			    "  P Tx Req Queue Space Avail (after queue): %d\n",
+			    tx_status.b.ptxqspcavail);
+		DWC_DEBUGPL(DBG_HCDV,
+			    "  P Tx FIFO Space Avail (after queue): %d\n",
+			    tx_status.b.ptxfspcavail);
+#endif
+		if (!DWC_LIST_EMPTY(&hcd->periodic_sched_assigned) ||
+		    no_queue_space || no_fifo_space) {
+			/*
+			 * May need to queue more transactions as the request
+			 * queue or Tx FIFO empties. Enable the periodic Tx
+			 * FIFO empty interrupt. (Always use the half-empty
+			 * level to ensure that new requests are loaded as
+			 * soon as possible.)
+			 */
+			DWC_MODIFY_REG32(&global_regs->gintmsk, 0,
+					 intr_mask.d32);
+		} else {
+			/*
+			 * Disable the Tx FIFO empty interrupt since there are
+			 * no more transactions that need to be queued right
+			 * now. This function is called from interrupt
+			 * handlers to queue more transactions as transfer
+			 * states change.
+			 */
+			DWC_MODIFY_REG32(&global_regs->gintmsk, intr_mask.d32,
+					 0);
+		}
+	}
+}
+
+/**
+ * Processes active non-periodic channels and queues transactions for these
+ * channels to the DWC_otg controller. After queueing transactions, the NP Tx
+ * FIFO Empty interrupt is enabled if there are more transactions to queue as
+ * NP Tx FIFO or request queue space becomes available. Otherwise, the NP Tx
+ * FIFO Empty interrupt is disabled.
+ */
+static void process_non_periodic_channels(dwc_otg_hcd_t * hcd)
+{
+	gnptxsts_data_t tx_status;
+	dwc_list_link_t *orig_qh_ptr;
+	dwc_otg_qh_t *qh;
+	int status;
+	int no_queue_space = 0;
+	int no_fifo_space = 0;
+	int more_to_do = 0;
+
+	dwc_otg_core_global_regs_t *global_regs =
+	    hcd->core_if->core_global_regs;
+
+	DWC_DEBUGPL(DBG_HCDV, "Queue non-periodic transactions\n");
+#ifdef DEBUG
+	tx_status.d32 = DWC_READ_REG32(&global_regs->gnptxsts);
+	DWC_DEBUGPL(DBG_HCDV,
+		    "  NP Tx Req Queue Space Avail (before queue): %d\n",
+		    tx_status.b.nptxqspcavail);
+	DWC_DEBUGPL(DBG_HCDV, "  NP Tx FIFO Space Avail (before queue): %d\n",
+		    tx_status.b.nptxfspcavail);
+#endif
+	/*
+	 * Keep track of the starting point. Skip over the start-of-list
+	 * entry.
+	 */
+	if (hcd->non_periodic_qh_ptr == &hcd->non_periodic_sched_active) {
+		hcd->non_periodic_qh_ptr = hcd->non_periodic_qh_ptr->next;
+	}
+	orig_qh_ptr = hcd->non_periodic_qh_ptr;
+
+	/*
+	 * Process once through the active list or until no more space is
+	 * available in the request queue or the Tx FIFO.
+	 */
+	do {
+		tx_status.d32 = DWC_READ_REG32(&global_regs->gnptxsts);
+		if (!hcd->core_if->dma_enable && tx_status.b.nptxqspcavail == 0) {
+			no_queue_space = 1;
+			break;
+		}
+
+		qh = DWC_LIST_ENTRY(hcd->non_periodic_qh_ptr, dwc_otg_qh_t,
+				    qh_list_entry);
+		status =
+		    queue_transaction(hcd, qh->channel,
+				      tx_status.b.nptxfspcavail);
+
+		if (status > 0) {
+			more_to_do = 1;
+		} else if (status < 0) {
+			no_fifo_space = 1;
+			break;
+		}
+
+		/* Advance to next QH, skipping start-of-list entry. */
+		hcd->non_periodic_qh_ptr = hcd->non_periodic_qh_ptr->next;
+		if (hcd->non_periodic_qh_ptr == &hcd->non_periodic_sched_active) {
+			hcd->non_periodic_qh_ptr =
+			    hcd->non_periodic_qh_ptr->next;
+		}
+
+	} while (hcd->non_periodic_qh_ptr != orig_qh_ptr);
+
+	if (!hcd->core_if->dma_enable) {
+		gintmsk_data_t intr_mask = {.d32 = 0 };
+		intr_mask.b.nptxfempty = 1;
+
+#ifdef DEBUG
+		tx_status.d32 = DWC_READ_REG32(&global_regs->gnptxsts);
+		DWC_DEBUGPL(DBG_HCDV,
+			    "  NP Tx Req Queue Space Avail (after queue): %d\n",
+			    tx_status.b.nptxqspcavail);
+		DWC_DEBUGPL(DBG_HCDV,
+			    "  NP Tx FIFO Space Avail (after queue): %d\n",
+			    tx_status.b.nptxfspcavail);
+#endif
+		if (more_to_do || no_queue_space || no_fifo_space) {
+			/*
+			 * May need to queue more transactions as the request
+			 * queue or Tx FIFO empties. Enable the non-periodic
+			 * Tx FIFO empty interrupt. (Always use the half-empty
+			 * level to ensure that new requests are loaded as
+			 * soon as possible.)
+			 */
+			DWC_MODIFY_REG32(&global_regs->gintmsk, 0,
+					 intr_mask.d32);
+		} else {
+			/*
+			 * Disable the Tx FIFO empty interrupt since there are
+			 * no more transactions that need to be queued right
+			 * now. This function is called from interrupt
+			 * handlers to queue more transactions as transfer
+			 * states change.
+			 */
+			DWC_MODIFY_REG32(&global_regs->gintmsk, intr_mask.d32,
+					 0);
+		}
+	}
+}
+
+/**
+ * This function processes the currently active host channels and queues
+ * transactions for these channels to the DWC_otg controller. It is called
+ * from HCD interrupt handler functions.
+ *
+ * @param hcd The HCD state structure.
+ * @param tr_type The type(s) of transactions to queue (non-periodic,
+ * periodic, or both).
+ */
+void dwc_otg_hcd_queue_transactions(dwc_otg_hcd_t * hcd,
+				    dwc_otg_transaction_type_e tr_type)
+{
+#ifdef DEBUG_SOF
+	DWC_DEBUGPL(DBG_HCD, "Queue Transactions\n");
+#endif
+	/* Process host channels associated with periodic transfers. */
+	if ((tr_type == DWC_OTG_TRANSACTION_PERIODIC ||
+	     tr_type == DWC_OTG_TRANSACTION_ALL) &&
+	    !DWC_LIST_EMPTY(&hcd->periodic_sched_assigned)) {
+
+		process_periodic_channels(hcd);
+	}
+
+	/* Process host channels associated with non-periodic transfers. */
+	if (tr_type == DWC_OTG_TRANSACTION_NON_PERIODIC ||
+	    tr_type == DWC_OTG_TRANSACTION_ALL) {
+		if (!DWC_LIST_EMPTY(&hcd->non_periodic_sched_active)) {
+			process_non_periodic_channels(hcd);
+		} else {
+			/*
+			 * Ensure NP Tx FIFO empty interrupt is disabled when
+			 * there are no non-periodic transfers to process.
+			 */
+			gintmsk_data_t gintmsk = {.d32 = 0 };
+			gintmsk.b.nptxfempty = 1;
+			DWC_MODIFY_REG32(&hcd->core_if->
+					 core_global_regs->gintmsk, gintmsk.d32,
+					 0);
+		}
+	}
+}
+
+#ifdef DWC_HS_ELECT_TST
+/*
+ * Quick and dirty hack to implement the HS Electrical Test
+ * SINGLE_STEP_GET_DEVICE_DESCRIPTOR feature.
+ *
+ * This code was copied from our userspace app "hset". It sends a
+ * Get Device Descriptor control sequence in two parts, first the
+ * Setup packet by itself, followed some time later by the In and
+ * Ack packets. Rather than trying to figure out how to add this
+ * functionality to the normal driver code, we just hijack the
+ * hardware, using these two function to drive the hardware
+ * directly.
+ */
+
+static dwc_otg_core_global_regs_t *global_regs;
+static dwc_otg_host_global_regs_t *hc_global_regs;
+static dwc_otg_hc_regs_t *hc_regs;
+static uint32_t *data_fifo;
+
+static void do_setup(void)
+{
+	gintsts_data_t gintsts;
+	hctsiz_data_t hctsiz;
+	hcchar_data_t hcchar;
+	haint_data_t haint;
+	hcint_data_t hcint;
+
+	/* Enable HAINTs */
+	DWC_WRITE_REG32(&hc_global_regs->haintmsk, 0x0001);
+
+	/* Enable HCINTs */
+	DWC_WRITE_REG32(&hc_regs->hcintmsk, 0x04a3);
+
+	/* Read GINTSTS */
+	gintsts.d32 = DWC_READ_REG32(&global_regs->gintsts);
+
+	/* Read HAINT */
+	haint.d32 = DWC_READ_REG32(&hc_global_regs->haint);
+
+	/* Read HCINT */
+	hcint.d32 = DWC_READ_REG32(&hc_regs->hcint);
+
+	/* Read HCCHAR */
+	hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+
+	/* Clear HCINT */
+	DWC_WRITE_REG32(&hc_regs->hcint, hcint.d32);
+
+	/* Clear HAINT */
+	DWC_WRITE_REG32(&hc_global_regs->haint, haint.d32);
+
+	/* Clear GINTSTS */
+	DWC_WRITE_REG32(&global_regs->gintsts, gintsts.d32);
+
+	/* Read GINTSTS */
+	gintsts.d32 = DWC_READ_REG32(&global_regs->gintsts);
+
+	/*
+	 * Send Setup packet (Get Device Descriptor)
+	 */
+
+	/* Make sure channel is disabled */
+	hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+	if (hcchar.b.chen) {
+		hcchar.b.chdis = 1;
+//              hcchar.b.chen = 1;
+		DWC_WRITE_REG32(&hc_regs->hcchar, hcchar.d32);
+		//sleep(1);
+		dwc_mdelay(1000);
+
+		/* Read GINTSTS */
+		gintsts.d32 = DWC_READ_REG32(&global_regs->gintsts);
+
+		/* Read HAINT */
+		haint.d32 = DWC_READ_REG32(&hc_global_regs->haint);
+
+		/* Read HCINT */
+		hcint.d32 = DWC_READ_REG32(&hc_regs->hcint);
+
+		/* Read HCCHAR */
+		hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+
+		/* Clear HCINT */
+		DWC_WRITE_REG32(&hc_regs->hcint, hcint.d32);
+
+		/* Clear HAINT */
+		DWC_WRITE_REG32(&hc_global_regs->haint, haint.d32);
+
+		/* Clear GINTSTS */
+		DWC_WRITE_REG32(&global_regs->gintsts, gintsts.d32);
+
+		hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+	}
+
+	/* Set HCTSIZ */
+	hctsiz.d32 = 0;
+	hctsiz.b.xfersize = 8;
+	hctsiz.b.pktcnt = 1;
+	hctsiz.b.pid = DWC_OTG_HC_PID_SETUP;
+	DWC_WRITE_REG32(&hc_regs->hctsiz, hctsiz.d32);
+
+	/* Set HCCHAR */
+	hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+	hcchar.b.eptype = DWC_OTG_EP_TYPE_CONTROL;
+	hcchar.b.epdir = 0;
+	hcchar.b.epnum = 0;
+	hcchar.b.mps = 8;
+	hcchar.b.chen = 1;
+	DWC_WRITE_REG32(&hc_regs->hcchar, hcchar.d32);
+
+	/* Fill FIFO with Setup data for Get Device Descriptor */
+	data_fifo = (uint32_t *) ((char *)global_regs + 0x1000);
+	DWC_WRITE_REG32(data_fifo++, 0x01000680);
+	DWC_WRITE_REG32(data_fifo++, 0x00080000);
+
+	gintsts.d32 = DWC_READ_REG32(&global_regs->gintsts);
+
+	/* Wait for host channel interrupt */
+	do {
+		gintsts.d32 = DWC_READ_REG32(&global_regs->gintsts);
+	} while (gintsts.b.hcintr == 0);
+
+	/* Disable HCINTs */
+	DWC_WRITE_REG32(&hc_regs->hcintmsk, 0x0000);
+
+	/* Disable HAINTs */
+	DWC_WRITE_REG32(&hc_global_regs->haintmsk, 0x0000);
+
+	/* Read HAINT */
+	haint.d32 = DWC_READ_REG32(&hc_global_regs->haint);
+
+	/* Read HCINT */
+	hcint.d32 = DWC_READ_REG32(&hc_regs->hcint);
+
+	/* Read HCCHAR */
+	hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+
+	/* Clear HCINT */
+	DWC_WRITE_REG32(&hc_regs->hcint, hcint.d32);
+
+	/* Clear HAINT */
+	DWC_WRITE_REG32(&hc_global_regs->haint, haint.d32);
+
+	/* Clear GINTSTS */
+	DWC_WRITE_REG32(&global_regs->gintsts, gintsts.d32);
+
+	/* Read GINTSTS */
+	gintsts.d32 = DWC_READ_REG32(&global_regs->gintsts);
+}
+
+static void do_in_ack(void)
+{
+	gintsts_data_t gintsts;
+	hctsiz_data_t hctsiz;
+	hcchar_data_t hcchar;
+	haint_data_t haint;
+	hcint_data_t hcint;
+	host_grxsts_data_t grxsts;
+
+	/* Enable HAINTs */
+	DWC_WRITE_REG32(&hc_global_regs->haintmsk, 0x0001);
+
+	/* Enable HCINTs */
+	DWC_WRITE_REG32(&hc_regs->hcintmsk, 0x04a3);
+
+	/* Read GINTSTS */
+	gintsts.d32 = DWC_READ_REG32(&global_regs->gintsts);
+
+	/* Read HAINT */
+	haint.d32 = DWC_READ_REG32(&hc_global_regs->haint);
+
+	/* Read HCINT */
+	hcint.d32 = DWC_READ_REG32(&hc_regs->hcint);
+
+	/* Read HCCHAR */
+	hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+
+	/* Clear HCINT */
+	DWC_WRITE_REG32(&hc_regs->hcint, hcint.d32);
+
+	/* Clear HAINT */
+	DWC_WRITE_REG32(&hc_global_regs->haint, haint.d32);
+
+	/* Clear GINTSTS */
+	DWC_WRITE_REG32(&global_regs->gintsts, gintsts.d32);
+
+	/* Read GINTSTS */
+	gintsts.d32 = DWC_READ_REG32(&global_regs->gintsts);
+
+	/*
+	 * Receive Control In packet
+	 */
+
+	/* Make sure channel is disabled */
+	hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+	if (hcchar.b.chen) {
+		hcchar.b.chdis = 1;
+		hcchar.b.chen = 1;
+		DWC_WRITE_REG32(&hc_regs->hcchar, hcchar.d32);
+		//sleep(1);
+		dwc_mdelay(1000);
+
+		/* Read GINTSTS */
+		gintsts.d32 = DWC_READ_REG32(&global_regs->gintsts);
+
+		/* Read HAINT */
+		haint.d32 = DWC_READ_REG32(&hc_global_regs->haint);
+
+		/* Read HCINT */
+		hcint.d32 = DWC_READ_REG32(&hc_regs->hcint);
+
+		/* Read HCCHAR */
+		hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+
+		/* Clear HCINT */
+		DWC_WRITE_REG32(&hc_regs->hcint, hcint.d32);
+
+		/* Clear HAINT */
+		DWC_WRITE_REG32(&hc_global_regs->haint, haint.d32);
+
+		/* Clear GINTSTS */
+		DWC_WRITE_REG32(&global_regs->gintsts, gintsts.d32);
+
+		hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+	}
+
+	/* Set HCTSIZ */
+	hctsiz.d32 = 0;
+	hctsiz.b.xfersize = 8;
+	hctsiz.b.pktcnt = 1;
+	hctsiz.b.pid = DWC_OTG_HC_PID_DATA1;
+	DWC_WRITE_REG32(&hc_regs->hctsiz, hctsiz.d32);
+
+	/* Set HCCHAR */
+	hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+	hcchar.b.eptype = DWC_OTG_EP_TYPE_CONTROL;
+	hcchar.b.epdir = 1;
+	hcchar.b.epnum = 0;
+	hcchar.b.mps = 8;
+	hcchar.b.chen = 1;
+	DWC_WRITE_REG32(&hc_regs->hcchar, hcchar.d32);
+
+	gintsts.d32 = DWC_READ_REG32(&global_regs->gintsts);
+
+	/* Wait for receive status queue interrupt */
+	do {
+		gintsts.d32 = DWC_READ_REG32(&global_regs->gintsts);
+	} while (gintsts.b.rxstsqlvl == 0);
+
+	/* Read RXSTS */
+	grxsts.d32 = DWC_READ_REG32(&global_regs->grxstsp);
+
+	/* Clear RXSTSQLVL in GINTSTS */
+	gintsts.d32 = 0;
+	gintsts.b.rxstsqlvl = 1;
+	DWC_WRITE_REG32(&global_regs->gintsts, gintsts.d32);
+
+	switch (grxsts.b.pktsts) {
+	case DWC_GRXSTS_PKTSTS_IN:
+		/* Read the data into the host buffer */
+		if (grxsts.b.bcnt > 0) {
+			int i;
+			int word_count = (grxsts.b.bcnt + 3) / 4;
+
+			data_fifo = (uint32_t *) ((char *)global_regs + 0x1000);
+
+			for (i = 0; i < word_count; i++) {
+				(void)DWC_READ_REG32(data_fifo++);
+			}
+		}
+		break;
+
+	default:
+		break;
+	}
+
+	gintsts.d32 = DWC_READ_REG32(&global_regs->gintsts);
+
+	/* Wait for receive status queue interrupt */
+	do {
+		gintsts.d32 = DWC_READ_REG32(&global_regs->gintsts);
+	} while (gintsts.b.rxstsqlvl == 0);
+
+	/* Read RXSTS */
+	grxsts.d32 = DWC_READ_REG32(&global_regs->grxstsp);
+
+	/* Clear RXSTSQLVL in GINTSTS */
+	gintsts.d32 = 0;
+	gintsts.b.rxstsqlvl = 1;
+	DWC_WRITE_REG32(&global_regs->gintsts, gintsts.d32);
+
+	switch (grxsts.b.pktsts) {
+	case DWC_GRXSTS_PKTSTS_IN_XFER_COMP:
+		break;
+
+	default:
+		break;
+	}
+
+	gintsts.d32 = DWC_READ_REG32(&global_regs->gintsts);
+
+	/* Wait for host channel interrupt */
+	do {
+		gintsts.d32 = DWC_READ_REG32(&global_regs->gintsts);
+	} while (gintsts.b.hcintr == 0);
+
+	/* Read HAINT */
+	haint.d32 = DWC_READ_REG32(&hc_global_regs->haint);
+
+	/* Read HCINT */
+	hcint.d32 = DWC_READ_REG32(&hc_regs->hcint);
+
+	/* Read HCCHAR */
+	hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+
+	/* Clear HCINT */
+	DWC_WRITE_REG32(&hc_regs->hcint, hcint.d32);
+
+	/* Clear HAINT */
+	DWC_WRITE_REG32(&hc_global_regs->haint, haint.d32);
+
+	/* Clear GINTSTS */
+	DWC_WRITE_REG32(&global_regs->gintsts, gintsts.d32);
+
+	/* Read GINTSTS */
+	gintsts.d32 = DWC_READ_REG32(&global_regs->gintsts);
+
+//      usleep(100000);
+//      mdelay(100);
+	dwc_mdelay(1);
+
+	/*
+	 * Send handshake packet
+	 */
+
+	/* Read HAINT */
+	haint.d32 = DWC_READ_REG32(&hc_global_regs->haint);
+
+	/* Read HCINT */
+	hcint.d32 = DWC_READ_REG32(&hc_regs->hcint);
+
+	/* Read HCCHAR */
+	hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+
+	/* Clear HCINT */
+	DWC_WRITE_REG32(&hc_regs->hcint, hcint.d32);
+
+	/* Clear HAINT */
+	DWC_WRITE_REG32(&hc_global_regs->haint, haint.d32);
+
+	/* Clear GINTSTS */
+	DWC_WRITE_REG32(&global_regs->gintsts, gintsts.d32);
+
+	/* Read GINTSTS */
+	gintsts.d32 = DWC_READ_REG32(&global_regs->gintsts);
+
+	/* Make sure channel is disabled */
+	hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+	if (hcchar.b.chen) {
+		hcchar.b.chdis = 1;
+		hcchar.b.chen = 1;
+		DWC_WRITE_REG32(&hc_regs->hcchar, hcchar.d32);
+		//sleep(1);
+		dwc_mdelay(1000);
+
+		/* Read GINTSTS */
+		gintsts.d32 = DWC_READ_REG32(&global_regs->gintsts);
+
+		/* Read HAINT */
+		haint.d32 = DWC_READ_REG32(&hc_global_regs->haint);
+
+		/* Read HCINT */
+		hcint.d32 = DWC_READ_REG32(&hc_regs->hcint);
+
+		/* Read HCCHAR */
+		hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+
+		/* Clear HCINT */
+		DWC_WRITE_REG32(&hc_regs->hcint, hcint.d32);
+
+		/* Clear HAINT */
+		DWC_WRITE_REG32(&hc_global_regs->haint, haint.d32);
+
+		/* Clear GINTSTS */
+		DWC_WRITE_REG32(&global_regs->gintsts, gintsts.d32);
+
+		hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+	}
+
+	/* Set HCTSIZ */
+	hctsiz.d32 = 0;
+	hctsiz.b.xfersize = 0;
+	hctsiz.b.pktcnt = 1;
+	hctsiz.b.pid = DWC_OTG_HC_PID_DATA1;
+	DWC_WRITE_REG32(&hc_regs->hctsiz, hctsiz.d32);
+
+	/* Set HCCHAR */
+	hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+	hcchar.b.eptype = DWC_OTG_EP_TYPE_CONTROL;
+	hcchar.b.epdir = 0;
+	hcchar.b.epnum = 0;
+	hcchar.b.mps = 8;
+	hcchar.b.chen = 1;
+	DWC_WRITE_REG32(&hc_regs->hcchar, hcchar.d32);
+
+	gintsts.d32 = DWC_READ_REG32(&global_regs->gintsts);
+
+	/* Wait for host channel interrupt */
+	do {
+		gintsts.d32 = DWC_READ_REG32(&global_regs->gintsts);
+	} while (gintsts.b.hcintr == 0);
+
+	/* Disable HCINTs */
+	DWC_WRITE_REG32(&hc_regs->hcintmsk, 0x0000);
+
+	/* Disable HAINTs */
+	DWC_WRITE_REG32(&hc_global_regs->haintmsk, 0x0000);
+
+	/* Read HAINT */
+	haint.d32 = DWC_READ_REG32(&hc_global_regs->haint);
+
+	/* Read HCINT */
+	hcint.d32 = DWC_READ_REG32(&hc_regs->hcint);
+
+	/* Read HCCHAR */
+	hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+
+	/* Clear HCINT */
+	DWC_WRITE_REG32(&hc_regs->hcint, hcint.d32);
+
+	/* Clear HAINT */
+	DWC_WRITE_REG32(&hc_global_regs->haint, haint.d32);
+
+	/* Clear GINTSTS */
+	DWC_WRITE_REG32(&global_regs->gintsts, gintsts.d32);
+
+	/* Read GINTSTS */
+	gintsts.d32 = DWC_READ_REG32(&global_regs->gintsts);
+}
+#endif
+
+/** Handles hub class-specific requests. */
+int dwc_otg_hcd_hub_control(dwc_otg_hcd_t * dwc_otg_hcd,
+			    uint16_t typeReq,
+			    uint16_t wValue,
+			    uint16_t wIndex, uint8_t * buf, uint16_t wLength)
+{
+	int retval = 0;
+
+	dwc_otg_core_if_t *core_if = dwc_otg_hcd->core_if;
+	usb_hub_descriptor_t *hub_desc;
+	hprt0_data_t hprt0 = {.d32 = 0 };
+
+	uint32_t port_status;
+
+	DWC_DEBUGPL(DBG_ANY, "DWC OTG HCD HUB CONTROL - "
+		    "typeReq 0x%04X, wValue 0x%04X, wIndex 0x%04X buf %p, wLength %u\n",
+		    typeReq, wValue, wIndex, buf, wLength);
+
+	switch (typeReq) {
+	case UCR_CLEAR_HUB_FEATURE:
+		DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD HUB CONTROL - "
+			    "ClearHubFeature 0x%x\n", wValue);
+		switch (wValue) {
+		case UHF_C_HUB_LOCAL_POWER:
+		case UHF_C_HUB_OVER_CURRENT:
+			/* Nothing required here */
+			break;
+		default:
+			retval = -DWC_E_INVALID;
+			DWC_ERROR("DWC OTG HCD - "
+				  "ClearHubFeature request %xh unknown\n",
+				  wValue);
+		}
+		break;
+	case UCR_CLEAR_PORT_FEATURE:
+#ifdef CONFIG_USB_DWC_OTG_LPM
+		if (wValue != UHF_PORT_L1)
+#endif
+			if (!wIndex || wIndex > 1)
+				goto error;
+
+		switch (wValue) {
+		case UHF_PORT_ENABLE:
+			DWC_DEBUGPL(DBG_ANY, "DWC OTG HCD HUB CONTROL - "
+				    "ClearPortFeature USB_PORT_FEAT_ENABLE\n");
+			hprt0.d32 = dwc_otg_read_hprt0(core_if);
+			hprt0.b.prtena = 1;
+			DWC_WRITE_REG32(core_if->host_if->hprt0, hprt0.d32);
+			break;
+		case UHF_PORT_SUSPEND:
+			DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD HUB CONTROL - "
+				    "ClearPortFeature USB_PORT_FEAT_SUSPEND\n");
+
+			if (core_if->power_down == 2) {
+				dwc_otg_host_hibernation_restore(core_if, 0, 0);
+			} else {
+				DWC_WRITE_REG32(core_if->pcgcctl, 0);
+				dwc_mdelay(5);
+
+				hprt0.d32 = dwc_otg_read_hprt0(core_if);
+				hprt0.b.prtres = 1;
+				DWC_WRITE_REG32(core_if->host_if->hprt0, hprt0.d32);
+				hprt0.b.prtsusp = 0;
+				/* Clear Resume bit */
+				dwc_mdelay(100);
+				hprt0.b.prtres = 0;
+				DWC_WRITE_REG32(core_if->host_if->hprt0, hprt0.d32);
+			}
+			break;
+#ifdef CONFIG_USB_DWC_OTG_LPM
+		case UHF_PORT_L1:
+			{
+			DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD HUB CONTROL - "
+				    "ClearPortFeature USB_PORT_L1\n");
+				pcgcctl_data_t pcgcctl = {.d32 = 0 };
+				glpmcfg_data_t lpmcfg = {.d32 = 0 };
+
+				lpmcfg.d32 =
+				    DWC_READ_REG32(&core_if->
+						   core_global_regs->glpmcfg);
+				lpmcfg.b.en_utmi_sleep = 0;
+				lpmcfg.b.hird_thres &= (~(1 << 4));
+				lpmcfg.b.prt_sleep_sts = 1;
+				DWC_WRITE_REG32(&core_if->
+						core_global_regs->glpmcfg,
+						lpmcfg.d32);
+
+				/* Clear Enbl_L1Gating bit. */
+				pcgcctl.b.enbl_sleep_gating = 1;
+				DWC_MODIFY_REG32(core_if->pcgcctl, pcgcctl.d32,
+						 0);
+
+				dwc_mdelay(5);
+
+				hprt0.d32 = dwc_otg_read_hprt0(core_if);
+				hprt0.b.prtres = 1;
+				DWC_WRITE_REG32(core_if->host_if->hprt0,
+						hprt0.d32);
+				/* This bit will be cleared in wakeup interrupt handle */
+				break;
+			}
+#endif
+		case UHF_PORT_POWER:
+			DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD HUB CONTROL - "
+				    "ClearPortFeature USB_PORT_FEAT_POWER\n");
+			hprt0.d32 = dwc_otg_read_hprt0(core_if);
+			hprt0.b.prtpwr = 0;
+			DWC_WRITE_REG32(core_if->host_if->hprt0, hprt0.d32);
+			break;
+		case UHF_PORT_INDICATOR:
+			DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD HUB CONTROL - "
+				    "ClearPortFeature USB_PORT_FEAT_INDICATOR\n");
+			/* Port inidicator not supported */
+			break;
+		case UHF_C_PORT_CONNECTION:
+			/* Clears drivers internal connect status change
+			 * flag */
+			DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD HUB CONTROL - "
+				    "ClearPortFeature USB_PORT_FEAT_C_CONNECTION\n");
+			dwc_otg_hcd->flags.b.port_connect_status_change = 0;
+			break;
+		case UHF_C_PORT_RESET:
+			/* Clears the driver's internal Port Reset Change
+			 * flag */
+			DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD HUB CONTROL - "
+				    "ClearPortFeature USB_PORT_FEAT_C_RESET\n");
+			dwc_otg_hcd->flags.b.port_reset_change = 0;
+			break;
+		case UHF_C_PORT_ENABLE:
+			/* Clears the driver's internal Port
+			 * Enable/Disable Change flag */
+			DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD HUB CONTROL - "
+				    "ClearPortFeature USB_PORT_FEAT_C_ENABLE\n");
+			dwc_otg_hcd->flags.b.port_enable_change = 0;
+			break;
+		case UHF_C_PORT_SUSPEND:
+			/* Clears the driver's internal Port Suspend
+			 * Change flag, which is set when resume signaling on
+			 * the host port is complete */
+			DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD HUB CONTROL - "
+				    "ClearPortFeature USB_PORT_FEAT_C_SUSPEND\n");
+			dwc_otg_hcd->flags.b.port_suspend_change = 0;
+			break;
+#ifdef CONFIG_USB_DWC_OTG_LPM
+		case UHF_C_PORT_L1:
+			dwc_otg_hcd->flags.b.port_l1_change = 0;
+			break;
+#endif
+		case UHF_C_PORT_OVER_CURRENT:
+			DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD HUB CONTROL - "
+				    "ClearPortFeature USB_PORT_FEAT_C_OVER_CURRENT\n");
+			dwc_otg_hcd->flags.b.port_over_current_change = 0;
+			break;
+		default:
+			retval = -DWC_E_INVALID;
+			DWC_ERROR("DWC OTG HCD - "
+				  "ClearPortFeature request %xh "
+				  "unknown or unsupported\n", wValue);
+		}
+		break;
+	case UCR_GET_HUB_DESCRIPTOR:
+		DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD HUB CONTROL - "
+			    "GetHubDescriptor\n");
+		hub_desc = (usb_hub_descriptor_t *) buf;
+		hub_desc->bDescLength = 9;
+		hub_desc->bDescriptorType = 0x29;
+		hub_desc->bNbrPorts = 1;
+		USETW(hub_desc->wHubCharacteristics, 0x08);
+		hub_desc->bPwrOn2PwrGood = 1;
+		hub_desc->bHubContrCurrent = 0;
+		hub_desc->DeviceRemovable[0] = 0;
+		hub_desc->DeviceRemovable[1] = 0xff;
+		break;
+	case UCR_GET_HUB_STATUS:
+		DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD HUB CONTROL - "
+			    "GetHubStatus\n");
+		DWC_MEMSET(buf, 0, 4);
+		break;
+	case UCR_GET_PORT_STATUS:
+		DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD HUB CONTROL - "
+			    "GetPortStatus wIndex = 0x%04x FLAGS=0x%08x\n",
+			    wIndex, dwc_otg_hcd->flags.d32);
+		if (!wIndex || wIndex > 1)
+			goto error;
+
+		port_status = 0;
+
+		if (dwc_otg_hcd->flags.b.port_connect_status_change)
+			port_status |= (1 << UHF_C_PORT_CONNECTION);
+
+		if (dwc_otg_hcd->flags.b.port_enable_change)
+			port_status |= (1 << UHF_C_PORT_ENABLE);
+
+		if (dwc_otg_hcd->flags.b.port_suspend_change)
+			port_status |= (1 << UHF_C_PORT_SUSPEND);
+
+		if (dwc_otg_hcd->flags.b.port_l1_change)
+			port_status |= (1 << UHF_C_PORT_L1);
+
+		if (dwc_otg_hcd->flags.b.port_reset_change) {
+			port_status |= (1 << UHF_C_PORT_RESET);
+		}
+
+		if (dwc_otg_hcd->flags.b.port_over_current_change) {
+			DWC_WARN("Overcurrent change detected\n");
+			port_status |= (1 << UHF_C_PORT_OVER_CURRENT);
+		}
+
+		if (!dwc_otg_hcd->flags.b.port_connect_status) {
+			/*
+			 * The port is disconnected, which means the core is
+			 * either in device mode or it soon will be. Just
+			 * return 0's for the remainder of the port status
+			 * since the port register can't be read if the core
+			 * is in device mode.
+			 */
+			*((__le32 *) buf) = dwc_cpu_to_le32(&port_status);
+			break;
+		}
+
+		hprt0.d32 = DWC_READ_REG32(core_if->host_if->hprt0);
+		DWC_DEBUGPL(DBG_HCDV, "  HPRT0: 0x%08x\n", hprt0.d32);
+
+		if (hprt0.b.prtconnsts)
+			port_status |= (1 << UHF_PORT_CONNECTION);
+
+		if (hprt0.b.prtena)
+			port_status |= (1 << UHF_PORT_ENABLE);
+
+		if (hprt0.b.prtsusp)
+			port_status |= (1 << UHF_PORT_SUSPEND);
+
+		if (hprt0.b.prtovrcurract)
+			port_status |= (1 << UHF_PORT_OVER_CURRENT);
+
+		if (hprt0.b.prtrst)
+			port_status |= (1 << UHF_PORT_RESET);
+
+		if (hprt0.b.prtpwr)
+			port_status |= (1 << UHF_PORT_POWER);
+
+		if (hprt0.b.prtspd == DWC_HPRT0_PRTSPD_HIGH_SPEED)
+			port_status |= (1 << UHF_PORT_HIGH_SPEED);
+		else if (hprt0.b.prtspd == DWC_HPRT0_PRTSPD_LOW_SPEED)
+			port_status |= (1 << UHF_PORT_LOW_SPEED);
+
+		if (hprt0.b.prttstctl)
+			port_status |= (1 << UHF_PORT_TEST);
+		if (dwc_otg_get_lpm_portsleepstatus(dwc_otg_hcd->core_if)) {
+			port_status |= (1 << UHF_PORT_L1);
+		}
+		/*
+		   For Synopsys HW emulation of Power down wkup_control asserts the
+		   hreset_n and prst_n on suspned. This causes the HPRT0 to be zero.
+		   We intentionally tell the software that port is in L2Suspend state.
+		   Only for STE.
+		*/
+		if ((core_if->power_down == 2)
+		    && (core_if->hibernation_suspend == 1)) {
+			port_status |= (1 << UHF_PORT_SUSPEND);
+		}
+		/* USB_PORT_FEAT_INDICATOR unsupported always 0 */
+
+		*((__le32 *) buf) = dwc_cpu_to_le32(&port_status);
+
+		break;
+	case UCR_SET_HUB_FEATURE:
+		DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD HUB CONTROL - "
+			    "SetHubFeature\n");
+		/* No HUB features supported */
+		break;
+	case UCR_SET_PORT_FEATURE:
+		DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD HUB CONTROL - "
+			    "SetPortFeature - wValue 0x%04X wIndex 0x%04X port_connect_status %u\n",
+			    wValue, wIndex, dwc_otg_hcd->flags.b.port_connect_status);
+		if (wValue != UHF_PORT_TEST && (!wIndex || wIndex > 1)) {
+			DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD HUB CONTROL - "
+				    "SetPortFeature - wValue not UHF_PORT_TEST and wIndex not valid\n");
+			goto error;
+		}
+
+		if (!dwc_otg_hcd->flags.b.port_connect_status) {
+			/*
+			 * The port is disconnected, which means the core is
+			 * either in device mode or it soon will be. Just
+			 * return without doing anything since the port
+			 * register can't be written if the core is in device
+			 * mode.
+			 */
+			DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD HUB CONTROL - "
+				    "SetPortFeature - port is disconnected, do nothing\n");
+			break;
+		}
+
+		switch (wValue) {
+		case UHF_PORT_SUSPEND:
+			DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD HUB CONTROL - "
+				    "SetPortFeature - USB_PORT_FEAT_SUSPEND\n");
+			if (dwc_otg_hcd_otg_port(dwc_otg_hcd) != wIndex) {
+				goto error;
+			}
+			if (core_if->power_down == 2) {
+				int timeout = 300;
+				dwc_irqflags_t flags;
+				pcgcctl_data_t pcgcctl = {.d32 = 0 };
+				gpwrdn_data_t gpwrdn = {.d32 = 0 };
+				gusbcfg_data_t gusbcfg = {.d32 = 0 };
+#ifdef DWC_DEV_SRPCAP
+				int32_t otg_cap_param = core_if->core_params->otg_cap;
+#endif
+				DWC_PRINTF("Preparing for complete power-off\n");
+
+				/* Save registers before hibernation */
+				dwc_otg_save_global_regs(core_if);
+				dwc_otg_save_host_regs(core_if);
+
+				hprt0.d32 = dwc_otg_read_hprt0(core_if);
+				hprt0.b.prtsusp = 1;
+				hprt0.b.prtena = 0;
+				DWC_WRITE_REG32(core_if->host_if->hprt0, hprt0.d32);
+				/* Spin hprt0.b.prtsusp to became 1 */
+				do {
+					hprt0.d32 = dwc_otg_read_hprt0(core_if);
+					if (hprt0.b.prtsusp) {
+						break;
+					}
+					dwc_mdelay(1);
+				} while (--timeout);
+				if (!timeout) {
+					DWC_WARN("Suspend wasn't genereted\n");
+				}
+				dwc_udelay(10);
+
+				/*
+				 * We need to disable interrupts to prevent servicing of any IRQ
+				 * during going to hibernation
+				 */
+				DWC_SPINLOCK_IRQSAVE(dwc_otg_hcd->lock, &flags);
+				core_if->lx_state = DWC_OTG_L2;
+#ifdef DWC_DEV_SRPCAP
+				hprt0.d32 = dwc_otg_read_hprt0(core_if);
+				hprt0.b.prtpwr = 0;
+				hprt0.b.prtena = 0;
+				DWC_WRITE_REG32(core_if->host_if->hprt0,
+						hprt0.d32);
+#endif
+				gusbcfg.d32 =
+				    DWC_READ_REG32(&core_if->core_global_regs->
+						   gusbcfg);
+				if (gusbcfg.b.ulpi_utmi_sel == 1) {
+					/* ULPI interface */
+					/* Suspend the Phy Clock */
+					pcgcctl.d32 = 0;
+					pcgcctl.b.stoppclk = 1;
+					DWC_MODIFY_REG32(core_if->pcgcctl, 0,
+							 pcgcctl.d32);
+					dwc_udelay(10);
+					gpwrdn.b.pmuactv = 1;
+					DWC_MODIFY_REG32(&core_if->
+							 core_global_regs->
+							 gpwrdn, 0, gpwrdn.d32);
+				} else {
+					/* UTMI+ Interface */
+					gpwrdn.b.pmuactv = 1;
+					DWC_MODIFY_REG32(&core_if->
+							 core_global_regs->
+							 gpwrdn, 0, gpwrdn.d32);
+					dwc_udelay(10);
+					pcgcctl.b.stoppclk = 1;
+					DWC_MODIFY_REG32(core_if->pcgcctl, 0, pcgcctl.d32);
+					dwc_udelay(10);
+				}
+#ifdef DWC_DEV_SRPCAP
+				gpwrdn.d32 = 0;
+				gpwrdn.b.dis_vbus = 1;
+				DWC_MODIFY_REG32(&core_if->core_global_regs->
+						 gpwrdn, 0, gpwrdn.d32);
+#endif
+				gpwrdn.d32 = 0;
+				gpwrdn.b.pmuintsel = 1;
+				DWC_MODIFY_REG32(&core_if->core_global_regs->
+						 gpwrdn, 0, gpwrdn.d32);
+				dwc_udelay(10);
+
+				gpwrdn.d32 = 0;
+#ifdef DWC_DEV_SRPCAP
+				gpwrdn.b.srp_det_msk = 1;
+#endif
+				gpwrdn.b.disconn_det_msk = 1;
+				gpwrdn.b.lnstchng_msk = 1;
+				gpwrdn.b.sts_chngint_msk = 1;
+				DWC_MODIFY_REG32(&core_if->core_global_regs->
+						 gpwrdn, 0, gpwrdn.d32);
+				dwc_udelay(10);
+
+				/* Enable Power Down Clamp and all interrupts in GPWRDN */
+				gpwrdn.d32 = 0;
+				gpwrdn.b.pwrdnclmp = 1;
+				DWC_MODIFY_REG32(&core_if->core_global_regs->
+						 gpwrdn, 0, gpwrdn.d32);
+				dwc_udelay(10);
+
+				/* Switch off VDD */
+				gpwrdn.d32 = 0;
+				gpwrdn.b.pwrdnswtch = 1;
+				DWC_MODIFY_REG32(&core_if->core_global_regs->
+						 gpwrdn, 0, gpwrdn.d32);
+
+#ifdef DWC_DEV_SRPCAP
+				if (otg_cap_param == DWC_OTG_CAP_PARAM_HNP_SRP_CAPABLE)
+				{
+					core_if->pwron_timer_started = 1;
+					DWC_TIMER_SCHEDULE(core_if->pwron_timer, 6000 /* 6 secs */ );
+				}
+#endif
+				/* Save gpwrdn register for further usage if stschng interrupt */
+				core_if->gr_backup->gpwrdn_local =
+						DWC_READ_REG32(&core_if->core_global_regs->gpwrdn);
+
+				/* Set flag to indicate that we are in hibernation */
+				core_if->hibernation_suspend = 1;
+				DWC_SPINUNLOCK_IRQRESTORE(dwc_otg_hcd->lock,flags);
+
+				DWC_PRINTF("Host hibernation completed\n");
+				// Exit from case statement
+				break;
+
+			}
+			if (dwc_otg_hcd_otg_port(dwc_otg_hcd) == wIndex &&
+			    dwc_otg_hcd->fops->get_b_hnp_enable(dwc_otg_hcd))
+			{
+#ifndef CONFIG_MACH_M822XX
+				/* Base value of GOTGCTL, VBUS checking enabled */
+				gotgctl_data_t gotgctl = {.d32 = 0 };
+#else  /* CONFIG_MACH_M822XX */
+				/* Base value of GOTGCTL, VBUS checking disabled */
+				gotgctl_data_t gotgctl = {.d32 = ((1<<3) | (1<<2)) };
+#endif	/* CONFIG_MACH_M822XX */
+
+				gotgctl.b.hstsethnpen = 1;
+				DWC_MODIFY_REG32(&core_if->core_global_regs->
+						 gotgctl, 0, gotgctl.d32);
+				core_if->op_state = A_SUSPEND;
+			}
+			hprt0.d32 = dwc_otg_read_hprt0(core_if);
+			hprt0.b.prtsusp = 1;
+			DWC_WRITE_REG32(core_if->host_if->hprt0, hprt0.d32);
+			{
+				dwc_irqflags_t flags;
+				/* Update lx_state */
+				DWC_SPINLOCK_IRQSAVE(dwc_otg_hcd->lock, &flags);
+				core_if->lx_state = DWC_OTG_L2;
+				DWC_SPINUNLOCK_IRQRESTORE(dwc_otg_hcd->lock, flags);
+			}
+			/* Suspend the Phy Clock */
+			{
+				pcgcctl_data_t pcgcctl = {.d32 = 0 };
+				pcgcctl.b.stoppclk = 1;
+				DWC_MODIFY_REG32(core_if->pcgcctl, 0,
+						 pcgcctl.d32);
+				dwc_udelay(10);
+			}
+
+			/* For HNP the bus must be suspended for at least 200ms. */
+			if (dwc_otg_hcd->fops->get_b_hnp_enable(dwc_otg_hcd)) {
+				pcgcctl_data_t pcgcctl = {.d32 = 0 };
+				pcgcctl.b.stoppclk = 1;
+                DWC_MODIFY_REG32(core_if->pcgcctl, pcgcctl.d32, 0);
+				dwc_mdelay(200);
+			}
+
+			/** @todo - check how sw can wait for 1 sec to check asesvld??? */
+			break;
+		case UHF_PORT_POWER:
+			DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD HUB CONTROL - "
+				    "SetPortFeature - USB_PORT_FEAT_POWER\n");
+			hprt0.d32 = dwc_otg_read_hprt0(core_if);
+			hprt0.b.prtpwr = 1;
+			DWC_WRITE_REG32(core_if->host_if->hprt0, hprt0.d32);
+			break;
+		case UHF_PORT_RESET:
+			DWC_DEBUGPL(DBG_HCD,
+				    "DWC OTG HCD HUB CONTROL - "
+				    "SetPortFeature - USB_PORT_FEAT_RESET - Command\n");
+			if ((core_if->power_down == 2)
+			    && (core_if->hibernation_suspend == 1)) {
+				/* If we are going to exit from Hibernated
+				 * state via USB RESET.
+				 */
+				DWC_DEBUGPL(DBG_HCD,
+					    "DWC OTG HCD HUB CONTROL - "
+					    "SetPortFeature - USB_PORT_FEAT_RESET - Exiting from hibernation\n");
+				dwc_otg_host_hibernation_restore(core_if, 0, 1);
+			} else {
+				hprt0.d32 = dwc_otg_read_hprt0(core_if);
+
+				DWC_DEBUGPL(DBG_HCD,
+					    "DWC OTG HCD HUB CONTROL - "
+					    "SetPortFeature - USB_PORT_FEAT_RESET\n");
+				{
+					pcgcctl_data_t pcgcctl = {.d32 = 0 };
+					pcgcctl.b.enbl_sleep_gating = 1;
+					pcgcctl.b.stoppclk = 1;
+					DWC_MODIFY_REG32(core_if->pcgcctl, pcgcctl.d32, 0);
+					DWC_WRITE_REG32(core_if->pcgcctl, 0);
+				}
+#ifdef CONFIG_USB_DWC_OTG_LPM
+				{
+					glpmcfg_data_t lpmcfg;
+					lpmcfg.d32 =
+						DWC_READ_REG32(&core_if->core_global_regs->glpmcfg);
+					if (lpmcfg.b.prt_sleep_sts) {
+						lpmcfg.b.en_utmi_sleep = 0;
+						lpmcfg.b.hird_thres &= (~(1 << 4));
+						DWC_WRITE_REG32
+						    (&core_if->core_global_regs->glpmcfg,
+						     lpmcfg.d32);
+						dwc_mdelay(1);
+					}
+				}
+#endif
+				hprt0.d32 = dwc_otg_read_hprt0(core_if);
+				/* Clear suspend bit if resetting from suspended state. */
+				hprt0.b.prtsusp = 0;
+				/* When B-Host the Port reset bit is set in
+				 * the Start HCD Callback function, so that
+				 * the reset is started within 1ms of the HNP
+				 * success interrupt. */
+				if (!dwc_otg_hcd_is_b_host(dwc_otg_hcd)) {
+					hprt0.b.prtpwr = 1;
+					hprt0.b.prtrst = 1;
+					DWC_PRINTF("Indeed it is in host mode hprt0 = %08x\n",hprt0.d32);
+					DWC_WRITE_REG32(core_if->host_if->hprt0,
+							hprt0.d32);
+				}
+				/* Clear reset bit in 10ms (FS/LS) or 50ms (HS) */
+				dwc_mdelay(60);
+				hprt0.b.prtrst = 0;
+				DWC_WRITE_REG32(core_if->host_if->hprt0, hprt0.d32);
+				core_if->lx_state = DWC_OTG_L0;	/* Now back to the on state */
+			}
+			break;
+#ifdef DWC_HS_ELECT_TST
+		case UHF_PORT_TEST:
+			{
+				uint32_t t;
+				gintmsk_data_t gintmsk;
+
+				t = (wIndex >> 8);	/* MSB wIndex USB */
+				DWC_DEBUGPL(DBG_HCD,
+					    "DWC OTG HCD HUB CONTROL - "
+					    "SetPortFeature - USB_PORT_FEAT_TEST %d\n",
+					    t);
+				DWC_WARN("USB_PORT_FEAT_TEST %d\n", t);
+				if (t < 6) {
+					hprt0.d32 = dwc_otg_read_hprt0(core_if);
+					hprt0.b.prttstctl = t;
+					DWC_WRITE_REG32(core_if->host_if->hprt0,
+							hprt0.d32);
+				} else {
+					/* Setup global vars with reg addresses (quick and
+					 * dirty hack, should be cleaned up)
+					 */
+					global_regs = core_if->core_global_regs;
+					hc_global_regs =
+					    core_if->host_if->host_global_regs;
+					hc_regs =
+					    (dwc_otg_hc_regs_t *) ((char *)
+								   global_regs +
+								   0x500);
+					data_fifo =
+					    (uint32_t *) ((char *)global_regs +
+							  0x1000);
+
+					if (t == 6) {	/* HS_HOST_PORT_SUSPEND_RESUME */
+						/* Save current interrupt mask */
+						gintmsk.d32 =
+						    DWC_READ_REG32
+						    (&global_regs->gintmsk);
+
+						/* Disable all interrupts while we muck with
+						 * the hardware directly
+						 */
+						DWC_WRITE_REG32(&global_regs->gintmsk, 0);
+
+						/* 15 second delay per the test spec */
+						dwc_mdelay(15000);
+
+						/* Drive suspend on the root port */
+						hprt0.d32 =
+						    dwc_otg_read_hprt0(core_if);
+						hprt0.b.prtsusp = 1;
+						hprt0.b.prtres = 0;
+						DWC_WRITE_REG32(core_if->host_if->hprt0, hprt0.d32);
+
+						/* 15 second delay per the test spec */
+						dwc_mdelay(15000);
+
+						/* Drive resume on the root port */
+						hprt0.d32 =
+						    dwc_otg_read_hprt0(core_if);
+						hprt0.b.prtsusp = 0;
+						hprt0.b.prtres = 1;
+						DWC_WRITE_REG32(core_if->host_if->hprt0, hprt0.d32);
+						dwc_mdelay(100);
+
+						/* Clear the resume bit */
+						hprt0.b.prtres = 0;
+						DWC_WRITE_REG32(core_if->host_if->hprt0, hprt0.d32);
+
+						/* Restore interrupts */
+						DWC_WRITE_REG32(&global_regs->gintmsk, gintmsk.d32);
+					} else if (t == 7) {	/* SINGLE_STEP_GET_DEVICE_DESCRIPTOR setup */
+						/* Save current interrupt mask */
+						gintmsk.d32 =
+						    DWC_READ_REG32
+						    (&global_regs->gintmsk);
+
+						/* Disable all interrupts while we muck with
+						 * the hardware directly
+						 */
+						DWC_WRITE_REG32(&global_regs->gintmsk, 0);
+
+						/* 15 second delay per the test spec */
+						dwc_mdelay(15000);
+
+						/* Send the Setup packet */
+						do_setup();
+
+						/* 15 second delay so nothing else happens for awhile */
+						dwc_mdelay(15000);
+
+						/* Restore interrupts */
+						DWC_WRITE_REG32(&global_regs->gintmsk, gintmsk.d32);
+					} else if (t == 8) {	/* SINGLE_STEP_GET_DEVICE_DESCRIPTOR execute */
+						/* Save current interrupt mask */
+						gintmsk.d32 =
+						    DWC_READ_REG32
+						    (&global_regs->gintmsk);
+
+						/* Disable all interrupts while we muck with
+						 * the hardware directly
+						 */
+						DWC_WRITE_REG32(&global_regs->gintmsk, 0);
+
+						/* Send the Setup packet */
+						do_setup();
+
+						/* 15 second delay so nothing else happens for awhile */
+						dwc_mdelay(15000);
+
+						/* Send the In and Ack packets */
+						do_in_ack();
+
+						/* 15 second delay so nothing else happens for awhile */
+						dwc_mdelay(15000);
+
+						/* Restore interrupts */
+						DWC_WRITE_REG32(&global_regs->gintmsk, gintmsk.d32);
+					}
+				}
+				break;
+			}
+#endif /* DWC_HS_ELECT_TST */
+
+		case UHF_PORT_INDICATOR:
+			DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD HUB CONTROL - "
+				    "SetPortFeature - USB_PORT_FEAT_INDICATOR\n");
+			/* Not supported */
+			break;
+		default:
+			retval = -DWC_E_INVALID;
+			DWC_ERROR("DWC OTG HCD - "
+				  "SetPortFeature request %xh "
+				  "unknown or unsupported\n", wValue);
+			break;
+		}
+		break;
+#ifdef CONFIG_USB_DWC_OTG_LPM
+	case UCR_SET_AND_TEST_PORT_FEATURE:
+		if (wValue != UHF_PORT_L1) {
+			goto error;
+		}
+		{
+			int portnum, hird, devaddr, remwake;
+			glpmcfg_data_t lpmcfg;
+			uint32_t time_usecs;
+			gintsts_data_t gintsts;
+			gintmsk_data_t gintmsk;
+
+			if (!dwc_otg_get_param_lpm_enable(core_if)) {
+				goto error;
+			}
+			if (wValue != UHF_PORT_L1 || wLength != 1) {
+				goto error;
+			}
+			/* Check if the port currently is in SLEEP state */
+			lpmcfg.d32 =
+			    DWC_READ_REG32(&core_if->core_global_regs->glpmcfg);
+			if (lpmcfg.b.prt_sleep_sts) {
+				DWC_INFO("Port is already in sleep mode\n");
+				buf[0] = 0;	/* Return success */
+				break;
+			}
+
+			portnum = wIndex & 0xf;
+			hird = (wIndex >> 4) & 0xf;
+			devaddr = (wIndex >> 8) & 0x7f;
+			remwake = (wIndex >> 15);
+
+			if (portnum != 1) {
+				retval = -DWC_E_INVALID;
+				DWC_WARN
+				    ("Wrong port number(%d) in SetandTestPortFeature request\n",
+				     portnum);
+				break;
+			}
+
+			DWC_PRINTF
+			    ("SetandTestPortFeature request: portnum = %d, hird = %d, devaddr = %d, rewake = %d\n",
+			     portnum, hird, devaddr, remwake);
+			/* Disable LPM interrupt */
+			gintmsk.d32 = 0;
+			gintmsk.b.lpmtranrcvd = 1;
+			DWC_MODIFY_REG32(&core_if->core_global_regs->gintmsk,
+					 gintmsk.d32, 0);
+
+			if (dwc_otg_hcd_send_lpm
+			    (dwc_otg_hcd, devaddr, hird, remwake)) {
+				retval = -DWC_E_INVALID;
+				break;
+			}
+
+			time_usecs = 10 * (lpmcfg.b.retry_count + 1);
+			/* We will consider timeout if time_usecs microseconds pass,
+			 * and we don't receive LPM transaction status.
+			 * After receiving non-error responce(ACK/NYET/STALL) from device,
+			 *  core will set lpmtranrcvd bit.
+			 */
+			do {
+				gintsts.d32 =
+				    DWC_READ_REG32(&core_if->core_global_regs->gintsts);
+				if (gintsts.b.lpmtranrcvd) {
+					break;
+				}
+				dwc_udelay(1);
+			} while (--time_usecs);
+			/* lpm_int bit will be cleared in LPM interrupt handler */
+
+			/* Now fill status
+			 * 0x00 - Success
+			 * 0x10 - NYET
+			 * 0x11 - Timeout
+			 */
+			if (!gintsts.b.lpmtranrcvd) {
+				buf[0] = 0x3;	/* Completion code is Timeout */
+				dwc_otg_hcd_free_hc_from_lpm(dwc_otg_hcd);
+			} else {
+				lpmcfg.d32 =
+				    DWC_READ_REG32(&core_if->core_global_regs->glpmcfg);
+				if (lpmcfg.b.lpm_resp == 0x3) {
+					/* ACK responce from the device */
+					buf[0] = 0x00;	/* Success */
+				} else if (lpmcfg.b.lpm_resp == 0x2) {
+					/* NYET responce from the device */
+					buf[0] = 0x2;
+				} else {
+					/* Otherwise responce with Timeout */
+					buf[0] = 0x3;
+				}
+			}
+			DWC_PRINTF("Device responce to LPM trans is %x\n",
+				   lpmcfg.b.lpm_resp);
+			DWC_MODIFY_REG32(&core_if->core_global_regs->gintmsk, 0,
+					 gintmsk.d32);
+
+			break;
+		}
+#endif /* CONFIG_USB_DWC_OTG_LPM */
+	default:
+error:
+		retval = -DWC_E_INVALID;
+		DWC_WARN("DWC OTG HCD - "
+			 "Unknown hub control request type or invalid typeReq: %xh wIndex: %xh wValue: %xh\n",
+			 typeReq, wIndex, wValue);
+		break;
+	}
+
+	return retval;
+}
+
+#ifdef CONFIG_USB_DWC_OTG_LPM
+/** Returns index of host channel to perform LPM transaction. */
+int dwc_otg_hcd_get_hc_for_lpm_tran(dwc_otg_hcd_t * hcd, uint8_t devaddr)
+{
+	dwc_otg_core_if_t *core_if = hcd->core_if;
+	dwc_hc_t *hc;
+	hcchar_data_t hcchar;
+	gintmsk_data_t gintmsk = {.d32 = 0 };
+
+	if (DWC_CIRCLEQ_EMPTY(&hcd->free_hc_list)) {
+		DWC_PRINTF("No free channel to select for LPM transaction\n");
+		return -1;
+	}
+
+	hc = DWC_CIRCLEQ_FIRST(&hcd->free_hc_list);
+
+	/* Mask host channel interrupts. */
+	gintmsk.b.hcintr = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gintmsk, gintmsk.d32, 0);
+
+	/* Fill fields that core needs for LPM transaction */
+	hcchar.b.devaddr = devaddr;
+	hcchar.b.epnum = 0;
+	hcchar.b.eptype = DWC_OTG_EP_TYPE_CONTROL;
+	hcchar.b.mps = 64;
+	hcchar.b.lspddev = (hc->speed == DWC_OTG_EP_SPEED_LOW);
+	hcchar.b.epdir = 0;	/* OUT */
+	DWC_WRITE_REG32(&core_if->host_if->hc_regs[hc->hc_num]->hcchar,
+			hcchar.d32);
+
+	/* Remove the host channel from the free list. */
+	DWC_CIRCLEQ_REMOVE_INIT(&hcd->free_hc_list, hc, hc_list_entry);
+
+	DWC_PRINTF("hcnum = %d devaddr = %d\n", hc->hc_num, devaddr);
+
+	return hc->hc_num;
+}
+
+/** Release hc after performing LPM transaction */
+void dwc_otg_hcd_free_hc_from_lpm(dwc_otg_hcd_t * hcd)
+{
+	dwc_hc_t *hc;
+	glpmcfg_data_t lpmcfg;
+	uint8_t hc_num;
+
+	lpmcfg.d32 = DWC_READ_REG32(&hcd->core_if->core_global_regs->glpmcfg);
+	hc_num = lpmcfg.b.lpm_chan_index;
+
+	hc = hcd->hc_ptr_array[hc_num];
+
+	DWC_PRINTF("Freeing channel %d after LPM\n", hc_num);
+	/* Return host channel to free list */
+	DWC_CIRCLEQ_INSERT_TAIL(&hcd->free_hc_list, hc, hc_list_entry);
+}
+
+int dwc_otg_hcd_send_lpm(dwc_otg_hcd_t * hcd, uint8_t devaddr, uint8_t hird,
+			 uint8_t bRemoteWake)
+{
+	glpmcfg_data_t lpmcfg;
+	pcgcctl_data_t pcgcctl = {.d32 = 0 };
+	int channel;
+
+	channel = dwc_otg_hcd_get_hc_for_lpm_tran(hcd, devaddr);
+	if (channel < 0) {
+		return channel;
+	}
+
+	pcgcctl.b.enbl_sleep_gating = 1;
+	DWC_MODIFY_REG32(hcd->core_if->pcgcctl, 0, pcgcctl.d32);
+
+	/* Read LPM config register */
+	lpmcfg.d32 = DWC_READ_REG32(&hcd->core_if->core_global_regs->glpmcfg);
+
+	/* Program LPM transaction fields */
+	lpmcfg.b.rem_wkup_en = bRemoteWake;
+	lpmcfg.b.hird = hird;
+	lpmcfg.b.hird_thres = 0x1c;
+	lpmcfg.b.lpm_chan_index = channel;
+	lpmcfg.b.en_utmi_sleep = 1;
+	/* Program LPM config register */
+	DWC_WRITE_REG32(&hcd->core_if->core_global_regs->glpmcfg, lpmcfg.d32);
+
+	/* Send LPM transaction */
+	lpmcfg.b.send_lpm = 1;
+	DWC_WRITE_REG32(&hcd->core_if->core_global_regs->glpmcfg, lpmcfg.d32);
+
+	return 0;
+}
+
+#endif /* CONFIG_USB_DWC_OTG_LPM */
+
+int dwc_otg_hcd_is_status_changed(dwc_otg_hcd_t * hcd, int port)
+{
+	int retval;
+
+	if (port != 1) {
+		return -DWC_E_INVALID;
+	}
+
+	retval = (hcd->flags.b.port_connect_status_change ||
+		  hcd->flags.b.port_reset_change ||
+		  hcd->flags.b.port_enable_change ||
+		  hcd->flags.b.port_suspend_change ||
+		  hcd->flags.b.port_over_current_change);
+#ifdef DEBUG
+	if (retval) {
+		DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD HUB STATUS DATA:"
+			    " Root port status changed\n");
+		DWC_DEBUGPL(DBG_HCDV, "  port_connect_status_change: %d\n",
+			    hcd->flags.b.port_connect_status_change);
+		DWC_DEBUGPL(DBG_HCDV, "  port_reset_change: %d\n",
+			    hcd->flags.b.port_reset_change);
+		DWC_DEBUGPL(DBG_HCDV, "  port_enable_change: %d\n",
+			    hcd->flags.b.port_enable_change);
+		DWC_DEBUGPL(DBG_HCDV, "  port_suspend_change: %d\n",
+			    hcd->flags.b.port_suspend_change);
+		DWC_DEBUGPL(DBG_HCDV, "  port_over_current_change: %d\n",
+			    hcd->flags.b.port_over_current_change);
+	}
+#endif
+	return retval;
+}
+
+int dwc_otg_hcd_get_frame_number(dwc_otg_hcd_t * dwc_otg_hcd)
+{
+	hfnum_data_t hfnum;
+	hfnum.d32 =
+	    DWC_READ_REG32(&dwc_otg_hcd->core_if->host_if->host_global_regs->
+			   hfnum);
+
+#ifdef DEBUG_SOF
+	DWC_DEBUGPL(DBG_HCDV, "DWC OTG HCD GET FRAME NUMBER %d\n",
+		    hfnum.b.frnum);
+#endif
+	return hfnum.b.frnum;
+}
+
+int dwc_otg_hcd_start(dwc_otg_hcd_t * hcd,
+		      struct dwc_otg_hcd_function_ops *fops)
+{
+	int retval = 0;
+
+	hcd->fops = fops;
+	if (!dwc_otg_is_device_mode(hcd->core_if) &&
+		(!hcd->core_if->adp_enable || hcd->core_if->adp.adp_started)) {
+		dwc_otg_hcd_reinit(hcd);
+	} else {
+		retval = -DWC_E_NO_DEVICE;
+	}
+
+	return retval;
+}
+
+void *dwc_otg_hcd_get_priv_data(dwc_otg_hcd_t * hcd)
+{
+	return hcd->priv;
+}
+
+void dwc_otg_hcd_set_priv_data(dwc_otg_hcd_t * hcd, void *priv_data)
+{
+	hcd->priv = priv_data;
+}
+
+uint32_t dwc_otg_hcd_otg_port(dwc_otg_hcd_t * hcd)
+{
+	return hcd->otg_port;
+}
+
+uint32_t dwc_otg_hcd_is_b_host(dwc_otg_hcd_t * hcd)
+{
+	uint32_t is_b_host;
+	if (hcd->core_if->op_state == B_HOST) {
+		is_b_host = 1;
+	} else {
+		is_b_host = 0;
+	}
+
+	return is_b_host;
+}
+
+dwc_otg_hcd_urb_t *dwc_otg_hcd_urb_alloc(dwc_otg_hcd_t * hcd,
+					 int iso_desc_count, int atomic_alloc)
+{
+	dwc_otg_hcd_urb_t *dwc_otg_urb;
+	uint32_t size;
+
+	size =
+	    sizeof(*dwc_otg_urb) +
+	    iso_desc_count * sizeof(struct dwc_otg_hcd_iso_packet_desc);
+	if (atomic_alloc)
+		dwc_otg_urb = DWC_ALLOC_ATOMIC(size);
+	else
+		dwc_otg_urb = DWC_ALLOC(size);
+
+	dwc_otg_urb->packet_count = iso_desc_count;
+
+	return dwc_otg_urb;
+}
+
+void dwc_otg_hcd_urb_set_pipeinfo(dwc_otg_hcd_urb_t * dwc_otg_urb,
+				  uint8_t dev_addr, uint8_t ep_num,
+				  uint8_t ep_type, uint8_t ep_dir, uint16_t mps)
+{
+	dwc_otg_hcd_fill_pipe(&dwc_otg_urb->pipe_info, dev_addr, ep_num,
+			      ep_type, ep_dir, mps);
+#if 0
+	DWC_PRINTF
+	    ("addr = %d, ep_num = %d, ep_dir = 0x%x, ep_type = 0x%x, mps = %d\n",
+	     dev_addr, ep_num, ep_dir, ep_type, mps);
+#endif
+}
+
+void dwc_otg_hcd_urb_set_params(dwc_otg_hcd_urb_t * dwc_otg_urb,
+				void *urb_handle, void *buf, dwc_dma_t dma,
+				uint32_t buflen, void *setup_packet,
+				dwc_dma_t setup_dma, uint32_t flags,
+				uint16_t interval)
+{
+	dwc_otg_urb->priv = urb_handle;
+	dwc_otg_urb->buf = buf;
+	dwc_otg_urb->dma = dma;
+	dwc_otg_urb->length = buflen;
+	dwc_otg_urb->setup_packet = setup_packet;
+	dwc_otg_urb->setup_dma = setup_dma;
+	dwc_otg_urb->flags = flags;
+	dwc_otg_urb->interval = interval;
+	dwc_otg_urb->status = -DWC_E_IN_PROGRESS;
+}
+
+uint32_t dwc_otg_hcd_urb_get_status(dwc_otg_hcd_urb_t * dwc_otg_urb)
+{
+	return dwc_otg_urb->status;
+}
+
+uint32_t dwc_otg_hcd_urb_get_actual_length(dwc_otg_hcd_urb_t * dwc_otg_urb)
+{
+	return dwc_otg_urb->actual_length;
+}
+
+uint32_t dwc_otg_hcd_urb_get_error_count(dwc_otg_hcd_urb_t * dwc_otg_urb)
+{
+	return dwc_otg_urb->error_count;
+}
+
+void dwc_otg_hcd_urb_set_iso_desc_params(dwc_otg_hcd_urb_t * dwc_otg_urb,
+					 int desc_num, uint32_t offset,
+					 uint32_t length)
+{
+	dwc_otg_urb->iso_descs[desc_num].offset = offset;
+	dwc_otg_urb->iso_descs[desc_num].length = length;
+}
+
+uint32_t dwc_otg_hcd_urb_get_iso_desc_status(dwc_otg_hcd_urb_t * dwc_otg_urb,
+					     int desc_num)
+{
+	return dwc_otg_urb->iso_descs[desc_num].status;
+}
+
+uint32_t dwc_otg_hcd_urb_get_iso_desc_actual_length(dwc_otg_hcd_urb_t *
+						    dwc_otg_urb, int desc_num)
+{
+	return dwc_otg_urb->iso_descs[desc_num].actual_length;
+}
+
+int dwc_otg_hcd_is_bandwidth_allocated(dwc_otg_hcd_t * hcd, void *ep_handle)
+{
+	int allocated = 0;
+	dwc_otg_qh_t *qh = (dwc_otg_qh_t *) ep_handle;
+
+	if (qh) {
+		if (!DWC_LIST_EMPTY(&qh->qh_list_entry)) {
+			allocated = 1;
+		}
+	}
+	return allocated;
+}
+
+int dwc_otg_hcd_is_bandwidth_freed(dwc_otg_hcd_t * hcd, void *ep_handle)
+{
+	dwc_otg_qh_t *qh = (dwc_otg_qh_t *) ep_handle;
+	int freed = 0;
+	DWC_ASSERT(qh, "qh is not allocated\n");
+
+	if (DWC_LIST_EMPTY(&qh->qh_list_entry)) {
+		freed = 1;
+	}
+
+	return freed;
+}
+
+uint8_t dwc_otg_hcd_get_ep_bandwidth(dwc_otg_hcd_t * hcd, void *ep_handle)
+{
+	dwc_otg_qh_t *qh = (dwc_otg_qh_t *) ep_handle;
+	DWC_ASSERT(qh, "qh is not allocated\n");
+	return qh->usecs;
+}
+
+void dwc_otg_hcd_dump_state(dwc_otg_hcd_t * hcd)
+{
+#ifdef DEBUG
+	int num_channels;
+	int i;
+	gnptxsts_data_t np_tx_status;
+	hptxsts_data_t p_tx_status;
+
+	num_channels = hcd->core_if->core_params->host_channels;
+	DWC_PRINTF("\n");
+	DWC_PRINTF
+	    ("************************************************************\n");
+	DWC_PRINTF("HCD State:\n");
+	DWC_PRINTF("  Num channels: %d\n", num_channels);
+	for (i = 0; i < num_channels; i++) {
+		dwc_hc_t *hc = hcd->hc_ptr_array[i];
+		DWC_PRINTF("  Channel %d:\n", i);
+		DWC_PRINTF("    dev_addr: %d, ep_num: %d, ep_is_in: %d\n",
+			   hc->dev_addr, hc->ep_num, hc->ep_is_in);
+		DWC_PRINTF("    speed: %d\n", hc->speed);
+		DWC_PRINTF("    ep_type: %d\n", hc->ep_type);
+		DWC_PRINTF("    max_packet: %d\n", hc->max_packet);
+		DWC_PRINTF("    data_pid_start: %d\n", hc->data_pid_start);
+		DWC_PRINTF("    multi_count: %d\n", hc->multi_count);
+		DWC_PRINTF("    xfer_started: %d\n", hc->xfer_started);
+		DWC_PRINTF("    xfer_buff: %p\n", hc->xfer_buff);
+		DWC_PRINTF("    xfer_len: %d\n", hc->xfer_len);
+		DWC_PRINTF("    xfer_count: %d\n", hc->xfer_count);
+		DWC_PRINTF("    halt_on_queue: %d\n", hc->halt_on_queue);
+		DWC_PRINTF("    halt_pending: %d\n", hc->halt_pending);
+		DWC_PRINTF("    halt_status: %d\n", hc->halt_status);
+		DWC_PRINTF("    do_split: %d\n", hc->do_split);
+		DWC_PRINTF("    complete_split: %d\n", hc->complete_split);
+		DWC_PRINTF("    hub_addr: %d\n", hc->hub_addr);
+		DWC_PRINTF("    port_addr: %d\n", hc->port_addr);
+		DWC_PRINTF("    xact_pos: %d\n", hc->xact_pos);
+		DWC_PRINTF("    requests: %d\n", hc->requests);
+		DWC_PRINTF("    qh: %p\n", hc->qh);
+		if (hc->xfer_started) {
+			hfnum_data_t hfnum;
+			hcchar_data_t hcchar;
+			hctsiz_data_t hctsiz;
+			hcint_data_t hcint;
+			hcintmsk_data_t hcintmsk;
+			hfnum.d32 =
+			    DWC_READ_REG32(&hcd->core_if->
+					   host_if->host_global_regs->hfnum);
+			hcchar.d32 =
+			    DWC_READ_REG32(&hcd->core_if->host_if->
+					   hc_regs[i]->hcchar);
+			hctsiz.d32 =
+			    DWC_READ_REG32(&hcd->core_if->host_if->
+					   hc_regs[i]->hctsiz);
+			hcint.d32 =
+			    DWC_READ_REG32(&hcd->core_if->host_if->
+					   hc_regs[i]->hcint);
+			hcintmsk.d32 =
+			    DWC_READ_REG32(&hcd->core_if->host_if->
+					   hc_regs[i]->hcintmsk);
+			DWC_PRINTF("    hfnum: 0x%08x\n", hfnum.d32);
+			DWC_PRINTF("    hcchar: 0x%08x\n", hcchar.d32);
+			DWC_PRINTF("    hctsiz: 0x%08x\n", hctsiz.d32);
+			DWC_PRINTF("    hcint: 0x%08x\n", hcint.d32);
+			DWC_PRINTF("    hcintmsk: 0x%08x\n", hcintmsk.d32);
+		}
+		if (hc->xfer_started && hc->qh) {
+			dwc_otg_qtd_t *qtd;
+			dwc_otg_hcd_urb_t *urb;
+
+			DWC_CIRCLEQ_FOREACH(qtd, &hc->qh->qtd_list, qtd_list_entry) {
+				if (!qtd->in_process)
+					break;
+
+				urb = qtd->urb;
+			DWC_PRINTF("    URB Info:\n");
+			DWC_PRINTF("      qtd: %p, urb: %p\n", qtd, urb);
+			if (urb) {
+				DWC_PRINTF("      Dev: %d, EP: %d %s\n",
+					   dwc_otg_hcd_get_dev_addr(&urb->
+								    pipe_info),
+					   dwc_otg_hcd_get_ep_num(&urb->
+								  pipe_info),
+					   dwc_otg_hcd_is_pipe_in(&urb->
+								  pipe_info) ?
+					   "IN" : "OUT");
+				DWC_PRINTF("      Max packet size: %d\n",
+					   dwc_otg_hcd_get_mps(&urb->
+							       pipe_info));
+				DWC_PRINTF("      transfer_buffer: %p\n",
+					   urb->buf);
+				DWC_PRINTF("      transfer_dma: %p\n",
+					   (void *)urb->dma);
+				DWC_PRINTF("      transfer_buffer_length: %d\n",
+					   urb->length);
+					DWC_PRINTF("      actual_length: %d\n",
+						   urb->actual_length);
+				}
+			}
+		}
+	}
+	DWC_PRINTF("  non_periodic_channels: %d\n", hcd->non_periodic_channels);
+	DWC_PRINTF("  periodic_channels: %d\n", hcd->periodic_channels);
+	DWC_PRINTF("  periodic_usecs: %d\n", hcd->periodic_usecs);
+	np_tx_status.d32 =
+	    DWC_READ_REG32(&hcd->core_if->core_global_regs->gnptxsts);
+	DWC_PRINTF("  NP Tx Req Queue Space Avail: %d\n",
+		   np_tx_status.b.nptxqspcavail);
+	DWC_PRINTF("  NP Tx FIFO Space Avail: %d\n",
+		   np_tx_status.b.nptxfspcavail);
+	p_tx_status.d32 =
+	    DWC_READ_REG32(&hcd->core_if->host_if->host_global_regs->hptxsts);
+	DWC_PRINTF("  P Tx Req Queue Space Avail: %d\n",
+		   p_tx_status.b.ptxqspcavail);
+	DWC_PRINTF("  P Tx FIFO Space Avail: %d\n", p_tx_status.b.ptxfspcavail);
+	dwc_otg_hcd_dump_frrem(hcd);
+	dwc_otg_dump_global_registers(hcd->core_if);
+	dwc_otg_dump_host_registers(hcd->core_if);
+	DWC_PRINTF
+	    ("************************************************************\n");
+	DWC_PRINTF("\n");
+#endif
+}
+
+#ifdef DEBUG
+void dwc_print_setup_data(uint8_t * setup)
+{
+	int i;
+	if (CHK_DEBUG_LEVEL(DBG_HCD)) {
+		DWC_PRINTF("Setup Data = MSB ");
+		for (i = 7; i >= 0; i--)
+			DWC_PRINTF("%02x ", setup[i]);
+		DWC_PRINTF("\n");
+		DWC_PRINTF("  bmRequestType Tranfer = %s\n",
+			   (setup[0] & 0x80) ? "Device-to-Host" :
+			   "Host-to-Device");
+		DWC_PRINTF("  bmRequestType Type = ");
+		switch ((setup[0] & 0x60) >> 5) {
+		case 0:
+			DWC_PRINTF("Standard\n");
+			break;
+		case 1:
+			DWC_PRINTF("Class\n");
+			break;
+		case 2:
+			DWC_PRINTF("Vendor\n");
+			break;
+		case 3:
+			DWC_PRINTF("Reserved\n");
+			break;
+		}
+		DWC_PRINTF("  bmRequestType Recipient = ");
+		switch (setup[0] & 0x1f) {
+		case 0:
+			DWC_PRINTF("Device\n");
+			break;
+		case 1:
+			DWC_PRINTF("Interface\n");
+			break;
+		case 2:
+			DWC_PRINTF("Endpoint\n");
+			break;
+		case 3:
+			DWC_PRINTF("Other\n");
+			break;
+		default:
+			DWC_PRINTF("Reserved\n");
+			break;
+		}
+		DWC_PRINTF("  bRequest = 0x%0x\n", setup[1]);
+		DWC_PRINTF("  wValue = 0x%0x\n", *((uint16_t *) & setup[2]));
+		DWC_PRINTF("  wIndex = 0x%0x\n", *((uint16_t *) & setup[4]));
+		DWC_PRINTF("  wLength = 0x%0x\n\n", *((uint16_t *) & setup[6]));
+	}
+}
+#endif
+
+void dwc_otg_hcd_dump_frrem(dwc_otg_hcd_t * hcd)
+{
+#if 0
+	DWC_PRINTF("Frame remaining at SOF:\n");
+	DWC_PRINTF("  samples %u, accum %llu, avg %llu\n",
+		   hcd->frrem_samples, hcd->frrem_accum,
+		   (hcd->frrem_samples > 0) ?
+		   hcd->frrem_accum / hcd->frrem_samples : 0);
+
+	DWC_PRINTF("\n");
+	DWC_PRINTF("Frame remaining at start_transfer (uframe 7):\n");
+	DWC_PRINTF("  samples %u, accum %llu, avg %llu\n",
+		   hcd->core_if->hfnum_7_samples,
+		   hcd->core_if->hfnum_7_frrem_accum,
+		   (hcd->core_if->hfnum_7_samples >
+		    0) ? hcd->core_if->hfnum_7_frrem_accum /
+		   hcd->core_if->hfnum_7_samples : 0);
+	DWC_PRINTF("Frame remaining at start_transfer (uframe 0):\n");
+	DWC_PRINTF("  samples %u, accum %llu, avg %llu\n",
+		   hcd->core_if->hfnum_0_samples,
+		   hcd->core_if->hfnum_0_frrem_accum,
+		   (hcd->core_if->hfnum_0_samples >
+		    0) ? hcd->core_if->hfnum_0_frrem_accum /
+		   hcd->core_if->hfnum_0_samples : 0);
+	DWC_PRINTF("Frame remaining at start_transfer (uframe 1-6):\n");
+	DWC_PRINTF("  samples %u, accum %llu, avg %llu\n",
+		   hcd->core_if->hfnum_other_samples,
+		   hcd->core_if->hfnum_other_frrem_accum,
+		   (hcd->core_if->hfnum_other_samples >
+		    0) ? hcd->core_if->hfnum_other_frrem_accum /
+		   hcd->core_if->hfnum_other_samples : 0);
+
+	DWC_PRINTF("\n");
+	DWC_PRINTF("Frame remaining at sample point A (uframe 7):\n");
+	DWC_PRINTF("  samples %u, accum %llu, avg %llu\n",
+		   hcd->hfnum_7_samples_a, hcd->hfnum_7_frrem_accum_a,
+		   (hcd->hfnum_7_samples_a > 0) ?
+		   hcd->hfnum_7_frrem_accum_a / hcd->hfnum_7_samples_a : 0);
+	DWC_PRINTF("Frame remaining at sample point A (uframe 0):\n");
+	DWC_PRINTF("  samples %u, accum %llu, avg %llu\n",
+		   hcd->hfnum_0_samples_a, hcd->hfnum_0_frrem_accum_a,
+		   (hcd->hfnum_0_samples_a > 0) ?
+		   hcd->hfnum_0_frrem_accum_a / hcd->hfnum_0_samples_a : 0);
+	DWC_PRINTF("Frame remaining at sample point A (uframe 1-6):\n");
+	DWC_PRINTF("  samples %u, accum %llu, avg %llu\n",
+		   hcd->hfnum_other_samples_a, hcd->hfnum_other_frrem_accum_a,
+		   (hcd->hfnum_other_samples_a > 0) ?
+		   hcd->hfnum_other_frrem_accum_a /
+		   hcd->hfnum_other_samples_a : 0);
+
+	DWC_PRINTF("\n");
+	DWC_PRINTF("Frame remaining at sample point B (uframe 7):\n");
+	DWC_PRINTF("  samples %u, accum %llu, avg %llu\n",
+		   hcd->hfnum_7_samples_b, hcd->hfnum_7_frrem_accum_b,
+		   (hcd->hfnum_7_samples_b > 0) ?
+		   hcd->hfnum_7_frrem_accum_b / hcd->hfnum_7_samples_b : 0);
+	DWC_PRINTF("Frame remaining at sample point B (uframe 0):\n");
+	DWC_PRINTF("  samples %u, accum %llu, avg %llu\n",
+		   hcd->hfnum_0_samples_b, hcd->hfnum_0_frrem_accum_b,
+		   (hcd->hfnum_0_samples_b > 0) ?
+		   hcd->hfnum_0_frrem_accum_b / hcd->hfnum_0_samples_b : 0);
+	DWC_PRINTF("Frame remaining at sample point B (uframe 1-6):\n");
+	DWC_PRINTF("  samples %u, accum %llu, avg %llu\n",
+		   hcd->hfnum_other_samples_b, hcd->hfnum_other_frrem_accum_b,
+		   (hcd->hfnum_other_samples_b > 0) ?
+		   hcd->hfnum_other_frrem_accum_b /
+		   hcd->hfnum_other_samples_b : 0);
+#endif
+}
+
+#endif /* DWC_DEVICE_ONLY */
diff --git a/drivers/usb/dwc_otg/dwc_otg_hcd.h b/drivers/usb/dwc_otg/dwc_otg_hcd.h
new file mode 100644
index 0000000..58a1293
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_otg_hcd.h
@@ -0,0 +1,803 @@
+/* ==========================================================================
+ * $File: //dwh/usb_iip/dev/software/otg/linux/drivers/dwc_otg_hcd.h $
+ * $Revision: #58 $
+ * $Date: 2011/09/15 $
+ * $Change: 1846647 $
+ *
+ * Synopsys HS OTG Linux Software Driver and documentation (hereinafter,
+ * "Software") is an Unsupported proprietary work of Synopsys, Inc. unless
+ * otherwise expressly agreed to in writing between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product under
+ * any End User Software License Agreement or Agreement for Licensed Product
+ * with Synopsys or any supplement thereto. You are permitted to use and
+ * redistribute this Software in source and binary forms, with or without
+ * modification, provided that redistributions of source code must retain this
+ * notice. You may not view, use, disclose, copy or distribute this file or
+ * any information contained herein except pursuant to this license grant from
+ * Synopsys. If you do not agree with this notice, including the disclaimer
+ * below, then you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS" BASIS
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE HEREBY DISCLAIMED. IN NO EVENT SHALL SYNOPSYS BE LIABLE FOR ANY DIRECT,
+ * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================== */
+#ifndef DWC_DEVICE_ONLY
+#ifndef __DWC_HCD_H__
+#define __DWC_HCD_H__
+
+#include "dwc_otg_os_dep.h"
+#include "usb.h"
+#include "dwc_otg_hcd_if.h"
+#include "dwc_otg_core_if.h"
+#include "dwc_list.h"
+#include "dwc_otg_cil.h"
+
+/**
+ * @file
+ *
+ * This file contains the structures, constants, and interfaces for
+ * the Host Contoller Driver (HCD).
+ *
+ * The Host Controller Driver (HCD) is responsible for translating requests
+ * from the USB Driver into the appropriate actions on the DWC_otg controller.
+ * It isolates the USBD from the specifics of the controller by providing an
+ * API to the USBD.
+ */
+
+struct dwc_otg_hcd_pipe_info {
+	uint8_t dev_addr;
+	uint8_t ep_num;
+	uint8_t pipe_type;
+	uint8_t pipe_dir;
+	uint16_t mps;
+};
+
+struct dwc_otg_hcd_iso_packet_desc {
+	uint32_t offset;
+	uint32_t length;
+	uint32_t actual_length;
+	uint32_t status;
+};
+
+struct dwc_otg_qtd;
+
+struct dwc_otg_hcd_urb {
+	void *priv;
+	struct dwc_otg_qtd *qtd;
+	void *buf;
+	dwc_dma_t dma;
+	void *setup_packet;
+	dwc_dma_t setup_dma;
+	uint32_t length;
+	uint32_t actual_length;
+	uint32_t status;
+	uint32_t error_count;
+	uint32_t packet_count;
+	uint32_t flags;
+	uint16_t interval;
+	struct dwc_otg_hcd_pipe_info pipe_info;
+	struct dwc_otg_hcd_iso_packet_desc iso_descs[0];
+};
+
+static inline uint8_t dwc_otg_hcd_get_ep_num(struct dwc_otg_hcd_pipe_info *pipe)
+{
+	return pipe->ep_num;
+}
+
+static inline uint8_t dwc_otg_hcd_get_pipe_type(struct dwc_otg_hcd_pipe_info
+						*pipe)
+{
+	return pipe->pipe_type;
+}
+
+static inline uint16_t dwc_otg_hcd_get_mps(struct dwc_otg_hcd_pipe_info *pipe)
+{
+	return pipe->mps;
+}
+
+static inline uint8_t dwc_otg_hcd_get_dev_addr(struct dwc_otg_hcd_pipe_info
+					       *pipe)
+{
+	return pipe->dev_addr;
+}
+
+static inline uint8_t dwc_otg_hcd_is_pipe_isoc(struct dwc_otg_hcd_pipe_info
+					       *pipe)
+{
+	return (pipe->pipe_type == UE_ISOCHRONOUS);
+}
+
+static inline uint8_t dwc_otg_hcd_is_pipe_int(struct dwc_otg_hcd_pipe_info
+					      *pipe)
+{
+	return (pipe->pipe_type == UE_INTERRUPT);
+}
+
+static inline uint8_t dwc_otg_hcd_is_pipe_bulk(struct dwc_otg_hcd_pipe_info
+					       *pipe)
+{
+	return (pipe->pipe_type == UE_BULK);
+}
+
+static inline uint8_t dwc_otg_hcd_is_pipe_control(struct dwc_otg_hcd_pipe_info
+						  *pipe)
+{
+	return (pipe->pipe_type == UE_CONTROL);
+}
+
+static inline uint8_t dwc_otg_hcd_is_pipe_in(struct dwc_otg_hcd_pipe_info *pipe)
+{
+	return (pipe->pipe_dir == UE_DIR_IN);
+}
+
+static inline uint8_t dwc_otg_hcd_is_pipe_out(struct dwc_otg_hcd_pipe_info
+					      *pipe)
+{
+	return (!dwc_otg_hcd_is_pipe_in(pipe));
+}
+
+static inline void dwc_otg_hcd_fill_pipe(struct dwc_otg_hcd_pipe_info *pipe,
+					 uint8_t devaddr, uint8_t ep_num,
+					 uint8_t pipe_type, uint8_t pipe_dir,
+					 uint16_t mps)
+{
+	pipe->dev_addr = devaddr;
+	pipe->ep_num = ep_num;
+	pipe->pipe_type = pipe_type;
+	pipe->pipe_dir = pipe_dir;
+	pipe->mps = mps;
+}
+
+/**
+ * Phases for control transfers.
+ */
+typedef enum dwc_otg_control_phase {
+	DWC_OTG_CONTROL_SETUP,
+	DWC_OTG_CONTROL_DATA,
+	DWC_OTG_CONTROL_STATUS
+} dwc_otg_control_phase_e;
+
+/** Transaction types. */
+typedef enum dwc_otg_transaction_type {
+	DWC_OTG_TRANSACTION_NONE,
+	DWC_OTG_TRANSACTION_PERIODIC,
+	DWC_OTG_TRANSACTION_NON_PERIODIC,
+	DWC_OTG_TRANSACTION_ALL
+} dwc_otg_transaction_type_e;
+
+struct dwc_otg_qh;
+
+/**
+ * A Queue Transfer Descriptor (QTD) holds the state of a bulk, control,
+ * interrupt, or isochronous transfer. A single QTD is created for each URB
+ * (of one of these types) submitted to the HCD. The transfer associated with
+ * a QTD may require one or multiple transactions.
+ *
+ * A QTD is linked to a Queue Head, which is entered in either the
+ * non-periodic or periodic schedule for execution. When a QTD is chosen for
+ * execution, some or all of its transactions may be executed. After
+ * execution, the state of the QTD is updated. The QTD may be retired if all
+ * its transactions are complete or if an error occurred. Otherwise, it
+ * remains in the schedule so more transactions can be executed later.
+ */
+typedef struct dwc_otg_qtd {
+	/**
+	 * Determines the PID of the next data packet for the data phase of
+	 * control transfers. Ignored for other transfer types.<br>
+	 * One of the following values:
+	 *	- DWC_OTG_HC_PID_DATA0
+	 *	- DWC_OTG_HC_PID_DATA1
+	 */
+	uint8_t data_toggle;
+
+	/** Current phase for control transfers (Setup, Data, or Status). */
+	dwc_otg_control_phase_e control_phase;
+
+	/** Keep track of the current split type
+	 * for FS/LS endpoints on a HS Hub */
+	uint8_t complete_split;
+
+	/** How many bytes transferred during SSPLIT OUT */
+	uint32_t ssplit_out_xfer_count;
+
+	/**
+	 * Holds the number of bus errors that have occurred for a transaction
+	 * within this transfer.
+	 */
+	uint8_t error_count;
+
+	/**
+	 * Index of the next frame descriptor for an isochronous transfer. A
+	 * frame descriptor describes the buffer position and length of the
+	 * data to be transferred in the next scheduled (micro)frame of an
+	 * isochronous transfer. It also holds status for that transaction.
+	 * The frame index starts at 0.
+	 */
+	uint16_t isoc_frame_index;
+
+	/** Position of the ISOC split on full/low speed */
+	uint8_t isoc_split_pos;
+
+	/** Position of the ISOC split in the buffer for the current frame */
+	uint16_t isoc_split_offset;
+
+	/** URB for this transfer */
+	struct dwc_otg_hcd_urb *urb;
+
+	struct dwc_otg_qh *qh;
+
+	/** This list of QTDs */
+	 DWC_CIRCLEQ_ENTRY(dwc_otg_qtd) qtd_list_entry;
+
+	/** Indicates if this QTD is currently processed by HW. */
+	uint8_t in_process;
+
+	/** Number of DMA descriptors for this QTD */
+	uint8_t n_desc;
+
+	/** 
+	 * Last activated frame(packet) index. 
+	 * Used in Descriptor DMA mode only.
+	 */
+	uint16_t isoc_frame_index_last;
+
+} dwc_otg_qtd_t;
+
+DWC_CIRCLEQ_HEAD(dwc_otg_qtd_list, dwc_otg_qtd);
+
+/**
+ * A Queue Head (QH) holds the static characteristics of an endpoint and
+ * maintains a list of transfers (QTDs) for that endpoint. A QH structure may
+ * be entered in either the non-periodic or periodic schedule.
+ */
+typedef struct dwc_otg_qh {
+	/**
+	 * Endpoint type.
+	 * One of the following values:
+	 *	- UE_CONTROL
+	 *	- UE_BULK
+	 *	- UE_INTERRUPT
+	 *	- UE_ISOCHRONOUS
+	 */
+	uint8_t ep_type;
+	uint8_t ep_is_in;
+
+	/** wMaxPacketSize Field of Endpoint Descriptor. */
+	uint16_t maxp;
+
+	/**
+	 * Device speed.
+	 * One of the following values:
+	 *	- DWC_OTG_EP_SPEED_LOW
+	 *	- DWC_OTG_EP_SPEED_FULL
+	 *	- DWC_OTG_EP_SPEED_HIGH
+	 */
+	uint8_t dev_speed;
+
+	/**
+	 * Determines the PID of the next data packet for non-control
+	 * transfers. Ignored for control transfers.<br>
+	 * One of the following values:
+	 *	- DWC_OTG_HC_PID_DATA0
+	 *	- DWC_OTG_HC_PID_DATA1
+	 */
+	uint8_t data_toggle;
+
+	/** Ping state if 1. */
+	uint8_t ping_state;
+
+	/**
+	 * List of QTDs for this QH.
+	 */
+	struct dwc_otg_qtd_list qtd_list;
+
+	/** Host channel currently processing transfers for this QH. */
+	struct dwc_hc *channel;
+
+	/** Full/low speed endpoint on high-speed hub requires split. */
+	uint8_t do_split;
+
+	/** @name Periodic schedule information */
+	/** @{ */
+
+	/** Bandwidth in microseconds per (micro)frame. */
+	uint16_t usecs;
+
+	/** Interval between transfers in (micro)frames. */
+	uint16_t interval;
+
+	/**
+	 * (micro)frame to initialize a periodic transfer. The transfer
+	 * executes in the following (micro)frame.
+	 */
+	uint16_t sched_frame;
+
+	/** (micro)frame at which last start split was initialized. */
+	uint16_t start_split_frame;
+
+	/** @} */
+
+	/** 
+	 * Used instead of original buffer if 
+	 * it(physical address) is not dword-aligned.
+	 */
+	uint8_t *dw_align_buf;
+	dwc_dma_t dw_align_buf_dma;
+
+	/** Entry for QH in either the periodic or non-periodic schedule. */
+	dwc_list_link_t qh_list_entry;
+
+	/** @name Descriptor DMA support */
+	/** @{ */
+
+	/** Descriptor List. */
+	dwc_otg_host_dma_desc_t *desc_list;
+
+	/** Descriptor List physical address. */
+	dwc_dma_t desc_list_dma;
+
+	/** 
+	 * Xfer Bytes array.
+	 * Each element corresponds to a descriptor and indicates 
+	 * original XferSize size value for the descriptor.
+	 */
+	uint32_t *n_bytes;
+
+	/** Actual number of transfer descriptors in a list. */
+	uint16_t ntd;
+
+	/** First activated isochronous transfer descriptor index. */
+	uint8_t td_first;
+	/** Last activated isochronous transfer descriptor index. */
+	uint8_t td_last;
+
+	/** @} */
+
+} dwc_otg_qh_t;
+
+DWC_CIRCLEQ_HEAD(hc_list, dwc_hc);
+
+/**
+ * This structure holds the state of the HCD, including the non-periodic and
+ * periodic schedules.
+ */
+struct dwc_otg_hcd {
+	/** The DWC otg device pointer */
+	struct dwc_otg_device *otg_dev;
+	/** DWC OTG Core Interface Layer */
+	dwc_otg_core_if_t *core_if;
+
+	/** Function HCD driver callbacks */
+	struct dwc_otg_hcd_function_ops *fops;
+
+	/** Internal DWC HCD Flags */
+	volatile union dwc_otg_hcd_internal_flags {
+		uint32_t d32;
+		struct {
+			unsigned port_connect_status_change:1;
+			unsigned port_connect_status:1;
+			unsigned port_reset_change:1;
+			unsigned port_enable_change:1;
+			unsigned port_suspend_change:1;
+			unsigned port_over_current_change:1;
+			unsigned port_l1_change:1;
+			unsigned reserved:26;
+		} b;
+	} flags;
+
+	/**
+	 * Inactive items in the non-periodic schedule. This is a list of
+	 * Queue Heads. Transfers associated with these Queue Heads are not
+	 * currently assigned to a host channel.
+	 */
+	dwc_list_link_t non_periodic_sched_inactive;
+
+	/**
+	 * Active items in the non-periodic schedule. This is a list of
+	 * Queue Heads. Transfers associated with these Queue Heads are
+	 * currently assigned to a host channel.
+	 */
+	dwc_list_link_t non_periodic_sched_active;
+
+	/**
+	 * Pointer to the next Queue Head to process in the active
+	 * non-periodic schedule.
+	 */
+	dwc_list_link_t *non_periodic_qh_ptr;
+
+	/**
+	 * Inactive items in the periodic schedule. This is a list of QHs for
+	 * periodic transfers that are _not_ scheduled for the next frame.
+	 * Each QH in the list has an interval counter that determines when it
+	 * needs to be scheduled for execution. This scheduling mechanism
+	 * allows only a simple calculation for periodic bandwidth used (i.e.
+	 * must assume that all periodic transfers may need to execute in the
+	 * same frame). However, it greatly simplifies scheduling and should
+	 * be sufficient for the vast majority of OTG hosts, which need to
+	 * connect to a small number of peripherals at one time.
+	 *
+	 * Items move from this list to periodic_sched_ready when the QH
+	 * interval counter is 0 at SOF.
+	 */
+	dwc_list_link_t periodic_sched_inactive;
+
+	/**
+	 * List of periodic QHs that are ready for execution in the next
+	 * frame, but have not yet been assigned to host channels.
+	 *
+	 * Items move from this list to periodic_sched_assigned as host
+	 * channels become available during the current frame.
+	 */
+	dwc_list_link_t periodic_sched_ready;
+
+	/**
+	 * List of periodic QHs to be executed in the next frame that are
+	 * assigned to host channels.
+	 *
+	 * Items move from this list to periodic_sched_queued as the
+	 * transactions for the QH are queued to the DWC_otg controller.
+	 */
+	dwc_list_link_t periodic_sched_assigned;
+
+	/**
+	 * List of periodic QHs that have been queued for execution.
+	 *
+	 * Items move from this list to either periodic_sched_inactive or
+	 * periodic_sched_ready when the channel associated with the transfer
+	 * is released. If the interval for the QH is 1, the item moves to
+	 * periodic_sched_ready because it must be rescheduled for the next
+	 * frame. Otherwise, the item moves to periodic_sched_inactive.
+	 */
+	dwc_list_link_t periodic_sched_queued;
+
+	/**
+	 * Total bandwidth claimed so far for periodic transfers. This value
+	 * is in microseconds per (micro)frame. The assumption is that all
+	 * periodic transfers may occur in the same (micro)frame.
+	 */
+	uint16_t periodic_usecs;
+
+	/**
+	 * Frame number read from the core at SOF. The value ranges from 0 to
+	 * DWC_HFNUM_MAX_FRNUM.
+	 */
+	uint16_t frame_number;
+
+	/**
+	 * Count of periodic QHs, if using several eps. For SOF enable/disable.
+	 */
+	uint16_t periodic_qh_count;
+
+	/**
+	 * Free host channels in the controller. This is a list of
+	 * dwc_hc_t items.
+	 */
+	struct hc_list free_hc_list;
+	/**
+	 * Number of host channels assigned to periodic transfers. Currently
+	 * assuming that there is a dedicated host channel for each periodic
+	 * transaction and at least one host channel available for
+	 * non-periodic transactions.
+	 */
+	int periodic_channels;
+
+	/**
+	 * Number of host channels assigned to non-periodic transfers.
+	 */
+	int non_periodic_channels;
+
+	/**
+	 * Array of pointers to the host channel descriptors. Allows accessing
+	 * a host channel descriptor given the host channel number. This is
+	 * useful in interrupt handlers.
+	 */
+	struct dwc_hc *hc_ptr_array[MAX_EPS_CHANNELS];
+
+	/**
+	 * Buffer to use for any data received during the status phase of a
+	 * control transfer. Normally no data is transferred during the status
+	 * phase. This buffer is used as a bit bucket.
+	 */
+	uint8_t *status_buf;
+
+	/**
+	 * DMA address for status_buf.
+	 */
+	dma_addr_t status_buf_dma;
+#define DWC_OTG_HCD_STATUS_BUF_SIZE 64
+
+	/**
+	 * Connection timer. An OTG host must display a message if the device
+	 * does not connect. Started when the VBus power is turned on via
+	 * sysfs attribute "buspower".
+	 */
+	dwc_timer_t *conn_timer;
+
+	/* Tasket to do a reset */
+	dwc_tasklet_t *reset_tasklet;
+
+	/*  */
+	dwc_spinlock_t *lock;
+
+	/**
+	 * Private data that could be used by OS wrapper.
+	 */
+	void *priv;
+
+	uint8_t otg_port;
+
+	/** Frame List */
+	uint32_t *frame_list;
+
+	/** Frame List DMA address */
+	dma_addr_t frame_list_dma;
+
+#ifdef DEBUG
+	uint32_t frrem_samples;
+	uint64_t frrem_accum;
+
+	uint32_t hfnum_7_samples_a;
+	uint64_t hfnum_7_frrem_accum_a;
+	uint32_t hfnum_0_samples_a;
+	uint64_t hfnum_0_frrem_accum_a;
+	uint32_t hfnum_other_samples_a;
+	uint64_t hfnum_other_frrem_accum_a;
+
+	uint32_t hfnum_7_samples_b;
+	uint64_t hfnum_7_frrem_accum_b;
+	uint32_t hfnum_0_samples_b;
+	uint64_t hfnum_0_frrem_accum_b;
+	uint32_t hfnum_other_samples_b;
+	uint64_t hfnum_other_frrem_accum_b;
+#endif
+};
+
+/** @name Transaction Execution Functions */
+/** @{ */
+extern dwc_otg_transaction_type_e dwc_otg_hcd_select_transactions(dwc_otg_hcd_t
+								  * hcd);
+extern void dwc_otg_hcd_queue_transactions(dwc_otg_hcd_t * hcd,
+					   dwc_otg_transaction_type_e tr_type);
+
+/** @} */
+
+/** @name Interrupt Handler Functions */
+/** @{ */
+extern int32_t dwc_otg_hcd_handle_intr(dwc_otg_hcd_t * dwc_otg_hcd);
+extern int32_t dwc_otg_hcd_handle_sof_intr(dwc_otg_hcd_t * dwc_otg_hcd);
+extern int32_t dwc_otg_hcd_handle_rx_status_q_level_intr(dwc_otg_hcd_t *
+							 dwc_otg_hcd);
+extern int32_t dwc_otg_hcd_handle_np_tx_fifo_empty_intr(dwc_otg_hcd_t *
+							dwc_otg_hcd);
+extern int32_t dwc_otg_hcd_handle_perio_tx_fifo_empty_intr(dwc_otg_hcd_t *
+							   dwc_otg_hcd);
+extern int32_t dwc_otg_hcd_handle_incomplete_periodic_intr(dwc_otg_hcd_t *
+							   dwc_otg_hcd);
+extern int32_t dwc_otg_hcd_handle_port_intr(dwc_otg_hcd_t * dwc_otg_hcd);
+extern int32_t dwc_otg_hcd_handle_conn_id_status_change_intr(dwc_otg_hcd_t *
+							     dwc_otg_hcd);
+extern int32_t dwc_otg_hcd_handle_disconnect_intr(dwc_otg_hcd_t * dwc_otg_hcd);
+extern int32_t dwc_otg_hcd_handle_hc_intr(dwc_otg_hcd_t * dwc_otg_hcd);
+extern int32_t dwc_otg_hcd_handle_hc_n_intr(dwc_otg_hcd_t * dwc_otg_hcd,
+					    uint32_t num);
+extern int32_t dwc_otg_hcd_handle_session_req_intr(dwc_otg_hcd_t * dwc_otg_hcd);
+extern int32_t dwc_otg_hcd_handle_wakeup_detected_intr(dwc_otg_hcd_t *
+						       dwc_otg_hcd);
+/** @} */
+
+/** @name Schedule Queue Functions */
+/** @{ */
+
+/* Implemented in dwc_otg_hcd_queue.c */
+extern dwc_otg_qh_t *dwc_otg_hcd_qh_create(dwc_otg_hcd_t * hcd,
+					   dwc_otg_hcd_urb_t * urb, int atomic_alloc);
+extern void dwc_otg_hcd_qh_free(dwc_otg_hcd_t * hcd, dwc_otg_qh_t * qh);
+extern int dwc_otg_hcd_qh_add(dwc_otg_hcd_t * hcd, dwc_otg_qh_t * qh);
+extern void dwc_otg_hcd_qh_remove(dwc_otg_hcd_t * hcd, dwc_otg_qh_t * qh);
+extern void dwc_otg_hcd_qh_deactivate(dwc_otg_hcd_t * hcd, dwc_otg_qh_t * qh,
+				      int sched_csplit);
+
+/** Remove and free a QH */
+static inline void dwc_otg_hcd_qh_remove_and_free(dwc_otg_hcd_t * hcd,
+						  dwc_otg_qh_t * qh)
+{
+	dwc_irqflags_t flags;
+	DWC_SPINLOCK_IRQSAVE(hcd->lock, &flags);
+	dwc_otg_hcd_qh_remove(hcd, qh);
+	DWC_SPINUNLOCK_IRQRESTORE(hcd->lock, flags);
+	dwc_otg_hcd_qh_free(hcd, qh);
+}
+
+/** Allocates memory for a QH structure.
+ * @return Returns the memory allocate or NULL on error. */
+static inline dwc_otg_qh_t *dwc_otg_hcd_qh_alloc(int atomic_alloc)
+{
+	if (atomic_alloc)
+		return (dwc_otg_qh_t *) DWC_ALLOC_ATOMIC(sizeof(dwc_otg_qh_t));
+	else
+		return (dwc_otg_qh_t *) DWC_ALLOC(sizeof(dwc_otg_qh_t));
+}
+
+extern dwc_otg_qtd_t *dwc_otg_hcd_qtd_create(dwc_otg_hcd_urb_t * urb,
+					     int atomic_alloc);
+extern void dwc_otg_hcd_qtd_init(dwc_otg_qtd_t * qtd, dwc_otg_hcd_urb_t * urb);
+extern int dwc_otg_hcd_qtd_add(dwc_otg_qtd_t * qtd, dwc_otg_hcd_t * dwc_otg_hcd,
+			       dwc_otg_qh_t ** qh, int atomic_alloc);
+
+/** Allocates memory for a QTD structure.
+ * @return Returns the memory allocate or NULL on error. */
+static inline dwc_otg_qtd_t *dwc_otg_hcd_qtd_alloc(int atomic_alloc)
+{
+	if (atomic_alloc)
+		return (dwc_otg_qtd_t *) DWC_ALLOC_ATOMIC(sizeof(dwc_otg_qtd_t));
+	else
+		return (dwc_otg_qtd_t *) DWC_ALLOC(sizeof(dwc_otg_qtd_t));
+}
+
+/** Frees the memory for a QTD structure.  QTD should already be removed from
+ * list.
+ * @param qtd QTD to free.*/
+static inline void dwc_otg_hcd_qtd_free(dwc_otg_qtd_t * qtd)
+{
+	DWC_FREE(qtd);
+}
+
+/** Removes a QTD from list.
+ * @param hcd HCD instance.
+ * @param qtd QTD to remove from list.
+ * @param qh QTD belongs to.
+ */
+static inline void dwc_otg_hcd_qtd_remove(dwc_otg_hcd_t * hcd,
+					  dwc_otg_qtd_t * qtd,
+					  dwc_otg_qh_t * qh)
+{
+	DWC_CIRCLEQ_REMOVE(&qh->qtd_list, qtd, qtd_list_entry);
+}
+
+/** Remove and free a QTD 
+  * Need to disable IRQ and hold hcd lock while calling this function out of 
+  * interrupt servicing chain */
+static inline void dwc_otg_hcd_qtd_remove_and_free(dwc_otg_hcd_t * hcd,
+						   dwc_otg_qtd_t * qtd,
+						   dwc_otg_qh_t * qh)
+{
+	dwc_otg_hcd_qtd_remove(hcd, qtd, qh);
+	dwc_otg_hcd_qtd_free(qtd);
+}
+
+/** @} */
+
+/** @name Descriptor DMA Supporting Functions */
+/** @{ */
+
+extern void dwc_otg_hcd_start_xfer_ddma(dwc_otg_hcd_t * hcd, dwc_otg_qh_t * qh);
+extern void dwc_otg_hcd_complete_xfer_ddma(dwc_otg_hcd_t * hcd,
+					   dwc_hc_t * hc,
+					   dwc_otg_hc_regs_t * hc_regs,
+					   dwc_otg_halt_status_e halt_status);
+
+extern int dwc_otg_hcd_qh_init_ddma(dwc_otg_hcd_t * hcd, dwc_otg_qh_t * qh);
+extern void dwc_otg_hcd_qh_free_ddma(dwc_otg_hcd_t * hcd, dwc_otg_qh_t * qh);
+
+/** @} */
+
+/** @name Internal Functions */
+/** @{ */
+dwc_otg_qh_t *dwc_urb_to_qh(dwc_otg_hcd_urb_t * urb);
+/** @} */
+
+#ifdef CONFIG_USB_DWC_OTG_LPM
+extern int dwc_otg_hcd_get_hc_for_lpm_tran(dwc_otg_hcd_t * hcd,
+					   uint8_t devaddr);
+extern void dwc_otg_hcd_free_hc_from_lpm(dwc_otg_hcd_t * hcd);
+#endif
+
+/** Gets the QH that contains the list_head */
+#define dwc_list_to_qh(_list_head_ptr_) container_of(_list_head_ptr_, dwc_otg_qh_t, qh_list_entry)
+
+/** Gets the QTD that contains the list_head */
+#define dwc_list_to_qtd(_list_head_ptr_) container_of(_list_head_ptr_, dwc_otg_qtd_t, qtd_list_entry)
+
+/** Check if QH is non-periodic  */
+#define dwc_qh_is_non_per(_qh_ptr_) ((_qh_ptr_->ep_type == UE_BULK) || \
+				     (_qh_ptr_->ep_type == UE_CONTROL))
+
+/** High bandwidth multiplier as encoded in highspeed endpoint descriptors */
+#define dwc_hb_mult(wMaxPacketSize) (1 + (((wMaxPacketSize) >> 11) & 0x03))
+
+/** Packet size for any kind of endpoint descriptor */
+#define dwc_max_packet(wMaxPacketSize) ((wMaxPacketSize) & 0x07ff)
+
+/**
+ * Returns true if _frame1 is less than or equal to _frame2. The comparison is
+ * done modulo DWC_HFNUM_MAX_FRNUM. This accounts for the rollover of the
+ * frame number when the max frame number is reached.
+ */
+static inline int dwc_frame_num_le(uint16_t frame1, uint16_t frame2)
+{
+	return ((frame2 - frame1) & DWC_HFNUM_MAX_FRNUM) <=
+	    (DWC_HFNUM_MAX_FRNUM >> 1);
+}
+
+/**
+ * Returns true if _frame1 is greater than _frame2. The comparison is done
+ * modulo DWC_HFNUM_MAX_FRNUM. This accounts for the rollover of the frame
+ * number when the max frame number is reached.
+ */
+static inline int dwc_frame_num_gt(uint16_t frame1, uint16_t frame2)
+{
+	return (frame1 != frame2) &&
+	    (((frame1 - frame2) & DWC_HFNUM_MAX_FRNUM) <
+	     (DWC_HFNUM_MAX_FRNUM >> 1));
+}
+
+/**
+ * Increments _frame by the amount specified by _inc. The addition is done
+ * modulo DWC_HFNUM_MAX_FRNUM. Returns the incremented value.
+ */
+static inline uint16_t dwc_frame_num_inc(uint16_t frame, uint16_t inc)
+{
+	return (frame + inc) & DWC_HFNUM_MAX_FRNUM;
+}
+
+static inline uint16_t dwc_full_frame_num(uint16_t frame)
+{
+	return (frame & DWC_HFNUM_MAX_FRNUM) >> 3;
+}
+
+static inline uint16_t dwc_micro_frame_num(uint16_t frame)
+{
+	return frame & 0x7;
+}
+
+void dwc_otg_hcd_save_data_toggle(dwc_hc_t * hc,
+				  dwc_otg_hc_regs_t * hc_regs,
+				  dwc_otg_qtd_t * qtd);
+
+#ifdef DEBUG
+/**
+ * Macro to sample the remaining PHY clocks left in the current frame. This
+ * may be used during debugging to determine the average time it takes to
+ * execute sections of code. There are two possible sample points, "a" and
+ * "b", so the _letter argument must be one of these values.
+ *
+ * To dump the average sample times, read the "hcd_frrem" sysfs attribute. For
+ * example, "cat /sys/devices/lm0/hcd_frrem".
+ */
+#define dwc_sample_frrem(_hcd, _qh, _letter) \
+{ \
+	hfnum_data_t hfnum; \
+	dwc_otg_qtd_t *qtd; \
+	qtd = list_entry(_qh->qtd_list.next, dwc_otg_qtd_t, qtd_list_entry); \
+	if (usb_pipeint(qtd->urb->pipe) && _qh->start_split_frame != 0 && !qtd->complete_split) { \
+		hfnum.d32 = DWC_READ_REG32(&_hcd->core_if->host_if->host_global_regs->hfnum); \
+		switch (hfnum.b.frnum & 0x7) { \
+		case 7: \
+			_hcd->hfnum_7_samples_##_letter++; \
+			_hcd->hfnum_7_frrem_accum_##_letter += hfnum.b.frrem; \
+			break; \
+		case 0: \
+			_hcd->hfnum_0_samples_##_letter++; \
+			_hcd->hfnum_0_frrem_accum_##_letter += hfnum.b.frrem; \
+			break; \
+		default: \
+			_hcd->hfnum_other_samples_##_letter++; \
+			_hcd->hfnum_other_frrem_accum_##_letter += hfnum.b.frrem; \
+			break; \
+		} \
+	} \
+}
+#else
+#define dwc_sample_frrem(_hcd, _qh, _letter)
+#endif
+#endif
+#endif /* DWC_DEVICE_ONLY */
diff --git a/drivers/usb/dwc_otg/dwc_otg_hcd_ddma.c b/drivers/usb/dwc_otg/dwc_otg_hcd_ddma.c
new file mode 100644
index 0000000..6911e04
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_otg_hcd_ddma.c
@@ -0,0 +1,1122 @@
+/*==========================================================================
+ * $File: //dwh/usb_iip/dev/software/otg/linux/drivers/dwc_otg_hcd_ddma.c $
+ * $Revision: #10 $
+ * $Date: 2011/10/20 $
+ * $Change: 1869464 $
+ *
+ * Synopsys HS OTG Linux Software Driver and documentation (hereinafter,
+ * "Software") is an Unsupported proprietary work of Synopsys, Inc. unless
+ * otherwise expressly agreed to in writing between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product under
+ * any End User Software License Agreement or Agreement for Licensed Product
+ * with Synopsys or any supplement thereto. You are permitted to use and
+ * redistribute this Software in source and binary forms, with or without
+ * modification, provided that redistributions of source code must retain this
+ * notice. You may not view, use, disclose, copy or distribute this file or
+ * any information contained herein except pursuant to this license grant from
+ * Synopsys. If you do not agree with this notice, including the disclaimer
+ * below, then you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS" BASIS
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE HEREBY DISCLAIMED. IN NO EVENT SHALL SYNOPSYS BE LIABLE FOR ANY DIRECT,
+ * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================== */
+#ifndef DWC_DEVICE_ONLY
+
+/** @file
+ * This file contains Descriptor DMA support implementation for host mode.
+ */
+
+#include "dwc_otg_hcd.h"
+#include "dwc_otg_regs.h"
+
+static inline uint8_t frame_list_idx(uint16_t frame)
+{
+	return (frame & (MAX_FRLIST_EN_NUM - 1));
+}
+
+static inline uint16_t desclist_idx_inc(uint16_t idx, uint16_t inc, uint8_t speed)
+{
+	return (idx + inc) &
+	    (((speed ==
+	       DWC_OTG_EP_SPEED_HIGH) ? MAX_DMA_DESC_NUM_HS_ISOC :
+	      MAX_DMA_DESC_NUM_GENERIC) - 1);
+}
+
+static inline uint16_t desclist_idx_dec(uint16_t idx, uint16_t inc, uint8_t speed)
+{
+	return (idx - inc) &
+	    (((speed ==
+	       DWC_OTG_EP_SPEED_HIGH) ? MAX_DMA_DESC_NUM_HS_ISOC :
+	      MAX_DMA_DESC_NUM_GENERIC) - 1);
+}
+
+static inline uint16_t max_desc_num(dwc_otg_qh_t * qh)
+{
+	return (((qh->ep_type == UE_ISOCHRONOUS)
+		 && (qh->dev_speed == DWC_OTG_EP_SPEED_HIGH))
+		? MAX_DMA_DESC_NUM_HS_ISOC : MAX_DMA_DESC_NUM_GENERIC);
+}
+static inline uint16_t frame_incr_val(dwc_otg_qh_t * qh)
+{
+	return ((qh->dev_speed == DWC_OTG_EP_SPEED_HIGH)
+		? ((qh->interval + 8 - 1) / 8)
+		: qh->interval);
+}
+
+static int desc_list_alloc(dwc_otg_qh_t * qh)
+{
+	int retval = 0;
+
+	qh->desc_list = (dwc_otg_host_dma_desc_t *)
+	    DWC_DMA_ALLOC(sizeof(dwc_otg_host_dma_desc_t) * max_desc_num(qh),
+			  &qh->desc_list_dma);
+
+	if (!qh->desc_list) {
+		retval = -DWC_E_NO_MEMORY;
+		DWC_ERROR("%s: DMA descriptor list allocation failed\n", __func__);
+		
+	}
+
+	dwc_memset(qh->desc_list, 0x00,
+		   sizeof(dwc_otg_host_dma_desc_t) * max_desc_num(qh));
+
+	qh->n_bytes =
+	    (uint32_t *) DWC_ALLOC(sizeof(uint32_t) * max_desc_num(qh));
+
+	if (!qh->n_bytes) {
+		retval = -DWC_E_NO_MEMORY;
+		DWC_ERROR
+		    ("%s: Failed to allocate array for descriptors' size actual values\n",
+		     __func__);
+
+	}
+	return retval;
+
+}
+
+static void desc_list_free(dwc_otg_qh_t * qh)
+{
+	if (qh->desc_list) {
+		DWC_DMA_FREE(max_desc_num(qh), qh->desc_list,
+			     qh->desc_list_dma);
+		qh->desc_list = NULL;
+	}
+
+	if (qh->n_bytes) {
+		DWC_FREE(qh->n_bytes);
+		qh->n_bytes = NULL;
+	}
+}
+
+static int frame_list_alloc(dwc_otg_hcd_t * hcd)
+{
+	int retval = 0;
+	if (hcd->frame_list)
+		return 0;
+
+	hcd->frame_list = DWC_DMA_ALLOC(4 * MAX_FRLIST_EN_NUM,
+					&hcd->frame_list_dma);
+	if (!hcd->frame_list) {
+		retval = -DWC_E_NO_MEMORY;
+		DWC_ERROR("%s: Frame List allocation failed\n", __func__);
+	}
+
+	dwc_memset(hcd->frame_list, 0x00, 4 * MAX_FRLIST_EN_NUM);
+
+	return retval;
+}
+
+static void frame_list_free(dwc_otg_hcd_t * hcd)
+{
+	if (!hcd->frame_list)
+		return;
+	
+	DWC_DMA_FREE(4 * MAX_FRLIST_EN_NUM, hcd->frame_list, hcd->frame_list_dma);
+	hcd->frame_list = NULL;
+}
+
+static void per_sched_enable(dwc_otg_hcd_t * hcd, uint16_t fr_list_en)
+{
+
+	hcfg_data_t hcfg;
+
+	hcfg.d32 = DWC_READ_REG32(&hcd->core_if->host_if->host_global_regs->hcfg);
+
+	if (hcfg.b.perschedena) {
+		/* already enabled */
+		return;
+	}
+
+	DWC_WRITE_REG32(&hcd->core_if->host_if->host_global_regs->hflbaddr,
+			hcd->frame_list_dma);
+
+	switch (fr_list_en) {
+	case 64:
+		hcfg.b.frlisten = 3;
+		break;
+	case 32:
+		hcfg.b.frlisten = 2;
+		break;
+	case 16:
+		hcfg.b.frlisten = 1;
+		break;
+	case 8:
+		hcfg.b.frlisten = 0;
+		break;
+	default:
+		break;
+	}
+
+	hcfg.b.perschedena = 1;
+
+	DWC_DEBUGPL(DBG_HCD, "Enabling Periodic schedule\n");
+	DWC_WRITE_REG32(&hcd->core_if->host_if->host_global_regs->hcfg, hcfg.d32);
+
+}
+
+static void per_sched_disable(dwc_otg_hcd_t * hcd)
+{
+	hcfg_data_t hcfg;
+
+	hcfg.d32 = DWC_READ_REG32(&hcd->core_if->host_if->host_global_regs->hcfg);
+	
+	if (!hcfg.b.perschedena) {
+		/* already disabled */
+		return;
+	}
+	hcfg.b.perschedena = 0;
+
+	DWC_DEBUGPL(DBG_HCD, "Disabling Periodic schedule\n");
+	DWC_WRITE_REG32(&hcd->core_if->host_if->host_global_regs->hcfg, hcfg.d32);
+}
+
+/* 
+ * Activates/Deactivates FrameList entries for the channel 
+ * based on endpoint servicing period.
+ */
+void update_frame_list(dwc_otg_hcd_t * hcd, dwc_otg_qh_t * qh, uint8_t enable)
+{
+	uint16_t i, j, inc;
+	dwc_hc_t *hc = NULL;
+
+	if (!qh->channel) {
+		DWC_ERROR("qh->channel = %p", qh->channel);
+		return;
+	}
+
+	if (!hcd) {
+		DWC_ERROR("------hcd = %p", hcd);
+		return;
+	}
+
+	if (!hcd->frame_list) {
+		DWC_ERROR("-------hcd->frame_list = %p", hcd->frame_list);
+		return;
+	}
+
+	hc = qh->channel;
+	inc = frame_incr_val(qh);
+	if (qh->ep_type == UE_ISOCHRONOUS)
+		i = frame_list_idx(qh->sched_frame);
+	else
+		i = 0;
+
+	j = i;
+	do {
+		if (enable)
+			hcd->frame_list[j] |= (1 << hc->hc_num);
+		else
+			hcd->frame_list[j] &= ~(1 << hc->hc_num);
+		j = (j + inc) & (MAX_FRLIST_EN_NUM - 1);
+	}
+	while (j != i);
+	if (!enable)
+		return;
+	hc->schinfo = 0;
+	if (qh->channel->speed == DWC_OTG_EP_SPEED_HIGH) {
+		j = 1;
+		/* TODO - check this */
+		inc = (8 + qh->interval - 1) / qh->interval;
+		for (i = 0; i < inc; i++) {
+			hc->schinfo |= j;
+			j = j << qh->interval;
+		}
+	} else {
+		hc->schinfo = 0xff;
+	}
+}
+
+#if 1
+void dump_frame_list(dwc_otg_hcd_t * hcd)
+{
+	int i = 0;
+	DWC_PRINTF("--FRAME LIST (hex) --\n");
+	for (i = 0; i < MAX_FRLIST_EN_NUM; i++) {
+		DWC_PRINTF("%x\t", hcd->frame_list[i]);
+		if (!(i % 8) && i)
+			DWC_PRINTF("\n");
+	}
+	DWC_PRINTF("\n----\n");
+
+}
+#endif
+
+static void release_channel_ddma(dwc_otg_hcd_t * hcd, dwc_otg_qh_t * qh)
+{
+	dwc_hc_t *hc = qh->channel;
+	if (dwc_qh_is_non_per(qh))
+		hcd->non_periodic_channels--;
+	else
+		update_frame_list(hcd, qh, 0);
+
+	/* 
+	 * The condition is added to prevent double cleanup try in case of device
+	 * disconnect. See channel cleanup in dwc_otg_hcd_disconnect_cb().
+	 */
+	if (hc->qh) {
+		dwc_otg_hc_cleanup(hcd->core_if, hc);
+		DWC_CIRCLEQ_INSERT_TAIL(&hcd->free_hc_list, hc, hc_list_entry);
+		hc->qh = NULL;
+	}
+
+	qh->channel = NULL;
+	qh->ntd = 0;
+
+	if (qh->desc_list) {
+		dwc_memset(qh->desc_list, 0x00,
+			   sizeof(dwc_otg_host_dma_desc_t) * max_desc_num(qh));
+	}
+}
+
+/** 
+ * Initializes a QH structure's Descriptor DMA related members.
+ * Allocates memory for descriptor list.
+ * On first periodic QH, allocates memory for FrameList 
+ * and enables periodic scheduling.
+ *
+ * @param hcd The HCD state structure for the DWC OTG controller.
+ * @param qh The QH to init.
+ *
+ * @return 0 if successful, negative error code otherwise.
+ */
+int dwc_otg_hcd_qh_init_ddma(dwc_otg_hcd_t * hcd, dwc_otg_qh_t * qh)
+{
+	int retval = 0;
+
+	if (qh->do_split) {
+		DWC_ERROR("SPLIT Transfers are not supported in Descriptor DMA.\n");
+    		return -1;
+    	}
+
+	retval = desc_list_alloc(qh);
+
+	if ((retval == 0)
+	    && (qh->ep_type == UE_ISOCHRONOUS || qh->ep_type == UE_INTERRUPT)) {
+		if (!hcd->frame_list) {
+			retval = frame_list_alloc(hcd);
+			/* Enable periodic schedule on first periodic QH */
+			if (retval == 0)
+				per_sched_enable(hcd, MAX_FRLIST_EN_NUM);
+		}
+	}
+
+	qh->ntd = 0;
+
+	return retval;
+}
+
+/** 
+ * Frees descriptor list memory associated with the QH. 
+ * If QH is periodic and the last, frees FrameList memory 
+ * and disables periodic scheduling. 
+ *
+ * @param hcd The HCD state structure for the DWC OTG controller.
+ * @param qh The QH to init.
+ */
+void dwc_otg_hcd_qh_free_ddma(dwc_otg_hcd_t * hcd, dwc_otg_qh_t * qh)
+{
+	desc_list_free(qh);
+
+	/* 
+	 * Channel still assigned due to some reasons. 
+	 * Seen on Isoc URB dequeue. Channel halted but no subsequent
+	 * ChHalted interrupt to release the channel. Afterwards
+	 * when it comes here from endpoint disable routine
+	 * channel remains assigned.
+	 */
+	if (qh->channel)
+		release_channel_ddma(hcd, qh);
+
+	if ((qh->ep_type == UE_ISOCHRONOUS || qh->ep_type == UE_INTERRUPT)
+	    && !hcd->periodic_channels && hcd->frame_list) {
+
+		per_sched_disable(hcd);
+		frame_list_free(hcd);
+	}
+}
+
+static uint8_t frame_to_desc_idx(dwc_otg_qh_t * qh, uint16_t frame_idx)
+{
+	if (qh->dev_speed == DWC_OTG_EP_SPEED_HIGH) {
+		/* 
+		 * Descriptor set(8 descriptors) index
+		 * which is 8-aligned.
+		 */
+		return (frame_idx & ((MAX_DMA_DESC_NUM_HS_ISOC / 8) - 1)) * 8;
+	} else {
+		return (frame_idx & (MAX_DMA_DESC_NUM_GENERIC - 1));
+	}
+}
+
+/* 
+ * Determine starting frame for Isochronous transfer. 
+ * Few frames skipped to prevent race condition with HC. 
+ */
+static uint8_t calc_starting_frame(dwc_otg_hcd_t * hcd, dwc_otg_qh_t * qh,
+				   uint8_t * skip_frames)
+{
+	uint16_t frame = 0;
+	hcd->frame_number = dwc_otg_hcd_get_frame_number(hcd);
+	
+	/* sched_frame is always frame number(not uFrame) both in FS and HS !! */
+	
+	/* 
+	 * skip_frames is used to limit activated descriptors number
+	 * to avoid the situation when HC services the last activated
+	 * descriptor firstly.
+	 * Example for FS:
+	 * Current frame is 1, scheduled frame is 3. Since HC always fetches the descriptor
+	 * corresponding to curr_frame+1, the descriptor corresponding to frame 2
+	 * will be fetched. If the number of descriptors is max=64 (or greather) the
+	 * list will be fully programmed with Active descriptors and it is possible
+	 * case(rare) that the latest descriptor(considering rollback) corresponding
+	 * to frame 2 will be serviced first. HS case is more probable because, in fact,
+	 * up to 11 uframes(16 in the code) may be skipped.
+	 */
+	if (qh->dev_speed == DWC_OTG_EP_SPEED_HIGH) {
+		/* 
+		 * Consider uframe counter also, to start xfer asap.
+		 * If half of the frame elapsed skip 2 frames otherwise
+		 * just 1 frame. 
+		 * Starting descriptor index must be 8-aligned, so
+		 * if the current frame is near to complete the next one
+		 * is skipped as well.
+		 */
+
+		if (dwc_micro_frame_num(hcd->frame_number) >= 5) {
+			*skip_frames = 2 * 8;
+		 	frame = dwc_frame_num_inc(hcd->frame_number, *skip_frames);
+		} else {
+			*skip_frames = 1 * 8;
+			frame = dwc_frame_num_inc(hcd->frame_number, *skip_frames);
+		}
+
+		frame = dwc_full_frame_num(frame);
+	} else {
+		/* 
+		 * Two frames are skipped for FS - the current and the next.
+		 * But for descriptor programming, 1 frame(descriptor) is enough,
+		 * see example above.
+		 */
+		*skip_frames = 1;
+		frame = dwc_frame_num_inc(hcd->frame_number, 2);
+	}
+
+	return frame;
+}
+
+/* 
+ * Calculate initial descriptor index for isochronous transfer
+ * based on scheduled frame. 
+ */
+static uint8_t recalc_initial_desc_idx(dwc_otg_hcd_t * hcd, dwc_otg_qh_t * qh)
+{
+	uint16_t frame = 0, fr_idx, fr_idx_tmp;
+	uint8_t skip_frames = 0;
+	/* 
+	 * With current ISOC processing algorithm the channel is being
+	 * released when no more QTDs in the list(qh->ntd == 0).
+	 * Thus this function is called only when qh->ntd == 0 and qh->channel == 0. 
+	 *
+	 * So qh->channel != NULL branch is not used and just not removed from the
+	 * source file. It is required for another possible approach which is,
+	 * do not disable and release the channel when ISOC session completed, 
+	 * just move QH to inactive schedule until new QTD arrives. 
+	 * On new QTD, the QH moved back to 'ready' schedule,
+	 * starting frame and therefore starting desc_index are recalculated.
+	 * In this case channel is released only on ep_disable.
+	 */
+
+	/* Calculate starting descriptor index. For INTERRUPT endpoint it is always 0. */
+	if (qh->channel) {
+		frame = calc_starting_frame(hcd, qh, &skip_frames);
+		/* 
+		 * Calculate initial descriptor index based on FrameList current bitmap
+		 * and servicing period.
+		 */
+		fr_idx_tmp = frame_list_idx(frame);
+		fr_idx =
+		    (MAX_FRLIST_EN_NUM + frame_list_idx(qh->sched_frame) -
+		     fr_idx_tmp)
+		    % frame_incr_val(qh);
+		fr_idx = (fr_idx + fr_idx_tmp) % MAX_FRLIST_EN_NUM;
+	} else {
+		qh->sched_frame = calc_starting_frame(hcd, qh, &skip_frames);
+		fr_idx = frame_list_idx(qh->sched_frame);
+	}
+
+	qh->td_first = qh->td_last = frame_to_desc_idx(qh, fr_idx);
+
+	return skip_frames;
+}
+
+#define	ISOC_URB_GIVEBACK_ASAP
+
+#define MAX_ISOC_XFER_SIZE_FS 1023
+#define MAX_ISOC_XFER_SIZE_HS 3072
+#define DESCNUM_THRESHOLD 4
+
+static void init_isoc_dma_desc(dwc_otg_hcd_t * hcd, dwc_otg_qh_t * qh,
+			       uint8_t skip_frames)
+{
+	struct dwc_otg_hcd_iso_packet_desc *frame_desc;
+	dwc_otg_qtd_t *qtd;
+	dwc_otg_host_dma_desc_t *dma_desc;
+	uint16_t idx, inc, n_desc, ntd_max, max_xfer_size;
+
+	idx = qh->td_last;
+	inc = qh->interval;
+	n_desc = 0;
+
+	ntd_max = (max_desc_num(qh) + qh->interval - 1) / qh->interval;
+	if (skip_frames && !qh->channel)
+		ntd_max = ntd_max - skip_frames / qh->interval;
+
+	max_xfer_size =
+	    (qh->dev_speed ==
+	     DWC_OTG_EP_SPEED_HIGH) ? MAX_ISOC_XFER_SIZE_HS :
+	    MAX_ISOC_XFER_SIZE_FS;
+
+	DWC_CIRCLEQ_FOREACH(qtd, &qh->qtd_list, qtd_list_entry) {
+		while ((qh->ntd < ntd_max)
+		       && (qtd->isoc_frame_index_last <
+			   qtd->urb->packet_count)) {
+
+			dma_desc = &qh->desc_list[idx];
+			dwc_memset(dma_desc, 0x00, sizeof(dwc_otg_host_dma_desc_t));
+
+			frame_desc = &qtd->urb->iso_descs[qtd->isoc_frame_index_last];
+
+			if (frame_desc->length > max_xfer_size)
+				qh->n_bytes[idx] = max_xfer_size;
+			else
+				qh->n_bytes[idx] = frame_desc->length;
+			dma_desc->status.b_isoc.n_bytes = qh->n_bytes[idx];
+			dma_desc->status.b_isoc.a = 1;
+			dma_desc->status.b_isoc.sts = 0;
+
+			dma_desc->buf = qtd->urb->dma + frame_desc->offset;
+
+			qh->ntd++;
+
+			qtd->isoc_frame_index_last++;
+
+#ifdef	ISOC_URB_GIVEBACK_ASAP
+			/* 
+			 * Set IOC for each descriptor corresponding to the 
+			 * last frame of the URB.
+			 */
+			if (qtd->isoc_frame_index_last ==
+			    qtd->urb->packet_count)
+				dma_desc->status.b_isoc.ioc = 1;
+
+#endif
+			idx = desclist_idx_inc(idx, inc, qh->dev_speed);
+			n_desc++;
+
+		}
+		qtd->in_process = 1;
+	}
+
+	qh->td_last = idx;
+
+#ifdef	ISOC_URB_GIVEBACK_ASAP
+	/* Set IOC for the last descriptor if descriptor list is full */
+	if (qh->ntd == ntd_max) {
+		idx = desclist_idx_dec(qh->td_last, inc, qh->dev_speed);
+		qh->desc_list[idx].status.b_isoc.ioc = 1;
+	}
+#else
+	/* 
+	 * Set IOC bit only for one descriptor. 
+	 * Always try to be ahead of HW processing,
+	 * i.e. on IOC generation driver activates next descriptors but
+	 * core continues to process descriptors followed the one with IOC set.
+	 */
+
+	if (n_desc > DESCNUM_THRESHOLD) {
+		/* 
+		 * Move IOC "up". Required even if there is only one QTD 
+		 * in the list, cause QTDs migth continue to be queued,
+		 * but during the activation it was only one queued.
+		 * Actually more than one QTD might be in the list if this function called 
+		 * from XferCompletion - QTDs was queued during HW processing of the previous
+		 * descriptor chunk.
+		 */
+		idx = dwc_desclist_idx_dec(idx, inc * ((qh->ntd + 1) / 2), qh->dev_speed);
+	} else {
+		/* 
+		 * Set the IOC for the latest descriptor
+		 * if either number of descriptor is not greather than threshold
+		 * or no more new descriptors activated.
+		 */
+		idx = dwc_desclist_idx_dec(qh->td_last, inc, qh->dev_speed);
+	}
+
+	qh->desc_list[idx].status.b_isoc.ioc = 1;
+#endif
+}
+
+static void init_non_isoc_dma_desc(dwc_otg_hcd_t * hcd, dwc_otg_qh_t * qh)
+{
+
+	dwc_hc_t *hc;
+	dwc_otg_host_dma_desc_t *dma_desc;
+	dwc_otg_qtd_t *qtd;
+	int num_packets, len, n_desc = 0;
+
+	hc = qh->channel;
+
+	/* 
+	 * Start with hc->xfer_buff initialized in 
+	 * assign_and_init_hc(), then if SG transfer consists of multiple URBs,
+	 * this pointer re-assigned to the buffer of the currently processed QTD.
+	 * For non-SG request there is always one QTD active.
+	 */
+
+	DWC_CIRCLEQ_FOREACH(qtd, &qh->qtd_list, qtd_list_entry) {
+
+		if (n_desc) {
+			/* SG request - more than 1 QTDs */
+	 		hc->xfer_buff = (uint8_t *)qtd->urb->dma + qtd->urb->actual_length;
+			hc->xfer_len = qtd->urb->length - qtd->urb->actual_length;
+		}
+
+		qtd->n_desc = 0;
+
+		do {
+			dma_desc = &qh->desc_list[n_desc];
+			len = hc->xfer_len;
+
+			if (len > MAX_DMA_DESC_SIZE)
+				len = MAX_DMA_DESC_SIZE - hc->max_packet + 1;
+
+			if (hc->ep_is_in) {
+				if (len > 0) {
+					num_packets = (len + hc->max_packet - 1) / hc->max_packet;
+				} else {
+					/* Need 1 packet for transfer length of 0. */
+					num_packets = 1;
+				}
+				/* Always program an integral # of max packets for IN transfers. */
+				len = num_packets * hc->max_packet;
+			}
+
+			dma_desc->status.b.n_bytes = len;
+
+			qh->n_bytes[n_desc] = len;
+
+			if ((qh->ep_type == UE_CONTROL)
+			    && (qtd->control_phase == DWC_OTG_CONTROL_SETUP))
+				dma_desc->status.b.sup = 1;	/* Setup Packet */
+
+			dma_desc->status.b.a = 1;	/* Active descriptor */
+			dma_desc->status.b.sts = 0;
+
+			dma_desc->buf =
+			    ((unsigned long)hc->xfer_buff & 0xffffffff);
+
+			/* 
+			 * Last descriptor(or single) of IN transfer 
+			 * with actual size less than MaxPacket.
+			 */
+			if (len > hc->xfer_len) {
+				hc->xfer_len = 0;
+			} else {
+				hc->xfer_buff += len;
+				hc->xfer_len -= len;
+			}
+
+			qtd->n_desc++;
+			n_desc++;
+		}
+		while ((hc->xfer_len > 0) && (n_desc != MAX_DMA_DESC_NUM_GENERIC));
+		
+
+		qtd->in_process = 1;
+
+		if (qh->ep_type == UE_CONTROL)
+			break;
+
+		if (n_desc == MAX_DMA_DESC_NUM_GENERIC)
+			break;
+	}
+
+	if (n_desc) {
+		/* Request Transfer Complete interrupt for the last descriptor */
+		qh->desc_list[n_desc - 1].status.b.ioc = 1;
+		/* End of List indicator */
+		qh->desc_list[n_desc - 1].status.b.eol = 1;
+
+		hc->ntd = n_desc;
+	}
+}
+
+/** 
+ * For Control and Bulk endpoints initializes descriptor list
+ * and starts the transfer.
+ *
+ * For Interrupt and Isochronous endpoints initializes descriptor list
+ * then updates FrameList, marking appropriate entries as active.
+ * In case of Isochronous, the starting descriptor index is calculated based
+ * on the scheduled frame, but only on the first transfer descriptor within a session.
+ * Then starts the transfer via enabling the channel. 
+ * For Isochronous endpoint the channel is not halted on XferComplete 
+ * interrupt so remains assigned to the endpoint(QH) until session is done.
+ *
+ * @param hcd The HCD state structure for the DWC OTG controller.
+ * @param qh The QH to init.
+ *
+ * @return 0 if successful, negative error code otherwise.
+ */
+void dwc_otg_hcd_start_xfer_ddma(dwc_otg_hcd_t * hcd, dwc_otg_qh_t * qh)
+{
+	/* Channel is already assigned */
+	dwc_hc_t *hc = qh->channel;
+	uint8_t skip_frames = 0;
+
+	switch (hc->ep_type) {
+	case DWC_OTG_EP_TYPE_CONTROL:
+	case DWC_OTG_EP_TYPE_BULK:
+		init_non_isoc_dma_desc(hcd, qh);
+
+		dwc_otg_hc_start_transfer_ddma(hcd->core_if, hc);
+		break;
+	case DWC_OTG_EP_TYPE_INTR:
+		init_non_isoc_dma_desc(hcd, qh);
+
+		update_frame_list(hcd, qh, 1);
+
+		dwc_otg_hc_start_transfer_ddma(hcd->core_if, hc);
+		break;
+	case DWC_OTG_EP_TYPE_ISOC:
+
+		if (!qh->ntd)
+			skip_frames = recalc_initial_desc_idx(hcd, qh);
+
+		init_isoc_dma_desc(hcd, qh, skip_frames);
+
+		if (!hc->xfer_started) {
+
+			update_frame_list(hcd, qh, 1);
+
+			/* 
+			 * Always set to max, instead of actual size.
+			 * Otherwise ntd will be changed with 
+			 * channel being enabled. Not recommended.
+			 *
+			 */
+			hc->ntd = max_desc_num(qh);
+			/* Enable channel only once for ISOC */
+			dwc_otg_hc_start_transfer_ddma(hcd->core_if, hc);
+		}
+
+		break;
+	default:
+
+		break;
+	}
+}
+
+static void complete_isoc_xfer_ddma(dwc_otg_hcd_t * hcd,
+				    dwc_hc_t * hc,
+				    dwc_otg_hc_regs_t * hc_regs,
+				    dwc_otg_halt_status_e halt_status)
+{
+	struct dwc_otg_hcd_iso_packet_desc *frame_desc;
+	dwc_otg_qtd_t *qtd, *qtd_tmp;
+	dwc_otg_qh_t *qh;
+	dwc_otg_host_dma_desc_t *dma_desc;
+	uint16_t idx, remain;
+	uint8_t urb_compl;
+
+	qh = hc->qh;
+	idx = qh->td_first;
+
+	if (hc->halt_status == DWC_OTG_HC_XFER_URB_DEQUEUE) {
+		DWC_CIRCLEQ_FOREACH_SAFE(qtd, qtd_tmp, &hc->qh->qtd_list, qtd_list_entry)
+		    qtd->in_process = 0;
+		return;
+	} else if ((halt_status == DWC_OTG_HC_XFER_AHB_ERR) ||
+		   (halt_status == DWC_OTG_HC_XFER_BABBLE_ERR)) {
+		/* 
+		 * Channel is halted in these error cases.
+		 * Considered as serious issues.
+		 * Complete all URBs marking all frames as failed, 
+		 * irrespective whether some of the descriptors(frames) succeeded or no.
+		 * Pass error code to completion routine as well, to
+		 * update urb->status, some of class drivers might use it to stop
+		 * queing transfer requests.
+		 */
+		int err = (halt_status == DWC_OTG_HC_XFER_AHB_ERR)
+		    ? (-DWC_E_IO)
+		    : (-DWC_E_OVERFLOW);
+						
+		DWC_CIRCLEQ_FOREACH_SAFE(qtd, qtd_tmp, &hc->qh->qtd_list, qtd_list_entry) {
+			for (idx = 0; idx < qtd->urb->packet_count; idx++) {
+				frame_desc = &qtd->urb->iso_descs[idx];
+				frame_desc->status = err;
+			}
+			hcd->fops->complete(hcd, qtd->urb->priv, qtd->urb, err);
+			dwc_otg_hcd_qtd_remove_and_free(hcd, qtd, qh);
+		}
+		return;
+	}
+
+	DWC_CIRCLEQ_FOREACH_SAFE(qtd, qtd_tmp, &hc->qh->qtd_list, qtd_list_entry) {
+
+		if (!qtd->in_process)
+			break;
+
+		urb_compl = 0;
+
+		do {
+
+			dma_desc = &qh->desc_list[idx];
+			
+			frame_desc = &qtd->urb->iso_descs[qtd->isoc_frame_index];
+			remain = hc->ep_is_in ? dma_desc->status.b_isoc.n_bytes : 0;
+
+			if (dma_desc->status.b_isoc.sts == DMA_DESC_STS_PKTERR) {
+				/* 
+				 * XactError or, unable to complete all the transactions 
+				 * in the scheduled micro-frame/frame, 
+				 * both indicated by DMA_DESC_STS_PKTERR.
+				 */
+				qtd->urb->error_count++;
+				frame_desc->actual_length = qh->n_bytes[idx] - remain;
+				frame_desc->status = -DWC_E_PROTOCOL;
+			} else {
+				/* Success */
+								
+				frame_desc->actual_length = qh->n_bytes[idx] - remain;
+				frame_desc->status = 0;
+			}
+
+			if (++qtd->isoc_frame_index == qtd->urb->packet_count) {
+				/*
+				 * urb->status is not used for isoc transfers here.
+				 * The individual frame_desc status are used instead.
+				 */
+
+				hcd->fops->complete(hcd, qtd->urb->priv, qtd->urb, 0);
+				dwc_otg_hcd_qtd_remove_and_free(hcd, qtd, qh);
+
+				/* 
+				 * This check is necessary because urb_dequeue can be called 
+				 * from urb complete callback(sound driver example).
+				 * All pending URBs are dequeued there, so no need for
+				 * further processing.
+				 */
+				if (hc->halt_status == DWC_OTG_HC_XFER_URB_DEQUEUE) {	
+					return;
+				}
+
+				urb_compl = 1;
+
+			}
+
+			qh->ntd--;
+
+			/* Stop if IOC requested descriptor reached */
+			if (dma_desc->status.b_isoc.ioc) {
+				idx = desclist_idx_inc(idx, qh->interval, hc->speed);	
+				goto stop_scan;
+			}
+
+			idx = desclist_idx_inc(idx, qh->interval, hc->speed);
+
+			if (urb_compl)
+				break;
+		}
+		while (idx != qh->td_first);
+	}
+stop_scan:
+	qh->td_first = idx;
+}
+
+uint8_t update_non_isoc_urb_state_ddma(dwc_otg_hcd_t * hcd,
+				       dwc_hc_t * hc,
+				       dwc_otg_qtd_t * qtd,
+				       dwc_otg_host_dma_desc_t * dma_desc,
+				       dwc_otg_halt_status_e halt_status,
+				       uint32_t n_bytes, uint8_t * xfer_done)
+{
+
+	uint16_t remain = hc->ep_is_in ? dma_desc->status.b.n_bytes : 0;
+	dwc_otg_hcd_urb_t *urb = qtd->urb;
+
+	if (halt_status == DWC_OTG_HC_XFER_AHB_ERR) {
+		urb->status = -DWC_E_IO;
+		return 1;
+	}
+	if (dma_desc->status.b.sts == DMA_DESC_STS_PKTERR) {
+		switch (halt_status) {
+		case DWC_OTG_HC_XFER_STALL:
+			urb->status = -DWC_E_PIPE;
+			break;
+		case DWC_OTG_HC_XFER_BABBLE_ERR:
+			urb->status = -DWC_E_OVERFLOW;
+			break;
+		case DWC_OTG_HC_XFER_XACT_ERR:
+			urb->status = -DWC_E_PROTOCOL;
+			break;
+		default:	
+			DWC_ERROR("%s: Unhandled descriptor error status (%d)\n", __func__,
+			  	  halt_status);
+			break;
+		}
+		return 1;
+	}
+
+	if (dma_desc->status.b.a == 1) {
+		DWC_DEBUGPL(DBG_HCDV,
+			    "Active descriptor encountered on channel %d\n",
+			    hc->hc_num);
+		return 0;
+	}
+
+	if (hc->ep_type == DWC_OTG_EP_TYPE_CONTROL) {
+		if (qtd->control_phase == DWC_OTG_CONTROL_DATA) {
+			urb->actual_length += n_bytes - remain;
+			if (remain || urb->actual_length == urb->length) {
+				/* 
+				 * For Control Data stage do not set urb->status=0 to prevent
+				 * URB callback. Set it when Status phase done. See below.
+				 */
+				*xfer_done = 1;
+			}
+
+		} else if (qtd->control_phase == DWC_OTG_CONTROL_STATUS) {
+			urb->status = 0;
+			*xfer_done = 1;
+		}
+		/* No handling for SETUP stage */
+	} else {
+		/* BULK and INTR */
+		urb->actual_length += n_bytes - remain;
+		if (remain || urb->actual_length == urb->length) {
+			urb->status = 0;
+			*xfer_done = 1;
+		}
+	}
+
+	return 0;
+}
+
+static void complete_non_isoc_xfer_ddma(dwc_otg_hcd_t * hcd,
+					dwc_hc_t * hc,
+					dwc_otg_hc_regs_t * hc_regs,
+					dwc_otg_halt_status_e halt_status)
+{
+	dwc_otg_hcd_urb_t *urb = NULL;
+	dwc_otg_qtd_t *qtd, *qtd_tmp;
+	dwc_otg_qh_t *qh;
+	dwc_otg_host_dma_desc_t *dma_desc;
+	uint32_t n_bytes, n_desc, i;
+	uint8_t failed = 0, xfer_done;
+
+	n_desc = 0;
+
+	qh = hc->qh;
+
+	if (hc->halt_status == DWC_OTG_HC_XFER_URB_DEQUEUE) {
+		DWC_CIRCLEQ_FOREACH_SAFE(qtd, qtd_tmp, &hc->qh->qtd_list, qtd_list_entry) {
+			qtd->in_process = 0;
+		}
+		return;
+	}
+
+	DWC_CIRCLEQ_FOREACH_SAFE(qtd, qtd_tmp, &qh->qtd_list, qtd_list_entry) {
+
+		urb = qtd->urb;
+
+		n_bytes = 0;
+		xfer_done = 0;
+
+		for (i = 0; i < qtd->n_desc; i++) {
+			dma_desc = &qh->desc_list[n_desc];
+
+			n_bytes = qh->n_bytes[n_desc];
+
+			failed =
+			    update_non_isoc_urb_state_ddma(hcd, hc, qtd,
+							   dma_desc,
+							   halt_status, n_bytes,
+							   &xfer_done);
+
+			if (failed
+			    || (xfer_done
+				&& (urb->status != -DWC_E_IN_PROGRESS))) {
+
+				hcd->fops->complete(hcd, urb->priv, urb,
+						    urb->status);
+				dwc_otg_hcd_qtd_remove_and_free(hcd, qtd, qh);
+
+				if (failed)
+					goto stop_scan;
+			} else if (qh->ep_type == UE_CONTROL) {
+				if (qtd->control_phase == DWC_OTG_CONTROL_SETUP) {
+					if (urb->length > 0) {
+						qtd->control_phase = DWC_OTG_CONTROL_DATA;
+					} else {
+						qtd->control_phase = DWC_OTG_CONTROL_STATUS;
+					}
+					DWC_DEBUGPL(DBG_HCDV, "  Control setup transaction done\n");
+				} else if (qtd->control_phase == DWC_OTG_CONTROL_DATA) {
+					if (xfer_done) {
+						qtd->control_phase = DWC_OTG_CONTROL_STATUS;
+						DWC_DEBUGPL(DBG_HCDV, "  Control data transfer done\n");
+					} else if (i + 1 == qtd->n_desc) {
+						/* 
+						 * Last descriptor for Control data stage which is
+						 * not completed yet.
+						 */
+						dwc_otg_hcd_save_data_toggle(hc, hc_regs, qtd);
+					}
+				}
+			}
+
+			n_desc++;
+		}
+
+	}
+
+stop_scan:
+
+	if (qh->ep_type != UE_CONTROL) {
+		/* 
+		 * Resetting the data toggle for bulk
+		 * and interrupt endpoints in case of stall. See handle_hc_stall_intr() 
+		 */
+		if (halt_status == DWC_OTG_HC_XFER_STALL)
+			qh->data_toggle = DWC_OTG_HC_PID_DATA0;
+		else
+			dwc_otg_hcd_save_data_toggle(hc, hc_regs, qtd);
+	}
+
+	if (halt_status == DWC_OTG_HC_XFER_COMPLETE) {
+		hcint_data_t hcint;
+		hcint.d32 = DWC_READ_REG32(&hc_regs->hcint);
+		if (hcint.b.nyet) {
+			/*
+			 * Got a NYET on the last transaction of the transfer. It
+			 * means that the endpoint should be in the PING state at the
+			 * beginning of the next transfer.
+			 */
+			qh->ping_state = 1;
+			clear_hc_int(hc_regs, nyet);
+		}
+
+	}
+
+}
+
+/**
+ * This function is called from interrupt handlers.
+ * Scans the descriptor list, updates URB's status and
+ * calls completion routine for the URB if it's done.
+ * Releases the channel to be used by other transfers.
+ * In case of Isochronous endpoint the channel is not halted until 
+ * the end of the session, i.e. QTD list is empty.
+ * If periodic channel released the FrameList is updated accordingly.
+ *
+ * Calls transaction selection routines to activate pending transfers.
+ *
+ * @param hcd The HCD state structure for the DWC OTG controller.
+ * @param hc Host channel, the transfer is completed on.
+ * @param hc_regs Host channel registers.
+ * @param halt_status Reason the channel is being halted, 
+ *		      or just XferComplete for isochronous transfer
+ */
+void dwc_otg_hcd_complete_xfer_ddma(dwc_otg_hcd_t * hcd,
+				    dwc_hc_t * hc,
+				    dwc_otg_hc_regs_t * hc_regs,
+				    dwc_otg_halt_status_e halt_status)
+{
+	uint8_t continue_isoc_xfer = 0;
+	dwc_otg_transaction_type_e tr_type;
+	dwc_otg_qh_t *qh = hc->qh;
+
+	if (hc->ep_type == DWC_OTG_EP_TYPE_ISOC) {
+
+		complete_isoc_xfer_ddma(hcd, hc, hc_regs, halt_status);
+
+		/* Release the channel if halted or session completed */
+		if (halt_status != DWC_OTG_HC_XFER_COMPLETE ||
+		    DWC_CIRCLEQ_EMPTY(&qh->qtd_list)) {
+
+			/* Halt the channel if session completed */
+			if (halt_status == DWC_OTG_HC_XFER_COMPLETE) {
+				dwc_otg_hc_halt(hcd->core_if, hc, halt_status);
+			}
+
+			release_channel_ddma(hcd, qh);
+			dwc_otg_hcd_qh_remove(hcd, qh);
+		} else {
+			/* Keep in assigned schedule to continue transfer */
+			DWC_LIST_MOVE_HEAD(&hcd->periodic_sched_assigned,
+					   &qh->qh_list_entry);
+			continue_isoc_xfer = 1;
+
+		}
+		/** @todo Consider the case when period exceeds FrameList size.
+		 *  Frame Rollover interrupt should be used. 
+		 */
+	} else {
+		/* Scan descriptor list to complete the URB(s), then release the channel */
+		complete_non_isoc_xfer_ddma(hcd, hc, hc_regs, halt_status);
+
+		release_channel_ddma(hcd, qh);
+		dwc_otg_hcd_qh_remove(hcd, qh);
+
+		if (!DWC_CIRCLEQ_EMPTY(&qh->qtd_list)) {
+			/* Add back to inactive non-periodic schedule on normal completion */
+			dwc_otg_hcd_qh_add(hcd, qh);
+		}
+
+	}
+	tr_type = dwc_otg_hcd_select_transactions(hcd);
+	if (tr_type != DWC_OTG_TRANSACTION_NONE || continue_isoc_xfer) {
+		if (continue_isoc_xfer) {
+			if (tr_type == DWC_OTG_TRANSACTION_NONE) {
+				tr_type = DWC_OTG_TRANSACTION_PERIODIC;
+			} else if (tr_type == DWC_OTG_TRANSACTION_NON_PERIODIC) {
+				tr_type = DWC_OTG_TRANSACTION_ALL;
+			}
+		}
+		dwc_otg_hcd_queue_transactions(hcd, tr_type);
+	}
+}
+
+#endif /* DWC_DEVICE_ONLY */
diff --git a/drivers/usb/dwc_otg/dwc_otg_hcd_if.h b/drivers/usb/dwc_otg/dwc_otg_hcd_if.h
new file mode 100644
index 0000000..b3dc806
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_otg_hcd_if.h
@@ -0,0 +1,412 @@
+/* ==========================================================================
+ * $File: //dwh/usb_iip/dev/software/otg/linux/drivers/dwc_otg_hcd_if.h $
+ * $Revision: #12 $
+ * $Date: 2011/10/26 $
+ * $Change: 1873028 $
+ *
+ * Synopsys HS OTG Linux Software Driver and documentation (hereinafter,
+ * "Software") is an Unsupported proprietary work of Synopsys, Inc. unless
+ * otherwise expressly agreed to in writing between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product under
+ * any End User Software License Agreement or Agreement for Licensed Product
+ * with Synopsys or any supplement thereto. You are permitted to use and
+ * redistribute this Software in source and binary forms, with or without
+ * modification, provided that redistributions of source code must retain this
+ * notice. You may not view, use, disclose, copy or distribute this file or
+ * any information contained herein except pursuant to this license grant from
+ * Synopsys. If you do not agree with this notice, including the disclaimer
+ * below, then you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS" BASIS
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE HEREBY DISCLAIMED. IN NO EVENT SHALL SYNOPSYS BE LIABLE FOR ANY DIRECT,
+ * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================== */
+#ifndef DWC_DEVICE_ONLY
+#ifndef __DWC_HCD_IF_H__
+#define __DWC_HCD_IF_H__
+
+#include "dwc_otg_core_if.h"
+
+/** @file
+ * This file defines DWC_OTG HCD Core API.
+ */
+
+struct dwc_otg_hcd;
+typedef struct dwc_otg_hcd dwc_otg_hcd_t;
+
+struct dwc_otg_hcd_urb;
+typedef struct dwc_otg_hcd_urb dwc_otg_hcd_urb_t;
+
+/** @name HCD Function Driver Callbacks */
+/** @{ */
+
+/** This function is called whenever core switches to host mode. */
+typedef int (*dwc_otg_hcd_start_cb_t) (dwc_otg_hcd_t * hcd);
+
+/** This function is called when device has been disconnected */
+typedef int (*dwc_otg_hcd_disconnect_cb_t) (dwc_otg_hcd_t * hcd);
+
+/** Wrapper provides this function to HCD to core, so it can get hub information to which device is connected */
+typedef int (*dwc_otg_hcd_hub_info_from_urb_cb_t) (dwc_otg_hcd_t * hcd,
+						   void *urb_handle,
+						   uint32_t * hub_addr,
+						   uint32_t * port_addr);
+/** Via this function HCD core gets device speed */
+typedef int (*dwc_otg_hcd_speed_from_urb_cb_t) (dwc_otg_hcd_t * hcd,
+						void *urb_handle);
+
+/** This function is called when urb is completed */
+typedef int (*dwc_otg_hcd_complete_urb_cb_t) (dwc_otg_hcd_t * hcd,
+					      void *urb_handle,
+					      dwc_otg_hcd_urb_t * dwc_otg_urb,
+					      int32_t status);
+
+/** Via this function HCD core gets b_hnp_enable parameter */
+typedef int (*dwc_otg_hcd_get_b_hnp_enable) (dwc_otg_hcd_t * hcd);
+
+struct dwc_otg_hcd_function_ops {
+	dwc_otg_hcd_start_cb_t start;
+	dwc_otg_hcd_disconnect_cb_t disconnect;
+	dwc_otg_hcd_hub_info_from_urb_cb_t hub_info;
+	dwc_otg_hcd_speed_from_urb_cb_t speed;
+	dwc_otg_hcd_complete_urb_cb_t complete;
+	dwc_otg_hcd_get_b_hnp_enable get_b_hnp_enable;
+};
+/** @} */
+
+/** @name HCD Core API */
+/** @{ */
+/** This function allocates dwc_otg_hcd structure and returns pointer on it. */
+extern dwc_otg_hcd_t *dwc_otg_hcd_alloc_hcd(void);
+
+/** This function should be called to initiate HCD Core.
+ *
+ * @param hcd The HCD
+ * @param core_if The DWC_OTG Core
+ *
+ * Returns -DWC_E_NO_MEMORY if no enough memory.
+ * Returns 0 on success  
+ */
+extern int dwc_otg_hcd_init(dwc_otg_hcd_t * hcd, dwc_otg_core_if_t * core_if);
+
+/** Frees HCD
+ *
+ * @param hcd The HCD
+ */
+extern void dwc_otg_hcd_remove(dwc_otg_hcd_t * hcd);
+
+/** This function should be called on every hardware interrupt.
+ *
+ * @param dwc_otg_hcd The HCD
+ *
+ * Returns non zero if interrupt is handled
+ * Return 0 if interrupt is not handled
+ */
+extern int32_t dwc_otg_hcd_handle_intr(dwc_otg_hcd_t * dwc_otg_hcd);
+
+/**
+ * Returns private data set by
+ * dwc_otg_hcd_set_priv_data function.
+ *
+ * @param hcd The HCD
+ */
+extern void *dwc_otg_hcd_get_priv_data(dwc_otg_hcd_t * hcd);
+
+/**
+ * Set private data.
+ *
+ * @param hcd The HCD
+ * @param priv_data pointer to be stored in private data
+ */
+extern void dwc_otg_hcd_set_priv_data(dwc_otg_hcd_t * hcd, void *priv_data);
+
+/**
+ * This function initializes the HCD Core.
+ *
+ * @param hcd The HCD
+ * @param fops The Function Driver Operations data structure containing pointers to all callbacks.
+ *
+ * Returns -DWC_E_NO_DEVICE if Core is currently is in device mode.
+ * Returns 0 on success
+ */
+extern int dwc_otg_hcd_start(dwc_otg_hcd_t * hcd,
+			     struct dwc_otg_hcd_function_ops *fops);
+
+/**
+ * Halts the DWC_otg host mode operations in a clean manner. USB transfers are
+ * stopped. 
+ *
+ * @param hcd The HCD
+ */
+extern void dwc_otg_hcd_stop(dwc_otg_hcd_t * hcd);
+
+/**
+ * Handles hub class-specific requests.
+ *
+ * @param dwc_otg_hcd The HCD
+ * @param typeReq Request Type
+ * @param wValue wValue from control request
+ * @param wIndex wIndex from control request
+ * @param buf data buffer 
+ * @param wLength data buffer length
+ *
+ * Returns -DWC_E_INVALID if invalid argument is passed
+ * Returns 0 on success
+ */
+extern int dwc_otg_hcd_hub_control(dwc_otg_hcd_t * dwc_otg_hcd,
+				   uint16_t typeReq, uint16_t wValue,
+				   uint16_t wIndex, uint8_t * buf,
+				   uint16_t wLength);
+
+/**
+ * Returns otg port number.
+ *
+ * @param hcd The HCD
+ */
+extern uint32_t dwc_otg_hcd_otg_port(dwc_otg_hcd_t * hcd);
+
+/**
+ * Returns OTG version - either 1.3 or 2.0.
+ *
+ * @param core_if The core_if structure pointer
+ */
+extern uint16_t dwc_otg_get_otg_version(dwc_otg_core_if_t * core_if);
+
+/**
+ * Returns 1 if currently core is acting as B host, and 0 otherwise.
+ *
+ * @param hcd The HCD
+ */
+extern uint32_t dwc_otg_hcd_is_b_host(dwc_otg_hcd_t * hcd);
+
+/**
+ * Returns current frame number.
+ *
+ * @param hcd The HCD
+ */
+extern int dwc_otg_hcd_get_frame_number(dwc_otg_hcd_t * hcd);
+
+/**
+ * Dumps hcd state.
+ *
+ * @param hcd The HCD
+ */
+extern void dwc_otg_hcd_dump_state(dwc_otg_hcd_t * hcd);
+
+/**
+ * Dump the average frame remaining at SOF. This can be used to
+ * determine average interrupt latency. Frame remaining is also shown for
+ * start transfer and two additional sample points.
+ * Currently this function is not implemented.
+ *
+ * @param hcd The HCD
+ */
+extern void dwc_otg_hcd_dump_frrem(dwc_otg_hcd_t * hcd);
+
+/**
+ * Sends LPM transaction to the local device.
+ *
+ * @param hcd The HCD
+ * @param devaddr Device Address
+ * @param hird Host initiated resume duration
+ * @param bRemoteWake Value of bRemoteWake field in LPM transaction
+ *
+ * Returns negative value if sending LPM transaction was not succeeded.
+ * Returns 0 on success.
+ */
+extern int dwc_otg_hcd_send_lpm(dwc_otg_hcd_t * hcd, uint8_t devaddr,
+				uint8_t hird, uint8_t bRemoteWake);
+
+/* URB interface */
+
+/**
+ * Allocates memory for dwc_otg_hcd_urb structure.
+ * Allocated memory should be freed by call of DWC_FREE.
+ *
+ * @param hcd The HCD
+ * @param iso_desc_count Count of ISOC descriptors
+ * @param atomic_alloc Specefies whether to perform atomic allocation.
+ */
+extern dwc_otg_hcd_urb_t *dwc_otg_hcd_urb_alloc(dwc_otg_hcd_t * hcd,
+						int iso_desc_count,
+						int atomic_alloc);
+
+/**
+ * Set pipe information in URB.
+ *
+ * @param hcd_urb DWC_OTG URB
+ * @param devaddr Device Address
+ * @param ep_num Endpoint Number
+ * @param ep_type Endpoint Type
+ * @param ep_dir Endpoint Direction
+ * @param mps Max Packet Size
+ */
+extern void dwc_otg_hcd_urb_set_pipeinfo(dwc_otg_hcd_urb_t * hcd_urb,
+					 uint8_t devaddr, uint8_t ep_num,
+					 uint8_t ep_type, uint8_t ep_dir,
+					 uint16_t mps);
+
+/* Transfer flags */
+#define URB_GIVEBACK_ASAP 0x1
+#define URB_SEND_ZERO_PACKET 0x2
+
+/**
+ * Sets dwc_otg_hcd_urb parameters.
+ *
+ * @param urb DWC_OTG URB allocated by dwc_otg_hcd_urb_alloc function.
+ * @param urb_handle Unique handle for request, this will be passed back
+ * to function driver in completion callback.
+ * @param buf The buffer for the data
+ * @param dma The DMA buffer for the data
+ * @param buflen Transfer length
+ * @param sp Buffer for setup data
+ * @param sp_dma DMA address of setup data buffer
+ * @param flags Transfer flags
+ * @param interval Polling interval for interrupt or isochronous transfers.
+ */
+extern void dwc_otg_hcd_urb_set_params(dwc_otg_hcd_urb_t * urb,
+				       void *urb_handle, void *buf,
+				       dwc_dma_t dma, uint32_t buflen, void *sp,
+				       dwc_dma_t sp_dma, uint32_t flags,
+				       uint16_t interval);
+
+/** Gets status from dwc_otg_hcd_urb
+ *
+ * @param dwc_otg_urb DWC_OTG URB
+ */
+extern uint32_t dwc_otg_hcd_urb_get_status(dwc_otg_hcd_urb_t * dwc_otg_urb);
+
+/** Gets actual length from dwc_otg_hcd_urb
+ *
+ * @param dwc_otg_urb DWC_OTG URB
+ */
+extern uint32_t dwc_otg_hcd_urb_get_actual_length(dwc_otg_hcd_urb_t *
+						  dwc_otg_urb);
+
+/** Gets error count from dwc_otg_hcd_urb. Only for ISOC URBs
+ *
+ * @param dwc_otg_urb DWC_OTG URB
+ */
+extern uint32_t dwc_otg_hcd_urb_get_error_count(dwc_otg_hcd_urb_t *
+						dwc_otg_urb);
+
+/** Set ISOC descriptor offset and length
+ *
+ * @param dwc_otg_urb DWC_OTG URB
+ * @param desc_num ISOC descriptor number
+ * @param offset Offset from beginig of buffer.
+ * @param length Transaction length
+ */
+extern void dwc_otg_hcd_urb_set_iso_desc_params(dwc_otg_hcd_urb_t * dwc_otg_urb,
+						int desc_num, uint32_t offset,
+						uint32_t length);
+
+/** Get status of ISOC descriptor, specified by desc_num
+ *
+ * @param dwc_otg_urb DWC_OTG URB
+ * @param desc_num ISOC descriptor number 
+ */
+extern uint32_t dwc_otg_hcd_urb_get_iso_desc_status(dwc_otg_hcd_urb_t *
+						    dwc_otg_urb, int desc_num);
+
+/** Get actual length of ISOC descriptor, specified by desc_num
+ *
+ * @param dwc_otg_urb DWC_OTG URB
+ * @param desc_num ISOC descriptor number
+ */
+extern uint32_t dwc_otg_hcd_urb_get_iso_desc_actual_length(dwc_otg_hcd_urb_t *
+							   dwc_otg_urb,
+							   int desc_num);
+
+/** Queue URB. After transfer is completes, the complete callback will be called with the URB status
+ *
+ * @param dwc_otg_hcd The HCD
+ * @param dwc_otg_urb DWC_OTG URB
+ * @param ep_handle Out parameter for returning endpoint handle
+ * @param atomic_alloc Flag to do atomic allocation if needed
+ *
+ * Returns -DWC_E_NO_DEVICE if no device is connected.
+ * Returns -DWC_E_NO_MEMORY if there is no enough memory.
+ * Returns 0 on success.
+ */
+extern int dwc_otg_hcd_urb_enqueue(dwc_otg_hcd_t * dwc_otg_hcd,
+				   dwc_otg_hcd_urb_t * dwc_otg_urb,
+				   void **ep_handle, int atomic_alloc);
+
+/** De-queue the specified URB
+ *
+ * @param dwc_otg_hcd The HCD
+ * @param dwc_otg_urb DWC_OTG URB
+ */
+extern int dwc_otg_hcd_urb_dequeue(dwc_otg_hcd_t * dwc_otg_hcd,
+				   dwc_otg_hcd_urb_t * dwc_otg_urb);
+
+/** Frees resources in the DWC_otg controller related to a given endpoint.
+ * Any URBs for the endpoint must already be dequeued.
+ *
+ * @param hcd The HCD
+ * @param ep_handle Endpoint handle, returned by dwc_otg_hcd_urb_enqueue function
+ * @param retry Number of retries if there are queued transfers.
+ *
+ * Returns -DWC_E_INVALID if invalid arguments are passed.
+ * Returns 0 on success
+ */
+extern int dwc_otg_hcd_endpoint_disable(dwc_otg_hcd_t * hcd, void *ep_handle,
+					int retry);
+
+/* Resets the data toggle in qh structure. This function can be called from
+ * usb_clear_halt routine.
+ *
+ * @param hcd The HCD
+ * @param ep_handle Endpoint handle, returned by dwc_otg_hcd_urb_enqueue function
+ *
+ * Returns -DWC_E_INVALID if invalid arguments are passed.
+ * Returns 0 on success
+ */
+extern int dwc_otg_hcd_endpoint_reset(dwc_otg_hcd_t * hcd, void *ep_handle);
+
+/** Returns 1 if status of specified port is changed and 0 otherwise.
+ *
+ * @param hcd The HCD
+ * @param port Port number
+ */
+extern int dwc_otg_hcd_is_status_changed(dwc_otg_hcd_t * hcd, int port);
+
+/** Call this function to check if bandwidth was allocated for specified endpoint.
+ * Only for ISOC and INTERRUPT endpoints.
+ *
+ * @param hcd The HCD
+ * @param ep_handle Endpoint handle
+ */
+extern int dwc_otg_hcd_is_bandwidth_allocated(dwc_otg_hcd_t * hcd,
+					      void *ep_handle);
+
+/** Call this function to check if bandwidth was freed for specified endpoint.
+ *
+ * @param hcd The HCD
+ * @param ep_handle Endpoint handle
+ */
+extern int dwc_otg_hcd_is_bandwidth_freed(dwc_otg_hcd_t * hcd, void *ep_handle);
+
+/** Returns bandwidth allocated for specified endpoint in microseconds.
+ * Only for ISOC and INTERRUPT endpoints.
+ *
+ * @param hcd The HCD
+ * @param ep_handle Endpoint handle
+ */
+extern uint8_t dwc_otg_hcd_get_ep_bandwidth(dwc_otg_hcd_t * hcd,
+					    void *ep_handle);
+
+/** @} */
+
+#endif /* __DWC_HCD_IF_H__ */
+#endif /* DWC_DEVICE_ONLY */
diff --git a/drivers/usb/dwc_otg/dwc_otg_hcd_intr.c b/drivers/usb/dwc_otg/dwc_otg_hcd_intr.c
new file mode 100644
index 0000000..d145fa0
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_otg_hcd_intr.c
@@ -0,0 +1,2095 @@
+/* ==========================================================================
+ * $File: //dwh/usb_iip/dev/software/otg/linux/drivers/dwc_otg_hcd_intr.c $
+ * $Revision: #89 $
+ * $Date: 2011/10/20 $
+ * $Change: 1869487 $
+ *
+ * Synopsys HS OTG Linux Software Driver and documentation (hereinafter,
+ * "Software") is an Unsupported proprietary work of Synopsys, Inc. unless
+ * otherwise expressly agreed to in writing between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product under
+ * any End User Software License Agreement or Agreement for Licensed Product
+ * with Synopsys or any supplement thereto. You are permitted to use and
+ * redistribute this Software in source and binary forms, with or without
+ * modification, provided that redistributions of source code must retain this
+ * notice. You may not view, use, disclose, copy or distribute this file or
+ * any information contained herein except pursuant to this license grant from
+ * Synopsys. If you do not agree with this notice, including the disclaimer
+ * below, then you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS" BASIS
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE HEREBY DISCLAIMED. IN NO EVENT SHALL SYNOPSYS BE LIABLE FOR ANY DIRECT,
+ * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================== */
+#ifndef DWC_DEVICE_ONLY
+
+#include "dwc_otg_hcd.h"
+#include "dwc_otg_regs.h"
+
+/** @file
+ * This file contains the implementation of the HCD Interrupt handlers.
+ */
+
+/** This function handles interrupts for the HCD. */
+int32_t dwc_otg_hcd_handle_intr(dwc_otg_hcd_t * dwc_otg_hcd)
+{
+	int retval = 0;
+
+	dwc_otg_core_if_t *core_if = dwc_otg_hcd->core_if;
+	gintsts_data_t gintsts;
+#ifdef DEBUG
+	dwc_otg_core_global_regs_t *global_regs = core_if->core_global_regs;
+#endif
+
+	/* Exit from ISR if core is hibernated */
+	if (core_if->hibernation_suspend == 1) {
+		return retval;
+	}
+	DWC_SPINLOCK(dwc_otg_hcd->lock);
+	/* Check if HOST Mode */
+	if (dwc_otg_is_host_mode(core_if)) {
+		gintsts.d32 = dwc_otg_read_core_intr(core_if);
+		if (!gintsts.d32) {
+			DWC_SPINUNLOCK(dwc_otg_hcd->lock);
+			return 0;
+		}
+#ifdef DEBUG
+		/* Don't print debug message in the interrupt handler on SOF */
+#ifndef DEBUG_SOF
+		if (gintsts.d32 != DWC_SOF_INTR_MASK)
+#endif
+			DWC_DEBUGPL(DBG_HCD, "\n");
+#endif
+
+#ifdef DEBUG
+#ifndef DEBUG_SOF
+		if (gintsts.d32 != DWC_SOF_INTR_MASK)
+#endif
+			DWC_DEBUGPL(DBG_HCD,
+				    "DWC OTG HCD Interrupt Detected gintsts&gintmsk=0x%08x\n",
+				    gintsts.d32);
+#endif
+
+		if (gintsts.b.sofintr) {
+			retval |= dwc_otg_hcd_handle_sof_intr(dwc_otg_hcd);
+		}
+		if (gintsts.b.rxstsqlvl) {
+			retval |=
+			    dwc_otg_hcd_handle_rx_status_q_level_intr
+			    (dwc_otg_hcd);
+		}
+		if (gintsts.b.nptxfempty) {
+			retval |=
+			    dwc_otg_hcd_handle_np_tx_fifo_empty_intr
+			    (dwc_otg_hcd);
+		}
+		if (gintsts.b.i2cintr) {
+			/** @todo Implement i2cintr handler. */
+		}
+		if (gintsts.b.portintr) {
+			retval |= dwc_otg_hcd_handle_port_intr(dwc_otg_hcd);
+		}
+		if (gintsts.b.hcintr) {
+			retval |= dwc_otg_hcd_handle_hc_intr(dwc_otg_hcd);
+		}
+		if (gintsts.b.ptxfempty) {
+			retval |=
+			    dwc_otg_hcd_handle_perio_tx_fifo_empty_intr
+			    (dwc_otg_hcd);
+		}
+#ifdef DEBUG
+#ifndef DEBUG_SOF
+		if (gintsts.d32 != DWC_SOF_INTR_MASK)
+#endif
+		{
+			DWC_DEBUGPL(DBG_HCD,
+				    "DWC OTG HCD Finished Servicing Interrupts\n");
+			DWC_DEBUGPL(DBG_HCDV, "DWC OTG HCD gintsts=0x%08x\n",
+				    DWC_READ_REG32(&global_regs->gintsts));
+			DWC_DEBUGPL(DBG_HCDV, "DWC OTG HCD gintmsk=0x%08x\n",
+				    DWC_READ_REG32(&global_regs->gintmsk));
+		}
+#endif
+
+#ifdef DEBUG
+#ifndef DEBUG_SOF
+		if (gintsts.d32 != DWC_SOF_INTR_MASK)
+#endif
+			DWC_DEBUGPL(DBG_HCD, "\n");
+#endif
+
+	}
+	DWC_SPINUNLOCK(dwc_otg_hcd->lock);
+	return retval;
+}
+
+#ifdef DWC_TRACK_MISSED_SOFS
+#warning Compiling code to track missed SOFs
+#define FRAME_NUM_ARRAY_SIZE 1000
+/**
+ * This function is for debug only.
+ */
+static inline void track_missed_sofs(uint16_t curr_frame_number)
+{
+	static uint16_t frame_num_array[FRAME_NUM_ARRAY_SIZE];
+	static uint16_t last_frame_num_array[FRAME_NUM_ARRAY_SIZE];
+	static int frame_num_idx = 0;
+	static uint16_t last_frame_num = DWC_HFNUM_MAX_FRNUM;
+	static int dumped_frame_num_array = 0;
+
+	if (frame_num_idx < FRAME_NUM_ARRAY_SIZE) {
+		if (((last_frame_num + 1) & DWC_HFNUM_MAX_FRNUM) !=
+		    curr_frame_number) {
+			frame_num_array[frame_num_idx] = curr_frame_number;
+			last_frame_num_array[frame_num_idx++] = last_frame_num;
+		}
+	} else if (!dumped_frame_num_array) {
+		int i;
+		DWC_PRINTF("Frame     Last Frame\n");
+		DWC_PRINTF("-----     ----------\n");
+		for (i = 0; i < FRAME_NUM_ARRAY_SIZE; i++) {
+			DWC_PRINTF("0x%04x    0x%04x\n",
+				   frame_num_array[i], last_frame_num_array[i]);
+		}
+		dumped_frame_num_array = 1;
+	}
+	last_frame_num = curr_frame_number;
+}
+#endif
+
+/**
+ * Handles the start-of-frame interrupt in host mode. Non-periodic
+ * transactions may be queued to the DWC_otg controller for the current
+ * (micro)frame. Periodic transactions may be queued to the controller for the
+ * next (micro)frame.
+ */
+int32_t dwc_otg_hcd_handle_sof_intr(dwc_otg_hcd_t * hcd)
+{
+	hfnum_data_t hfnum;
+	dwc_list_link_t *qh_entry;
+	dwc_otg_qh_t *qh;
+	dwc_otg_transaction_type_e tr_type;
+	gintsts_data_t gintsts = {.d32 = 0 };
+
+	hfnum.d32 =
+	    DWC_READ_REG32(&hcd->core_if->host_if->host_global_regs->hfnum);
+
+#ifdef DEBUG_SOF
+	DWC_DEBUGPL(DBG_HCD, "--Start of Frame Interrupt--\n");
+#endif
+	hcd->frame_number = hfnum.b.frnum;
+
+#ifdef DEBUG
+	hcd->frrem_accum += hfnum.b.frrem;
+	hcd->frrem_samples++;
+#endif
+
+#ifdef DWC_TRACK_MISSED_SOFS
+	track_missed_sofs(hcd->frame_number);
+#endif
+	/* Determine whether any periodic QHs should be executed. */
+	qh_entry = DWC_LIST_FIRST(&hcd->periodic_sched_inactive);
+	while (qh_entry != &hcd->periodic_sched_inactive) {
+		qh = DWC_LIST_ENTRY(qh_entry, dwc_otg_qh_t, qh_list_entry);
+		qh_entry = qh_entry->next;
+		if (dwc_frame_num_le(qh->sched_frame, hcd->frame_number)) {
+			/*
+			 * Move QH to the ready list to be executed next
+			 * (micro)frame.
+			 */
+			DWC_LIST_MOVE_HEAD(&hcd->periodic_sched_ready,
+					   &qh->qh_list_entry);
+		}
+	}
+	tr_type = dwc_otg_hcd_select_transactions(hcd);
+	if (tr_type != DWC_OTG_TRANSACTION_NONE) {
+		dwc_otg_hcd_queue_transactions(hcd, tr_type);
+	}
+
+	/* Clear interrupt */
+	gintsts.b.sofintr = 1;
+	DWC_WRITE_REG32(&hcd->core_if->core_global_regs->gintsts, gintsts.d32);
+
+	return 1;
+}
+
+/** Handles the Rx Status Queue Level Interrupt, which indicates that there is at
+ * least one packet in the Rx FIFO.  The packets are moved from the FIFO to
+ * memory if the DWC_otg controller is operating in Slave mode. */
+int32_t dwc_otg_hcd_handle_rx_status_q_level_intr(dwc_otg_hcd_t * dwc_otg_hcd)
+{
+	host_grxsts_data_t grxsts;
+	dwc_hc_t *hc = NULL;
+
+	DWC_DEBUGPL(DBG_HCD, "--RxStsQ Level Interrupt--\n");
+
+	grxsts.d32 =
+	    DWC_READ_REG32(&dwc_otg_hcd->core_if->core_global_regs->grxstsp);
+
+	hc = dwc_otg_hcd->hc_ptr_array[grxsts.b.chnum];
+	if (!hc) {
+		DWC_ERROR("Unable to get corresponding channel\n");
+		return 0;
+	}
+
+	/* Packet Status */
+	DWC_DEBUGPL(DBG_HCDV, "    Ch num = %d\n", grxsts.b.chnum);
+	DWC_DEBUGPL(DBG_HCDV, "    Count = %d\n", grxsts.b.bcnt);
+	DWC_DEBUGPL(DBG_HCDV, "    DPID = %d, hc.dpid = %d\n", grxsts.b.dpid,
+		    hc->data_pid_start);
+	DWC_DEBUGPL(DBG_HCDV, "    PStatus = %d\n", grxsts.b.pktsts);
+
+	switch (grxsts.b.pktsts) {
+	case DWC_GRXSTS_PKTSTS_IN:
+		/* Read the data into the host buffer. */
+		if (grxsts.b.bcnt > 0) {
+			dwc_otg_read_packet(dwc_otg_hcd->core_if,
+					    hc->xfer_buff, grxsts.b.bcnt);
+
+			/* Update the HC fields for the next packet received. */
+			hc->xfer_count += grxsts.b.bcnt;
+			hc->xfer_buff += grxsts.b.bcnt;
+		}
+
+	case DWC_GRXSTS_PKTSTS_IN_XFER_COMP:
+	case DWC_GRXSTS_PKTSTS_DATA_TOGGLE_ERR:
+	case DWC_GRXSTS_PKTSTS_CH_HALTED:
+		/* Handled in interrupt, just ignore data */
+		break;
+	default:
+		DWC_ERROR("RX_STS_Q Interrupt: Unknown status %d\n",
+			  grxsts.b.pktsts);
+		break;
+	}
+
+	return 1;
+}
+
+/** This interrupt occurs when the non-periodic Tx FIFO is half-empty. More
+ * data packets may be written to the FIFO for OUT transfers. More requests
+ * may be written to the non-periodic request queue for IN transfers. This
+ * interrupt is enabled only in Slave mode. */
+int32_t dwc_otg_hcd_handle_np_tx_fifo_empty_intr(dwc_otg_hcd_t * dwc_otg_hcd)
+{
+	DWC_DEBUGPL(DBG_HCD, "--Non-Periodic TxFIFO Empty Interrupt--\n");
+	dwc_otg_hcd_queue_transactions(dwc_otg_hcd,
+				       DWC_OTG_TRANSACTION_NON_PERIODIC);
+	return 1;
+}
+
+/** This interrupt occurs when the periodic Tx FIFO is half-empty. More data
+ * packets may be written to the FIFO for OUT transfers. More requests may be
+ * written to the periodic request queue for IN transfers. This interrupt is
+ * enabled only in Slave mode. */
+int32_t dwc_otg_hcd_handle_perio_tx_fifo_empty_intr(dwc_otg_hcd_t * dwc_otg_hcd)
+{
+	DWC_DEBUGPL(DBG_HCD, "--Periodic TxFIFO Empty Interrupt--\n");
+	dwc_otg_hcd_queue_transactions(dwc_otg_hcd,
+				       DWC_OTG_TRANSACTION_PERIODIC);
+	return 1;
+}
+
+/** There are multiple conditions that can cause a port interrupt. This function
+ * determines which interrupt conditions have occurred and handles them
+ * appropriately. */
+int32_t dwc_otg_hcd_handle_port_intr(dwc_otg_hcd_t * dwc_otg_hcd)
+{
+	int retval = 0;
+	hprt0_data_t hprt0;
+	hprt0_data_t hprt0_modify;
+
+	hprt0.d32 = DWC_READ_REG32(dwc_otg_hcd->core_if->host_if->hprt0);
+	hprt0_modify.d32 = DWC_READ_REG32(dwc_otg_hcd->core_if->host_if->hprt0);
+
+	/* Clear appropriate bits in HPRT0 to clear the interrupt bit in
+	 * GINTSTS */
+
+	hprt0_modify.b.prtena = 0;
+	hprt0_modify.b.prtconndet = 0;
+	hprt0_modify.b.prtenchng = 0;
+	hprt0_modify.b.prtovrcurrchng = 0;
+
+	/* Port Connect Detected
+	 * Set flag and clear if detected */
+	if (dwc_otg_hcd->core_if->hibernation_suspend == 1) {
+		// Dont modify port status if we are in hibernation state
+		hprt0_modify.b.prtconndet = 1;
+		hprt0_modify.b.prtenchng = 1;
+		DWC_WRITE_REG32(dwc_otg_hcd->core_if->host_if->hprt0, hprt0_modify.d32);
+		hprt0.d32 = DWC_READ_REG32(dwc_otg_hcd->core_if->host_if->hprt0);
+		return retval;
+	}
+
+	if (hprt0.b.prtconndet) {
+		/** @todo - check if steps performed in 'else' block should be perfromed regardles adp */
+		if (dwc_otg_hcd->core_if->adp_enable &&
+				dwc_otg_hcd->core_if->adp.vbuson_timer_started == 1) {
+			DWC_PRINTF("PORT CONNECT DETECTED ----------------\n");
+			DWC_TIMER_CANCEL(dwc_otg_hcd->core_if->adp.vbuson_timer);
+			dwc_otg_hcd->core_if->adp.vbuson_timer_started = 0;
+			/* TODO - check if this is required, as
+			 * host initialization was already performed
+			 * after initial ADP probing
+			 */
+			/*dwc_otg_hcd->core_if->adp.vbuson_timer_started = 0;
+			dwc_otg_core_init(dwc_otg_hcd->core_if);
+			dwc_otg_enable_global_interrupts(dwc_otg_hcd->core_if);
+			cil_hcd_start(dwc_otg_hcd->core_if);*/
+		} else {
+
+			DWC_DEBUGPL(DBG_HCD, "--Port Interrupt HPRT0=0x%08x "
+				    "Port Connect Detected--\n", hprt0.d32);
+			dwc_otg_hcd->flags.b.port_connect_status_change = 1;
+			dwc_otg_hcd->flags.b.port_connect_status = 1;
+			hprt0_modify.b.prtconndet = 1;
+
+			/* B-Device has connected, Delete the connection timer. */
+			DWC_TIMER_CANCEL(dwc_otg_hcd->conn_timer);
+		}
+		/* The Hub driver asserts a reset when it sees port connect
+		 * status change flag */
+		retval |= 1;
+	}
+
+	/* Port Enable Changed
+	 * Clear if detected - Set internal flag if disabled */
+	if (hprt0.b.prtenchng) {
+		DWC_DEBUGPL(DBG_HCD, "  --Port Interrupt HPRT0=0x%08x "
+			    "Port Enable Changed--\n", hprt0.d32);
+		hprt0_modify.b.prtenchng = 1;
+		if (hprt0.b.prtena == 1) {
+			hfir_data_t hfir;
+			int do_reset = 0;
+			dwc_otg_core_params_t *params =
+			    dwc_otg_hcd->core_if->core_params;
+			dwc_otg_core_global_regs_t *global_regs =
+			    dwc_otg_hcd->core_if->core_global_regs;
+			dwc_otg_host_if_t *host_if =
+			    dwc_otg_hcd->core_if->host_if;
+
+			/* Every time when port enables calculate
+			 * HFIR.FrInterval
+			 */
+			hfir.d32 = DWC_READ_REG32(&host_if->host_global_regs->hfir);
+			hfir.b.frint = calc_frame_interval(dwc_otg_hcd->core_if);
+			DWC_WRITE_REG32(&host_if->host_global_regs->hfir, hfir.d32);
+
+			/* Check if we need to adjust the PHY clock speed for
+			 * low power and adjust it */
+			if (params->host_support_fs_ls_low_power) {
+				gusbcfg_data_t usbcfg;
+
+				usbcfg.d32 =
+				    DWC_READ_REG32(&global_regs->gusbcfg);
+
+				if (hprt0.b.prtspd == DWC_HPRT0_PRTSPD_LOW_SPEED
+				    || hprt0.b.prtspd ==
+				    DWC_HPRT0_PRTSPD_FULL_SPEED) {
+					/*
+					 * Low power
+					 */
+					hcfg_data_t hcfg;
+					if (usbcfg.b.phylpwrclksel == 0) {
+						/* Set PHY low power clock select for FS/LS devices */
+						usbcfg.b.phylpwrclksel = 1;
+						DWC_WRITE_REG32
+						    (&global_regs->gusbcfg,
+						     usbcfg.d32);
+						do_reset = 1;
+					}
+
+					hcfg.d32 =
+					    DWC_READ_REG32
+					    (&host_if->host_global_regs->hcfg);
+
+					if (hprt0.b.prtspd ==
+					    DWC_HPRT0_PRTSPD_LOW_SPEED
+					    && params->host_ls_low_power_phy_clk
+					    ==
+					    DWC_HOST_LS_LOW_POWER_PHY_CLK_PARAM_6MHZ)
+					{
+						/* 6 MHZ */
+						DWC_DEBUGPL(DBG_CIL,
+							    "FS_PHY programming HCFG to 6 MHz (Low Power)\n");
+						if (hcfg.b.fslspclksel !=
+						    DWC_HCFG_6_MHZ) {
+							hcfg.b.fslspclksel =
+							    DWC_HCFG_6_MHZ;
+							DWC_WRITE_REG32
+							    (&host_if->host_global_regs->hcfg,
+							     hcfg.d32);
+							do_reset = 1;
+						}
+					} else {
+						/* 48 MHZ */
+						DWC_DEBUGPL(DBG_CIL,
+							    "FS_PHY programming HCFG to 48 MHz ()\n");
+						if (hcfg.b.fslspclksel !=
+						    DWC_HCFG_48_MHZ) {
+							hcfg.b.fslspclksel =
+							    DWC_HCFG_48_MHZ;
+							DWC_WRITE_REG32
+							    (&host_if->host_global_regs->hcfg,
+							     hcfg.d32);
+							do_reset = 1;
+						}
+					}
+				} else {
+					/*
+					 * Not low power
+					 */
+					if (usbcfg.b.phylpwrclksel == 1) {
+						usbcfg.b.phylpwrclksel = 0;
+						DWC_WRITE_REG32
+						    (&global_regs->gusbcfg,
+						     usbcfg.d32);
+						do_reset = 1;
+					}
+				}
+
+				if (do_reset) {
+					DWC_TASK_SCHEDULE(dwc_otg_hcd->reset_tasklet);
+				}
+			}
+
+			if (!do_reset) {
+				/* Port has been enabled set the reset change flag */
+				dwc_otg_hcd->flags.b.port_reset_change = 1;
+			}
+		} else {
+			dwc_otg_hcd->flags.b.port_enable_change = 1;
+		}
+		retval |= 1;
+	}
+
+	/** Overcurrent Change Interrupt */
+	if (hprt0.b.prtovrcurrchng) {
+		DWC_DEBUGPL(DBG_HCD, "  --Port Interrupt HPRT0=0x%08x "
+			    "Port Overcurrent Changed--\n", hprt0.d32);
+		dwc_otg_hcd->flags.b.port_over_current_change = 1;
+		hprt0_modify.b.prtovrcurrchng = 1;
+		retval |= 1;
+	}
+
+	/* Clear Port Interrupts */
+	DWC_WRITE_REG32(dwc_otg_hcd->core_if->host_if->hprt0, hprt0_modify.d32);
+
+	return retval;
+}
+
+/** This interrupt indicates that one or more host channels has a pending
+ * interrupt. There are multiple conditions that can cause each host channel
+ * interrupt. This function determines which conditions have occurred for each
+ * host channel interrupt and handles them appropriately. */
+int32_t dwc_otg_hcd_handle_hc_intr(dwc_otg_hcd_t * dwc_otg_hcd)
+{
+	int i;
+	int retval = 0;
+	haint_data_t haint;
+
+	/* Clear appropriate bits in HCINTn to clear the interrupt bit in
+	 * GINTSTS */
+
+	haint.d32 = dwc_otg_read_host_all_channels_intr(dwc_otg_hcd->core_if);
+
+	for (i = 0; i < dwc_otg_hcd->core_if->core_params->host_channels; i++) {
+		if (haint.b2.chint & (1 << i)) {
+			retval |= dwc_otg_hcd_handle_hc_n_intr(dwc_otg_hcd, i);
+		}
+	}
+
+	return retval;
+}
+
+/**
+ * Gets the actual length of a transfer after the transfer halts. _halt_status
+ * holds the reason for the halt.
+ *
+ * For IN transfers where halt_status is DWC_OTG_HC_XFER_COMPLETE,
+ * *short_read is set to 1 upon return if less than the requested
+ * number of bytes were transferred. Otherwise, *short_read is set to 0 upon
+ * return. short_read may also be NULL on entry, in which case it remains
+ * unchanged.
+ */
+static uint32_t get_actual_xfer_length(dwc_hc_t * hc,
+				       dwc_otg_hc_regs_t * hc_regs,
+				       dwc_otg_qtd_t * qtd,
+				       dwc_otg_halt_status_e halt_status,
+				       int *short_read)
+{
+	hctsiz_data_t hctsiz;
+	uint32_t length;
+
+	if (short_read != NULL) {
+		*short_read = 0;
+	}
+	hctsiz.d32 = DWC_READ_REG32(&hc_regs->hctsiz);
+
+	if (halt_status == DWC_OTG_HC_XFER_COMPLETE) {
+		if (hc->ep_is_in) {
+			length = hc->xfer_len - hctsiz.b.xfersize;
+			if (short_read != NULL) {
+				*short_read = (hctsiz.b.xfersize != 0);
+			}
+		} else if (hc->qh->do_split) {
+			length = qtd->ssplit_out_xfer_count;
+		} else {
+			length = hc->xfer_len;
+		}
+	} else {
+		/*
+		 * Must use the hctsiz.pktcnt field to determine how much data
+		 * has been transferred. This field reflects the number of
+		 * packets that have been transferred via the USB. This is
+		 * always an integral number of packets if the transfer was
+		 * halted before its normal completion. (Can't use the
+		 * hctsiz.xfersize field because that reflects the number of
+		 * bytes transferred via the AHB, not the USB).
+		 */
+		length =
+		    (hc->start_pkt_count - hctsiz.b.pktcnt) * hc->max_packet;
+	}
+
+	return length;
+}
+
+/**
+ * Updates the state of the URB after a Transfer Complete interrupt on the
+ * host channel. Updates the actual_length field of the URB based on the
+ * number of bytes transferred via the host channel. Sets the URB status
+ * if the data transfer is finished.
+ *
+ * @return 1 if the data transfer specified by the URB is completely finished,
+ * 0 otherwise.
+ */
+static int update_urb_state_xfer_comp(dwc_hc_t * hc,
+				      dwc_otg_hc_regs_t * hc_regs,
+				      dwc_otg_hcd_urb_t * urb,
+				      dwc_otg_qtd_t * qtd)
+{
+	int xfer_done = 0;
+	int short_read = 0;
+
+	int xfer_length;
+
+	xfer_length = get_actual_xfer_length(hc, hc_regs, qtd,
+					     DWC_OTG_HC_XFER_COMPLETE,
+					     &short_read);
+
+
+	/* non DWORD-aligned buffer case handling. */
+	if (hc->align_buff && xfer_length && hc->ep_is_in) {
+		dwc_memcpy(urb->buf + urb->actual_length, hc->qh->dw_align_buf,
+			   xfer_length);
+	}
+
+	urb->actual_length += xfer_length;
+
+	if (xfer_length && (hc->ep_type == DWC_OTG_EP_TYPE_BULK) &&
+	    (urb->flags & URB_SEND_ZERO_PACKET)
+	    && (urb->actual_length == urb->length)
+	    && !(urb->length % hc->max_packet)) {
+		xfer_done = 0;
+	} else if (short_read || urb->actual_length == urb->length) {
+		xfer_done = 1;
+		urb->status = 0;
+	}
+
+#ifdef DEBUG
+	{
+		hctsiz_data_t hctsiz;
+		hctsiz.d32 = DWC_READ_REG32(&hc_regs->hctsiz);
+		DWC_DEBUGPL(DBG_HCDV, "DWC_otg: %s: %s, channel %d\n",
+			    __func__, (hc->ep_is_in ? "IN" : "OUT"),
+			    hc->hc_num);
+		DWC_DEBUGPL(DBG_HCDV, "  hc->xfer_len %d\n", hc->xfer_len);
+		DWC_DEBUGPL(DBG_HCDV, "  hctsiz.xfersize %d\n",
+			    hctsiz.b.xfersize);
+		DWC_DEBUGPL(DBG_HCDV, "  urb->transfer_buffer_length %d\n",
+			    urb->length);
+		DWC_DEBUGPL(DBG_HCDV, "  urb->actual_length %d\n",
+			    urb->actual_length);
+		DWC_DEBUGPL(DBG_HCDV, "  short_read %d, xfer_done %d\n",
+			    short_read, xfer_done);
+	}
+#endif
+
+	return xfer_done;
+}
+
+/*
+ * Save the starting data toggle for the next transfer. The data toggle is
+ * saved in the QH for non-control transfers and it's saved in the QTD for
+ * control transfers.
+ */
+void dwc_otg_hcd_save_data_toggle(dwc_hc_t * hc,
+			     dwc_otg_hc_regs_t * hc_regs, dwc_otg_qtd_t * qtd)
+{
+	hctsiz_data_t hctsiz;
+	hctsiz.d32 = DWC_READ_REG32(&hc_regs->hctsiz);
+
+	if (hc->ep_type != DWC_OTG_EP_TYPE_CONTROL) {
+		dwc_otg_qh_t *qh = hc->qh;
+		if (hctsiz.b.pid == DWC_HCTSIZ_DATA0) {
+			qh->data_toggle = DWC_OTG_HC_PID_DATA0;
+		} else {
+			qh->data_toggle = DWC_OTG_HC_PID_DATA1;
+		}
+	} else {
+		if (hctsiz.b.pid == DWC_HCTSIZ_DATA0) {
+			qtd->data_toggle = DWC_OTG_HC_PID_DATA0;
+		} else {
+			qtd->data_toggle = DWC_OTG_HC_PID_DATA1;
+		}
+	}
+}
+
+/**
+ * Updates the state of an Isochronous URB when the transfer is stopped for
+ * any reason. The fields of the current entry in the frame descriptor array
+ * are set based on the transfer state and the input _halt_status. Completes
+ * the Isochronous URB if all the URB frames have been completed.
+ *
+ * @return DWC_OTG_HC_XFER_COMPLETE if there are more frames remaining to be
+ * transferred in the URB. Otherwise return DWC_OTG_HC_XFER_URB_COMPLETE.
+ */
+static dwc_otg_halt_status_e
+update_isoc_urb_state(dwc_otg_hcd_t * hcd,
+		      dwc_hc_t * hc,
+		      dwc_otg_hc_regs_t * hc_regs,
+		      dwc_otg_qtd_t * qtd, dwc_otg_halt_status_e halt_status)
+{
+	dwc_otg_hcd_urb_t *urb = qtd->urb;
+	dwc_otg_halt_status_e ret_val = halt_status;
+	struct dwc_otg_hcd_iso_packet_desc *frame_desc;
+
+	frame_desc = &urb->iso_descs[qtd->isoc_frame_index];
+	switch (halt_status) {
+	case DWC_OTG_HC_XFER_COMPLETE:
+		frame_desc->status = 0;
+		frame_desc->actual_length =
+		    get_actual_xfer_length(hc, hc_regs, qtd, halt_status, NULL);
+
+		/* non DWORD-aligned buffer case handling. */
+		if (hc->align_buff && frame_desc->actual_length && hc->ep_is_in) {
+			dwc_memcpy(urb->buf + frame_desc->offset + qtd->isoc_split_offset,
+				   hc->qh->dw_align_buf, frame_desc->actual_length);
+		}
+
+		break;
+	case DWC_OTG_HC_XFER_FRAME_OVERRUN:
+		urb->error_count++;
+		if (hc->ep_is_in) {
+			frame_desc->status = -DWC_E_NO_STREAM_RES;
+		} else {
+			frame_desc->status = -DWC_E_COMMUNICATION;
+		}
+		frame_desc->actual_length = 0;
+		break;
+	case DWC_OTG_HC_XFER_BABBLE_ERR:
+		urb->error_count++;
+		frame_desc->status = -DWC_E_OVERFLOW;
+		/* Don't need to update actual_length in this case. */
+		break;
+	case DWC_OTG_HC_XFER_XACT_ERR:
+		urb->error_count++;
+		frame_desc->status = -DWC_E_PROTOCOL;
+		frame_desc->actual_length =
+		    get_actual_xfer_length(hc, hc_regs, qtd, halt_status, NULL);
+
+		/* non DWORD-aligned buffer case handling. */
+		if (hc->align_buff && frame_desc->actual_length && hc->ep_is_in) {
+			dwc_memcpy(urb->buf + frame_desc->offset + qtd->isoc_split_offset,
+				   hc->qh->dw_align_buf, frame_desc->actual_length);
+		}
+		/* Skip whole frame */
+		if (hc->qh->do_split && (hc->ep_type == DWC_OTG_EP_TYPE_ISOC) &&
+		    hc->ep_is_in && hcd->core_if->dma_enable) {
+			qtd->complete_split = 0;
+			qtd->isoc_split_offset = 0;
+		}
+
+		break;
+	default:
+		DWC_ASSERT(1, "Unhandled _halt_status (%d)\n", halt_status);
+		break;
+	}
+	if (++qtd->isoc_frame_index == urb->packet_count) {
+		/*
+		 * urb->status is not used for isoc transfers.
+		 * The individual frame_desc statuses are used instead.
+		 */
+		hcd->fops->complete(hcd, urb->priv, urb, 0);
+		ret_val = DWC_OTG_HC_XFER_URB_COMPLETE;
+	} else {
+		ret_val = DWC_OTG_HC_XFER_COMPLETE;
+	}
+	return ret_val;
+}
+
+/**
+ * Frees the first QTD in the QH's list if free_qtd is 1. For non-periodic
+ * QHs, removes the QH from the active non-periodic schedule. If any QTDs are
+ * still linked to the QH, the QH is added to the end of the inactive
+ * non-periodic schedule. For periodic QHs, removes the QH from the periodic
+ * schedule if no more QTDs are linked to the QH.
+ */
+static void deactivate_qh(dwc_otg_hcd_t * hcd, dwc_otg_qh_t * qh, int free_qtd)
+{
+	int continue_split = 0;
+	dwc_otg_qtd_t *qtd;
+
+	DWC_DEBUGPL(DBG_HCDV, "  %s(%p,%p,%d)\n", __func__, hcd, qh, free_qtd);
+
+	qtd = DWC_CIRCLEQ_FIRST(&qh->qtd_list);
+
+	if (qtd->complete_split) {
+		continue_split = 1;
+	} else if (qtd->isoc_split_pos == DWC_HCSPLIT_XACTPOS_MID ||
+		   qtd->isoc_split_pos == DWC_HCSPLIT_XACTPOS_END) {
+		continue_split = 1;
+	}
+
+	if (free_qtd) {
+		dwc_otg_hcd_qtd_remove_and_free(hcd, qtd, qh);
+		continue_split = 0;
+	}
+
+	qh->channel = NULL;
+	dwc_otg_hcd_qh_deactivate(hcd, qh, continue_split);
+}
+
+/**
+ * Releases a host channel for use by other transfers. Attempts to select and
+ * queue more transactions since at least one host channel is available.
+ *
+ * @param hcd The HCD state structure.
+ * @param hc The host channel to release.
+ * @param qtd The QTD associated with the host channel. This QTD may be freed
+ * if the transfer is complete or an error has occurred.
+ * @param halt_status Reason the channel is being released. This status
+ * determines the actions taken by this function.
+ */
+static void release_channel(dwc_otg_hcd_t * hcd,
+			    dwc_hc_t * hc,
+			    dwc_otg_qtd_t * qtd,
+			    dwc_otg_halt_status_e halt_status)
+{
+	dwc_otg_transaction_type_e tr_type;
+	int free_qtd;
+
+	DWC_DEBUGPL(DBG_HCDV, "  %s: channel %d, halt_status %d\n",
+		    __func__, hc->hc_num, halt_status);
+
+	switch (halt_status) {
+	case DWC_OTG_HC_XFER_URB_COMPLETE:
+		free_qtd = 1;
+		break;
+	case DWC_OTG_HC_XFER_AHB_ERR:
+	case DWC_OTG_HC_XFER_STALL:
+	case DWC_OTG_HC_XFER_BABBLE_ERR:
+		free_qtd = 1;
+		break;
+	case DWC_OTG_HC_XFER_XACT_ERR:
+		if (qtd->error_count >= 3) {
+			DWC_DEBUGPL(DBG_HCDV,
+				    "  Complete URB with transaction error\n");
+			free_qtd = 1;
+			qtd->urb->status = -DWC_E_PROTOCOL;
+			hcd->fops->complete(hcd, qtd->urb->priv,
+					    qtd->urb, -DWC_E_PROTOCOL);
+		} else {
+			free_qtd = 0;
+		}
+		break;
+	case DWC_OTG_HC_XFER_URB_DEQUEUE:
+		/*
+		 * The QTD has already been removed and the QH has been
+		 * deactivated. Don't want to do anything except release the
+		 * host channel and try to queue more transfers.
+		 */
+		goto cleanup;
+	case DWC_OTG_HC_XFER_NO_HALT_STATUS:
+		free_qtd = 0;
+		break;
+	case DWC_OTG_HC_XFER_PERIODIC_INCOMPLETE:
+		DWC_DEBUGPL(DBG_HCDV,
+			"  Complete URB with I/O error\n");
+		free_qtd = 1;
+		qtd->urb->status = -DWC_E_IO;
+		hcd->fops->complete(hcd, qtd->urb->priv,
+			qtd->urb, -DWC_E_IO);
+		break;
+	default:
+		free_qtd = 0;
+		break;
+	}
+
+	deactivate_qh(hcd, hc->qh, free_qtd);
+
+cleanup:
+	/*
+	 * Release the host channel for use by other transfers. The cleanup
+	 * function clears the channel interrupt enables and conditions, so
+	 * there's no need to clear the Channel Halted interrupt separately.
+	 */
+	dwc_otg_hc_cleanup(hcd->core_if, hc);
+	DWC_CIRCLEQ_INSERT_TAIL(&hcd->free_hc_list, hc, hc_list_entry);
+
+	switch (hc->ep_type) {
+	case DWC_OTG_EP_TYPE_CONTROL:
+	case DWC_OTG_EP_TYPE_BULK:
+		hcd->non_periodic_channels--;
+		break;
+
+	default:
+		/*
+		 * Don't release reservations for periodic channels here.
+		 * That's done when a periodic transfer is descheduled (i.e.
+		 * when the QH is removed from the periodic schedule).
+		 */
+		break;
+	}
+
+	/* Try to queue more transfers now that there's a free channel. */
+	tr_type = dwc_otg_hcd_select_transactions(hcd);
+	if (tr_type != DWC_OTG_TRANSACTION_NONE) {
+		dwc_otg_hcd_queue_transactions(hcd, tr_type);
+	}
+}
+
+/**
+ * Halts a host channel. If the channel cannot be halted immediately because
+ * the request queue is full, this function ensures that the FIFO empty
+ * interrupt for the appropriate queue is enabled so that the halt request can
+ * be queued when there is space in the request queue.
+ *
+ * This function may also be called in DMA mode. In that case, the channel is
+ * simply released since the core always halts the channel automatically in
+ * DMA mode.
+ */
+static void halt_channel(dwc_otg_hcd_t * hcd,
+			 dwc_hc_t * hc,
+			 dwc_otg_qtd_t * qtd, dwc_otg_halt_status_e halt_status)
+{
+	if (hcd->core_if->dma_enable) {
+		release_channel(hcd, hc, qtd, halt_status);
+		return;
+	}
+
+	/* Slave mode processing... */
+	dwc_otg_hc_halt(hcd->core_if, hc, halt_status);
+
+	if (hc->halt_on_queue) {
+		gintmsk_data_t gintmsk = {.d32 = 0 };
+		dwc_otg_core_global_regs_t *global_regs;
+		global_regs = hcd->core_if->core_global_regs;
+
+		if (hc->ep_type == DWC_OTG_EP_TYPE_CONTROL ||
+		    hc->ep_type == DWC_OTG_EP_TYPE_BULK) {
+			/*
+			 * Make sure the Non-periodic Tx FIFO empty interrupt
+			 * is enabled so that the non-periodic schedule will
+			 * be processed.
+			 */
+			gintmsk.b.nptxfempty = 1;
+			DWC_MODIFY_REG32(&global_regs->gintmsk, 0, gintmsk.d32);
+		} else {
+			/*
+			 * Move the QH from the periodic queued schedule to
+			 * the periodic assigned schedule. This allows the
+			 * halt to be queued when the periodic schedule is
+			 * processed.
+			 */
+			DWC_LIST_MOVE_HEAD(&hcd->periodic_sched_assigned,
+					   &hc->qh->qh_list_entry);
+
+			/*
+			 * Make sure the Periodic Tx FIFO Empty interrupt is
+			 * enabled so that the periodic schedule will be
+			 * processed.
+			 */
+			gintmsk.b.ptxfempty = 1;
+			DWC_MODIFY_REG32(&global_regs->gintmsk, 0, gintmsk.d32);
+		}
+	}
+}
+
+/**
+ * Performs common cleanup for non-periodic transfers after a Transfer
+ * Complete interrupt. This function should be called after any endpoint type
+ * specific handling is finished to release the host channel.
+ */
+static void complete_non_periodic_xfer(dwc_otg_hcd_t * hcd,
+				       dwc_hc_t * hc,
+				       dwc_otg_hc_regs_t * hc_regs,
+				       dwc_otg_qtd_t * qtd,
+				       dwc_otg_halt_status_e halt_status)
+{
+	hcint_data_t hcint;
+
+	qtd->error_count = 0;
+
+	hcint.d32 = DWC_READ_REG32(&hc_regs->hcint);
+	if (hcint.b.nyet) {
+		/*
+		 * Got a NYET on the last transaction of the transfer. This
+		 * means that the endpoint should be in the PING state at the
+		 * beginning of the next transfer.
+		 */
+		hc->qh->ping_state = 1;
+		clear_hc_int(hc_regs, nyet);
+	}
+
+	/*
+	 * Always halt and release the host channel to make it available for
+	 * more transfers. There may still be more phases for a control
+	 * transfer or more data packets for a bulk transfer at this point,
+	 * but the host channel is still halted. A channel will be reassigned
+	 * to the transfer when the non-periodic schedule is processed after
+	 * the channel is released. This allows transactions to be queued
+	 * properly via dwc_otg_hcd_queue_transactions, which also enables the
+	 * Tx FIFO Empty interrupt if necessary.
+	 */
+	if (hc->ep_is_in) {
+		/*
+		 * IN transfers in Slave mode require an explicit disable to
+		 * halt the channel. (In DMA mode, this call simply releases
+		 * the channel.)
+		 */
+		halt_channel(hcd, hc, qtd, halt_status);
+	} else {
+		/*
+		 * The channel is automatically disabled by the core for OUT
+		 * transfers in Slave mode.
+		 */
+		release_channel(hcd, hc, qtd, halt_status);
+	}
+}
+
+/**
+ * Performs common cleanup for periodic transfers after a Transfer Complete
+ * interrupt. This function should be called after any endpoint type specific
+ * handling is finished to release the host channel.
+ */
+static void complete_periodic_xfer(dwc_otg_hcd_t * hcd,
+				   dwc_hc_t * hc,
+				   dwc_otg_hc_regs_t * hc_regs,
+				   dwc_otg_qtd_t * qtd,
+				   dwc_otg_halt_status_e halt_status)
+{
+	hctsiz_data_t hctsiz;
+	qtd->error_count = 0;
+
+	hctsiz.d32 = DWC_READ_REG32(&hc_regs->hctsiz);
+	if (!hc->ep_is_in || hctsiz.b.pktcnt == 0) {
+		/* Core halts channel in these cases. */
+		release_channel(hcd, hc, qtd, halt_status);
+	} else {
+		/* Flush any outstanding requests from the Tx queue. */
+		halt_channel(hcd, hc, qtd, halt_status);
+	}
+}
+
+static int32_t handle_xfercomp_isoc_split_in(dwc_otg_hcd_t * hcd,
+					     dwc_hc_t * hc,
+					     dwc_otg_hc_regs_t * hc_regs,
+					     dwc_otg_qtd_t * qtd)
+{
+	uint32_t len;
+	struct dwc_otg_hcd_iso_packet_desc *frame_desc;
+	frame_desc = &qtd->urb->iso_descs[qtd->isoc_frame_index];
+
+	len = get_actual_xfer_length(hc, hc_regs, qtd,
+				     DWC_OTG_HC_XFER_COMPLETE, NULL);
+
+	if (!len) {
+		qtd->complete_split = 0;
+		qtd->isoc_split_offset = 0;
+		return 0;
+	}
+	frame_desc->actual_length += len;
+
+	if (hc->align_buff && len)
+		dwc_memcpy(qtd->urb->buf + frame_desc->offset +
+			   qtd->isoc_split_offset, hc->qh->dw_align_buf, len);
+	qtd->isoc_split_offset += len;
+
+	if (frame_desc->length == frame_desc->actual_length) {
+		frame_desc->status = 0;
+		qtd->isoc_frame_index++;
+		qtd->complete_split = 0;
+		qtd->isoc_split_offset = 0;
+	}
+
+	if (qtd->isoc_frame_index == qtd->urb->packet_count) {
+		hcd->fops->complete(hcd, qtd->urb->priv, qtd->urb, 0);
+		release_channel(hcd, hc, qtd, DWC_OTG_HC_XFER_URB_COMPLETE);
+	} else {
+		release_channel(hcd, hc, qtd, DWC_OTG_HC_XFER_NO_HALT_STATUS);
+	}
+
+	return 1;		/* Indicates that channel released */
+}
+
+/**
+ * Handles a host channel Transfer Complete interrupt. This handler may be
+ * called in either DMA mode or Slave mode.
+ */
+static int32_t handle_hc_xfercomp_intr(dwc_otg_hcd_t * hcd,
+				       dwc_hc_t * hc,
+				       dwc_otg_hc_regs_t * hc_regs,
+				       dwc_otg_qtd_t * qtd)
+{
+	int urb_xfer_done;
+	dwc_otg_halt_status_e halt_status = DWC_OTG_HC_XFER_COMPLETE;
+	dwc_otg_hcd_urb_t *urb = qtd->urb;
+	int pipe_type = dwc_otg_hcd_get_pipe_type(&urb->pipe_info);
+
+	DWC_DEBUGPL(DBG_HCD, "--Host Channel %d Interrupt: "
+		    "Transfer Complete--\n", hc->hc_num);
+
+	if (hcd->core_if->dma_desc_enable) {
+		dwc_otg_hcd_complete_xfer_ddma(hcd, hc, hc_regs, halt_status);
+		if (pipe_type == UE_ISOCHRONOUS) {
+			/* Do not disable the interrupt, just clear it */
+			clear_hc_int(hc_regs, xfercomp);
+			return 1;
+		}
+		goto handle_xfercomp_done;
+	}
+
+	/*
+	 * Handle xfer complete on CSPLIT.
+	 */
+
+	if (hc->qh->do_split) {
+		if ((hc->ep_type == DWC_OTG_EP_TYPE_ISOC) && hc->ep_is_in
+		    && hcd->core_if->dma_enable) {
+			if (qtd->complete_split
+			    && handle_xfercomp_isoc_split_in(hcd, hc, hc_regs,
+							     qtd))
+				goto handle_xfercomp_done;
+		} else {
+			qtd->complete_split = 0;
+		}
+	}
+
+	/* Update the QTD and URB states. */
+	switch (pipe_type) {
+	case UE_CONTROL:
+		switch (qtd->control_phase) {
+		case DWC_OTG_CONTROL_SETUP:
+			if (urb->length > 0) {
+				qtd->control_phase = DWC_OTG_CONTROL_DATA;
+			} else {
+				qtd->control_phase = DWC_OTG_CONTROL_STATUS;
+			}
+			DWC_DEBUGPL(DBG_HCDV,
+				    "  Control setup transaction done\n");
+			halt_status = DWC_OTG_HC_XFER_COMPLETE;
+			break;
+		case DWC_OTG_CONTROL_DATA:{
+				urb_xfer_done =
+				    update_urb_state_xfer_comp(hc, hc_regs, urb,
+							       qtd);
+				if (urb_xfer_done) {
+					qtd->control_phase =
+					    DWC_OTG_CONTROL_STATUS;
+					DWC_DEBUGPL(DBG_HCDV,
+						    "  Control data transfer done\n");
+				} else {
+					dwc_otg_hcd_save_data_toggle(hc, hc_regs, qtd);
+				}
+				halt_status = DWC_OTG_HC_XFER_COMPLETE;
+				break;
+			}
+		case DWC_OTG_CONTROL_STATUS:
+			DWC_DEBUGPL(DBG_HCDV, "  Control transfer complete\n");
+			if (urb->status == -DWC_E_IN_PROGRESS) {
+				urb->status = 0;
+			}
+			hcd->fops->complete(hcd, urb->priv, urb, urb->status);
+			halt_status = DWC_OTG_HC_XFER_URB_COMPLETE;
+			break;
+		}
+
+		complete_non_periodic_xfer(hcd, hc, hc_regs, qtd, halt_status);
+		break;
+	case UE_BULK:
+		DWC_DEBUGPL(DBG_HCDV, "  Bulk transfer complete\n");
+		urb_xfer_done =
+		    update_urb_state_xfer_comp(hc, hc_regs, urb, qtd);
+		if (urb_xfer_done) {
+			hcd->fops->complete(hcd, urb->priv, urb, urb->status);
+			halt_status = DWC_OTG_HC_XFER_URB_COMPLETE;
+		} else {
+			halt_status = DWC_OTG_HC_XFER_COMPLETE;
+		}
+
+		dwc_otg_hcd_save_data_toggle(hc, hc_regs, qtd);
+		complete_non_periodic_xfer(hcd, hc, hc_regs, qtd, halt_status);
+		break;
+	case UE_INTERRUPT:
+		DWC_DEBUGPL(DBG_HCDV, "  Interrupt transfer complete\n");
+		urb_xfer_done =
+			update_urb_state_xfer_comp(hc, hc_regs, urb, qtd);
+
+		/*
+		 * Interrupt URB is done on the first transfer complete
+		 * interrupt.
+		 */
+		if (urb_xfer_done) {
+				hcd->fops->complete(hcd, urb->priv, urb, urb->status);
+				halt_status = DWC_OTG_HC_XFER_URB_COMPLETE;
+		} else {
+				halt_status = DWC_OTG_HC_XFER_COMPLETE;
+		}
+
+		dwc_otg_hcd_save_data_toggle(hc, hc_regs, qtd);
+		complete_periodic_xfer(hcd, hc, hc_regs, qtd, halt_status);
+		break;
+	case UE_ISOCHRONOUS:
+		DWC_DEBUGPL(DBG_HCDV, "  Isochronous transfer complete\n");
+		if (qtd->isoc_split_pos == DWC_HCSPLIT_XACTPOS_ALL) {
+			halt_status =
+			    update_isoc_urb_state(hcd, hc, hc_regs, qtd,
+						  DWC_OTG_HC_XFER_COMPLETE);
+		}
+		complete_periodic_xfer(hcd, hc, hc_regs, qtd, halt_status);
+		break;
+	}
+
+handle_xfercomp_done:
+	disable_hc_int(hc_regs, xfercompl);
+
+	return 1;
+}
+
+/**
+ * Handles a host channel STALL interrupt. This handler may be called in
+ * either DMA mode or Slave mode.
+ */
+static int32_t handle_hc_stall_intr(dwc_otg_hcd_t * hcd,
+				    dwc_hc_t * hc,
+				    dwc_otg_hc_regs_t * hc_regs,
+				    dwc_otg_qtd_t * qtd)
+{
+	dwc_otg_hcd_urb_t *urb = qtd->urb;
+	int pipe_type = dwc_otg_hcd_get_pipe_type(&urb->pipe_info);
+
+	DWC_DEBUGPL(DBG_HCD, "--Host Channel %d Interrupt: "
+		    "STALL Received--\n", hc->hc_num);
+
+	if (hcd->core_if->dma_desc_enable) {
+		dwc_otg_hcd_complete_xfer_ddma(hcd, hc, hc_regs, DWC_OTG_HC_XFER_STALL);
+		goto handle_stall_done;
+	}
+
+	if (pipe_type == UE_CONTROL) {
+		hcd->fops->complete(hcd, urb->priv, urb, -DWC_E_PIPE);
+	}
+
+	if (pipe_type == UE_BULK || pipe_type == UE_INTERRUPT) {
+		hcd->fops->complete(hcd, urb->priv, urb, -DWC_E_PIPE);
+		/*
+		 * USB protocol requires resetting the data toggle for bulk
+		 * and interrupt endpoints when a CLEAR_FEATURE(ENDPOINT_HALT)
+		 * setup command is issued to the endpoint. Anticipate the
+		 * CLEAR_FEATURE command since a STALL has occurred and reset
+		 * the data toggle now.
+		 */
+		hc->qh->data_toggle = 0;
+	}
+
+	halt_channel(hcd, hc, qtd, DWC_OTG_HC_XFER_STALL);
+
+handle_stall_done:
+	disable_hc_int(hc_regs, stall);
+
+	return 1;
+}
+
+/*
+ * Updates the state of the URB when a transfer has been stopped due to an
+ * abnormal condition before the transfer completes. Modifies the
+ * actual_length field of the URB to reflect the number of bytes that have
+ * actually been transferred via the host channel.
+ */
+static void update_urb_state_xfer_intr(dwc_hc_t * hc,
+				       dwc_otg_hc_regs_t * hc_regs,
+				       dwc_otg_hcd_urb_t * urb,
+				       dwc_otg_qtd_t * qtd,
+				       dwc_otg_halt_status_e halt_status)
+{
+	uint32_t bytes_transferred = get_actual_xfer_length(hc, hc_regs, qtd,
+							    halt_status, NULL);
+	/* non DWORD-aligned buffer case handling. */
+	if (hc->align_buff && bytes_transferred && hc->ep_is_in) {
+		dwc_memcpy(urb->buf + urb->actual_length, hc->qh->dw_align_buf,
+			   bytes_transferred);
+	}
+
+	urb->actual_length += bytes_transferred;
+
+#ifdef DEBUG
+	{
+		hctsiz_data_t hctsiz;
+		hctsiz.d32 = DWC_READ_REG32(&hc_regs->hctsiz);
+		DWC_DEBUGPL(DBG_HCDV, "DWC_otg: %s: %s, channel %d\n",
+			    __func__, (hc->ep_is_in ? "IN" : "OUT"),
+			    hc->hc_num);
+		DWC_DEBUGPL(DBG_HCDV, "  hc->start_pkt_count %d\n",
+			    hc->start_pkt_count);
+		DWC_DEBUGPL(DBG_HCDV, "  hctsiz.pktcnt %d\n", hctsiz.b.pktcnt);
+		DWC_DEBUGPL(DBG_HCDV, "  hc->max_packet %d\n", hc->max_packet);
+		DWC_DEBUGPL(DBG_HCDV, "  bytes_transferred %d\n",
+			    bytes_transferred);
+		DWC_DEBUGPL(DBG_HCDV, "  urb->actual_length %d\n",
+			    urb->actual_length);
+		DWC_DEBUGPL(DBG_HCDV, "  urb->transfer_buffer_length %d\n",
+			    urb->length);
+	}
+#endif
+}
+
+/**
+ * Handles a host channel NAK interrupt. This handler may be called in either
+ * DMA mode or Slave mode.
+ */
+static int32_t handle_hc_nak_intr(dwc_otg_hcd_t * hcd,
+				  dwc_hc_t * hc,
+				  dwc_otg_hc_regs_t * hc_regs,
+				  dwc_otg_qtd_t * qtd)
+{
+	DWC_DEBUGPL(DBG_HCD, "--Host Channel %d Interrupt: "
+		    "NAK Received--\n", hc->hc_num);
+
+	/*
+	 * Handle NAK for IN/OUT SSPLIT/CSPLIT transfers, bulk, control, and
+	 * interrupt.  Re-start the SSPLIT transfer.
+	 */
+	if (hc->do_split) {
+		if (hc->complete_split) {
+			qtd->error_count = 0;
+		}
+		qtd->complete_split = 0;
+		halt_channel(hcd, hc, qtd, DWC_OTG_HC_XFER_NAK);
+		goto handle_nak_done;
+	}
+
+	switch (dwc_otg_hcd_get_pipe_type(&qtd->urb->pipe_info)) {
+	case UE_CONTROL:
+	case UE_BULK:
+		if (hcd->core_if->dma_enable && hc->ep_is_in) {
+			/*
+			 * NAK interrupts are enabled on bulk/control IN
+			 * transfers in DMA mode for the sole purpose of
+			 * resetting the error count after a transaction error
+			 * occurs. The core will continue transferring data.
+			 */
+			qtd->error_count = 0;
+			goto handle_nak_done;
+		}
+
+		/*
+		 * NAK interrupts normally occur during OUT transfers in DMA
+		 * or Slave mode. For IN transfers, more requests will be
+		 * queued as request queue space is available.
+		 */
+		qtd->error_count = 0;
+
+		if (!hc->qh->ping_state) {
+			update_urb_state_xfer_intr(hc, hc_regs,
+						   qtd->urb, qtd,
+						   DWC_OTG_HC_XFER_NAK);
+			dwc_otg_hcd_save_data_toggle(hc, hc_regs, qtd);
+
+			if (hc->speed == DWC_OTG_EP_SPEED_HIGH)
+				hc->qh->ping_state = 1;
+		}
+
+		/*
+		 * Halt the channel so the transfer can be re-started from
+		 * the appropriate point or the PING protocol will
+		 * start/continue.
+		 */
+		halt_channel(hcd, hc, qtd, DWC_OTG_HC_XFER_NAK);
+		break;
+	case UE_INTERRUPT:
+		qtd->error_count = 0;
+		halt_channel(hcd, hc, qtd, DWC_OTG_HC_XFER_NAK);
+		break;
+	case UE_ISOCHRONOUS:
+		/* Should never get called for isochronous transfers. */
+		DWC_ASSERT(1, "NACK interrupt for ISOC transfer\n");
+		break;
+	}
+
+handle_nak_done:
+	disable_hc_int(hc_regs, nak);
+
+	return 1;
+}
+
+/**
+ * Handles a host channel ACK interrupt. This interrupt is enabled when
+ * performing the PING protocol in Slave mode, when errors occur during
+ * either Slave mode or DMA mode, and during Start Split transactions.
+ */
+static int32_t handle_hc_ack_intr(dwc_otg_hcd_t * hcd,
+				  dwc_hc_t * hc,
+				  dwc_otg_hc_regs_t * hc_regs,
+				  dwc_otg_qtd_t * qtd)
+{
+	DWC_DEBUGPL(DBG_HCD, "--Host Channel %d Interrupt: "
+		    "ACK Received--\n", hc->hc_num);
+
+	if (hc->do_split) {
+		/*
+		 * Handle ACK on SSPLIT.
+		 * ACK should not occur in CSPLIT.
+		 */
+		if (!hc->ep_is_in && hc->data_pid_start != DWC_OTG_HC_PID_SETUP) {
+			qtd->ssplit_out_xfer_count = hc->xfer_len;
+		}
+		if (!(hc->ep_type == DWC_OTG_EP_TYPE_ISOC && !hc->ep_is_in)) {
+			/* Don't need complete for isochronous out transfers. */
+			qtd->complete_split = 1;
+		}
+
+		/* ISOC OUT */
+		if (hc->ep_type == DWC_OTG_EP_TYPE_ISOC && !hc->ep_is_in) {
+			switch (hc->xact_pos) {
+			case DWC_HCSPLIT_XACTPOS_ALL:
+				break;
+			case DWC_HCSPLIT_XACTPOS_END:
+				qtd->isoc_split_pos = DWC_HCSPLIT_XACTPOS_ALL;
+				qtd->isoc_split_offset = 0;
+				break;
+			case DWC_HCSPLIT_XACTPOS_BEGIN:
+			case DWC_HCSPLIT_XACTPOS_MID:
+				/*
+				 * For BEGIN or MID, calculate the length for
+				 * the next microframe to determine the correct
+				 * SSPLIT token, either MID or END.
+				 */
+				{
+					struct dwc_otg_hcd_iso_packet_desc
+					*frame_desc;
+
+					frame_desc =
+					    &qtd->urb->
+					    iso_descs[qtd->isoc_frame_index];
+					qtd->isoc_split_offset += 188;
+
+					if ((frame_desc->length -
+					     qtd->isoc_split_offset) <= 188) {
+						qtd->isoc_split_pos =
+						    DWC_HCSPLIT_XACTPOS_END;
+					} else {
+						qtd->isoc_split_pos =
+						    DWC_HCSPLIT_XACTPOS_MID;
+					}
+
+				}
+				break;
+			}
+		} else {
+			halt_channel(hcd, hc, qtd, DWC_OTG_HC_XFER_ACK);
+		}
+	} else {
+		qtd->error_count = 0;
+
+		if (hc->qh->ping_state) {
+			hc->qh->ping_state = 0;
+			/*
+			 * Halt the channel so the transfer can be re-started
+			 * from the appropriate point. This only happens in
+			 * Slave mode. In DMA mode, the ping_state is cleared
+			 * when the transfer is started because the core
+			 * automatically executes the PING, then the transfer.
+			 */
+			halt_channel(hcd, hc, qtd, DWC_OTG_HC_XFER_ACK);
+		}
+	}
+
+	/*
+	 * If the ACK occurred when _not_ in the PING state, let the channel
+	 * continue transferring data after clearing the error count.
+	 */
+
+	disable_hc_int(hc_regs, ack);
+
+	return 1;
+}
+
+/**
+ * Handles a host channel NYET interrupt. This interrupt should only occur on
+ * Bulk and Control OUT endpoints and for complete split transactions. If a
+ * NYET occurs at the same time as a Transfer Complete interrupt, it is
+ * handled in the xfercomp interrupt handler, not here. This handler may be
+ * called in either DMA mode or Slave mode.
+ */
+static int32_t handle_hc_nyet_intr(dwc_otg_hcd_t * hcd,
+				   dwc_hc_t * hc,
+				   dwc_otg_hc_regs_t * hc_regs,
+				   dwc_otg_qtd_t * qtd)
+{
+	DWC_DEBUGPL(DBG_HCD, "--Host Channel %d Interrupt: "
+		    "NYET Received--\n", hc->hc_num);
+
+	/*
+	 * NYET on CSPLIT
+	 * re-do the CSPLIT immediately on non-periodic
+	 */
+	if (hc->do_split && hc->complete_split) {
+		if (hc->ep_is_in && (hc->ep_type == DWC_OTG_EP_TYPE_ISOC)
+		    && hcd->core_if->dma_enable) {
+			qtd->complete_split = 0;
+			qtd->isoc_split_offset = 0;
+			if (++qtd->isoc_frame_index == qtd->urb->packet_count) {
+				hcd->fops->complete(hcd, qtd->urb->priv, qtd->urb, 0);
+				release_channel(hcd, hc, qtd, DWC_OTG_HC_XFER_URB_COMPLETE);
+			}
+			else
+				release_channel(hcd, hc, qtd, DWC_OTG_HC_XFER_NO_HALT_STATUS);
+			goto handle_nyet_done;
+		}
+
+		if (hc->ep_type == DWC_OTG_EP_TYPE_INTR ||
+		    hc->ep_type == DWC_OTG_EP_TYPE_ISOC) {
+			int frnum = dwc_otg_hcd_get_frame_number(hcd);
+
+			if (dwc_full_frame_num(frnum) !=
+			    dwc_full_frame_num(hc->qh->sched_frame)) {
+				/*
+				 * No longer in the same full speed frame.
+				 * Treat this as a transaction error.
+				 */
+#if 0
+				/** @todo Fix system performance so this can
+				 * be treated as an error. Right now complete
+				 * splits cannot be scheduled precisely enough
+				 * due to other system activity, so this error
+				 * occurs regularly in Slave mode.
+				 */
+				qtd->error_count++;
+#endif
+				qtd->complete_split = 0;
+				halt_channel(hcd, hc, qtd,
+					     DWC_OTG_HC_XFER_XACT_ERR);
+				/** @todo add support for isoc release */
+				goto handle_nyet_done;
+			}
+		}
+
+		halt_channel(hcd, hc, qtd, DWC_OTG_HC_XFER_NYET);
+		goto handle_nyet_done;
+	}
+
+	hc->qh->ping_state = 1;
+	qtd->error_count = 0;
+
+	update_urb_state_xfer_intr(hc, hc_regs, qtd->urb, qtd,
+				   DWC_OTG_HC_XFER_NYET);
+	dwc_otg_hcd_save_data_toggle(hc, hc_regs, qtd);
+
+	/*
+	 * Halt the channel and re-start the transfer so the PING
+	 * protocol will start.
+	 */
+	halt_channel(hcd, hc, qtd, DWC_OTG_HC_XFER_NYET);
+
+handle_nyet_done:
+	disable_hc_int(hc_regs, nyet);
+	return 1;
+}
+
+/**
+ * Handles a host channel babble interrupt. This handler may be called in
+ * either DMA mode or Slave mode.
+ */
+static int32_t handle_hc_babble_intr(dwc_otg_hcd_t * hcd,
+				     dwc_hc_t * hc,
+				     dwc_otg_hc_regs_t * hc_regs,
+				     dwc_otg_qtd_t * qtd)
+{
+	DWC_DEBUGPL(DBG_HCD, "--Host Channel %d Interrupt: "
+		    "Babble Error--\n", hc->hc_num);
+
+	if (hcd->core_if->dma_desc_enable) {
+		dwc_otg_hcd_complete_xfer_ddma(hcd, hc, hc_regs,
+					       DWC_OTG_HC_XFER_BABBLE_ERR);
+		goto handle_babble_done;
+	}
+
+	if (hc->ep_type != DWC_OTG_EP_TYPE_ISOC) {
+		hcd->fops->complete(hcd, qtd->urb->priv,
+				    qtd->urb, -DWC_E_OVERFLOW);
+		halt_channel(hcd, hc, qtd, DWC_OTG_HC_XFER_BABBLE_ERR);
+	} else {
+		dwc_otg_halt_status_e halt_status;
+		halt_status = update_isoc_urb_state(hcd, hc, hc_regs, qtd,
+						    DWC_OTG_HC_XFER_BABBLE_ERR);
+		halt_channel(hcd, hc, qtd, halt_status);
+	}
+
+handle_babble_done:
+	disable_hc_int(hc_regs, bblerr);
+	return 1;
+}
+
+/**
+ * Handles a host channel AHB error interrupt. This handler is only called in
+ * DMA mode.
+ */
+static int32_t handle_hc_ahberr_intr(dwc_otg_hcd_t * hcd,
+				     dwc_hc_t * hc,
+				     dwc_otg_hc_regs_t * hc_regs,
+				     dwc_otg_qtd_t * qtd)
+{
+	hcchar_data_t hcchar;
+	hcsplt_data_t hcsplt;
+	hctsiz_data_t hctsiz;
+	uint32_t hcdma;
+	char *pipetype, *speed;
+
+	dwc_otg_hcd_urb_t *urb = qtd->urb;
+
+	DWC_DEBUGPL(DBG_HCD, "--Host Channel %d Interrupt: "
+		    "AHB Error--\n", hc->hc_num);
+
+	hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+	hcsplt.d32 = DWC_READ_REG32(&hc_regs->hcsplt);
+	hctsiz.d32 = DWC_READ_REG32(&hc_regs->hctsiz);
+	hcdma = DWC_READ_REG32(&hc_regs->hcdma);
+
+	DWC_ERROR("AHB ERROR, Channel %d\n", hc->hc_num);
+	DWC_ERROR("  hcchar 0x%08x, hcsplt 0x%08x\n", hcchar.d32, hcsplt.d32);
+	DWC_ERROR("  hctsiz 0x%08x, hcdma 0x%08x\n", hctsiz.d32, hcdma);
+	DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD URB Enqueue\n");
+	DWC_ERROR("  Device address: %d\n",
+		  dwc_otg_hcd_get_dev_addr(&urb->pipe_info));
+	DWC_ERROR("  Endpoint: %d, %s\n",
+		  dwc_otg_hcd_get_ep_num(&urb->pipe_info),
+		  (dwc_otg_hcd_is_pipe_in(&urb->pipe_info) ? "IN" : "OUT"));
+
+	switch (dwc_otg_hcd_get_pipe_type(&urb->pipe_info)) {
+	case UE_CONTROL:
+		pipetype = "CONTROL";
+		break;
+	case UE_BULK:
+		pipetype = "BULK";
+		break;
+	case UE_INTERRUPT:
+		pipetype = "INTERRUPT";
+		break;
+	case UE_ISOCHRONOUS:
+		pipetype = "ISOCHRONOUS";
+		break;
+	default:
+		pipetype = "UNKNOWN";
+		break;
+	}
+
+	DWC_ERROR("  Endpoint type: %s\n", pipetype);
+
+	switch (hc->speed) {
+	case DWC_OTG_EP_SPEED_HIGH:
+		speed = "HIGH";
+		break;
+	case DWC_OTG_EP_SPEED_FULL:
+		speed = "FULL";
+		break;
+	case DWC_OTG_EP_SPEED_LOW:
+		speed = "LOW";
+		break;
+	default:
+		speed = "UNKNOWN";
+		break;
+	};
+
+	DWC_ERROR("  Speed: %s\n", speed);
+
+	DWC_ERROR("  Max packet size: %d\n",
+		  dwc_otg_hcd_get_mps(&urb->pipe_info));
+	DWC_ERROR("  Data buffer length: %d\n", urb->length);
+	DWC_ERROR("  Transfer buffer: %p, Transfer DMA: %p\n",
+		  urb->buf, (void *)urb->dma);
+	DWC_ERROR("  Setup buffer: %p, Setup DMA: %p\n",
+		  urb->setup_packet, (void *)urb->setup_dma);
+	DWC_ERROR("  Interval: %d\n", urb->interval);
+
+	/* Core haltes the channel for Descriptor DMA mode */
+	if (hcd->core_if->dma_desc_enable) {
+		dwc_otg_hcd_complete_xfer_ddma(hcd, hc, hc_regs,
+					       DWC_OTG_HC_XFER_AHB_ERR);
+		goto handle_ahberr_done;
+	}
+
+	hcd->fops->complete(hcd, urb->priv, urb, -DWC_E_IO);
+
+	/*
+	 * Force a channel halt. Don't call halt_channel because that won't
+	 * write to the HCCHARn register in DMA mode to force the halt.
+	 */
+	dwc_otg_hc_halt(hcd->core_if, hc, DWC_OTG_HC_XFER_AHB_ERR);
+handle_ahberr_done:
+	disable_hc_int(hc_regs, ahberr);
+	return 1;
+}
+
+/**
+ * Handles a host channel transaction error interrupt. This handler may be
+ * called in either DMA mode or Slave mode.
+ */
+static int32_t handle_hc_xacterr_intr(dwc_otg_hcd_t * hcd,
+				      dwc_hc_t * hc,
+				      dwc_otg_hc_regs_t * hc_regs,
+				      dwc_otg_qtd_t * qtd)
+{
+	DWC_DEBUGPL(DBG_HCD, "--Host Channel %d Interrupt: "
+		    "Transaction Error--\n", hc->hc_num);
+
+	if (hcd->core_if->dma_desc_enable) {
+		dwc_otg_hcd_complete_xfer_ddma(hcd, hc, hc_regs,
+					       DWC_OTG_HC_XFER_XACT_ERR);
+		goto handle_xacterr_done;
+	}
+
+	if (qtd == NULL)
+		goto handle_xacterr_done;
+
+	if(qtd->urb == NULL)
+		goto handle_xacterr_done;
+
+	if (&qtd->urb->pipe_info == NULL)
+		goto handle_xacterr_done;
+
+	switch (dwc_otg_hcd_get_pipe_type(&qtd->urb->pipe_info)) {
+	case UE_CONTROL:
+	case UE_BULK:
+		qtd->error_count++;
+		if (!hc->qh->ping_state) {
+
+			update_urb_state_xfer_intr(hc, hc_regs,
+						   qtd->urb, qtd,
+						   DWC_OTG_HC_XFER_XACT_ERR);
+			dwc_otg_hcd_save_data_toggle(hc, hc_regs, qtd);
+			if (!hc->ep_is_in && hc->speed == DWC_OTG_EP_SPEED_HIGH) {
+				hc->qh->ping_state = 1;
+			}
+		}
+
+		/*
+		 * Halt the channel so the transfer can be re-started from
+		 * the appropriate point or the PING protocol will start.
+		 */
+		halt_channel(hcd, hc, qtd, DWC_OTG_HC_XFER_XACT_ERR);
+		break;
+	case UE_INTERRUPT:
+		qtd->error_count++;
+		if (hc->do_split && hc->complete_split) {
+			qtd->complete_split = 0;
+		}
+		halt_channel(hcd, hc, qtd, DWC_OTG_HC_XFER_XACT_ERR);
+		break;
+	case UE_ISOCHRONOUS:
+		{
+			dwc_otg_halt_status_e halt_status;
+			halt_status =
+			    update_isoc_urb_state(hcd, hc, hc_regs, qtd,
+						  DWC_OTG_HC_XFER_XACT_ERR);
+
+			halt_channel(hcd, hc, qtd, halt_status);
+		}
+		break;
+	}
+handle_xacterr_done:
+	disable_hc_int(hc_regs, xacterr);
+
+	return 1;
+}
+
+/**
+ * Handles a host channel frame overrun interrupt. This handler may be called
+ * in either DMA mode or Slave mode.
+ */
+static int32_t handle_hc_frmovrun_intr(dwc_otg_hcd_t * hcd,
+				       dwc_hc_t * hc,
+				       dwc_otg_hc_regs_t * hc_regs,
+				       dwc_otg_qtd_t * qtd)
+{
+	DWC_DEBUGPL(DBG_HCD, "--Host Channel %d Interrupt: "
+		    "Frame Overrun--\n", hc->hc_num);
+
+	switch (dwc_otg_hcd_get_pipe_type(&qtd->urb->pipe_info)) {
+	case UE_CONTROL:
+	case UE_BULK:
+		break;
+	case UE_INTERRUPT:
+		halt_channel(hcd, hc, qtd, DWC_OTG_HC_XFER_FRAME_OVERRUN);
+		break;
+	case UE_ISOCHRONOUS:
+		{
+			dwc_otg_halt_status_e halt_status;
+			halt_status =
+			    update_isoc_urb_state(hcd, hc, hc_regs, qtd,
+						  DWC_OTG_HC_XFER_FRAME_OVERRUN);
+
+			halt_channel(hcd, hc, qtd, halt_status);
+		}
+		break;
+	}
+
+	disable_hc_int(hc_regs, frmovrun);
+
+	return 1;
+}
+
+/**
+ * Handles a host channel data toggle error interrupt. This handler may be
+ * called in either DMA mode or Slave mode.
+ */
+static int32_t handle_hc_datatglerr_intr(dwc_otg_hcd_t * hcd,
+					 dwc_hc_t * hc,
+					 dwc_otg_hc_regs_t * hc_regs,
+					 dwc_otg_qtd_t * qtd)
+{
+	DWC_DEBUGPL(DBG_HCD, "--Host Channel %d Interrupt: "
+		    "Data Toggle Error--\n", hc->hc_num);
+
+	if (hc->ep_is_in) {
+		qtd->error_count = 0;
+	} else {
+		DWC_ERROR("Data Toggle Error on OUT transfer,"
+			  "channel %d\n", hc->hc_num);
+	}
+
+	disable_hc_int(hc_regs, datatglerr);
+
+	return 1;
+}
+
+#ifdef DEBUG
+/**
+ * This function is for debug only. It checks that a valid halt status is set
+ * and that HCCHARn.chdis is clear. If there's a problem, corrective action is
+ * taken and a warning is issued.
+ * @return 1 if halt status is ok, 0 otherwise.
+ */
+static inline int halt_status_ok(dwc_otg_hcd_t * hcd,
+				 dwc_hc_t * hc,
+				 dwc_otg_hc_regs_t * hc_regs,
+				 dwc_otg_qtd_t * qtd)
+{
+	hcchar_data_t hcchar;
+	hctsiz_data_t hctsiz;
+	hcint_data_t hcint;
+	hcintmsk_data_t hcintmsk;
+	hcsplt_data_t hcsplt;
+
+	if (hc->halt_status == DWC_OTG_HC_XFER_NO_HALT_STATUS) {
+		/*
+		 * This code is here only as a check. This condition should
+		 * never happen. Ignore the halt if it does occur.
+		 */
+		hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+		hctsiz.d32 = DWC_READ_REG32(&hc_regs->hctsiz);
+		hcint.d32 = DWC_READ_REG32(&hc_regs->hcint);
+		hcintmsk.d32 = DWC_READ_REG32(&hc_regs->hcintmsk);
+		hcsplt.d32 = DWC_READ_REG32(&hc_regs->hcsplt);
+		DWC_WARN
+		    ("%s: hc->halt_status == DWC_OTG_HC_XFER_NO_HALT_STATUS, "
+		     "channel %d, hcchar 0x%08x, hctsiz 0x%08x, "
+		     "hcint 0x%08x, hcintmsk 0x%08x, "
+		     "hcsplt 0x%08x, qtd->complete_split %d\n", __func__,
+		     hc->hc_num, hcchar.d32, hctsiz.d32, hcint.d32,
+		     hcintmsk.d32, hcsplt.d32, qtd->complete_split);
+
+		DWC_WARN("%s: no halt status, channel %d, ignoring interrupt\n",
+			 __func__, hc->hc_num);
+		DWC_WARN("\n");
+		clear_hc_int(hc_regs, chhltd);
+		return 0;
+	}
+
+	/*
+	 * This code is here only as a check. hcchar.chdis should
+	 * never be set when the halt interrupt occurs. Halt the
+	 * channel again if it does occur.
+	 */
+	hcchar.d32 = DWC_READ_REG32(&hc_regs->hcchar);
+	if (hcchar.b.chdis) {
+		DWC_WARN("%s: hcchar.chdis set unexpectedly, "
+			 "hcchar 0x%08x, trying to halt again\n",
+			 __func__, hcchar.d32);
+		clear_hc_int(hc_regs, chhltd);
+		hc->halt_pending = 0;
+		halt_channel(hcd, hc, qtd, hc->halt_status);
+		return 0;
+	}
+
+	return 1;
+}
+#endif
+
+/**
+ * Handles a host Channel Halted interrupt in DMA mode. This handler
+ * determines the reason the channel halted and proceeds accordingly.
+ */
+static void handle_hc_chhltd_intr_dma(dwc_otg_hcd_t * hcd,
+				      dwc_hc_t * hc,
+				      dwc_otg_hc_regs_t * hc_regs,
+				      dwc_otg_qtd_t * qtd)
+{
+	hcint_data_t hcint;
+	hcintmsk_data_t hcintmsk;
+	int out_nak_enh = 0;
+
+	/* For core with OUT NAK enhancement, the flow for high-
+	 * speed CONTROL/BULK OUT is handled a little differently.
+	 */
+	if (hcd->core_if->snpsid >= OTG_CORE_REV_2_71a) {
+		if (hc->speed == DWC_OTG_EP_SPEED_HIGH && !hc->ep_is_in &&
+		    (hc->ep_type == DWC_OTG_EP_TYPE_CONTROL ||
+		     hc->ep_type == DWC_OTG_EP_TYPE_BULK)) {
+			out_nak_enh = 1;
+		}
+	}
+
+	if (hc->halt_status == DWC_OTG_HC_XFER_URB_DEQUEUE ||
+	    (hc->halt_status == DWC_OTG_HC_XFER_AHB_ERR
+	     && !hcd->core_if->dma_desc_enable)) {
+		/*
+		 * Just release the channel. A dequeue can happen on a
+		 * transfer timeout. In the case of an AHB Error, the channel
+		 * was forced to halt because there's no way to gracefully
+		 * recover.
+		 */
+		if (hcd->core_if->dma_desc_enable)
+			dwc_otg_hcd_complete_xfer_ddma(hcd, hc, hc_regs,
+						       hc->halt_status);
+		else
+			release_channel(hcd, hc, qtd, hc->halt_status);
+		return;
+	}
+
+	/* Read the HCINTn register to determine the cause for the halt. */
+	hcint.d32 = DWC_READ_REG32(&hc_regs->hcint);
+	hcintmsk.d32 = DWC_READ_REG32(&hc_regs->hcintmsk);
+
+	if (hcint.b.xfercomp) {
+		/** @todo This is here because of a possible hardware bug.  Spec
+		 * says that on SPLIT-ISOC OUT transfers in DMA mode that a HALT
+		 * interrupt w/ACK bit set should occur, but I only see the
+		 * XFERCOMP bit, even with it masked out.  This is a workaround
+		 * for that behavior.  Should fix this when hardware is fixed.
+		 */
+		if (hc->ep_type == DWC_OTG_EP_TYPE_ISOC && !hc->ep_is_in) {
+			handle_hc_ack_intr(hcd, hc, hc_regs, qtd);
+		}
+		handle_hc_xfercomp_intr(hcd, hc, hc_regs, qtd);
+	} else if (hcint.b.stall) {
+		handle_hc_stall_intr(hcd, hc, hc_regs, qtd);
+	} else if (hcint.b.xacterr && !hcd->core_if->dma_desc_enable) {
+		if (out_nak_enh) {
+			if (hcint.b.nyet || hcint.b.nak || hcint.b.ack) {
+				DWC_DEBUG("XactErr with NYET/NAK/ACK\n");
+				qtd->error_count = 0;
+			} else {
+				DWC_DEBUG("XactErr without NYET/NAK/ACK\n");
+			}
+		}
+
+		/*
+		 * Must handle xacterr before nak or ack. Could get a xacterr
+		 * at the same time as either of these on a BULK/CONTROL OUT
+		 * that started with a PING. The xacterr takes precedence.
+		 */
+		handle_hc_xacterr_intr(hcd, hc, hc_regs, qtd);
+	} else if (hcint.b.xcs_xact && hcd->core_if->dma_desc_enable) {
+		handle_hc_xacterr_intr(hcd, hc, hc_regs, qtd);
+	} else if (hcint.b.ahberr && hcd->core_if->dma_desc_enable) {
+		handle_hc_ahberr_intr(hcd, hc, hc_regs, qtd);
+	} else if (hcint.b.bblerr) {
+		handle_hc_babble_intr(hcd, hc, hc_regs, qtd);
+	} else if (hcint.b.frmovrun) {
+		handle_hc_frmovrun_intr(hcd, hc, hc_regs, qtd);
+	} else if (!out_nak_enh) {
+		if (hcint.b.nyet) {
+			/*
+			 * Must handle nyet before nak or ack. Could get a nyet at the
+			 * same time as either of those on a BULK/CONTROL OUT that
+			 * started with a PING. The nyet takes precedence.
+			 */
+			handle_hc_nyet_intr(hcd, hc, hc_regs, qtd);
+		} else if (hcint.b.nak && !hcintmsk.b.nak) {
+			/*
+			 * If nak is not masked, it's because a non-split IN transfer
+			 * is in an error state. In that case, the nak is handled by
+			 * the nak interrupt handler, not here. Handle nak here for
+			 * BULK/CONTROL OUT transfers, which halt on a NAK to allow
+			 * rewinding the buffer pointer.
+			 */
+			handle_hc_nak_intr(hcd, hc, hc_regs, qtd);
+		} else if (hcint.b.ack && !hcintmsk.b.ack) {
+			/*
+			 * If ack is not masked, it's because a non-split IN transfer
+			 * is in an error state. In that case, the ack is handled by
+			 * the ack interrupt handler, not here. Handle ack here for
+			 * split transfers. Start splits halt on ACK.
+			 */
+			handle_hc_ack_intr(hcd, hc, hc_regs, qtd);
+		} else {
+			if (hc->ep_type == DWC_OTG_EP_TYPE_INTR ||
+			    hc->ep_type == DWC_OTG_EP_TYPE_ISOC) {
+				/*
+				 * A periodic transfer halted with no other channel
+				 * interrupts set. Assume it was halted by the core
+				 * because it could not be completed in its scheduled
+				 * (micro)frame.
+				 */
+#ifdef DEBUG
+				DWC_PRINTF
+				    ("%s: Halt channel %d (assume incomplete periodic transfer)\n",
+				     __func__, hc->hc_num);
+#endif
+				halt_channel(hcd, hc, qtd,
+					     DWC_OTG_HC_XFER_PERIODIC_INCOMPLETE);
+			} else {
+				DWC_ERROR
+				    ("%s: Channel %d, DMA Mode -- ChHltd set, but reason "
+				     "for halting is unknown, hcint 0x%08x, intsts 0x%08x\n",
+				     __func__, hc->hc_num, hcint.d32,
+				     DWC_READ_REG32(&hcd->
+						    core_if->core_global_regs->
+						    gintsts));
+			}
+
+		}
+	} else {
+		DWC_PRINTF("NYET/NAK/ACK/other in non-error case, 0x%08x\n",
+			   hcint.d32);
+	}
+}
+
+/**
+ * Handles a host channel Channel Halted interrupt.
+ *
+ * In slave mode, this handler is called only when the driver specifically
+ * requests a halt. This occurs during handling other host channel interrupts
+ * (e.g. nak, xacterr, stall, nyet, etc.).
+ *
+ * In DMA mode, this is the interrupt that occurs when the core has finished
+ * processing a transfer on a channel. Other host channel interrupts (except
+ * ahberr) are disabled in DMA mode.
+ */
+static int32_t handle_hc_chhltd_intr(dwc_otg_hcd_t * hcd,
+				     dwc_hc_t * hc,
+				     dwc_otg_hc_regs_t * hc_regs,
+				     dwc_otg_qtd_t * qtd)
+{
+	DWC_DEBUGPL(DBG_HCD, "--Host Channel %d Interrupt: "
+		    "Channel Halted--\n", hc->hc_num);
+
+	if (hcd->core_if->dma_enable) {
+		handle_hc_chhltd_intr_dma(hcd, hc, hc_regs, qtd);
+	} else {
+#ifdef DEBUG
+		if (!halt_status_ok(hcd, hc, hc_regs, qtd)) {
+			return 1;
+		}
+#endif
+		release_channel(hcd, hc, qtd, hc->halt_status);
+	}
+
+	return 1;
+}
+
+/** Handles interrupt for a specific Host Channel */
+int32_t dwc_otg_hcd_handle_hc_n_intr(dwc_otg_hcd_t * dwc_otg_hcd, uint32_t num)
+{
+	int retval = 0;
+	hcint_data_t hcint;
+	hcintmsk_data_t hcintmsk;
+	dwc_hc_t *hc;
+	dwc_otg_hc_regs_t *hc_regs;
+	dwc_otg_qtd_t *qtd;
+
+	DWC_DEBUGPL(DBG_HCDV, "--Host Channel Interrupt--, Channel %d\n", num);
+
+	hc = dwc_otg_hcd->hc_ptr_array[num];
+	hc_regs = dwc_otg_hcd->core_if->host_if->hc_regs[num];
+	qtd = DWC_CIRCLEQ_FIRST(&hc->qh->qtd_list);
+
+	hcint.d32 = DWC_READ_REG32(&hc_regs->hcint);
+	hcintmsk.d32 = DWC_READ_REG32(&hc_regs->hcintmsk);
+	DWC_DEBUGPL(DBG_HCDV,
+		    "  hcint 0x%08x, hcintmsk 0x%08x, hcint&hcintmsk 0x%08x\n",
+		    hcint.d32, hcintmsk.d32, (hcint.d32 & hcintmsk.d32));
+	hcint.d32 = hcint.d32 & hcintmsk.d32;
+
+	if (!dwc_otg_hcd->core_if->dma_enable) {
+		if (hcint.b.chhltd && hcint.d32 != 0x2) {
+			hcint.b.chhltd = 0;
+		}
+	}
+
+	if (hcint.b.xfercomp) {
+		retval |=
+		    handle_hc_xfercomp_intr(dwc_otg_hcd, hc, hc_regs, qtd);
+		/*
+		 * If NYET occurred at same time as Xfer Complete, the NYET is
+		 * handled by the Xfer Complete interrupt handler. Don't want
+		 * to call the NYET interrupt handler in this case.
+		 */
+		hcint.b.nyet = 0;
+	}
+	if (hcint.b.chhltd) {
+		retval |= handle_hc_chhltd_intr(dwc_otg_hcd, hc, hc_regs, qtd);
+	}
+	if (hcint.b.ahberr) {
+		retval |= handle_hc_ahberr_intr(dwc_otg_hcd, hc, hc_regs, qtd);
+	}
+	if (hcint.b.stall) {
+		retval |= handle_hc_stall_intr(dwc_otg_hcd, hc, hc_regs, qtd);
+	}
+	if (hcint.b.nak) {
+		retval |= handle_hc_nak_intr(dwc_otg_hcd, hc, hc_regs, qtd);
+	}
+	if (hcint.b.ack) {
+		retval |= handle_hc_ack_intr(dwc_otg_hcd, hc, hc_regs, qtd);
+	}
+	if (hcint.b.nyet) {
+		retval |= handle_hc_nyet_intr(dwc_otg_hcd, hc, hc_regs, qtd);
+	}
+	if (hcint.b.xacterr) {
+		retval |= handle_hc_xacterr_intr(dwc_otg_hcd, hc, hc_regs, qtd);
+	}
+	if (hcint.b.bblerr) {
+		retval |= handle_hc_babble_intr(dwc_otg_hcd, hc, hc_regs, qtd);
+	}
+	if (hcint.b.frmovrun) {
+		retval |=
+		    handle_hc_frmovrun_intr(dwc_otg_hcd, hc, hc_regs, qtd);
+	}
+	if (hcint.b.datatglerr) {
+		retval |=
+		    handle_hc_datatglerr_intr(dwc_otg_hcd, hc, hc_regs, qtd);
+	}
+
+	return retval;
+}
+
+#endif /* DWC_DEVICE_ONLY */
diff --git a/drivers/usb/dwc_otg/dwc_otg_hcd_linux.c b/drivers/usb/dwc_otg/dwc_otg_hcd_linux.c
new file mode 100644
index 0000000..7f4c71a
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_otg_hcd_linux.c
@@ -0,0 +1,885 @@
+/* ==========================================================================
+ * $File: //dwh/usb_iip/dev/software/otg/linux/drivers/dwc_otg_hcd_linux.c $
+ * $Revision: #20 $
+ * $Date: 2011/10/26 $
+ * $Change: 1872981 $
+ *
+ * Synopsys HS OTG Linux Software Driver and documentation (hereinafter,
+ * "Software") is an Unsupported proprietary work of Synopsys, Inc. unless
+ * otherwise expressly agreed to in writing between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product under
+ * any End User Software License Agreement or Agreement for Licensed Product
+ * with Synopsys or any supplement thereto. You are permitted to use and
+ * redistribute this Software in source and binary forms, with or without
+ * modification, provided that redistributions of source code must retain this
+ * notice. You may not view, use, disclose, copy or distribute this file or
+ * any information contained herein except pursuant to this license grant from
+ * Synopsys. If you do not agree with this notice, including the disclaimer
+ * below, then you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS" BASIS
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE HEREBY DISCLAIMED. IN NO EVENT SHALL SYNOPSYS BE LIABLE FOR ANY DIRECT,
+ * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================== */
+#ifndef DWC_DEVICE_ONLY
+
+/**
+ * @file
+ *
+ * This file contains the implementation of the HCD. In Linux, the HCD
+ * implements the hc_driver API.
+ */
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/init.h>
+#include <linux/device.h>
+#include <linux/errno.h>
+#include <linux/list.h>
+#include <linux/interrupt.h>
+#include <linux/string.h>
+#include <linux/dma-mapping.h>
+#include <linux/version.h>
+#include <asm/io.h>
+#include <linux/usb.h>
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,35)
+#include <../drivers/usb/core/hcd.h>
+#else
+#include <linux/usb/hcd.h>
+#endif
+
+#include "dwc_otg_hcd_if.h"
+#include "dwc_otg_dbg.h"
+#include "dwc_otg_driver.h"
+#include "dwc_otg_hcd.h"
+/**
+ * Gets the endpoint number from a _bEndpointAddress argument. The endpoint is
+ * qualified with its direction (possible 32 endpoints per device).
+ */
+#define dwc_ep_addr_to_endpoint(_bEndpointAddress_) ((_bEndpointAddress_ & USB_ENDPOINT_NUMBER_MASK) | \
+						     ((_bEndpointAddress_ & USB_DIR_IN) != 0) << 4)
+
+//static const char dwc_otg_hcd_name[] = "dwc_otg_hcd";
+static const char dwc_otg_hcd_name[] = "dwc_otg";
+
+/** @name Linux HC Driver API Functions */
+/** @{ */
+static int urb_enqueue(struct usb_hcd *hcd,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,28)
+		       struct usb_host_endpoint *ep,
+#endif
+		       struct urb *urb, gfp_t mem_flags);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,28)
+static int urb_dequeue(struct usb_hcd *hcd, struct urb *urb);
+#else
+static int urb_dequeue(struct usb_hcd *hcd, struct urb *urb, int status);
+#endif
+
+static void unmap_urb_for_dma(struct usb_hcd *hcd, struct urb *urb);
+static void endpoint_disable(struct usb_hcd *hcd, struct usb_host_endpoint *ep);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,30)
+static void endpoint_reset(struct usb_hcd *hcd, struct usb_host_endpoint *ep);
+#endif
+static irqreturn_t dwc_otg_hcd_irq(struct usb_hcd *hcd);
+extern int hcd_start(struct usb_hcd *hcd);
+extern void hcd_stop(struct usb_hcd *hcd);
+static int get_frame_number(struct usb_hcd *hcd);
+extern int hub_status_data(struct usb_hcd *hcd, char *buf);
+extern int hub_control(struct usb_hcd *hcd,
+		       u16 typeReq,
+		       u16 wValue, u16 wIndex, char *buf, u16 wLength);
+
+struct wrapper_priv_data {
+	dwc_otg_hcd_t *dwc_otg_hcd;
+};
+
+/** @} */
+
+static struct hc_driver dwc_otg_hc_driver = {
+
+	.description = dwc_otg_hcd_name,
+	.product_desc = "DWC OTG Controller",
+	.hcd_priv_size = sizeof(struct wrapper_priv_data),
+
+	.irq = dwc_otg_hcd_irq,
+
+	.flags = HCD_MEMORY | HCD_USB2,
+
+	//.reset =
+	.start = hcd_start,
+	//.suspend =
+	//.resume =
+	.stop = hcd_stop,
+
+	.urb_enqueue = urb_enqueue,
+	.urb_dequeue = urb_dequeue,
+	.endpoint_disable = endpoint_disable,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,30)
+	.endpoint_reset = endpoint_reset,
+#endif
+	.get_frame_number = get_frame_number,
+
+	.unmap_urb_for_dma = unmap_urb_for_dma,
+	.hub_status_data = hub_status_data,
+	.hub_control = hub_control,
+	//.bus_suspend =
+	//.bus_resume =
+};
+
+/** Gets the dwc_otg_hcd from a struct usb_hcd */
+static inline dwc_otg_hcd_t *hcd_to_dwc_otg_hcd(struct usb_hcd *hcd)
+{
+	struct wrapper_priv_data *p;
+	p = (struct wrapper_priv_data *)(hcd->hcd_priv);
+	return p->dwc_otg_hcd;
+}
+
+/** Gets the struct usb_hcd that contains a dwc_otg_hcd_t. */
+static inline struct usb_hcd *dwc_otg_hcd_to_hcd(dwc_otg_hcd_t * dwc_otg_hcd)
+{
+	return dwc_otg_hcd_get_priv_data(dwc_otg_hcd);
+}
+
+/** Gets the usb_host_endpoint associated with an URB. */
+inline struct usb_host_endpoint *dwc_urb_to_endpoint(struct urb *urb)
+{
+	struct usb_device *dev = urb->dev;
+	int ep_num = usb_pipeendpoint(urb->pipe);
+
+	if (usb_pipein(urb->pipe))
+		return dev->ep_in[ep_num];
+	else
+		return dev->ep_out[ep_num];
+}
+
+static int _disconnect(dwc_otg_hcd_t * hcd)
+{
+	struct usb_hcd *usb_hcd = dwc_otg_hcd_to_hcd(hcd);
+
+	usb_hcd->self.is_b_host = 0;
+	return 0;
+}
+
+static int _start(dwc_otg_hcd_t * hcd)
+{
+	struct usb_hcd *usb_hcd = dwc_otg_hcd_to_hcd(hcd);
+
+	usb_hcd->self.is_b_host = dwc_otg_hcd_is_b_host(hcd);
+	hcd_start(usb_hcd);
+
+	return 0;
+}
+
+static int _hub_info(dwc_otg_hcd_t * hcd, void *urb_handle, uint32_t * hub_addr,
+		     uint32_t * port_addr)
+{
+	struct urb *urb = (struct urb *)urb_handle;
+	if (urb->dev->tt) {
+		*hub_addr = urb->dev->tt->hub->devnum;
+	} else {
+		*hub_addr = 0;
+	}
+	*port_addr = urb->dev->ttport;
+	return 0;
+}
+
+static int _speed(dwc_otg_hcd_t * hcd, void *urb_handle)
+{
+	struct urb *urb = (struct urb *)urb_handle;
+	return urb->dev->speed;
+}
+
+static int _get_b_hnp_enable(dwc_otg_hcd_t * hcd)
+{
+	struct usb_hcd *usb_hcd = dwc_otg_hcd_to_hcd(hcd);
+	return usb_hcd->self.b_hnp_enable;
+}
+
+static void allocate_bus_bandwidth(struct usb_hcd *hcd, uint32_t bw,
+				   struct urb *urb)
+{
+	hcd_to_bus(hcd)->bandwidth_allocated += bw / urb->interval;
+	if (usb_pipetype(urb->pipe) == PIPE_ISOCHRONOUS) {
+		hcd_to_bus(hcd)->bandwidth_isoc_reqs++;
+	} else {
+		hcd_to_bus(hcd)->bandwidth_int_reqs++;
+	}
+}
+
+static void free_bus_bandwidth(struct usb_hcd *hcd, uint32_t bw,
+			       struct urb *urb)
+{
+	hcd_to_bus(hcd)->bandwidth_allocated -= bw / urb->interval;
+	if (usb_pipetype(urb->pipe) == PIPE_ISOCHRONOUS) {
+		hcd_to_bus(hcd)->bandwidth_isoc_reqs--;
+	} else {
+		hcd_to_bus(hcd)->bandwidth_int_reqs--;
+	}
+}
+
+/**
+ * Sets the final status of an URB and returns it to the device driver. Any
+ * required cleanup of the URB is performed.
+ */
+static int _complete(dwc_otg_hcd_t * hcd, void *urb_handle,
+		     dwc_otg_hcd_urb_t * dwc_otg_urb, int32_t status)
+{
+	struct urb *urb = (struct urb *)urb_handle;
+#ifdef DEBUG
+	if (CHK_DEBUG_LEVEL(DBG_HCDV | DBG_HCD_URB)) {
+		DWC_PRINTF("%s: urb %p, device %d, ep %d %s, status=%d\n",
+			   __func__, urb, usb_pipedevice(urb->pipe),
+			   usb_pipeendpoint(urb->pipe),
+			   usb_pipein(urb->pipe) ? "IN" : "OUT", status);
+		if (usb_pipetype(urb->pipe) == PIPE_ISOCHRONOUS) {
+			int i;
+			for (i = 0; i < urb->number_of_packets; i++) {
+				DWC_PRINTF("  ISO Desc %d status: %d\n",
+					   i, urb->iso_frame_desc[i].status);
+			}
+		}
+	}
+#endif
+
+	urb->actual_length = dwc_otg_hcd_urb_get_actual_length(dwc_otg_urb);
+	/* Convert status value. */
+	switch (status) {
+	case -DWC_E_PROTOCOL:
+		status = -EPROTO;
+		break;
+	case -DWC_E_IN_PROGRESS:
+		status = -EINPROGRESS;
+		break;
+	case -DWC_E_PIPE:
+		status = -EPIPE;
+		break;
+	case -DWC_E_IO:
+		status = -EIO;
+		break;
+	case -DWC_E_TIMEOUT:
+		status = -ETIMEDOUT;
+		break;
+	case -DWC_E_OVERFLOW:
+		status = -EOVERFLOW;
+		break;
+	default:
+		if (status) {
+			DWC_PRINTF("Uknown urb status %d\n", status);
+
+		}
+	}
+
+	if (usb_pipetype(urb->pipe) == PIPE_ISOCHRONOUS) {
+		int i;
+
+		urb->error_count = dwc_otg_hcd_urb_get_error_count(dwc_otg_urb);
+		for (i = 0; i < urb->number_of_packets; ++i) {
+			urb->iso_frame_desc[i].actual_length =
+			    dwc_otg_hcd_urb_get_iso_desc_actual_length
+			    (dwc_otg_urb, i);
+			urb->iso_frame_desc[i].status =
+			    dwc_otg_hcd_urb_get_iso_desc_status(dwc_otg_urb, i);
+		}
+	}
+
+	urb->status = status;
+	urb->hcpriv = NULL;
+	if (!status) {
+		if ((urb->transfer_flags & URB_SHORT_NOT_OK) &&
+		    (urb->actual_length < urb->transfer_buffer_length)) {
+			urb->status = -EREMOTEIO;
+		}
+	}
+
+	if ((usb_pipetype(urb->pipe) == PIPE_ISOCHRONOUS) ||
+	    (usb_pipetype(urb->pipe) == PIPE_INTERRUPT)) {
+		struct usb_host_endpoint *ep = dwc_urb_to_endpoint(urb);
+		if (ep) {
+			free_bus_bandwidth(dwc_otg_hcd_to_hcd(hcd),
+					   dwc_otg_hcd_get_ep_bandwidth(hcd,
+									ep->hcpriv),
+					   urb);
+		}
+	}
+
+	DWC_FREE(dwc_otg_urb);
+
+	DWC_SPINUNLOCK(hcd->lock);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,28)
+	usb_hcd_giveback_urb(dwc_otg_hcd_to_hcd(hcd), urb);
+#else
+	usb_hcd_giveback_urb(dwc_otg_hcd_to_hcd(hcd), urb, status);
+#endif
+	DWC_SPINLOCK(hcd->lock);
+
+	return 0;
+}
+
+static struct dwc_otg_hcd_function_ops hcd_fops = {
+	.start = _start,
+	.disconnect = _disconnect,
+	.hub_info = _hub_info,
+	.speed = _speed,
+	.complete = _complete,
+	.get_b_hnp_enable = _get_b_hnp_enable,
+};
+
+
+/*
+ * Controller Port Suspend Routiene.
+ */
+void dwc_otg_host_port_suspend(struct usb_hcd *hcd)
+{
+	dwc_otg_host_if_t host_if;
+	hprt0_data_t hprt;
+
+	host_if.hprt0 = hcd->regs + DWC_OTG_HOST_PORT_REGS_OFFSET;
+
+	hprt.d32= DWC_READ_REG32(host_if.hprt0);
+	hprt.b.prtsusp = 1;
+	DWC_WRITE_REG32(host_if.hprt0, hprt.d32);
+}
+
+
+/*
+ * Controller Port Resume Routiene.
+ */
+void dwc_otg_host_port_resume(struct usb_hcd *hcd)
+{
+	dwc_otg_host_if_t host_if;
+	hprt0_data_t hprt;
+
+	host_if.hprt0 = hcd->regs + DWC_OTG_HOST_PORT_REGS_OFFSET;
+
+	hprt.d32= DWC_READ_REG32(host_if.hprt0);
+	hprt.b.prtsusp = 0;
+	DWC_WRITE_REG32(host_if.hprt0, hprt.d32);
+}
+
+
+/**
+ * Initializes the HCD. This function allocates memory for and initializes the
+ * static parts of the usb_hcd and dwc_otg_hcd structures. It also registers the
+ * USB bus with the core and calls the hc_driver->start() function. It returns
+ * a negative error on failure.
+ */
+int hcd_init(
+#ifdef LM_INTERFACE
+		    struct lm_device *_dev
+#elif  defined(PCI_INTERFACE)
+		    struct pci_dev *_dev
+#else
+			struct platform_device *_dev
+#endif
+    )
+{
+	struct usb_hcd *hcd = NULL;
+	dwc_otg_hcd_t *dwc_otg_hcd = NULL;
+#ifdef LM_INTERFACE
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(_dev);
+#elif  defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = pci_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = platform_get_drvdata(_dev);
+#endif
+
+	int retval = 0;
+
+	DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD INIT\n");
+
+	/* Set device flags indicating whether the HCD supports DMA. */
+	if (dwc_otg_is_dma_enable(otg_dev->core_if)) {
+#ifdef LM_INTERFACE
+		_dev->dev.dma_mask = (void *)~0;
+		_dev->dev.coherent_dma_mask = ~0;
+#elif  defined(PCI_INTERFACE)
+		pci_set_dma_mask(_dev, DMA_32BIT_MASK);
+		pci_set_consistent_dma_mask(_dev, DMA_32BIT_MASK);
+#endif
+
+	} else {
+#ifdef LM_INTERFACE
+		_dev->dev.dma_mask = (void *)0;
+		_dev->dev.coherent_dma_mask = 0;
+#elif  defined(PCI_INTERFACE)
+		pci_set_dma_mask(_dev, 0);
+		pci_set_consistent_dma_mask(_dev, 0);
+#endif
+	}
+
+	/*
+	 * Allocate memory for the base HCD plus the DWC OTG HCD.
+	 * Initialize the base HCD.
+	 */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,30)
+	hcd = usb_create_hcd(&dwc_otg_hc_driver, &_dev->dev, _dev->dev.bus_id);
+#else
+	hcd = usb_create_hcd(&dwc_otg_hc_driver, &_dev->dev, dev_name(&_dev->dev));
+	hcd->has_tt = 1;
+//      hcd->uses_new_polling = 1;
+//      hcd->poll_rh = 0;
+#endif
+	if (!hcd) {
+		retval = -ENOMEM;
+		goto error1;
+	}
+
+	hcd->regs = otg_dev->os_dep.base;
+
+	/* Initialize the DWC OTG HCD. */
+	dwc_otg_hcd = dwc_otg_hcd_alloc_hcd();
+	if (!dwc_otg_hcd) {
+		goto error2;
+	}
+	((struct wrapper_priv_data *)(hcd->hcd_priv))->dwc_otg_hcd =
+	    dwc_otg_hcd;
+	otg_dev->hcd = dwc_otg_hcd;
+
+	if (dwc_otg_hcd_init(dwc_otg_hcd, otg_dev->core_if)) {
+		goto error2;
+	}
+
+	otg_dev->hcd->otg_dev = otg_dev;
+	hcd->self.otg_port = dwc_otg_hcd_otg_port(dwc_otg_hcd);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,33) //don't support for LM(with 2.6.20.1 kernel)
+	//hcd->self.otg_version = dwc_otg_get_otg_version(otg_dev->core_if); // Makarand
+	/* Don't support SG list at this point */
+	hcd->self.sg_tablesize = 0;
+#endif
+	/*
+	 * Finish generic HCD initialization and start the HCD. This function
+	 * allocates the DMA buffer pool, registers the USB bus, requests the
+	 * IRQ line, and calls hcd_start method.
+	 */
+	retval = usb_add_hcd(hcd, otg_dev->irq, IRQF_SHARED | IRQF_DISABLED);
+	if (retval < 0) {
+		goto error2;
+	}
+
+	dwc_otg_hcd_set_priv_data(dwc_otg_hcd, hcd);
+	return 0;
+
+error2:
+	usb_put_hcd(hcd);
+error1:
+	return retval;
+}
+
+/**
+ * Removes the HCD.
+ * Frees memory and resources associated with the HCD and deregisters the bus.
+ */
+void hcd_remove(
+#ifdef LM_INTERFACE
+		       struct lm_device *_dev
+#elif  defined(PCI_INTERFACE)
+		       struct pci_dev *_dev
+#else
+				struct platform_device *_dev
+#endif
+    )
+{
+#ifdef LM_INTERFACE
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(_dev);
+#elif  defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = pci_get_drvdata(_dev);
+#else
+	dwc_otg_device_t *otg_dev = platform_get_drvdata(_dev);
+#endif
+
+	dwc_otg_hcd_t *dwc_otg_hcd;
+	struct usb_hcd *hcd;
+
+	DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD REMOVE\n");
+
+	if (!otg_dev) {
+		DWC_DEBUGPL(DBG_ANY, "%s: otg_dev NULL!\n", __func__);
+		return;
+	}
+
+	dwc_otg_hcd = otg_dev->hcd;
+
+	if (!dwc_otg_hcd) {
+		DWC_DEBUGPL(DBG_ANY, "%s: otg_dev->hcd NULL!\n", __func__);
+		return;
+	}
+
+	hcd = dwc_otg_hcd_to_hcd(dwc_otg_hcd);
+
+	if (!hcd) {
+		DWC_DEBUGPL(DBG_ANY,
+			    "%s: dwc_otg_hcd_to_hcd(dwc_otg_hcd) NULL!\n",
+			    __func__);
+		return;
+	}
+	usb_remove_hcd(hcd);
+	dwc_otg_hcd_set_priv_data(dwc_otg_hcd, NULL);
+	dwc_otg_hcd_remove(dwc_otg_hcd);
+	usb_put_hcd(hcd);
+}
+
+/* =========================================================================
+ *  Linux HC Driver Functions
+ * ========================================================================= */
+
+/** Initializes the DWC_otg controller and its root hub and prepares it for host
+ * mode operation. Activates the root port. Returns 0 on success and a negative
+ * error code on failure. */
+int hcd_start(struct usb_hcd *hcd)
+{
+	dwc_otg_hcd_t *dwc_otg_hcd = hcd_to_dwc_otg_hcd(hcd);
+	struct usb_bus *bus;
+
+	DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD START\n");
+	bus = hcd_to_bus(hcd);
+
+	hcd->state = HC_STATE_RUNNING;
+	if (dwc_otg_hcd_start(dwc_otg_hcd, &hcd_fops)) {
+		return 0;
+	}
+
+	/* Initialize and connect root hub if one is not already attached */
+	if (bus->root_hub) {
+		DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD Has Root Hub\n");
+		/* Inform the HUB driver to resume. */
+		usb_hcd_resume_root_hub(hcd);
+	}
+
+	return 0;
+}
+
+/**
+ * Halts the DWC_otg host mode operations in a clean manner. USB transfers are
+ * stopped.
+ */
+void hcd_stop(struct usb_hcd *hcd)
+{
+	dwc_otg_hcd_t *dwc_otg_hcd = hcd_to_dwc_otg_hcd(hcd);
+
+	dwc_otg_hcd_stop(dwc_otg_hcd);
+}
+
+/** Returns the current frame number. */
+static int get_frame_number(struct usb_hcd *hcd)
+{
+	dwc_otg_hcd_t *dwc_otg_hcd = hcd_to_dwc_otg_hcd(hcd);
+
+	return dwc_otg_hcd_get_frame_number(dwc_otg_hcd);
+}
+
+#ifdef DEBUG
+static void dump_urb_info(struct urb *urb, char *fn_name)
+{
+	DWC_PRINTF("%s, urb %p\n", fn_name, urb);
+	DWC_PRINTF("  Device address: %d\n", usb_pipedevice(urb->pipe));
+	DWC_PRINTF("  Endpoint: %d, %s\n", usb_pipeendpoint(urb->pipe),
+		   (usb_pipein(urb->pipe) ? "IN" : "OUT"));
+	DWC_PRINTF("  Endpoint type: %s\n", ( {
+					     char *pipetype;
+					     switch (usb_pipetype(urb->pipe)) {
+case PIPE_CONTROL:
+pipetype = "CONTROL"; break; case PIPE_BULK:
+pipetype = "BULK"; break; case PIPE_INTERRUPT:
+pipetype = "INTERRUPT"; break; case PIPE_ISOCHRONOUS:
+pipetype = "ISOCHRONOUS"; break; default:
+					     pipetype = "UNKNOWN"; break;};
+					     pipetype;}
+		   )) ;
+	DWC_PRINTF("  Speed: %s\n", ( {
+				     char *speed; switch (urb->dev->speed) {
+case USB_SPEED_HIGH:
+speed = "HIGH"; break; case USB_SPEED_FULL:
+speed = "FULL"; break; case USB_SPEED_LOW:
+speed = "LOW"; break; default:
+				     speed = "UNKNOWN"; break;};
+				     speed;}
+		   )) ;
+	DWC_PRINTF("  Max packet size: %d\n",
+		   usb_maxpacket(urb->dev, urb->pipe, usb_pipeout(urb->pipe)));
+	DWC_PRINTF("  Data buffer length: %d\n", urb->transfer_buffer_length);
+	DWC_PRINTF("  Transfer buffer: %p, Transfer DMA: %p\n",
+		   urb->transfer_buffer, (void *)urb->transfer_dma);
+	DWC_PRINTF("  Setup buffer: %p, Setup DMA: %p\n",
+		   urb->setup_packet, (void *)urb->setup_dma);
+	DWC_PRINTF("  Interval: %d\n", urb->interval);
+	if (usb_pipetype(urb->pipe) == PIPE_ISOCHRONOUS) {
+		int i;
+		for (i = 0; i < urb->number_of_packets; i++) {
+			DWC_PRINTF("  ISO Desc %d:\n", i);
+			DWC_PRINTF("    offset: %d, length %d\n",
+				   urb->iso_frame_desc[i].offset,
+				   urb->iso_frame_desc[i].length);
+		}
+	}
+}
+
+#endif
+
+/** Starts processing a USB transfer request specified by a USB Request Block
+ * (URB). mem_flags indicates the type of memory allocation to use while
+ * processing this URB. */
+static int urb_enqueue(struct usb_hcd *hcd,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,28)
+		       struct usb_host_endpoint *ep,
+#endif
+		       struct urb *urb, gfp_t mem_flags)
+{
+	int retval = 0;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,28)
+	struct usb_host_endpoint *ep = urb->ep;
+#endif
+	dwc_otg_hcd_t *dwc_otg_hcd = hcd_to_dwc_otg_hcd(hcd);
+	dwc_otg_hcd_urb_t *dwc_otg_urb;
+	int i;
+	int alloc_bandwidth = 0;
+	uint8_t ep_type = 0;
+	uint32_t flags = 0;
+	void *buf;
+
+#ifdef DEBUG
+	if (CHK_DEBUG_LEVEL(DBG_HCDV | DBG_HCD_URB)) {
+		dump_urb_info(urb, "urb_enqueue");
+	}
+#endif
+
+	if ((usb_pipetype(urb->pipe) == PIPE_ISOCHRONOUS)
+	    || (usb_pipetype(urb->pipe) == PIPE_INTERRUPT)) {
+		if (!dwc_otg_hcd_is_bandwidth_allocated
+		    (dwc_otg_hcd, &ep->hcpriv)) {
+			alloc_bandwidth = 1;
+		}
+	}
+
+	switch (usb_pipetype(urb->pipe)) {
+	case PIPE_CONTROL:
+		ep_type = USB_ENDPOINT_XFER_CONTROL;
+		break;
+	case PIPE_ISOCHRONOUS:
+		ep_type = USB_ENDPOINT_XFER_ISOC;
+		break;
+	case PIPE_BULK:
+		ep_type = USB_ENDPOINT_XFER_BULK;
+		break;
+	case PIPE_INTERRUPT:
+		ep_type = USB_ENDPOINT_XFER_INT;
+		break;
+	default:
+		DWC_WARN("Wrong ep type\n");
+	}
+
+	dwc_otg_urb = dwc_otg_hcd_urb_alloc(dwc_otg_hcd,
+					    urb->number_of_packets,
+					    mem_flags == GFP_ATOMIC ? 1 : 0);
+
+	dwc_otg_hcd_urb_set_pipeinfo(dwc_otg_urb, usb_pipedevice(urb->pipe),
+				     usb_pipeendpoint(urb->pipe), ep_type,
+				     usb_pipein(urb->pipe),
+				     usb_maxpacket(urb->dev, urb->pipe,
+						   !(usb_pipein(urb->pipe))));
+
+	buf = urb->transfer_buffer;
+	if (hcd->self.uses_dma) {
+		/*
+		 * Calculate virtual address from physical address,
+		 * because some class driver may not fill transfer_buffer.
+		 * In Buffer DMA mode virual address is used,
+		 * when handling non DWORD aligned buffers.
+		 */
+		buf = phys_to_virt(urb->transfer_dma);
+	}
+
+	if (!(urb->transfer_flags & URB_NO_INTERRUPT))
+		flags |= URB_GIVEBACK_ASAP;
+	if (urb->transfer_flags & URB_ZERO_PACKET)
+		flags |= URB_SEND_ZERO_PACKET;
+
+	dwc_otg_hcd_urb_set_params(dwc_otg_urb, urb, buf,
+				   urb->transfer_dma,
+				   urb->transfer_buffer_length,
+				   urb->setup_packet,
+				   urb->setup_dma, flags, urb->interval);
+
+	for (i = 0; i < urb->number_of_packets; ++i) {
+		dwc_otg_hcd_urb_set_iso_desc_params(dwc_otg_urb, i,
+						    urb->
+						    iso_frame_desc[i].offset,
+						    urb->
+						    iso_frame_desc[i].length);
+	}
+
+	urb->hcpriv = dwc_otg_urb;
+	retval = dwc_otg_hcd_urb_enqueue(dwc_otg_hcd, dwc_otg_urb, &ep->hcpriv,
+					 mem_flags == GFP_ATOMIC ? 1 : 0);
+	if (!retval) {
+		if (alloc_bandwidth) {
+			allocate_bus_bandwidth(hcd,
+					       dwc_otg_hcd_get_ep_bandwidth
+					       (dwc_otg_hcd, ep->hcpriv), urb);
+		}
+	} else {
+		if (retval == -DWC_E_NO_DEVICE) {
+			retval = -ENODEV;
+		}
+	}
+
+	return retval;
+}
+
+static void unmap_urb_for_dma(struct usb_hcd *hcd, struct urb *urb)
+{
+	return;
+}
+
+/** Aborts/cancels a USB transfer request. Always returns 0 to indicate
+ * success.  */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,28)
+static int urb_dequeue(struct usb_hcd *hcd, struct urb *urb)
+#else
+static int urb_dequeue(struct usb_hcd *hcd, struct urb *urb, int status)
+#endif
+{
+	dwc_irqflags_t flags;
+	dwc_otg_hcd_t *dwc_otg_hcd;
+	DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD URB Dequeue\n");
+
+	dwc_otg_hcd = hcd_to_dwc_otg_hcd(hcd);
+
+#ifdef DEBUG
+	if (CHK_DEBUG_LEVEL(DBG_HCDV | DBG_HCD_URB)) {
+		dump_urb_info(urb, "urb_dequeue");
+	}
+#endif
+
+	DWC_SPINLOCK_IRQSAVE(dwc_otg_hcd->lock, &flags);
+
+	dwc_otg_hcd_urb_dequeue(dwc_otg_hcd, urb->hcpriv);
+
+	DWC_FREE(urb->hcpriv);
+	urb->hcpriv = NULL;
+	DWC_SPINUNLOCK_IRQRESTORE(dwc_otg_hcd->lock, flags);
+
+	/* Higher layer software sets URB status. */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,28)
+	usb_hcd_giveback_urb(hcd, urb);
+#else
+	usb_hcd_giveback_urb(hcd, urb, status);
+#endif
+	if (CHK_DEBUG_LEVEL(DBG_HCDV | DBG_HCD_URB)) {
+		DWC_PRINTF("Called usb_hcd_giveback_urb()\n");
+		DWC_PRINTF("  urb->status = %d\n", urb->status);
+	}
+
+	return 0;
+}
+
+/* Frees resources in the DWC_otg controller related to a given endpoint. Also
+ * clears state in the HCD related to the endpoint. Any URBs for the endpoint
+ * must already be dequeued. */
+static void endpoint_disable(struct usb_hcd *hcd, struct usb_host_endpoint *ep)
+{
+	dwc_otg_hcd_t *dwc_otg_hcd = hcd_to_dwc_otg_hcd(hcd);
+
+	DWC_DEBUGPL(DBG_HCD,
+		    "DWC OTG HCD EP DISABLE: _bEndpointAddress=0x%02x, "
+		    "endpoint=%d\n", ep->desc.bEndpointAddress,
+		    dwc_ep_addr_to_endpoint(ep->desc.bEndpointAddress));
+	dwc_otg_hcd_endpoint_disable(dwc_otg_hcd, ep->hcpriv, 250);
+	ep->hcpriv = NULL;
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,30)
+/* Resets endpoint specific parameter values, in current version used to reset
+ * the data toggle(as a WA). This function can be called from usb_clear_halt routine */
+static void endpoint_reset(struct usb_hcd *hcd, struct usb_host_endpoint *ep)
+{
+	dwc_irqflags_t flags;
+	struct usb_device *udev = NULL;
+	int epnum = usb_endpoint_num(&ep->desc);
+	int is_out = usb_endpoint_dir_out(&ep->desc);
+	int is_control = usb_endpoint_xfer_control(&ep->desc);
+	dwc_otg_hcd_t *dwc_otg_hcd = hcd_to_dwc_otg_hcd(hcd);
+#ifdef LM_INTERFACE
+	struct lm_device *_dev = dwc_otg_hcd->otg_dev->os_dep.lmdev;
+#elif defined(PCI_INTERFACE)
+	struct pci_dev *_dev = dwc_otg_hcd->otg_dev->os_dep.pcidev;
+#else
+	struct device *_dev = dwc_otg_hcd->otg_dev->os_dep.parent;
+#endif
+
+	if (_dev)
+		//udev = to_usb_device(&_dev->dev); // Makarand
+		udev = to_usb_device(_dev);
+	else
+		return;
+
+	DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD EP RESET: Endpoint Num=0x%02d\n", epnum);
+
+	DWC_SPINLOCK_IRQSAVE(dwc_otg_hcd->lock, &flags);
+	usb_settoggle(udev, epnum, is_out, 0);
+	if (is_control)
+		usb_settoggle(udev, epnum, !is_out, 0);
+
+	if (ep->hcpriv) {
+		dwc_otg_hcd_endpoint_reset(dwc_otg_hcd, ep->hcpriv);
+	}
+	DWC_SPINUNLOCK_IRQRESTORE(dwc_otg_hcd->lock, flags);
+}
+#endif
+
+/** Handles host mode interrupts for the DWC_otg controller. Returns IRQ_NONE if
+ * there was no interrupt to handle. Returns IRQ_HANDLED if there was a valid
+ * interrupt.
+ *
+ * This function is called by the USB core when an interrupt occurs */
+static irqreturn_t dwc_otg_hcd_irq(struct usb_hcd *hcd)
+{
+	dwc_otg_hcd_t *dwc_otg_hcd = hcd_to_dwc_otg_hcd(hcd);
+	int32_t retval = dwc_otg_hcd_handle_intr(dwc_otg_hcd);
+	if (retval != 0) {
+		S3C2410X_CLEAR_EINTPEND();
+	}
+	return IRQ_RETVAL(retval);
+}
+
+/** Creates Status Change bitmap for the root hub and root port. The bitmap is
+ * returned in buf. Bit 0 is the status change indicator for the root hub. Bit 1
+ * is the status change indicator for the single root port. Returns 1 if either
+ * change indicator is 1, otherwise returns 0. */
+int hub_status_data(struct usb_hcd *hcd, char *buf)
+{
+	dwc_otg_hcd_t *dwc_otg_hcd = hcd_to_dwc_otg_hcd(hcd);
+
+	buf[0] = 0;
+	buf[0] |= (dwc_otg_hcd_is_status_changed(dwc_otg_hcd, 1)) << 1;
+
+	return (buf[0] != 0);
+}
+
+/** Handles hub class-specific requests. */
+int hub_control(struct usb_hcd *hcd,
+		u16 typeReq, u16 wValue, u16 wIndex, char *buf, u16 wLength)
+{
+	int retval;
+
+	retval = dwc_otg_hcd_hub_control(hcd_to_dwc_otg_hcd(hcd),
+					 typeReq, wValue, wIndex, buf, wLength);
+
+	switch (retval) {
+	case -DWC_E_INVALID:
+		retval = -EINVAL;
+		break;
+	}
+
+	return retval;
+}
+
+#endif /* DWC_DEVICE_ONLY */
diff --git a/drivers/usb/dwc_otg/dwc_otg_hcd_queue.c b/drivers/usb/dwc_otg/dwc_otg_hcd_queue.c
new file mode 100644
index 0000000..903a902
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_otg_hcd_queue.c
@@ -0,0 +1,726 @@
+/* ==========================================================================
+ * $File: //dwh/usb_iip/dev/software/otg/linux/drivers/dwc_otg_hcd_queue.c $
+ * $Revision: #44 $
+ * $Date: 2011/10/26 $
+ * $Change: 1873028 $
+ *
+ * Synopsys HS OTG Linux Software Driver and documentation (hereinafter,
+ * "Software") is an Unsupported proprietary work of Synopsys, Inc. unless
+ * otherwise expressly agreed to in writing between Synopsys and you.
+ * 
+ * The Software IS NOT an item of Licensed Software or Licensed Product under
+ * any End User Software License Agreement or Agreement for Licensed Product
+ * with Synopsys or any supplement thereto. You are permitted to use and
+ * redistribute this Software in source and binary forms, with or without
+ * modification, provided that redistributions of source code must retain this
+ * notice. You may not view, use, disclose, copy or distribute this file or
+ * any information contained herein except pursuant to this license grant from
+ * Synopsys. If you do not agree with this notice, including the disclaimer
+ * below, then you are not authorized to use the Software.
+ * 
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS" BASIS
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE HEREBY DISCLAIMED. IN NO EVENT SHALL SYNOPSYS BE LIABLE FOR ANY DIRECT,
+ * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================== */
+#ifndef DWC_DEVICE_ONLY
+
+/**
+ * @file
+ *
+ * This file contains the functions to manage Queue Heads and Queue
+ * Transfer Descriptors.
+ */
+
+#include "dwc_otg_hcd.h"
+#include "dwc_otg_regs.h"
+
+/** 
+ * Free each QTD in the QH's QTD-list then free the QH.  QH should already be
+ * removed from a list.  QTD list should already be empty if called from URB
+ * Dequeue.
+ *
+ * @param hcd HCD instance.
+ * @param qh The QH to free.
+ */
+void dwc_otg_hcd_qh_free(dwc_otg_hcd_t * hcd, dwc_otg_qh_t * qh)
+{
+	dwc_otg_qtd_t *qtd, *qtd_tmp;
+
+	/* Free each QTD in the QTD list */
+	DWC_SPINLOCK(hcd->lock);
+	DWC_CIRCLEQ_FOREACH_SAFE(qtd, qtd_tmp, &qh->qtd_list, qtd_list_entry) {
+		DWC_CIRCLEQ_REMOVE(&qh->qtd_list, qtd, qtd_list_entry);
+		dwc_otg_hcd_qtd_free(qtd);
+	}
+
+	if (hcd->core_if->dma_desc_enable) {
+		dwc_otg_hcd_qh_free_ddma(hcd, qh);
+	} else if (qh->dw_align_buf) {
+		uint32_t buf_size;
+		if (qh->ep_type == UE_ISOCHRONOUS) {
+			buf_size = 4096;
+		} else {
+			buf_size = hcd->core_if->core_params->max_transfer_size;
+		}
+		DWC_DMA_FREE(buf_size, qh->dw_align_buf, qh->dw_align_buf_dma);
+	}
+
+	DWC_FREE(qh);
+	DWC_SPINUNLOCK(hcd->lock);
+	return;
+}
+
+#define BitStuffTime(bytecount)  ((8 * 7* bytecount) / 6)
+#define HS_HOST_DELAY		5	/* nanoseconds */
+#define FS_LS_HOST_DELAY	1000	/* nanoseconds */
+#define HUB_LS_SETUP		333	/* nanoseconds */
+#define NS_TO_US(ns)		((ns + 500) / 1000)
+				/* convert & round nanoseconds to microseconds */
+
+static uint32_t calc_bus_time(int speed, int is_in, int is_isoc, int bytecount)
+{
+	unsigned long retval;
+
+	switch (speed) {
+	case USB_SPEED_HIGH:
+		if (is_isoc) {
+			retval =
+			    ((38 * 8 * 2083) +
+			     (2083 * (3 + BitStuffTime(bytecount)))) / 1000 +
+			    HS_HOST_DELAY;
+		} else {
+			retval =
+			    ((55 * 8 * 2083) +
+			     (2083 * (3 + BitStuffTime(bytecount)))) / 1000 +
+			    HS_HOST_DELAY;
+		}
+		break;
+	case USB_SPEED_FULL:
+		if (is_isoc) {
+			retval =
+			    (8354 * (31 + 10 * BitStuffTime(bytecount))) / 1000;
+			if (is_in) {
+				retval = 7268 + FS_LS_HOST_DELAY + retval;
+			} else {
+				retval = 6265 + FS_LS_HOST_DELAY + retval;
+			}
+		} else {
+			retval =
+			    (8354 * (31 + 10 * BitStuffTime(bytecount))) / 1000;
+			retval = 9107 + FS_LS_HOST_DELAY + retval;
+		}
+		break;
+	case USB_SPEED_LOW:
+		if (is_in) {
+			retval =
+			    (67667 * (31 + 10 * BitStuffTime(bytecount))) /
+			    1000;
+			retval =
+			    64060 + (2 * HUB_LS_SETUP) + FS_LS_HOST_DELAY +
+			    retval;
+		} else {
+			retval =
+			    (66700 * (31 + 10 * BitStuffTime(bytecount))) /
+			    1000;
+			retval =
+			    64107 + (2 * HUB_LS_SETUP) + FS_LS_HOST_DELAY +
+			    retval;
+		}
+		break;
+	default:
+		DWC_WARN("Unknown device speed\n");
+		retval = -1;
+	}
+
+	return NS_TO_US(retval);
+}
+
+/** 
+ * Initializes a QH structure.
+ *
+ * @param hcd The HCD state structure for the DWC OTG controller.
+ * @param qh  The QH to init.
+ * @param urb Holds the information about the device/endpoint that we need
+ * 	      to initialize the QH. 
+ */
+#define SCHEDULE_SLOP 10
+void qh_init(dwc_otg_hcd_t * hcd, dwc_otg_qh_t * qh, dwc_otg_hcd_urb_t * urb)
+{
+	char *speed, *type;
+	int dev_speed;
+	uint32_t hub_addr, hub_port;
+
+	dwc_memset(qh, 0, sizeof(dwc_otg_qh_t));
+
+	/* Initialize QH */
+	qh->ep_type = dwc_otg_hcd_get_pipe_type(&urb->pipe_info);
+	qh->ep_is_in = dwc_otg_hcd_is_pipe_in(&urb->pipe_info) ? 1 : 0;
+
+	qh->data_toggle = DWC_OTG_HC_PID_DATA0;
+	qh->maxp = dwc_otg_hcd_get_mps(&urb->pipe_info);
+	DWC_CIRCLEQ_INIT(&qh->qtd_list);
+	DWC_LIST_INIT(&qh->qh_list_entry);
+	qh->channel = NULL;
+
+	/* FS/LS Enpoint on HS Hub 
+	 * NOT virtual root hub */
+	dev_speed = hcd->fops->speed(hcd, urb->priv);
+
+	hcd->fops->hub_info(hcd, urb->priv, &hub_addr, &hub_port);
+	qh->do_split = 0;
+
+	if (((dev_speed == USB_SPEED_LOW) ||
+	     (dev_speed == USB_SPEED_FULL)) &&
+	    (hub_addr != 0 && hub_addr != 1)) {
+		DWC_DEBUGPL(DBG_HCD,
+			    "QH init: EP %d: TT found at hub addr %d, for port %d\n",
+			    dwc_otg_hcd_get_ep_num(&urb->pipe_info), hub_addr,
+			    hub_port);
+		qh->do_split = 1;
+	}
+
+	if (qh->ep_type == UE_INTERRUPT || qh->ep_type == UE_ISOCHRONOUS) {
+		/* Compute scheduling parameters once and save them. */
+		hprt0_data_t hprt;
+
+		/** @todo Account for split transfers in the bus time. */
+		int bytecount =
+		    dwc_hb_mult(qh->maxp) * dwc_max_packet(qh->maxp);
+
+		qh->usecs =
+		    calc_bus_time((qh->do_split ? USB_SPEED_HIGH : dev_speed),
+				  qh->ep_is_in, (qh->ep_type == UE_ISOCHRONOUS),
+				  bytecount);
+		/* Start in a slightly future (micro)frame. */
+		qh->sched_frame = dwc_frame_num_inc(hcd->frame_number,
+						    SCHEDULE_SLOP);
+		qh->interval = urb->interval;
+
+#if 0
+		/* Increase interrupt polling rate for debugging. */
+		if (qh->ep_type == UE_INTERRUPT) {
+			qh->interval = 8;
+		}
+#endif
+		hprt.d32 = DWC_READ_REG32(hcd->core_if->host_if->hprt0);
+		if ((hprt.b.prtspd == DWC_HPRT0_PRTSPD_HIGH_SPEED) &&
+		    ((dev_speed == USB_SPEED_LOW) ||
+		     (dev_speed == USB_SPEED_FULL))) {
+			qh->interval *= 8;
+			qh->sched_frame |= 0x7;
+			qh->start_split_frame = qh->sched_frame;
+		}
+
+	}
+
+	DWC_DEBUGPL(DBG_HCD, "DWC OTG HCD QH Initialized\n");
+	DWC_DEBUGPL(DBG_HCDV, "DWC OTG HCD QH  - qh = %p\n", qh);
+	DWC_DEBUGPL(DBG_HCDV, "DWC OTG HCD QH  - Device Address = %d\n",
+		    dwc_otg_hcd_get_dev_addr(&urb->pipe_info));
+	DWC_DEBUGPL(DBG_HCDV, "DWC OTG HCD QH  - Endpoint %d, %s\n",
+		    dwc_otg_hcd_get_ep_num(&urb->pipe_info),
+		    dwc_otg_hcd_is_pipe_in(&urb->pipe_info) ? "IN" : "OUT");
+	switch (dev_speed) {
+	case USB_SPEED_LOW:
+		qh->dev_speed = DWC_OTG_EP_SPEED_LOW;
+		speed = "low";
+		break;
+	case USB_SPEED_FULL:
+		qh->dev_speed = DWC_OTG_EP_SPEED_FULL;
+		speed = "full";
+		break;
+	case USB_SPEED_HIGH:
+		qh->dev_speed = DWC_OTG_EP_SPEED_HIGH;
+		speed = "high";
+		break;
+	default:
+		speed = "?";
+		break;
+	}
+	DWC_DEBUGPL(DBG_HCDV, "DWC OTG HCD QH  - Speed = %s\n", speed);
+
+	switch (qh->ep_type) {
+	case UE_ISOCHRONOUS:
+		type = "isochronous";
+		break;
+	case UE_INTERRUPT:
+		type = "interrupt";
+		break;
+	case UE_CONTROL:
+		type = "control";
+		break;
+	case UE_BULK:
+		type = "bulk";
+		break;
+	default:
+		type = "?";
+		break;
+	}
+
+	DWC_DEBUGPL(DBG_HCDV, "DWC OTG HCD QH  - Type = %s\n", type);
+
+#ifdef DEBUG
+	if (qh->ep_type == UE_INTERRUPT) {
+		DWC_DEBUGPL(DBG_HCDV, "DWC OTG HCD QH - usecs = %d\n",
+			    qh->usecs);
+		DWC_DEBUGPL(DBG_HCDV, "DWC OTG HCD QH - interval = %d\n",
+			    qh->interval);
+	}
+#endif
+
+}
+
+/**
+ * This function allocates and initializes a QH.
+ *
+ * @param hcd The HCD state structure for the DWC OTG controller.
+ * @param urb Holds the information about the device/endpoint that we need
+ * 	      to initialize the QH.
+ * @param atomic_alloc Flag to do atomic allocation if needed
+ *
+ * @return Returns pointer to the newly allocated QH, or NULL on error. */
+dwc_otg_qh_t *dwc_otg_hcd_qh_create(dwc_otg_hcd_t * hcd,
+				    dwc_otg_hcd_urb_t * urb, int atomic_alloc)
+{
+	dwc_otg_qh_t *qh;
+
+	/* Allocate memory */
+	/** @todo add memflags argument */
+	qh = dwc_otg_hcd_qh_alloc(atomic_alloc);
+	if (qh == NULL) {
+		DWC_ERROR("qh allocation failed");
+		return NULL;
+	}
+
+	qh_init(hcd, qh, urb);
+
+	if (hcd->core_if->dma_desc_enable
+	    && (dwc_otg_hcd_qh_init_ddma(hcd, qh) < 0)) {
+		dwc_otg_hcd_qh_free(hcd, qh);
+		return NULL;
+	}
+
+	return qh;
+}
+
+/**
+ * Checks that a channel is available for a periodic transfer.
+ *
+ * @return 0 if successful, negative error code otherise.
+ */
+static int periodic_channel_available(dwc_otg_hcd_t * hcd)
+{
+	/*
+	 * Currently assuming that there is a dedicated host channnel for each
+	 * periodic transaction plus at least one host channel for
+	 * non-periodic transactions.
+	 */
+	int status;
+	int num_channels;
+
+	num_channels = hcd->core_if->core_params->host_channels;
+	if ((hcd->periodic_channels + hcd->non_periodic_channels < num_channels)
+	    && (hcd->periodic_channels < num_channels - 1)) {
+		status = 0;
+	} else {
+		DWC_INFO("%s: Total channels: %d, Periodic: %d, Non-periodic: %d\n",
+			__func__, num_channels, hcd->periodic_channels, hcd->non_periodic_channels);	//NOTICE
+		status = -DWC_E_NO_SPACE;
+	}
+
+	return status;
+}
+
+/**
+ * Checks that there is sufficient bandwidth for the specified QH in the
+ * periodic schedule. For simplicity, this calculation assumes that all the
+ * transfers in the periodic schedule may occur in the same (micro)frame.
+ *
+ * @param hcd The HCD state structure for the DWC OTG controller.
+ * @param qh QH containing periodic bandwidth required.
+ *
+ * @return 0 if successful, negative error code otherwise.
+ */
+static int check_periodic_bandwidth(dwc_otg_hcd_t * hcd, dwc_otg_qh_t * qh)
+{
+	int status;
+	int16_t max_claimed_usecs;
+
+	status = 0;
+
+	if ((qh->dev_speed == DWC_OTG_EP_SPEED_HIGH) || qh->do_split) {
+		/*
+		 * High speed mode.
+		 * Max periodic usecs is 80% x 125 usec = 100 usec.
+		 */
+
+		max_claimed_usecs = 100 - qh->usecs;
+	} else {
+		/*
+		 * Full speed mode.
+		 * Max periodic usecs is 90% x 1000 usec = 900 usec.
+		 */
+		max_claimed_usecs = 900 - qh->usecs;
+	}
+
+	if (hcd->periodic_usecs > max_claimed_usecs) {
+		DWC_INFO("%s: already claimed usecs %d, required usecs %d\n", __func__, hcd->periodic_usecs, qh->usecs);	//NOTICE
+		status = -DWC_E_NO_SPACE;
+	}
+
+	return status;
+}
+
+/**
+ * Checks that the max transfer size allowed in a host channel is large enough
+ * to handle the maximum data transfer in a single (micro)frame for a periodic
+ * transfer.
+ *
+ * @param hcd The HCD state structure for the DWC OTG controller.
+ * @param qh QH for a periodic endpoint.
+ *
+ * @return 0 if successful, negative error code otherwise.
+ */
+static int check_max_xfer_size(dwc_otg_hcd_t * hcd, dwc_otg_qh_t * qh)
+{
+	int status;
+	uint32_t max_xfer_size;
+	uint32_t max_channel_xfer_size;
+
+	status = 0;
+
+	max_xfer_size = dwc_max_packet(qh->maxp) * dwc_hb_mult(qh->maxp);
+	max_channel_xfer_size = hcd->core_if->core_params->max_transfer_size;
+
+	if (max_xfer_size > max_channel_xfer_size) {
+		DWC_INFO("%s: Periodic xfer length %d > " "max xfer length for channel %d\n",
+				__func__, max_xfer_size, max_channel_xfer_size);	//NOTICE
+		status = -DWC_E_NO_SPACE;
+	}
+
+	return status;
+}
+
+/**
+ * Schedules an interrupt or isochronous transfer in the periodic schedule.
+ *
+ * @param hcd The HCD state structure for the DWC OTG controller.
+ * @param qh QH for the periodic transfer. The QH should already contain the
+ * scheduling information.
+ *
+ * @return 0 if successful, negative error code otherwise.
+ */
+static int schedule_periodic(dwc_otg_hcd_t * hcd, dwc_otg_qh_t * qh)
+{
+	int status = 0;
+
+	status = periodic_channel_available(hcd);
+	if (status) {
+		DWC_INFO("%s: No host channel available for periodic " "transfer.\n", __func__);	//NOTICE
+		return status;
+	}
+
+	status = check_periodic_bandwidth(hcd, qh);
+	if (status) {
+		DWC_INFO("%s: Insufficient periodic bandwidth for " "periodic transfer.\n", __func__);	//NOTICE
+		return status;
+	}
+
+	status = check_max_xfer_size(hcd, qh);
+	if (status) {
+		DWC_INFO("%s: Channel max transfer size too small " "for periodic transfer.\n", __func__);	//NOTICE
+		return status;
+	}
+
+	if (hcd->core_if->dma_desc_enable) {
+		/* Don't rely on SOF and start in ready schedule */
+		DWC_LIST_INSERT_TAIL(&hcd->periodic_sched_ready, &qh->qh_list_entry);
+	}
+	else {
+	/* Always start in the inactive schedule. */
+	DWC_LIST_INSERT_TAIL(&hcd->periodic_sched_inactive, &qh->qh_list_entry);
+	}
+
+	/* Reserve the periodic channel. */
+	hcd->periodic_channels++;
+
+	/* Update claimed usecs per (micro)frame. */
+	hcd->periodic_usecs += qh->usecs;
+
+	return status;
+}
+
+/**
+ * This function adds a QH to either the non periodic or periodic schedule if
+ * it is not already in the schedule. If the QH is already in the schedule, no
+ * action is taken.
+ *
+ * @return 0 if successful, negative error code otherwise.
+ */
+int dwc_otg_hcd_qh_add(dwc_otg_hcd_t * hcd, dwc_otg_qh_t * qh)
+{
+	int status = 0;
+	gintmsk_data_t intr_mask = {.d32 = 0 };
+
+	if (!DWC_LIST_EMPTY(&qh->qh_list_entry)) {
+		/* QH already in a schedule. */
+		return status;
+	}
+
+	/* Add the new QH to the appropriate schedule */
+	if (dwc_qh_is_non_per(qh)) {
+		/* Always start in the inactive schedule. */
+		DWC_LIST_INSERT_TAIL(&hcd->non_periodic_sched_inactive,
+				     &qh->qh_list_entry);
+	} else {
+		status = schedule_periodic(hcd, qh);
+		if ( !hcd->periodic_qh_count ) {
+			intr_mask.b.sofintr = 1;
+			DWC_MODIFY_REG32(&hcd->core_if->core_global_regs->gintmsk,
+								intr_mask.d32, intr_mask.d32);
+		}
+		hcd->periodic_qh_count++;
+	}
+
+	return status;
+}
+
+/**
+ * Removes an interrupt or isochronous transfer from the periodic schedule.
+ *
+ * @param hcd The HCD state structure for the DWC OTG controller.
+ * @param qh QH for the periodic transfer.
+ */
+static void deschedule_periodic(dwc_otg_hcd_t * hcd, dwc_otg_qh_t * qh)
+{
+	DWC_LIST_REMOVE_INIT(&qh->qh_list_entry);
+
+	/* Release the periodic channel reservation. */
+	hcd->periodic_channels--;
+
+	/* Update claimed usecs per (micro)frame. */
+	hcd->periodic_usecs -= qh->usecs;
+}
+
+/** 
+ * Removes a QH from either the non-periodic or periodic schedule.  Memory is
+ * not freed.
+ *
+ * @param hcd The HCD state structure.
+ * @param qh QH to remove from schedule. */
+void dwc_otg_hcd_qh_remove(dwc_otg_hcd_t * hcd, dwc_otg_qh_t * qh)
+{
+	gintmsk_data_t intr_mask = {.d32 = 0 };
+
+	if (DWC_LIST_EMPTY(&qh->qh_list_entry)) {
+		/* QH is not in a schedule. */
+		return;
+	}
+
+	if (dwc_qh_is_non_per(qh)) {
+		if (hcd->non_periodic_qh_ptr == &qh->qh_list_entry) {
+			hcd->non_periodic_qh_ptr =
+			    hcd->non_periodic_qh_ptr->next;
+		}
+		DWC_LIST_REMOVE_INIT(&qh->qh_list_entry);
+	} else {
+		deschedule_periodic(hcd, qh);
+		hcd->periodic_qh_count--;
+		if( !hcd->periodic_qh_count ) {
+			intr_mask.b.sofintr = 1;
+				DWC_MODIFY_REG32(&hcd->core_if->core_global_regs->gintmsk,
+									intr_mask.d32, 0);
+		}
+	}
+}
+
+/**
+ * Deactivates a QH. For non-periodic QHs, removes the QH from the active
+ * non-periodic schedule. The QH is added to the inactive non-periodic
+ * schedule if any QTDs are still attached to the QH.
+ *
+ * For periodic QHs, the QH is removed from the periodic queued schedule. If
+ * there are any QTDs still attached to the QH, the QH is added to either the
+ * periodic inactive schedule or the periodic ready schedule and its next
+ * scheduled frame is calculated. The QH is placed in the ready schedule if
+ * the scheduled frame has been reached already. Otherwise it's placed in the
+ * inactive schedule. If there are no QTDs attached to the QH, the QH is
+ * completely removed from the periodic schedule.
+ */
+void dwc_otg_hcd_qh_deactivate(dwc_otg_hcd_t * hcd, dwc_otg_qh_t * qh,
+			       int sched_next_periodic_split)
+{	
+	if (dwc_qh_is_non_per(qh)) {
+		dwc_otg_hcd_qh_remove(hcd, qh);
+		if (!DWC_CIRCLEQ_EMPTY(&qh->qtd_list)) {
+			/* Add back to inactive non-periodic schedule. */
+			dwc_otg_hcd_qh_add(hcd, qh);
+		}
+	} else {
+		uint16_t frame_number = dwc_otg_hcd_get_frame_number(hcd);
+
+		if (qh->do_split) {
+			/* Schedule the next continuing periodic split transfer */
+			if (sched_next_periodic_split) {
+
+				qh->sched_frame = frame_number;
+				if (dwc_frame_num_le(frame_number,
+						     dwc_frame_num_inc
+						     (qh->start_split_frame,
+						      1))) {
+					/*
+					 * Allow one frame to elapse after start
+					 * split microframe before scheduling
+					 * complete split, but DONT if we are
+					 * doing the next start split in the
+					 * same frame for an ISOC out.
+					 */
+					if ((qh->ep_type != UE_ISOCHRONOUS) ||
+					    (qh->ep_is_in != 0)) {
+						qh->sched_frame =
+						    dwc_frame_num_inc(qh->sched_frame, 1);
+					}
+				}
+			} else {
+				qh->sched_frame =
+				    dwc_frame_num_inc(qh->start_split_frame,
+						      qh->interval);
+				if (dwc_frame_num_le
+				    (qh->sched_frame, frame_number)) {
+					qh->sched_frame = frame_number;
+				}
+				qh->sched_frame |= 0x7;
+				qh->start_split_frame = qh->sched_frame;
+			}
+		} else {
+			qh->sched_frame =
+			    dwc_frame_num_inc(qh->sched_frame, qh->interval);
+			if (dwc_frame_num_le(qh->sched_frame, frame_number)) {
+				qh->sched_frame = frame_number;
+			}
+		}
+
+		if (DWC_CIRCLEQ_EMPTY(&qh->qtd_list)) {
+			dwc_otg_hcd_qh_remove(hcd, qh);
+		} else {
+			/*
+			 * Remove from periodic_sched_queued and move to
+			 * appropriate queue.
+			 */
+			if (qh->sched_frame == frame_number) {
+				DWC_LIST_MOVE_HEAD(&hcd->periodic_sched_ready,
+						   &qh->qh_list_entry);
+			} else {
+				DWC_LIST_MOVE_HEAD
+				    (&hcd->periodic_sched_inactive,
+				     &qh->qh_list_entry);
+			}
+		}
+	}
+}
+
+/** 
+ * This function allocates and initializes a QTD. 
+ *
+ * @param urb The URB to create a QTD from.  Each URB-QTD pair will end up
+ * 	      pointing to each other so each pair should have a unique correlation.
+ * @param atomic_alloc Flag to do atomic alloc if needed
+ *
+ * @return Returns pointer to the newly allocated QTD, or NULL on error. */
+dwc_otg_qtd_t *dwc_otg_hcd_qtd_create(dwc_otg_hcd_urb_t * urb, int atomic_alloc)
+{
+	dwc_otg_qtd_t *qtd;
+
+	qtd = dwc_otg_hcd_qtd_alloc(atomic_alloc);
+	if (qtd == NULL) {
+		return NULL;
+	}
+
+	dwc_otg_hcd_qtd_init(qtd, urb);
+	return qtd;
+}
+
+/** 
+ * Initializes a QTD structure.
+ *
+ * @param qtd The QTD to initialize.
+ * @param urb The URB to use for initialization.  */
+void dwc_otg_hcd_qtd_init(dwc_otg_qtd_t * qtd, dwc_otg_hcd_urb_t * urb)
+{
+	dwc_memset(qtd, 0, sizeof(dwc_otg_qtd_t));
+	qtd->urb = urb;
+	if (dwc_otg_hcd_get_pipe_type(&urb->pipe_info) == UE_CONTROL) {
+		/*
+		 * The only time the QTD data toggle is used is on the data
+		 * phase of control transfers. This phase always starts with
+		 * DATA1.
+		 */
+		qtd->data_toggle = DWC_OTG_HC_PID_DATA1;
+		qtd->control_phase = DWC_OTG_CONTROL_SETUP;
+	}
+
+	/* start split */
+	qtd->complete_split = 0;
+	qtd->isoc_split_pos = DWC_HCSPLIT_XACTPOS_ALL;
+	qtd->isoc_split_offset = 0;
+	qtd->in_process = 0;
+
+	/* Store the qtd ptr in the urb to reference what QTD. */
+	urb->qtd = qtd;
+	return;
+}
+
+/**
+ * This function adds a QTD to the QTD-list of a QH.  It will find the correct
+ * QH to place the QTD into.  If it does not find a QH, then it will create a
+ * new QH. If the QH to which the QTD is added is not currently scheduled, it
+ * is placed into the proper schedule based on its EP type.
+ *
+ * @param[in] qtd The QTD to add
+ * @param[in] hcd The DWC HCD structure
+ * @param[out] qh out parameter to return queue head
+ * @param atomic_alloc Flag to do atomic alloc if needed
+ *
+ * @return 0 if successful, negative error code otherwise.
+ */
+int dwc_otg_hcd_qtd_add(dwc_otg_qtd_t * qtd,
+			dwc_otg_hcd_t * hcd, dwc_otg_qh_t ** qh, int atomic_alloc)
+{
+	int retval = 0;
+	dwc_irqflags_t flags;
+
+	dwc_otg_hcd_urb_t *urb = qtd->urb;
+
+	/*
+	 * Get the QH which holds the QTD-list to insert to. Create QH if it
+	 * doesn't exist.
+	 */
+	if (*qh == NULL) {
+		*qh = dwc_otg_hcd_qh_create(hcd, urb, atomic_alloc);
+		if (*qh == NULL) {
+			retval = -1;
+			goto done;
+		}
+	}
+	DWC_SPINLOCK_IRQSAVE(hcd->lock, &flags);
+	retval = dwc_otg_hcd_qh_add(hcd, *qh);
+	if (retval == 0) {
+		DWC_CIRCLEQ_INSERT_TAIL(&((*qh)->qtd_list), qtd,
+					qtd_list_entry);
+	}
+	DWC_SPINUNLOCK_IRQRESTORE(hcd->lock, flags);
+
+done:
+
+	return retval;
+}
+
+#endif /* DWC_DEVICE_ONLY */
diff --git a/drivers/usb/dwc_otg/dwc_otg_os_dep.h b/drivers/usb/dwc_otg/dwc_otg_os_dep.h
new file mode 100644
index 0000000..81e7655
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_otg_os_dep.h
@@ -0,0 +1,112 @@
+#ifndef _DWC_OS_DEP_H_
+#define _DWC_OS_DEP_H_
+
+/**
+ * @file
+ *
+ * This file contains OS dependent structures.
+ *
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/init.h>
+#include <linux/device.h>
+#include <linux/errno.h>
+#include <linux/types.h>
+#include <linux/slab.h>
+#include <linux/list.h>
+#include <linux/interrupt.h>
+#include <linux/ctype.h>
+#include <linux/string.h>
+#include <linux/dma-mapping.h>
+#include <linux/jiffies.h>
+#include <linux/delay.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/stat.h>
+#include <linux/pci.h>
+#include <linux/platform_device.h>
+
+#include <linux/version.h>
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,20)
+# include <linux/irq.h>
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,21)
+# include <linux/usb/ch9.h>
+#else
+# include <linux/usb_ch9.h>
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,24)
+# include <linux/usb/gadget.h>
+#else
+# include <linux/usb_gadget.h>
+#endif
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20)
+# include <asm/irq.h>
+#endif
+
+#ifdef PCI_INTERFACE
+# include <asm/io.h>
+#endif
+
+#ifdef LM_INTERFACE
+# include <asm/unaligned.h>
+# include <asm/sizes.h>
+# include <asm/param.h>
+# include <asm/io.h>
+# include <asm/arch/lm.h>
+# include <asm/arch/irqs.h>
+# include <asm/arch/regs-irq.h>
+#else
+# include <asm/unaligned.h>
+# include <asm/sizes.h>
+# include <asm/param.h>
+# include <asm/io.h>
+# include <mach/irqs.h>
+//# include <asm/arch/regs-irq.h>
+#endif
+
+/** The OS page size */
+#define DWC_OS_PAGE_SIZE	PAGE_SIZE
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,14)
+typedef int gfp_t;
+#endif
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,18)
+# define IRQF_SHARED SA_SHIRQ
+#endif
+
+typedef struct os_dependent {
+	/** Base address returned from ioremap() */
+	void *base;
+
+	/** Register offset for Diagnostic API */
+	uint32_t reg_offset;
+
+#ifdef LM_INTERFACE
+	struct lm_device *lmdev;
+#elif  defined(PCI_INTERFACE)
+	struct pci_dev *pcidev;
+
+	/** Start address of a PCI region */
+	resource_size_t rsrc_start;
+	
+	/** Length address of a PCI region */
+	resource_size_t rsrc_len;
+#else
+	struct device *parent;	
+#endif
+} os_dependent_t;
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* _DWC_OS_DEP_H_ */
diff --git a/drivers/usb/dwc_otg/dwc_otg_pcd.c b/drivers/usb/dwc_otg/dwc_otg_pcd.c
new file mode 100644
index 0000000..485a087
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_otg_pcd.c
@@ -0,0 +1,2641 @@
+/* ==========================================================================
+ * $File: //dwh/usb_iip/dev/software/otg/linux/drivers/dwc_otg_pcd.c $
+ * $Revision: #99 $
+ * $Date: 2011/10/24 $
+ * $Change: 1871160 $
+ *
+ * Synopsys HS OTG Linux Software Driver and documentation (hereinafter,
+ * "Software") is an Unsupported proprietary work of Synopsys, Inc. unless
+ * otherwise expressly agreed to in writing between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product under
+ * any End User Software License Agreement or Agreement for Licensed Product
+ * with Synopsys or any supplement thereto. You are permitted to use and
+ * redistribute this Software in source and binary forms, with or without
+ * modification, provided that redistributions of source code must retain this
+ * notice. You may not view, use, disclose, copy or distribute this file or
+ * any information contained herein except pursuant to this license grant from
+ * Synopsys. If you do not agree with this notice, including the disclaimer
+ * below, then you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS" BASIS
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE HEREBY DISCLAIMED. IN NO EVENT SHALL SYNOPSYS BE LIABLE FOR ANY DIRECT,
+ * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================== */
+#ifndef DWC_HOST_ONLY
+
+/** @file
+ * This file implements PCD Core. All code in this file is portable and doesn't
+ * use any OS specific functions.
+ * PCD Core provides Interface, defined in <code><dwc_otg_pcd_if.h></code>
+ * header file, which can be used to implement OS specific PCD interface.
+ *
+ * An important function of the PCD is managing interrupts generated
+ * by the DWC_otg controller. The implementation of the DWC_otg device
+ * mode interrupt service routines is in dwc_otg_pcd_intr.c.
+ *
+ * @todo Add Device Mode test modes (Test J mode, Test K mode, etc).
+ * @todo Does it work when the request size is greater than DEPTSIZ
+ * transfer size
+ *
+ */
+
+#include "dwc_otg_pcd.h"
+
+#ifdef DWC_UTE_CFI
+#include "dwc_otg_cfi.h"
+
+extern int init_cfi(cfiobject_t * cfiobj);
+#endif
+
+/**
+ * Choose endpoint from ep arrays using usb_ep structure.
+ */
+static dwc_otg_pcd_ep_t *get_ep_from_handle(dwc_otg_pcd_t * pcd, void *handle)
+{
+	int i;
+	if (pcd->ep0.priv == handle) {
+		return &pcd->ep0;
+	}
+	for (i = 0; i < MAX_EPS_CHANNELS - 1; i++) {
+		if (pcd->in_ep[i].priv == handle)
+			return &pcd->in_ep[i];
+		if (pcd->out_ep[i].priv == handle)
+			return &pcd->out_ep[i];
+	}
+
+	return NULL;
+}
+
+/**
+ * This function completes a request.  It call's the request call back.
+ */
+void dwc_otg_request_done(dwc_otg_pcd_ep_t * ep, dwc_otg_pcd_request_t * req,
+			  int32_t status)
+{
+	unsigned stopped = ep->stopped;
+
+	DWC_DEBUGPL(DBG_PCDV, "%s(ep %p req %p)\n", __func__, ep, req);
+	DWC_CIRCLEQ_REMOVE_INIT(&ep->queue, req, queue_entry);
+
+	/* don't modify queue heads during completion callback */
+	ep->stopped = 1;
+	/* spin_unlock/spin_lock now done in fops->complete() */
+	ep->pcd->fops->complete(ep->pcd, ep->priv, req->priv, status,
+				req->actual);
+
+	if (ep->pcd->request_pending > 0) {
+		--ep->pcd->request_pending;
+	}
+
+	ep->stopped = stopped;
+	DWC_FREE(req);
+}
+
+/**
+ * This function terminates all the requsts in the EP request queue.
+ */
+void dwc_otg_request_nuke(dwc_otg_pcd_ep_t * ep)
+{
+	dwc_otg_pcd_request_t *req;
+
+	ep->stopped = 1;
+
+	/* called with irqs blocked?? */
+	while (!DWC_CIRCLEQ_EMPTY(&ep->queue)) {
+		req = DWC_CIRCLEQ_FIRST(&ep->queue);
+		dwc_otg_request_done(ep, req, -DWC_E_SHUTDOWN);
+	}
+}
+
+void dwc_otg_pcd_start(dwc_otg_pcd_t * pcd,
+		       const struct dwc_otg_pcd_function_ops *fops)
+{
+	pcd->fops = fops;
+}
+
+/**
+ * PCD Callback function for initializing the PCD when switching to
+ * device mode.
+ *
+ * @param p void pointer to the <code>dwc_otg_pcd_t</code>
+ */
+static int32_t dwc_otg_pcd_start_cb(void *p)
+{
+	dwc_otg_pcd_t *pcd = (dwc_otg_pcd_t *) p;
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(pcd);
+
+	/*
+	 * Initialized the Core for Device mode.
+	 */
+	if (dwc_otg_is_device_mode(core_if)) {
+		dwc_otg_core_dev_init(core_if);
+		/* Set core_if's lock pointer to the pcd->lock */
+		core_if->lock = pcd->lock;
+	}
+	return 1;
+}
+
+/** CFI-specific buffer allocation function for EP */
+#ifdef DWC_UTE_CFI
+uint8_t *cfiw_ep_alloc_buffer(dwc_otg_pcd_t * pcd, void *pep, dwc_dma_t * addr,
+			      size_t buflen, int flags)
+{
+	dwc_otg_pcd_ep_t *ep;
+	ep = get_ep_from_handle(pcd, pep);
+	if (!ep) {
+		DWC_WARN("bad ep\n");
+		return -DWC_E_INVALID;
+	}
+
+	return pcd->cfi->ops.ep_alloc_buf(pcd->cfi, pcd, ep, addr, buflen,
+					  flags);
+}
+#else
+uint8_t *cfiw_ep_alloc_buffer(dwc_otg_pcd_t * pcd, void *pep, dwc_dma_t * addr,
+			      size_t buflen, int flags);
+#endif
+
+/**
+ * PCD Callback function for notifying the PCD when resuming from
+ * suspend.
+ *
+ * @param p void pointer to the <code>dwc_otg_pcd_t</code>
+ */
+static int32_t dwc_otg_pcd_resume_cb(void *p)
+{
+	dwc_otg_pcd_t *pcd = (dwc_otg_pcd_t *) p;
+
+	if (pcd->fops->resume) {
+		pcd->fops->resume(pcd);
+	}
+
+	/* Stop the SRP timeout timer. */
+	if ((GET_CORE_IF(pcd)->core_params->phy_type != DWC_PHY_TYPE_PARAM_FS)
+	    || (!GET_CORE_IF(pcd)->core_params->i2c_enable)) {
+		if (GET_CORE_IF(pcd)->srp_timer_started) {
+			GET_CORE_IF(pcd)->srp_timer_started = 0;
+			DWC_TIMER_CANCEL(GET_CORE_IF(pcd)->srp_timer);
+		}
+	}
+	return 1;
+}
+
+/**
+ * PCD Callback function for notifying the PCD device is suspended.
+ *
+ * @param p void pointer to the <code>dwc_otg_pcd_t</code>
+ */
+static int32_t dwc_otg_pcd_suspend_cb(void *p)
+{
+	dwc_otg_pcd_t *pcd = (dwc_otg_pcd_t *) p;
+
+	if (pcd->fops->suspend) {
+		DWC_SPINUNLOCK(pcd->lock);
+		pcd->fops->suspend(pcd);
+		DWC_SPINLOCK(pcd->lock);
+	}
+
+	return 1;
+}
+
+/**
+ * PCD Callback function for stopping the PCD when switching to Host
+ * mode.
+ *
+ * @param p void pointer to the <code>dwc_otg_pcd_t</code>
+ */
+static int32_t dwc_otg_pcd_stop_cb(void *p)
+{
+	dwc_otg_pcd_t *pcd = (dwc_otg_pcd_t *) p;
+	extern void dwc_otg_pcd_stop(dwc_otg_pcd_t * _pcd);
+
+	dwc_otg_pcd_stop(pcd);
+	return 1;
+}
+
+/**
+ * PCD Callback structure for handling mode switching.
+ */
+static dwc_otg_cil_callbacks_t pcd_callbacks = {
+	.start = dwc_otg_pcd_start_cb,
+	.stop = dwc_otg_pcd_stop_cb,
+	.suspend = dwc_otg_pcd_suspend_cb,
+	.resume_wakeup = dwc_otg_pcd_resume_cb,
+	.p = 0,			/* Set at registration */
+};
+
+/**
+ * This function allocates a DMA Descriptor chain for the Endpoint
+ * buffer to be used for a transfer to/from the specified endpoint.
+ */
+dwc_otg_dev_dma_desc_t *dwc_otg_ep_alloc_desc_chain(dwc_dma_t * dma_desc_addr,
+						    uint32_t count)
+{
+	return DWC_DMA_ALLOC_ATOMIC(count * sizeof(dwc_otg_dev_dma_desc_t),
+							dma_desc_addr);
+}
+
+/**
+ * This function frees a DMA Descriptor chain that was allocated by ep_alloc_desc.
+ */
+void dwc_otg_ep_free_desc_chain(dwc_otg_dev_dma_desc_t * desc_addr,
+				uint32_t dma_desc_addr, uint32_t count)
+{
+	DWC_DMA_FREE(count * sizeof(dwc_otg_dev_dma_desc_t), desc_addr,
+		     dma_desc_addr);
+}
+
+#ifdef DWC_EN_ISOC
+
+/**
+ * This function initializes a descriptor chain for Isochronous transfer
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param dwc_ep The EP to start the transfer on.
+ *
+ */
+void dwc_otg_iso_ep_start_ddma_transfer(dwc_otg_core_if_t * core_if,
+					dwc_ep_t * dwc_ep)
+{
+
+	dsts_data_t dsts = {.d32 = 0 };
+	depctl_data_t depctl = {.d32 = 0 };
+	volatile uint32_t *addr;
+	int i, j;
+	uint32_t len;
+
+	if (dwc_ep->is_in)
+		dwc_ep->desc_cnt = dwc_ep->buf_proc_intrvl / dwc_ep->bInterval;
+	else
+		dwc_ep->desc_cnt =
+		    dwc_ep->buf_proc_intrvl * dwc_ep->pkt_per_frm /
+		    dwc_ep->bInterval;
+
+	/** Allocate descriptors for double buffering */
+	dwc_ep->iso_desc_addr =
+	    dwc_otg_ep_alloc_desc_chain(&dwc_ep->iso_dma_desc_addr,
+					dwc_ep->desc_cnt * 2);
+	if (dwc_ep->desc_addr) {
+		DWC_WARN("%s, can't allocate DMA descriptor chain\n", __func__);
+		return;
+	}
+
+	dsts.d32 = DWC_READ_REG32(&core_if->dev_if->dev_global_regs->dsts);
+
+	/** ISO OUT EP */
+	if (dwc_ep->is_in == 0) {
+		dev_dma_desc_sts_t sts = {.d32 = 0 };
+		dwc_otg_dev_dma_desc_t *dma_desc = dwc_ep->iso_desc_addr;
+		dma_addr_t dma_ad;
+		uint32_t data_per_desc;
+		dwc_otg_dev_out_ep_regs_t *out_regs =
+		    core_if->dev_if->out_ep_regs[dwc_ep->num];
+		int offset;
+
+		addr = &core_if->dev_if->out_ep_regs[dwc_ep->num]->doepctl;
+		dma_ad = (dma_addr_t) DWC_READ_REG32(&(out_regs->doepdma));
+
+		/** Buffer 0 descriptors setup */
+		dma_ad = dwc_ep->dma_addr0;
+
+		sts.b_iso_out.bs = BS_HOST_READY;
+		sts.b_iso_out.rxsts = 0;
+		sts.b_iso_out.l = 0;
+		sts.b_iso_out.sp = 0;
+		sts.b_iso_out.ioc = 0;
+		sts.b_iso_out.pid = 0;
+		sts.b_iso_out.framenum = 0;
+
+		offset = 0;
+		for (i = 0; i < dwc_ep->desc_cnt - dwc_ep->pkt_per_frm;
+		     i += dwc_ep->pkt_per_frm) {
+
+			for (j = 0; j < dwc_ep->pkt_per_frm; ++j) {
+				uint32_t len = (j + 1) * dwc_ep->maxpacket;
+				if (len > dwc_ep->data_per_frame)
+					data_per_desc =
+					    dwc_ep->data_per_frame -
+					    j * dwc_ep->maxpacket;
+				else
+					data_per_desc = dwc_ep->maxpacket;
+				len = data_per_desc % 4;
+				if (len)
+					data_per_desc += 4 - len;
+
+				sts.b_iso_out.rxbytes = data_per_desc;
+				dma_desc->buf = dma_ad;
+				dma_desc->status.d32 = sts.d32;
+
+				offset += data_per_desc;
+				dma_desc++;
+				dma_ad += data_per_desc;
+			}
+		}
+
+		for (j = 0; j < dwc_ep->pkt_per_frm - 1; ++j) {
+			uint32_t len = (j + 1) * dwc_ep->maxpacket;
+			if (len > dwc_ep->data_per_frame)
+				data_per_desc =
+				    dwc_ep->data_per_frame -
+				    j * dwc_ep->maxpacket;
+			else
+				data_per_desc = dwc_ep->maxpacket;
+			len = data_per_desc % 4;
+			if (len)
+				data_per_desc += 4 - len;
+			sts.b_iso_out.rxbytes = data_per_desc;
+			dma_desc->buf = dma_ad;
+			dma_desc->status.d32 = sts.d32;
+
+			offset += data_per_desc;
+			dma_desc++;
+			dma_ad += data_per_desc;
+		}
+
+		sts.b_iso_out.ioc = 1;
+		len = (j + 1) * dwc_ep->maxpacket;
+		if (len > dwc_ep->data_per_frame)
+			data_per_desc =
+			    dwc_ep->data_per_frame - j * dwc_ep->maxpacket;
+		else
+			data_per_desc = dwc_ep->maxpacket;
+		len = data_per_desc % 4;
+		if (len)
+			data_per_desc += 4 - len;
+		sts.b_iso_out.rxbytes = data_per_desc;
+
+		dma_desc->buf = dma_ad;
+		dma_desc->status.d32 = sts.d32;
+		dma_desc++;
+
+		/** Buffer 1 descriptors setup */
+		sts.b_iso_out.ioc = 0;
+		dma_ad = dwc_ep->dma_addr1;
+
+		offset = 0;
+		for (i = 0; i < dwc_ep->desc_cnt - dwc_ep->pkt_per_frm;
+		     i += dwc_ep->pkt_per_frm) {
+			for (j = 0; j < dwc_ep->pkt_per_frm; ++j) {
+				uint32_t len = (j + 1) * dwc_ep->maxpacket;
+				if (len > dwc_ep->data_per_frame)
+					data_per_desc =
+					    dwc_ep->data_per_frame -
+					    j * dwc_ep->maxpacket;
+				else
+					data_per_desc = dwc_ep->maxpacket;
+				len = data_per_desc % 4;
+				if (len)
+					data_per_desc += 4 - len;
+
+				data_per_desc =
+				    sts.b_iso_out.rxbytes = data_per_desc;
+				dma_desc->buf = dma_ad;
+				dma_desc->status.d32 = sts.d32;
+
+				offset += data_per_desc;
+				dma_desc++;
+				dma_ad += data_per_desc;
+			}
+		}
+		for (j = 0; j < dwc_ep->pkt_per_frm - 1; ++j) {
+			data_per_desc =
+			    ((j + 1) * dwc_ep->maxpacket >
+			     dwc_ep->data_per_frame) ? dwc_ep->data_per_frame -
+			    j * dwc_ep->maxpacket : dwc_ep->maxpacket;
+			data_per_desc +=
+			    (data_per_desc % 4) ? (4 - data_per_desc % 4) : 0;
+			sts.b_iso_out.rxbytes = data_per_desc;
+			dma_desc->buf = dma_ad;
+			dma_desc->status.d32 = sts.d32;
+
+			offset += data_per_desc;
+			dma_desc++;
+			dma_ad += data_per_desc;
+		}
+
+		sts.b_iso_out.ioc = 1;
+		sts.b_iso_out.l = 1;
+		data_per_desc =
+		    ((j + 1) * dwc_ep->maxpacket >
+		     dwc_ep->data_per_frame) ? dwc_ep->data_per_frame -
+		    j * dwc_ep->maxpacket : dwc_ep->maxpacket;
+		data_per_desc +=
+		    (data_per_desc % 4) ? (4 - data_per_desc % 4) : 0;
+		sts.b_iso_out.rxbytes = data_per_desc;
+
+		dma_desc->buf = dma_ad;
+		dma_desc->status.d32 = sts.d32;
+
+		dwc_ep->next_frame = 0;
+
+		/** Write dma_ad into DOEPDMA register */
+		DWC_WRITE_REG32(&(out_regs->doepdma),
+				(uint32_t) dwc_ep->iso_dma_desc_addr);
+
+	}
+	/** ISO IN EP */
+	else {
+		dev_dma_desc_sts_t sts = {.d32 = 0 };
+		dwc_otg_dev_dma_desc_t *dma_desc = dwc_ep->iso_desc_addr;
+		dma_addr_t dma_ad;
+		dwc_otg_dev_in_ep_regs_t *in_regs =
+		    core_if->dev_if->in_ep_regs[dwc_ep->num];
+		unsigned int frmnumber;
+		fifosize_data_t txfifosize, rxfifosize;
+
+		txfifosize.d32 =
+		    DWC_READ_REG32(&core_if->dev_if->
+				   in_ep_regs[dwc_ep->num]->dtxfsts);
+		rxfifosize.d32 =
+		    DWC_READ_REG32(&core_if->core_global_regs->grxfsiz);
+
+		addr = &core_if->dev_if->in_ep_regs[dwc_ep->num]->diepctl;
+
+		dma_ad = dwc_ep->dma_addr0;
+
+		dsts.d32 =
+		    DWC_READ_REG32(&core_if->dev_if->dev_global_regs->dsts);
+
+		sts.b_iso_in.bs = BS_HOST_READY;
+		sts.b_iso_in.txsts = 0;
+		sts.b_iso_in.sp =
+		    (dwc_ep->data_per_frame % dwc_ep->maxpacket) ? 1 : 0;
+		sts.b_iso_in.ioc = 0;
+		sts.b_iso_in.pid = dwc_ep->pkt_per_frm;
+
+		frmnumber = dwc_ep->next_frame;
+
+		sts.b_iso_in.framenum = frmnumber;
+		sts.b_iso_in.txbytes = dwc_ep->data_per_frame;
+		sts.b_iso_in.l = 0;
+
+		/** Buffer 0 descriptors setup */
+		for (i = 0; i < dwc_ep->desc_cnt - 1; i++) {
+			dma_desc->buf = dma_ad;
+			dma_desc->status.d32 = sts.d32;
+			dma_desc++;
+
+			dma_ad += dwc_ep->data_per_frame;
+			sts.b_iso_in.framenum += dwc_ep->bInterval;
+		}
+
+		sts.b_iso_in.ioc = 1;
+		dma_desc->buf = dma_ad;
+		dma_desc->status.d32 = sts.d32;
+		++dma_desc;
+
+		/** Buffer 1 descriptors setup */
+		sts.b_iso_in.ioc = 0;
+		dma_ad = dwc_ep->dma_addr1;
+
+		for (i = 0; i < dwc_ep->desc_cnt - dwc_ep->pkt_per_frm;
+		     i += dwc_ep->pkt_per_frm) {
+			dma_desc->buf = dma_ad;
+			dma_desc->status.d32 = sts.d32;
+			dma_desc++;
+
+			dma_ad += dwc_ep->data_per_frame;
+			sts.b_iso_in.framenum += dwc_ep->bInterval;
+
+			sts.b_iso_in.ioc = 0;
+		}
+		sts.b_iso_in.ioc = 1;
+		sts.b_iso_in.l = 1;
+
+		dma_desc->buf = dma_ad;
+		dma_desc->status.d32 = sts.d32;
+
+		dwc_ep->next_frame = sts.b_iso_in.framenum + dwc_ep->bInterval;
+
+		/** Write dma_ad into diepdma register */
+		DWC_WRITE_REG32(&(in_regs->diepdma),
+				(uint32_t) dwc_ep->iso_dma_desc_addr);
+	}
+	/** Enable endpoint, clear nak  */
+	depctl.d32 = 0;
+	depctl.b.epena = 1;
+	depctl.b.usbactep = 1;
+	depctl.b.cnak = 1;
+
+	DWC_MODIFY_REG32(addr, depctl.d32, depctl.d32);
+	depctl.d32 = DWC_READ_REG32(addr);
+}
+
+/**
+ * This function initializes a descriptor chain for Isochronous transfer
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param ep The EP to start the transfer on.
+ *
+ */
+void dwc_otg_iso_ep_start_buf_transfer(dwc_otg_core_if_t * core_if,
+				       dwc_ep_t * ep)
+{
+	depctl_data_t depctl = {.d32 = 0 };
+	volatile uint32_t *addr;
+
+	if (ep->is_in) {
+		addr = &core_if->dev_if->in_ep_regs[ep->num]->diepctl;
+	} else {
+		addr = &core_if->dev_if->out_ep_regs[ep->num]->doepctl;
+	}
+
+	if (core_if->dma_enable == 0 || core_if->dma_desc_enable != 0) {
+		return;
+	} else {
+		deptsiz_data_t deptsiz = {.d32 = 0 };
+
+		ep->xfer_len =
+		    ep->data_per_frame * ep->buf_proc_intrvl / ep->bInterval;
+		ep->pkt_cnt =
+		    (ep->xfer_len - 1 + ep->maxpacket) / ep->maxpacket;
+		ep->xfer_count = 0;
+		ep->xfer_buff =
+		    (ep->proc_buf_num) ? ep->xfer_buff1 : ep->xfer_buff0;
+		ep->dma_addr =
+		    (ep->proc_buf_num) ? ep->dma_addr1 : ep->dma_addr0;
+
+		if (ep->is_in) {
+			/* Program the transfer size and packet count
+			 *      as follows: xfersize = N * maxpacket +
+			 *      short_packet pktcnt = N + (short_packet
+			 *      exist ? 1 : 0)
+			 */
+			deptsiz.b.mc = ep->pkt_per_frm;
+			deptsiz.b.xfersize = ep->xfer_len;
+			deptsiz.b.pktcnt =
+			    (ep->xfer_len - 1 + ep->maxpacket) / ep->maxpacket;
+			DWC_WRITE_REG32(&core_if->dev_if->
+					in_ep_regs[ep->num]->dieptsiz,
+					deptsiz.d32);
+
+			/* Write the DMA register */
+			DWC_WRITE_REG32(&
+					(core_if->dev_if->
+					 in_ep_regs[ep->num]->diepdma),
+					(uint32_t) ep->dma_addr);
+
+		} else {
+			deptsiz.b.pktcnt =
+			    (ep->xfer_len + (ep->maxpacket - 1)) /
+			    ep->maxpacket;
+			deptsiz.b.xfersize = deptsiz.b.pktcnt * ep->maxpacket;
+
+			DWC_WRITE_REG32(&core_if->dev_if->
+					out_ep_regs[ep->num]->doeptsiz,
+					deptsiz.d32);
+
+			/* Write the DMA register */
+			DWC_WRITE_REG32(&
+					(core_if->dev_if->
+					 out_ep_regs[ep->num]->doepdma),
+					(uint32_t) ep->dma_addr);
+
+		}
+		/** Enable endpoint, clear nak  */
+		depctl.d32 = 0;
+		depctl.b.epena = 1;
+		depctl.b.cnak = 1;
+
+		DWC_MODIFY_REG32(addr, depctl.d32, depctl.d32);
+	}
+}
+
+/**
+ * This function does the setup for a data transfer for an EP and
+ * starts the transfer. For an IN transfer, the packets will be
+ * loaded into the appropriate Tx FIFO in the ISR. For OUT transfers,
+ * the packets are unloaded from the Rx FIFO in the ISR.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param ep The EP to start the transfer on.
+ */
+
+static void dwc_otg_iso_ep_start_transfer(dwc_otg_core_if_t * core_if,
+					  dwc_ep_t * ep)
+{
+	if (core_if->dma_enable) {
+		if (core_if->dma_desc_enable) {
+			if (ep->is_in) {
+				ep->desc_cnt = ep->pkt_cnt / ep->pkt_per_frm;
+			} else {
+				ep->desc_cnt = ep->pkt_cnt;
+			}
+			dwc_otg_iso_ep_start_ddma_transfer(core_if, ep);
+		} else {
+			if (core_if->pti_enh_enable) {
+				dwc_otg_iso_ep_start_buf_transfer(core_if, ep);
+			} else {
+				ep->cur_pkt_addr =
+				    (ep->proc_buf_num) ? ep->
+				    xfer_buff1 : ep->xfer_buff0;
+				ep->cur_pkt_dma_addr =
+				    (ep->proc_buf_num) ? ep->
+				    dma_addr1 : ep->dma_addr0;
+				dwc_otg_iso_ep_start_frm_transfer(core_if, ep);
+			}
+		}
+	} else {
+		ep->cur_pkt_addr =
+		    (ep->proc_buf_num) ? ep->xfer_buff1 : ep->xfer_buff0;
+		ep->cur_pkt_dma_addr =
+		    (ep->proc_buf_num) ? ep->dma_addr1 : ep->dma_addr0;
+		dwc_otg_iso_ep_start_frm_transfer(core_if, ep);
+	}
+}
+
+/**
+ * This function stops transfer for an EP and
+ * resets the ep's variables.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param ep The EP to start the transfer on.
+ */
+
+void dwc_otg_iso_ep_stop_transfer(dwc_otg_core_if_t * core_if, dwc_ep_t * ep)
+{
+	depctl_data_t depctl = {.d32 = 0 };
+	volatile uint32_t *addr;
+
+	if (ep->is_in == 1) {
+		addr = &core_if->dev_if->in_ep_regs[ep->num]->diepctl;
+	} else {
+		addr = &core_if->dev_if->out_ep_regs[ep->num]->doepctl;
+	}
+
+	/* disable the ep */
+	depctl.d32 = DWC_READ_REG32(addr);
+
+	depctl.b.epdis = 1;
+	depctl.b.snak = 1;
+
+	DWC_WRITE_REG32(addr, depctl.d32);
+
+	if (core_if->dma_desc_enable &&
+	    ep->iso_desc_addr && ep->iso_dma_desc_addr) {
+		dwc_otg_ep_free_desc_chain(ep->iso_desc_addr,
+					   ep->iso_dma_desc_addr,
+					   ep->desc_cnt * 2);
+	}
+
+	/* reset varibales */
+	ep->dma_addr0 = 0;
+	ep->dma_addr1 = 0;
+	ep->xfer_buff0 = 0;
+	ep->xfer_buff1 = 0;
+	ep->data_per_frame = 0;
+	ep->data_pattern_frame = 0;
+	ep->sync_frame = 0;
+	ep->buf_proc_intrvl = 0;
+	ep->bInterval = 0;
+	ep->proc_buf_num = 0;
+	ep->pkt_per_frm = 0;
+	ep->pkt_per_frm = 0;
+	ep->desc_cnt = 0;
+	ep->iso_desc_addr = 0;
+	ep->iso_dma_desc_addr = 0;
+}
+
+int dwc_otg_pcd_iso_ep_start(dwc_otg_pcd_t * pcd, void *ep_handle,
+			     uint8_t * buf0, uint8_t * buf1, dwc_dma_t dma0,
+			     dwc_dma_t dma1, int sync_frame, int dp_frame,
+			     int data_per_frame, int start_frame,
+			     int buf_proc_intrvl, void *req_handle,
+			     int atomic_alloc)
+{
+	dwc_otg_pcd_ep_t *ep;
+	dwc_irqflags_t flags = 0;
+	dwc_ep_t *dwc_ep;
+	int32_t frm_data;
+	dsts_data_t dsts;
+	dwc_otg_core_if_t *core_if;
+
+	ep = get_ep_from_handle(pcd, ep_handle);
+
+	if (!ep || !ep->desc || ep->dwc_ep.num == 0) {
+		DWC_WARN("bad ep\n");
+		return -DWC_E_INVALID;
+	}
+
+	DWC_SPINLOCK_IRQSAVE(pcd->lock, &flags);
+	core_if = GET_CORE_IF(pcd);
+	dwc_ep = &ep->dwc_ep;
+
+	if (ep->iso_req_handle) {
+		DWC_WARN("ISO request in progress\n");
+	}
+
+	dwc_ep->dma_addr0 = dma0;
+	dwc_ep->dma_addr1 = dma1;
+
+	dwc_ep->xfer_buff0 = buf0;
+	dwc_ep->xfer_buff1 = buf1;
+
+	dwc_ep->data_per_frame = data_per_frame;
+
+	/** @todo - pattern data support is to be implemented in the future */
+	dwc_ep->data_pattern_frame = dp_frame;
+	dwc_ep->sync_frame = sync_frame;
+
+	dwc_ep->buf_proc_intrvl = buf_proc_intrvl;
+
+	dwc_ep->bInterval = 1 << (ep->desc->bInterval - 1);
+
+	dwc_ep->proc_buf_num = 0;
+
+	dwc_ep->pkt_per_frm = 0;
+	frm_data = ep->dwc_ep.data_per_frame;
+	while (frm_data > 0) {
+		dwc_ep->pkt_per_frm++;
+		frm_data -= ep->dwc_ep.maxpacket;
+	}
+
+	dsts.d32 = DWC_READ_REG32(&core_if->dev_if->dev_global_regs->dsts);
+
+	if (start_frame == -1) {
+		dwc_ep->next_frame = dsts.b.soffn + 1;
+		if (dwc_ep->bInterval != 1) {
+			dwc_ep->next_frame =
+			    dwc_ep->next_frame + (dwc_ep->bInterval - 1 -
+						  dwc_ep->next_frame %
+						  dwc_ep->bInterval);
+		}
+	} else {
+		dwc_ep->next_frame = start_frame;
+	}
+
+	if (!core_if->pti_enh_enable) {
+		dwc_ep->pkt_cnt =
+		    dwc_ep->buf_proc_intrvl * dwc_ep->pkt_per_frm /
+		    dwc_ep->bInterval;
+	} else {
+		dwc_ep->pkt_cnt =
+		    (dwc_ep->data_per_frame *
+		     (dwc_ep->buf_proc_intrvl / dwc_ep->bInterval)
+		     - 1 + dwc_ep->maxpacket) / dwc_ep->maxpacket;
+	}
+
+	if (core_if->dma_desc_enable) {
+		dwc_ep->desc_cnt =
+		    dwc_ep->buf_proc_intrvl * dwc_ep->pkt_per_frm /
+		    dwc_ep->bInterval;
+	}
+
+	if (atomic_alloc) {
+		dwc_ep->pkt_info =
+		    DWC_ALLOC_ATOMIC(sizeof(iso_pkt_info_t) * dwc_ep->pkt_cnt);
+	} else {
+		dwc_ep->pkt_info =
+		    DWC_ALLOC(sizeof(iso_pkt_info_t) * dwc_ep->pkt_cnt);
+	}
+	if (!dwc_ep->pkt_info) {
+		DWC_SPINUNLOCK_IRQRESTORE(pcd->lock, flags);
+		return -DWC_E_NO_MEMORY;
+	}
+	if (core_if->pti_enh_enable) {
+		dwc_memset(dwc_ep->pkt_info, 0,
+			   sizeof(iso_pkt_info_t) * dwc_ep->pkt_cnt);
+	}
+
+	dwc_ep->cur_pkt = 0;
+	ep->iso_req_handle = req_handle;
+
+	DWC_SPINUNLOCK_IRQRESTORE(pcd->lock, flags);
+	dwc_otg_iso_ep_start_transfer(core_if, dwc_ep);
+	return 0;
+}
+
+int dwc_otg_pcd_iso_ep_stop(dwc_otg_pcd_t * pcd, void *ep_handle,
+			    void *req_handle)
+{
+	dwc_irqflags_t flags = 0;
+	dwc_otg_pcd_ep_t *ep;
+	dwc_ep_t *dwc_ep;
+
+	ep = get_ep_from_handle(pcd, ep_handle);
+	if (!ep || !ep->desc || ep->dwc_ep.num == 0) {
+		DWC_WARN("bad ep\n");
+		return -DWC_E_INVALID;
+	}
+	dwc_ep = &ep->dwc_ep;
+
+	dwc_otg_iso_ep_stop_transfer(GET_CORE_IF(pcd), dwc_ep);
+
+	DWC_FREE(dwc_ep->pkt_info);
+	DWC_SPINLOCK_IRQSAVE(pcd->lock, &flags);
+	if (ep->iso_req_handle != req_handle) {
+		DWC_SPINUNLOCK_IRQRESTORE(pcd->lock, flags);
+		return -DWC_E_INVALID;
+	}
+
+	DWC_SPINUNLOCK_IRQRESTORE(pcd->lock, flags);
+
+	ep->iso_req_handle = 0;
+	return 0;
+}
+
+/**
+ * This function is used for perodical data exchnage between PCD and gadget drivers.
+ * for Isochronous EPs
+ *
+ *	- Every time a sync period completes this function is called to
+ *	  perform data exchange between PCD and gadget
+ */
+void dwc_otg_iso_buffer_done(dwc_otg_pcd_t * pcd, dwc_otg_pcd_ep_t * ep,
+			     void *req_handle)
+{
+	int i;
+	dwc_ep_t *dwc_ep;
+
+	dwc_ep = &ep->dwc_ep;
+
+	DWC_SPINUNLOCK(ep->pcd->lock);
+	pcd->fops->isoc_complete(pcd, ep->priv, ep->iso_req_handle,
+				 dwc_ep->proc_buf_num ^ 0x1);
+	DWC_SPINLOCK(ep->pcd->lock);
+
+	for (i = 0; i < dwc_ep->pkt_cnt; ++i) {
+		dwc_ep->pkt_info[i].status = 0;
+		dwc_ep->pkt_info[i].offset = 0;
+		dwc_ep->pkt_info[i].length = 0;
+	}
+}
+
+int dwc_otg_pcd_get_iso_packet_count(dwc_otg_pcd_t * pcd, void *ep_handle,
+				     void *iso_req_handle)
+{
+	dwc_otg_pcd_ep_t *ep;
+	dwc_ep_t *dwc_ep;
+
+	ep = get_ep_from_handle(pcd, ep_handle);
+	if (!ep->desc || ep->dwc_ep.num == 0) {
+		DWC_WARN("bad ep\n");
+		return -DWC_E_INVALID;
+	}
+	dwc_ep = &ep->dwc_ep;
+
+	return dwc_ep->pkt_cnt;
+}
+
+void dwc_otg_pcd_get_iso_packet_params(dwc_otg_pcd_t * pcd, void *ep_handle,
+				       void *iso_req_handle, int packet,
+				       int *status, int *actual, int *offset)
+{
+	dwc_otg_pcd_ep_t *ep;
+	dwc_ep_t *dwc_ep;
+
+	ep = get_ep_from_handle(pcd, ep_handle);
+	if (!ep)
+		DWC_WARN("bad ep\n");
+
+	dwc_ep = &ep->dwc_ep;
+
+	*status = dwc_ep->pkt_info[packet].status;
+	*actual = dwc_ep->pkt_info[packet].length;
+	*offset = dwc_ep->pkt_info[packet].offset;
+}
+
+#endif /* DWC_EN_ISOC */
+
+static void dwc_otg_pcd_init_ep(dwc_otg_pcd_t * pcd, dwc_otg_pcd_ep_t * pcd_ep,
+				uint32_t is_in, uint32_t ep_num)
+{
+	/* Init EP structure */
+	pcd_ep->desc = 0;
+	pcd_ep->pcd = pcd;
+	pcd_ep->stopped = 1;
+	pcd_ep->queue_sof = 0;
+
+	/* Init DWC ep structure */
+	pcd_ep->dwc_ep.is_in = is_in;
+	pcd_ep->dwc_ep.num = ep_num;
+	pcd_ep->dwc_ep.active = 0;
+	pcd_ep->dwc_ep.tx_fifo_num = 0;
+	/* Control until ep is actvated */
+	pcd_ep->dwc_ep.type = DWC_OTG_EP_TYPE_CONTROL;
+	pcd_ep->dwc_ep.maxpacket = MAX_PACKET_SIZE;
+	pcd_ep->dwc_ep.dma_addr = 0;
+	pcd_ep->dwc_ep.start_xfer_buff = 0;
+	pcd_ep->dwc_ep.xfer_buff = 0;
+	pcd_ep->dwc_ep.xfer_len = 0;
+	pcd_ep->dwc_ep.xfer_count = 0;
+	pcd_ep->dwc_ep.sent_zlp = 0;
+	pcd_ep->dwc_ep.total_len = 0;
+	pcd_ep->dwc_ep.desc_addr = 0;
+	pcd_ep->dwc_ep.dma_desc_addr = 0;
+	DWC_CIRCLEQ_INIT(&pcd_ep->queue);
+}
+
+/**
+ * Initialize ep's
+ */
+static void dwc_otg_pcd_reinit(dwc_otg_pcd_t * pcd)
+{
+	int i;
+	uint32_t hwcfg1;
+	dwc_otg_pcd_ep_t *ep;
+	int in_ep_cntr, out_ep_cntr;
+	uint32_t num_in_eps = (GET_CORE_IF(pcd))->dev_if->num_in_eps;
+	uint32_t num_out_eps = (GET_CORE_IF(pcd))->dev_if->num_out_eps;
+
+	/**
+	 * Initialize the EP0 structure.
+	 */
+	ep = &pcd->ep0;
+	dwc_otg_pcd_init_ep(pcd, ep, 0, 0);
+
+	in_ep_cntr = 0;
+	hwcfg1 = (GET_CORE_IF(pcd))->hwcfg1.d32 >> 3;
+	for (i = 1; in_ep_cntr < num_in_eps; i++) {
+		if ((hwcfg1 & 0x1) == 0) {
+			dwc_otg_pcd_ep_t *ep = &pcd->in_ep[in_ep_cntr];
+			in_ep_cntr++;
+			/**
+			 * @todo NGS: Add direction to EP, based on contents
+			 * of HWCFG1.  Need a copy of HWCFG1 in pcd structure?
+			 * sprintf(";r
+			 */
+			dwc_otg_pcd_init_ep(pcd, ep, 1 /* IN */ , i);
+
+			DWC_CIRCLEQ_INIT(&ep->queue);
+		}
+		hwcfg1 >>= 2;
+	}
+
+	out_ep_cntr = 0;
+	hwcfg1 = (GET_CORE_IF(pcd))->hwcfg1.d32 >> 2;
+	for (i = 1; out_ep_cntr < num_out_eps; i++) {
+		if ((hwcfg1 & 0x1) == 0) {
+			dwc_otg_pcd_ep_t *ep = &pcd->out_ep[out_ep_cntr];
+			out_ep_cntr++;
+			/**
+			 * @todo NGS: Add direction to EP, based on contents
+			 * of HWCFG1.  Need a copy of HWCFG1 in pcd structure?
+			 * sprintf(";r
+			 */
+			dwc_otg_pcd_init_ep(pcd, ep, 0 /* OUT */ , i);
+			DWC_CIRCLEQ_INIT(&ep->queue);
+		}
+		hwcfg1 >>= 2;
+	}
+
+	pcd->ep0state = EP0_DISCONNECT;
+	pcd->ep0.dwc_ep.maxpacket = MAX_EP0_SIZE;
+	pcd->ep0.dwc_ep.type = DWC_OTG_EP_TYPE_CONTROL;
+}
+
+/**
+ * This function is called when the SRP timer expires. The SRP should
+ * complete within 6 seconds.
+ */
+static void srp_timeout(void *ptr)
+{
+	gotgctl_data_t gotgctl;
+	dwc_otg_core_if_t *core_if = (dwc_otg_core_if_t *) ptr;
+	volatile uint32_t *addr = &core_if->core_global_regs->gotgctl;
+
+	gotgctl.d32 = DWC_READ_REG32(addr);
+
+	core_if->srp_timer_started = 0;
+
+	if (core_if->adp_enable) {
+		if (gotgctl.b.bsesvld == 0) {
+			gpwrdn_data_t gpwrdn = {.d32 = 0 };
+			DWC_PRINTF("SRP Timeout BSESSVLD = 0\n");
+			/* Power off the core */
+			if (core_if->power_down == 2) {
+				gpwrdn.b.pwrdnswtch = 1;
+				DWC_MODIFY_REG32(&core_if->core_global_regs->
+					gpwrdn, gpwrdn.d32, 0);
+			}
+
+			gpwrdn.d32 = 0;
+			gpwrdn.b.pmuintsel = 1;
+			gpwrdn.b.pmuactv = 1;
+			DWC_MODIFY_REG32(&core_if->core_global_regs->gpwrdn, 0, gpwrdn.d32);
+			dwc_otg_adp_probe_start(core_if);
+		} else {
+			DWC_PRINTF("SRP Timeout BSESSVLD = 1\n");
+			core_if->op_state = B_PERIPHERAL;
+			dwc_otg_core_init(core_if);
+			dwc_otg_enable_global_interrupts(core_if);
+			cil_pcd_start(core_if);
+		}
+	}
+
+	if ((core_if->core_params->phy_type == DWC_PHY_TYPE_PARAM_FS) &&
+	    (core_if->core_params->i2c_enable)) {
+		DWC_PRINTF("SRP Timeout\n");
+
+		if ((core_if->srp_success) && (gotgctl.b.bsesvld)) {
+			if (core_if->pcd_cb && core_if->pcd_cb->resume_wakeup) {
+				core_if->pcd_cb->resume_wakeup(core_if->pcd_cb->p);
+			}
+
+			/* Clear Session Request */
+#ifndef CONFIG_MACH_M822XX
+			/* Base value of GOTGCTL, VBUS checking enabled */
+			gotgctl.d32 = 0;
+#else  /* CONFIG_MACH_M822XX */
+			/* Base value of GOTGCTL, VBUS checking disabled */
+			gotgctl.d32 = ((1<<3) | (1<<2));
+#endif	/* CONFIG_MACH_M822XX */
+
+			gotgctl.b.sesreq = 1;
+			DWC_MODIFY_REG32(&core_if->core_global_regs->gotgctl,
+					 gotgctl.d32, 0);
+
+			core_if->srp_success = 0;
+		} else {
+			__DWC_ERROR("Device not connected/responding\n");
+			gotgctl.b.sesreq = 0;
+			DWC_WRITE_REG32(addr, gotgctl.d32);
+		}
+	} else if (gotgctl.b.sesreq) {
+		DWC_PRINTF("SRP Timeout\n");
+
+		__DWC_ERROR("Device not connected/responding\n");
+		gotgctl.b.sesreq = 0;
+		DWC_WRITE_REG32(addr, gotgctl.d32);
+	} else {
+		DWC_PRINTF(" SRP GOTGCTL=%0x\n", gotgctl.d32);
+	}
+}
+
+/**
+ * Tasklet
+ *
+ */
+extern void start_next_request(dwc_otg_pcd_ep_t * ep);
+
+static void start_xfer_tasklet_func(void *data)
+{
+	dwc_otg_pcd_t *pcd = (dwc_otg_pcd_t *) data;
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(pcd);
+
+	int i;
+	depctl_data_t diepctl;
+
+	DWC_DEBUGPL(DBG_PCDV, "Start xfer tasklet\n");
+
+	diepctl.d32 = DWC_READ_REG32(&core_if->dev_if->in_ep_regs[0]->diepctl);
+
+	if (pcd->ep0.queue_sof) {
+		pcd->ep0.queue_sof = 0;
+		start_next_request(&pcd->ep0);
+		// break;
+	}
+
+	for (i = 0; i < core_if->dev_if->num_in_eps; i++) {
+		depctl_data_t diepctl;
+		diepctl.d32 =
+		    DWC_READ_REG32(&core_if->dev_if->in_ep_regs[i]->diepctl);
+
+		if (pcd->in_ep[i].queue_sof) {
+			pcd->in_ep[i].queue_sof = 0;
+			start_next_request(&pcd->in_ep[i]);
+			// break;
+		}
+	}
+
+	return;
+}
+
+/**
+ * This function initialized the PCD portion of the driver.
+ *
+ */
+dwc_otg_pcd_t *dwc_otg_pcd_init(dwc_otg_core_if_t * core_if)
+{
+	dwc_otg_pcd_t *pcd = NULL;
+	dwc_otg_dev_if_t *dev_if;
+	int i;
+
+	/*
+	 * Allocate PCD structure
+	 */
+	pcd = DWC_ALLOC(sizeof(dwc_otg_pcd_t));
+
+	if (pcd == NULL) {
+		return NULL;
+	}
+
+	pcd->lock = DWC_SPINLOCK_ALLOC();
+	if (!pcd->lock) {
+		DWC_ERROR("Could not allocate lock for pcd");
+		DWC_FREE(pcd);
+		return NULL;
+	}
+	/* Set core_if's lock pointer to hcd->lock */
+	core_if->lock = pcd->lock;
+	pcd->core_if = core_if;
+
+	dev_if = core_if->dev_if;
+	dev_if->isoc_ep = NULL;
+
+	if (core_if->hwcfg4.b.ded_fifo_en) {
+		DWC_PRINTF("Dedicated Tx FIFOs mode\n");
+	} else {
+		DWC_PRINTF("Shared Tx FIFO mode\n");
+	}
+
+	/*
+	 * Initialized the Core for Device mode here if there is nod ADP support.
+	 * Otherwise it will be done later in dwc_otg_adp_start routine.
+	 */
+	if (dwc_otg_is_device_mode(core_if) /*&& !core_if->adp_enable*/) {
+		dwc_otg_core_dev_init(core_if);
+	}
+
+	/*
+	 * Register the PCD Callbacks.
+	 */
+	dwc_otg_cil_register_pcd_callbacks(core_if, &pcd_callbacks, pcd);
+
+	/*
+	 * Initialize the DMA buffer for SETUP packets
+	 */
+	if (GET_CORE_IF(pcd)->dma_enable) {
+		pcd->setup_pkt =
+		    DWC_DMA_ALLOC(sizeof(*pcd->setup_pkt) * 5,
+				  &pcd->setup_pkt_dma_handle);
+		if (pcd->setup_pkt == NULL) {
+			DWC_FREE(pcd);
+			return NULL;
+		}
+
+		pcd->status_buf =
+		    DWC_DMA_ALLOC(sizeof(uint16_t),
+				  &pcd->status_buf_dma_handle);
+		if (pcd->status_buf == NULL) {
+			DWC_DMA_FREE(sizeof(*pcd->setup_pkt) * 5,
+				     pcd->setup_pkt, pcd->setup_pkt_dma_handle);
+			DWC_FREE(pcd);
+			return NULL;
+		}
+
+		if (GET_CORE_IF(pcd)->dma_desc_enable) {
+			dev_if->setup_desc_addr[0] =
+			    dwc_otg_ep_alloc_desc_chain(&dev_if->
+							dma_setup_desc_addr[0],
+							1);
+			dev_if->setup_desc_addr[1] =
+			    dwc_otg_ep_alloc_desc_chain(&dev_if->
+							dma_setup_desc_addr[1],
+							1);
+			dev_if->in_desc_addr =
+			    dwc_otg_ep_alloc_desc_chain(&dev_if->
+							dma_in_desc_addr, 1);
+			dev_if->out_desc_addr =
+			    dwc_otg_ep_alloc_desc_chain(&dev_if->
+							dma_out_desc_addr, 1);
+
+			if (dev_if->setup_desc_addr[0] == 0
+			    || dev_if->setup_desc_addr[1] == 0
+			    || dev_if->in_desc_addr == 0
+			    || dev_if->out_desc_addr == 0) {
+
+				if (dev_if->out_desc_addr)
+					dwc_otg_ep_free_desc_chain(dev_if->
+								   out_desc_addr,
+								   dev_if->
+								   dma_out_desc_addr,
+								   1);
+				if (dev_if->in_desc_addr)
+					dwc_otg_ep_free_desc_chain(dev_if->
+								   in_desc_addr,
+								   dev_if->
+								   dma_in_desc_addr,
+								   1);
+				if (dev_if->setup_desc_addr[1])
+					dwc_otg_ep_free_desc_chain(dev_if->
+								   setup_desc_addr
+								   [1],
+								   dev_if->
+								   dma_setup_desc_addr
+								   [1], 1);
+				if (dev_if->setup_desc_addr[0])
+					dwc_otg_ep_free_desc_chain(dev_if->
+								   setup_desc_addr
+								   [0],
+								   dev_if->
+								   dma_setup_desc_addr
+								   [0], 1);
+
+				DWC_DMA_FREE(sizeof(*pcd->setup_pkt) * 5,
+					     pcd->setup_pkt,
+					     pcd->setup_pkt_dma_handle);
+				DWC_DMA_FREE(sizeof(*pcd->status_buf),
+					     pcd->status_buf,
+					     pcd->status_buf_dma_handle);
+
+				DWC_FREE(pcd);
+
+				return NULL;
+			}
+		}
+	} else {
+		pcd->setup_pkt = DWC_ALLOC(sizeof(*pcd->setup_pkt) * 5);
+		if (pcd->setup_pkt == NULL) {
+			DWC_FREE(pcd);
+			return NULL;
+		}
+
+		pcd->status_buf = DWC_ALLOC(sizeof(uint16_t));
+		if (pcd->status_buf == NULL) {
+			DWC_FREE(pcd->setup_pkt);
+			DWC_FREE(pcd);
+			return NULL;
+		}
+	}
+
+	dwc_otg_pcd_reinit(pcd);
+
+	/* Allocate the cfi object for the PCD */
+#ifdef DWC_UTE_CFI
+	pcd->cfi = DWC_ALLOC(sizeof(cfiobject_t));
+	if (NULL == pcd->cfi)
+		goto fail;
+	if (init_cfi(pcd->cfi)) {
+		CFI_INFO("%s: Failed to init the CFI object\n", __func__);
+		goto fail;
+	}
+#endif
+
+	/* Initialize tasklets */
+	pcd->start_xfer_tasklet = DWC_TASK_ALLOC("xfer_tasklet",
+						 start_xfer_tasklet_func, pcd);
+	pcd->test_mode_tasklet = DWC_TASK_ALLOC("test_mode_tasklet",
+						do_test_mode, pcd);
+
+	/* Initialize SRP timer */
+	core_if->srp_timer = DWC_TIMER_ALLOC("SRP TIMER", srp_timeout, core_if);
+
+	if (core_if->core_params->dev_out_nak) {
+		/**
+		* Initialize xfer timeout timer. Implemented for
+		* 2.93a feature "Device DDMA OUT NAK Enhancement"
+		*/
+		for(i = 0; i < MAX_EPS_CHANNELS; i++) {
+			pcd->core_if->ep_xfer_timer[i] =
+				DWC_TIMER_ALLOC("ep timer", ep_xfer_timeout,
+				&pcd->core_if->ep_xfer_info[i]);
+		}
+	}
+
+	return pcd;
+#ifdef DWC_UTE_CFI
+fail:
+#endif
+	if (pcd->setup_pkt)
+		DWC_FREE(pcd->setup_pkt);
+	if (pcd->status_buf)
+		DWC_FREE(pcd->status_buf);
+#ifdef DWC_UTE_CFI
+	if (pcd->cfi)
+		DWC_FREE(pcd->cfi);
+#endif
+	if (pcd)
+		DWC_FREE(pcd);
+	return NULL;
+
+}
+
+/**
+ * Remove PCD specific data
+ */
+void dwc_otg_pcd_remove(dwc_otg_pcd_t * pcd)
+{
+	dwc_otg_dev_if_t *dev_if = GET_CORE_IF(pcd)->dev_if;
+	int i;
+	if (pcd->core_if->core_params->dev_out_nak) {
+		for (i = 0; i < MAX_EPS_CHANNELS; i++) {
+			DWC_TIMER_CANCEL(pcd->core_if->ep_xfer_timer[i]);
+			pcd->core_if->ep_xfer_info[i].state = 0;
+		}
+	}
+
+	if (GET_CORE_IF(pcd)->dma_enable) {
+		DWC_DMA_FREE(sizeof(*pcd->setup_pkt) * 5, pcd->setup_pkt,
+			     pcd->setup_pkt_dma_handle);
+		DWC_DMA_FREE(sizeof(uint16_t), pcd->status_buf,
+			     pcd->status_buf_dma_handle);
+		if (GET_CORE_IF(pcd)->dma_desc_enable) {
+			dwc_otg_ep_free_desc_chain(dev_if->setup_desc_addr[0],
+						   dev_if->dma_setup_desc_addr
+						   [0], 1);
+			dwc_otg_ep_free_desc_chain(dev_if->setup_desc_addr[1],
+						   dev_if->dma_setup_desc_addr
+						   [1], 1);
+			dwc_otg_ep_free_desc_chain(dev_if->in_desc_addr,
+						   dev_if->dma_in_desc_addr, 1);
+			dwc_otg_ep_free_desc_chain(dev_if->out_desc_addr,
+						   dev_if->dma_out_desc_addr,
+						   1);
+		}
+	} else {
+		DWC_FREE(pcd->setup_pkt);
+		DWC_FREE(pcd->status_buf);
+	}
+	DWC_SPINLOCK_FREE(pcd->lock);
+	/* Set core_if's lock pointer to NULL */
+	pcd->core_if->lock = NULL;
+
+	DWC_TASK_FREE(pcd->start_xfer_tasklet);
+	DWC_TASK_FREE(pcd->test_mode_tasklet);
+	if (pcd->core_if->core_params->dev_out_nak) {
+		for (i = 0; i < MAX_EPS_CHANNELS; i++) {
+			if (pcd->core_if->ep_xfer_timer[i]) {
+					DWC_TIMER_FREE(pcd->core_if->ep_xfer_timer[i]);
+			}
+		}
+	}
+
+/* Release the CFI object's dynamic memory */
+#ifdef DWC_UTE_CFI
+	if (pcd->cfi->ops.release) {
+		pcd->cfi->ops.release(pcd->cfi);
+	}
+#endif
+
+	DWC_FREE(pcd);
+}
+
+/**
+ * Returns whether registered pcd is dual speed or not
+ */
+uint32_t dwc_otg_pcd_is_dualspeed(dwc_otg_pcd_t * pcd)
+{
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(pcd);
+
+	if ((core_if->core_params->speed == DWC_SPEED_PARAM_FULL) ||
+	    ((core_if->hwcfg2.b.hs_phy_type == 2) &&
+	     (core_if->hwcfg2.b.fs_phy_type == 1) &&
+	     (core_if->core_params->ulpi_fs_ls))) {
+		return 0;
+	}
+
+	return 1;
+}
+
+/**
+ * Returns whether registered pcd is OTG capable or not
+ */
+uint32_t dwc_otg_pcd_is_otg(dwc_otg_pcd_t * pcd)
+{
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(pcd);
+	gusbcfg_data_t usbcfg = {.d32 = 0 };
+
+	usbcfg.d32 = DWC_READ_REG32(&core_if->core_global_regs->gusbcfg);
+	if (!usbcfg.b.srpcap || !usbcfg.b.hnpcap) {
+		return 0;
+	}
+
+	return 1;
+}
+
+/**
+ * This function assigns periodic Tx FIFO to an periodic EP
+ * in shared Tx FIFO mode
+ */
+static uint32_t assign_tx_fifo(dwc_otg_core_if_t * core_if)
+{
+	uint32_t TxMsk = 1;
+	int i;
+
+	for (i = 0; i < core_if->hwcfg4.b.num_in_eps; ++i) {
+		if ((TxMsk & core_if->tx_msk) == 0) {
+			core_if->tx_msk |= TxMsk;
+			return i + 1;
+		}
+		TxMsk <<= 1;
+	}
+	return 0;
+}
+
+/**
+ * This function assigns periodic Tx FIFO to an periodic EP
+ * in shared Tx FIFO mode
+ */
+static uint32_t assign_perio_tx_fifo(dwc_otg_core_if_t * core_if)
+{
+	uint32_t PerTxMsk = 1;
+	int i;
+	for (i = 0; i < core_if->hwcfg4.b.num_dev_perio_in_ep; ++i) {
+		if ((PerTxMsk & core_if->p_tx_msk) == 0) {
+			core_if->p_tx_msk |= PerTxMsk;
+			return i + 1;
+		}
+		PerTxMsk <<= 1;
+	}
+	return 0;
+}
+
+/**
+ * This function releases periodic Tx FIFO
+ * in shared Tx FIFO mode
+ */
+static void release_perio_tx_fifo(dwc_otg_core_if_t * core_if,
+				  uint32_t fifo_num)
+{
+	core_if->p_tx_msk =
+	    (core_if->p_tx_msk & (1 << (fifo_num - 1))) ^ core_if->p_tx_msk;
+}
+
+/**
+ * This function releases periodic Tx FIFO
+ * in shared Tx FIFO mode
+ */
+static void release_tx_fifo(dwc_otg_core_if_t * core_if, uint32_t fifo_num)
+{
+	core_if->tx_msk =
+	    (core_if->tx_msk & (1 << (fifo_num - 1))) ^ core_if->tx_msk;
+}
+
+/**
+ * This function is being called from gadget
+ * to enable PCD endpoint.
+ */
+int dwc_otg_pcd_ep_enable(dwc_otg_pcd_t * pcd,
+			  const uint8_t * ep_desc, void *usb_ep)
+{
+	int num, dir;
+	dwc_otg_pcd_ep_t *ep = NULL;
+	const usb_endpoint_descriptor_t *desc;
+	dwc_irqflags_t flags;
+	fifosize_data_t dptxfsiz = {.d32 = 0 };
+	gdfifocfg_data_t gdfifocfg = {.d32 = 0 };
+	gdfifocfg_data_t gdfifocfgbase = {.d32 = 0 };
+	int retval = 0;
+	int i, epcount;
+
+	desc = (const usb_endpoint_descriptor_t *)ep_desc;
+
+	if (!desc) {
+		pcd->ep0.priv = usb_ep;
+		ep = &pcd->ep0;
+		retval = -DWC_E_INVALID;
+		goto out;
+	}
+
+	num = UE_GET_ADDR(desc->bEndpointAddress);
+	dir = UE_GET_DIR(desc->bEndpointAddress);
+
+	if (!desc->wMaxPacketSize) {
+		DWC_WARN("bad maxpacketsize\n");
+		retval = -DWC_E_INVALID;
+		goto out;
+	}
+
+	if (dir == UE_DIR_IN) {
+		epcount = pcd->core_if->dev_if->num_in_eps;
+		for (i = 0; i < epcount; i++) {
+			if (num == pcd->in_ep[i].dwc_ep.num) {
+				ep = &pcd->in_ep[i];
+				break;
+			}
+		}
+	} else {
+		epcount = pcd->core_if->dev_if->num_out_eps;
+		for (i = 0; i < epcount; i++) {
+			if (num == pcd->out_ep[i].dwc_ep.num) {
+				ep = &pcd->out_ep[i];
+				break;
+			}
+		}
+	}
+
+	if (!ep) {
+		DWC_WARN("bad address\n");
+		retval = -DWC_E_INVALID;
+		goto out;
+	}
+
+	DWC_SPINLOCK_IRQSAVE(pcd->lock, &flags);
+
+	ep->desc = desc;
+	ep->priv = usb_ep;
+
+	/*
+	 * Activate the EP
+	 */
+	ep->stopped = 0;
+
+	ep->dwc_ep.is_in = (dir == UE_DIR_IN);
+	ep->dwc_ep.maxpacket = UGETW(desc->wMaxPacketSize);
+
+	ep->dwc_ep.type = desc->bmAttributes & UE_XFERTYPE;
+
+	if (ep->dwc_ep.is_in) {
+		if (!GET_CORE_IF(pcd)->en_multiple_tx_fifo) {
+			ep->dwc_ep.tx_fifo_num = 0;
+
+			if (ep->dwc_ep.type == UE_ISOCHRONOUS) {
+				/*
+				 * if ISOC EP then assign a Periodic Tx FIFO.
+				 */
+				ep->dwc_ep.tx_fifo_num =
+				    assign_perio_tx_fifo(GET_CORE_IF(pcd));
+			}
+		} else {
+			/*
+			 * if Dedicated FIFOs mode is on then assign a Tx FIFO.
+			 */
+			ep->dwc_ep.tx_fifo_num =
+			    assign_tx_fifo(GET_CORE_IF(pcd));
+		}
+
+		/* Calculating EP info controller base address */
+		if (ep->dwc_ep.tx_fifo_num && GET_CORE_IF(pcd)->en_multiple_tx_fifo) {
+			gdfifocfg.d32 =
+			    DWC_READ_REG32(&GET_CORE_IF(pcd)->core_global_regs->
+					   gdfifocfg);
+			gdfifocfgbase.d32 = gdfifocfg.d32 >> 16;
+			dptxfsiz.d32 =
+			    (DWC_READ_REG32
+			     (&GET_CORE_IF(pcd)->
+			      core_global_regs->dtxfsiz[ep->dwc_ep.
+							tx_fifo_num-1]) >> 16);
+			gdfifocfg.b.epinfobase =
+			    gdfifocfgbase.d32 + dptxfsiz.d32;
+			DWC_WRITE_REG32(&GET_CORE_IF(pcd)->core_global_regs->
+					gdfifocfg, gdfifocfg.d32);
+		}
+	}
+	/* Set initial data PID. */
+	if (ep->dwc_ep.type == UE_BULK) {
+		ep->dwc_ep.data_pid_start = 0;
+	}
+
+	/* Alloc DMA Descriptors */
+	if (GET_CORE_IF(pcd)->dma_desc_enable) {
+#ifndef DWC_UTE_PER_IO
+		if (ep->dwc_ep.type != UE_ISOCHRONOUS) {
+#endif
+			ep->dwc_ep.desc_addr =
+			    dwc_otg_ep_alloc_desc_chain(&ep->
+							dwc_ep.dma_desc_addr,
+							MAX_DMA_DESC_CNT);
+			if (!ep->dwc_ep.desc_addr) {
+				DWC_WARN("%s, can't allocate DMA descriptor\n",
+					 __func__);
+				retval = -DWC_E_SHUTDOWN;
+				DWC_SPINUNLOCK_IRQRESTORE(pcd->lock, flags);
+				goto out;
+			}
+#ifndef DWC_UTE_PER_IO
+		}
+#endif
+	}
+
+	DWC_DEBUGPL(DBG_PCD, "Activate %s: type=%d, mps=%d desc=%p\n",
+		    (ep->dwc_ep.is_in ? "IN" : "OUT"),
+		    ep->dwc_ep.type, ep->dwc_ep.maxpacket, ep->desc);
+#ifdef DWC_UTE_PER_IO
+	ep->dwc_ep.xiso_bInterval = 1 << (ep->desc->bInterval - 1);
+#endif
+	if (ep->dwc_ep.type == DWC_OTG_EP_TYPE_ISOC) {
+		ep->dwc_ep.bInterval = 1 << (ep->desc->bInterval - 1);
+		ep->dwc_ep.frame_num = 0xFFFFFFFF;
+	}
+
+	dwc_otg_ep_activate(GET_CORE_IF(pcd), &ep->dwc_ep);
+
+#ifdef DWC_UTE_CFI
+	if (pcd->cfi->ops.ep_enable) {
+		pcd->cfi->ops.ep_enable(pcd->cfi, pcd, ep);
+	}
+#endif
+
+	DWC_SPINUNLOCK_IRQRESTORE(pcd->lock, flags);
+
+out:
+	return retval;
+}
+
+/**
+ * This function is being called from gadget
+ * to disable PCD endpoint.
+ */
+int dwc_otg_pcd_ep_disable(dwc_otg_pcd_t * pcd, void *ep_handle)
+{
+	dwc_otg_pcd_ep_t *ep;
+	dwc_irqflags_t flags;
+	dwc_otg_dev_dma_desc_t *desc_addr;
+	dwc_dma_t dma_desc_addr;
+	gdfifocfg_data_t gdfifocfgbase = {.d32 = 0 };
+	gdfifocfg_data_t gdfifocfg = {.d32 = 0 };
+	fifosize_data_t dptxfsiz = {.d32 = 0 };
+
+	ep = get_ep_from_handle(pcd, ep_handle);
+
+	if (!ep || !ep->desc) {
+		DWC_DEBUGPL(DBG_PCD, "bad ep address\n");
+		return -DWC_E_INVALID;
+	}
+
+	DWC_SPINLOCK_IRQSAVE(pcd->lock, &flags);
+
+	dwc_otg_request_nuke(ep);
+
+	dwc_otg_ep_deactivate(GET_CORE_IF(pcd), &ep->dwc_ep);
+	if (pcd->core_if->core_params->dev_out_nak)
+	{
+		DWC_TIMER_CANCEL(pcd->core_if->ep_xfer_timer[ep->dwc_ep.num]);
+		pcd->core_if->ep_xfer_info[ep->dwc_ep.num].state = 0;
+	}
+	ep->desc = NULL;
+	ep->stopped = 1;
+
+	gdfifocfg.d32 =
+	    DWC_READ_REG32(&GET_CORE_IF(pcd)->core_global_regs->gdfifocfg);
+	gdfifocfgbase.d32 = gdfifocfg.d32 >> 16;
+
+	if (ep->dwc_ep.is_in) {
+		if (GET_CORE_IF(pcd)->en_multiple_tx_fifo) {
+			/* Flush the Tx FIFO */
+			dwc_otg_flush_tx_fifo(GET_CORE_IF(pcd), ep->dwc_ep.tx_fifo_num);
+		}
+		release_perio_tx_fifo(GET_CORE_IF(pcd), ep->dwc_ep.tx_fifo_num);
+		release_tx_fifo(GET_CORE_IF(pcd), ep->dwc_ep.tx_fifo_num);
+		if (GET_CORE_IF(pcd)->en_multiple_tx_fifo) {
+			/* Decreasing EPinfo Base Addr */
+			dptxfsiz.d32 =
+			    (DWC_READ_REG32
+			     (&GET_CORE_IF(pcd)->
+		      		core_global_regs->dtxfsiz[ep->dwc_ep.tx_fifo_num-1]) >> 16);
+			gdfifocfg.b.epinfobase = gdfifocfgbase.d32 - dptxfsiz.d32;
+			DWC_WRITE_REG32(&GET_CORE_IF(pcd)->core_global_regs->gdfifocfg,
+					gdfifocfg.d32);
+		}
+	}
+
+	/* Free DMA Descriptors */
+	if (GET_CORE_IF(pcd)->dma_desc_enable) {
+		if (ep->dwc_ep.type != UE_ISOCHRONOUS) {
+			desc_addr = ep->dwc_ep.desc_addr;
+			dma_desc_addr = ep->dwc_ep.dma_desc_addr;
+
+			/* Cannot call dma_free_coherent() with IRQs disabled */
+			DWC_SPINUNLOCK_IRQRESTORE(pcd->lock, flags);
+			dwc_otg_ep_free_desc_chain(desc_addr, dma_desc_addr,
+						   MAX_DMA_DESC_CNT);
+
+			goto out_unlocked;
+		}
+	}
+	DWC_SPINUNLOCK_IRQRESTORE(pcd->lock, flags);
+
+out_unlocked:
+	DWC_DEBUGPL(DBG_PCD, "%d %s disabled\n", ep->dwc_ep.num,
+		    ep->dwc_ep.is_in ? "IN" : "OUT");
+	return 0;
+
+}
+
+/******************************************************************************/
+#ifdef DWC_UTE_PER_IO
+
+/**
+ * Free the request and its extended parts
+ *
+ */
+void dwc_pcd_xiso_ereq_free(dwc_otg_pcd_ep_t * ep, dwc_otg_pcd_request_t * req)
+{
+	DWC_FREE(req->ext_req.per_io_frame_descs);
+	DWC_FREE(req);
+}
+
+/**
+ * Start the next request in the endpoint's queue.
+ *
+ */
+int dwc_otg_pcd_xiso_start_next_request(dwc_otg_pcd_t * pcd,
+					dwc_otg_pcd_ep_t * ep)
+{
+	int i;
+	dwc_otg_pcd_request_t *req = NULL;
+	dwc_ep_t *dwcep = NULL;
+	struct dwc_iso_xreq_port *ereq = NULL;
+	struct dwc_iso_pkt_desc_port *ddesc_iso;
+	uint16_t nat;
+	depctl_data_t diepctl;
+
+	dwcep = &ep->dwc_ep;
+
+	if (dwcep->xiso_active_xfers > 0) {
+#if 0	//Disable this to decrease s/w overhead that is crucial for Isoc transfers
+		DWC_WARN("There are currently active transfers for EP%d \
+				(active=%d; queued=%d)", dwcep->num, dwcep->xiso_active_xfers,
+				dwcep->xiso_queued_xfers);
+#endif
+		return 0;
+	}
+
+	nat = UGETW(ep->desc->wMaxPacketSize);
+	nat = (nat >> 11) & 0x03;
+
+	if (!DWC_CIRCLEQ_EMPTY(&ep->queue)) {
+		req = DWC_CIRCLEQ_FIRST(&ep->queue);
+		ereq = &req->ext_req;
+		ep->stopped = 0;
+
+		/* Get the frame number */
+		dwcep->xiso_frame_num =
+		    dwc_otg_get_frame_number(GET_CORE_IF(pcd));
+		DWC_DEBUG("FRM_NUM=%d", dwcep->xiso_frame_num);
+
+		ddesc_iso = ereq->per_io_frame_descs;
+
+		if (dwcep->is_in) {
+			/* Setup DMA Descriptor chain for IN Isoc request */
+			for (i = 0; i < ereq->pio_pkt_count; i++) {
+				//if ((i % (nat + 1)) == 0)
+				if ( i > 0 )
+					dwcep->xiso_frame_num = (dwcep->xiso_bInterval +
+										dwcep->xiso_frame_num) & 0x3FFF;
+				dwcep->desc_addr[i].buf =
+				    req->dma + ddesc_iso[i].offset;
+				dwcep->desc_addr[i].status.b_iso_in.txbytes =
+				    ddesc_iso[i].length;
+				dwcep->desc_addr[i].status.b_iso_in.framenum =
+				    dwcep->xiso_frame_num;
+				dwcep->desc_addr[i].status.b_iso_in.bs =
+				    BS_HOST_READY;
+				dwcep->desc_addr[i].status.b_iso_in.txsts = 0;
+				dwcep->desc_addr[i].status.b_iso_in.sp =
+				    (ddesc_iso[i].length %
+				     dwcep->maxpacket) ? 1 : 0;
+				dwcep->desc_addr[i].status.b_iso_in.ioc = 0;
+				dwcep->desc_addr[i].status.b_iso_in.pid = nat + 1;
+				dwcep->desc_addr[i].status.b_iso_in.l = 0;
+
+				/* Process the last descriptor */
+				if (i == ereq->pio_pkt_count - 1) {
+					dwcep->desc_addr[i].status.b_iso_in.ioc = 1;
+					dwcep->desc_addr[i].status.b_iso_in.l = 1;
+				}
+			}
+
+			/* Setup and start the transfer for this endpoint */
+			dwcep->xiso_active_xfers++;
+			DWC_WRITE_REG32(&GET_CORE_IF(pcd)->dev_if->
+					in_ep_regs[dwcep->num]->diepdma,
+					dwcep->dma_desc_addr);
+			diepctl.d32 = 0;
+			diepctl.b.epena = 1;
+			diepctl.b.cnak = 1;
+			DWC_MODIFY_REG32(&GET_CORE_IF(pcd)->dev_if->
+					 in_ep_regs[dwcep->num]->diepctl, 0,
+					 diepctl.d32);
+		} else {
+			/* Setup DMA Descriptor chain for OUT Isoc request */
+			for (i = 0; i < ereq->pio_pkt_count; i++) {
+				//if ((i % (nat + 1)) == 0)
+				dwcep->xiso_frame_num = (dwcep->xiso_bInterval +
+										dwcep->xiso_frame_num) & 0x3FFF;
+				dwcep->desc_addr[i].buf =
+				    req->dma + ddesc_iso[i].offset;
+				dwcep->desc_addr[i].status.b_iso_out.rxbytes =
+				    ddesc_iso[i].length;
+				dwcep->desc_addr[i].status.b_iso_out.framenum =
+				    dwcep->xiso_frame_num;
+				dwcep->desc_addr[i].status.b_iso_out.bs =
+				    BS_HOST_READY;
+				dwcep->desc_addr[i].status.b_iso_out.rxsts = 0;
+				dwcep->desc_addr[i].status.b_iso_out.sp =
+				    (ddesc_iso[i].length %
+				     dwcep->maxpacket) ? 1 : 0;
+				dwcep->desc_addr[i].status.b_iso_out.ioc = 0;
+				dwcep->desc_addr[i].status.b_iso_out.pid = nat + 1;
+				dwcep->desc_addr[i].status.b_iso_out.l = 0;
+
+				/* Process the last descriptor */
+				if (i == ereq->pio_pkt_count - 1) {
+					dwcep->desc_addr[i].status.b_iso_out.ioc = 1;
+					dwcep->desc_addr[i].status.b_iso_out.l = 1;
+				}
+			}
+
+			/* Setup and start the transfer for this endpoint */
+			dwcep->xiso_active_xfers++;
+			DWC_WRITE_REG32(&GET_CORE_IF(pcd)->dev_if->
+					out_ep_regs[dwcep->num]->doepdma,
+					dwcep->dma_desc_addr);
+			diepctl.d32 = 0;
+			diepctl.b.epena = 1;
+			diepctl.b.cnak = 1;
+			DWC_MODIFY_REG32(&GET_CORE_IF(pcd)->dev_if->
+					 out_ep_regs[dwcep->num]->doepctl, 0,
+					 diepctl.d32);
+		}
+
+	} else {
+		ep->stopped = 1;
+	}
+
+	return 0;
+}
+
+/**
+ *	- Remove the request from the queue
+ */
+void complete_xiso_ep(dwc_otg_pcd_ep_t * ep)
+{
+	dwc_otg_pcd_request_t *req = NULL;
+	struct dwc_iso_xreq_port *ereq = NULL;
+	struct dwc_iso_pkt_desc_port *ddesc_iso = NULL;
+	dwc_ep_t *dwcep = NULL;
+	int i;
+
+	//DWC_DEBUG();
+	dwcep = &ep->dwc_ep;
+
+	/* Get the first pending request from the queue */
+	if (!DWC_CIRCLEQ_EMPTY(&ep->queue)) {
+		req = DWC_CIRCLEQ_FIRST(&ep->queue);
+		if (!req) {
+			DWC_PRINTF("complete_ep 0x%p, req = NULL!\n", ep);
+			return;
+		}
+		dwcep->xiso_active_xfers--;
+		dwcep->xiso_queued_xfers--;
+		/* Remove this request from the queue */
+		DWC_CIRCLEQ_REMOVE_INIT(&ep->queue, req, queue_entry);
+	} else {
+		DWC_PRINTF("complete_ep 0x%p, ep->queue empty!\n", ep);
+		return;
+	}
+
+	ep->stopped = 1;
+	ereq = &req->ext_req;
+	ddesc_iso = ereq->per_io_frame_descs;
+
+	if (dwcep->xiso_active_xfers < 0) {
+		DWC_WARN("EP#%d (xiso_active_xfers=%d)", dwcep->num,
+			 dwcep->xiso_active_xfers);
+	}
+
+	/* Fill the Isoc descs of portable extended req from dma descriptors */
+	for (i = 0; i < ereq->pio_pkt_count; i++) {
+		if (dwcep->is_in) {	/* IN endpoints */
+			ddesc_iso[i].actual_length = ddesc_iso[i].length -
+			    dwcep->desc_addr[i].status.b_iso_in.txbytes;
+			ddesc_iso[i].status =
+			    dwcep->desc_addr[i].status.b_iso_in.txsts;
+		} else {	/* OUT endpoints */
+			ddesc_iso[i].actual_length = ddesc_iso[i].length -
+			    dwcep->desc_addr[i].status.b_iso_out.rxbytes;
+			ddesc_iso[i].status =
+			    dwcep->desc_addr[i].status.b_iso_out.rxsts;
+		}
+	}
+
+	DWC_SPINUNLOCK(ep->pcd->lock);
+
+	/* Call the completion function in the non-portable logic */
+	ep->pcd->fops->xisoc_complete(ep->pcd, ep->priv, req->priv, 0,
+				      &req->ext_req);
+
+	DWC_SPINLOCK(ep->pcd->lock);
+
+	/* Free the request - specific freeing needed for extended request object */
+	dwc_pcd_xiso_ereq_free(ep, req);
+
+	/* Start the next request */
+	dwc_otg_pcd_xiso_start_next_request(ep->pcd, ep);
+
+	return;
+}
+
+/**
+ * Create and initialize the Isoc pkt descriptors of the extended request.
+ *
+ */
+static int dwc_otg_pcd_xiso_create_pkt_descs(dwc_otg_pcd_request_t * req,
+					     void *ereq_nonport,
+					     int atomic_alloc)
+{
+	struct dwc_iso_xreq_port *ereq = NULL;
+	struct dwc_iso_xreq_port *req_mapped = NULL;
+	struct dwc_iso_pkt_desc_port *ipds = NULL;	/* To be created in this function */
+	uint32_t pkt_count;
+	int i;
+
+	ereq = &req->ext_req;
+	req_mapped = (struct dwc_iso_xreq_port *)ereq_nonport;
+	pkt_count = req_mapped->pio_pkt_count;
+
+	/* Create the isoc descs */
+	if (atomic_alloc) {
+		ipds = DWC_ALLOC_ATOMIC(sizeof(*ipds) * pkt_count);
+	} else {
+		ipds = DWC_ALLOC(sizeof(*ipds) * pkt_count);
+	}
+
+	if (!ipds) {
+		DWC_ERROR("Failed to allocate isoc descriptors");
+		return -DWC_E_NO_MEMORY;
+	}
+
+	/* Initialize the extended request fields */
+	ereq->per_io_frame_descs = ipds;
+	ereq->error_count = 0;
+	ereq->pio_alloc_pkt_count = pkt_count;
+	ereq->pio_pkt_count = pkt_count;
+	ereq->tr_sub_flags = req_mapped->tr_sub_flags;
+
+	/* Init the Isoc descriptors */
+	for (i = 0; i < pkt_count; i++) {
+		ipds[i].length = req_mapped->per_io_frame_descs[i].length;
+		ipds[i].offset = req_mapped->per_io_frame_descs[i].offset;
+		ipds[i].status = req_mapped->per_io_frame_descs[i].status;	/* 0 */
+		ipds[i].actual_length =
+		    req_mapped->per_io_frame_descs[i].actual_length;
+	}
+
+	return 0;
+}
+
+static void prn_ext_request(struct dwc_iso_xreq_port *ereq)
+{
+	struct dwc_iso_pkt_desc_port *xfd = NULL;
+	int i;
+
+	DWC_DEBUG("per_io_frame_descs=%p", ereq->per_io_frame_descs);
+	DWC_DEBUG("tr_sub_flags=%d", ereq->tr_sub_flags);
+	DWC_DEBUG("error_count=%d", ereq->error_count);
+	DWC_DEBUG("pio_alloc_pkt_count=%d", ereq->pio_alloc_pkt_count);
+	DWC_DEBUG("pio_pkt_count=%d", ereq->pio_pkt_count);
+	DWC_DEBUG("res=%d", ereq->res);
+
+	for (i = 0; i < ereq->pio_pkt_count; i++) {
+		xfd = &ereq->per_io_frame_descs[0];
+		DWC_DEBUG("FD #%d", i);
+
+		DWC_DEBUG("xfd->actual_length=%d", xfd->actual_length);
+		DWC_DEBUG("xfd->length=%d", xfd->length);
+		DWC_DEBUG("xfd->offset=%d", xfd->offset);
+		DWC_DEBUG("xfd->status=%d", xfd->status);
+	}
+}
+
+/**
+ *
+ */
+int dwc_otg_pcd_xiso_ep_queue(dwc_otg_pcd_t * pcd, void *ep_handle,
+			      uint8_t * buf, dwc_dma_t dma_buf, uint32_t buflen,
+			      int zero, void *req_handle, int atomic_alloc,
+			      void *ereq_nonport)
+{
+	dwc_otg_pcd_request_t *req = NULL;
+	dwc_otg_pcd_ep_t *ep;
+	dwc_irqflags_t flags;
+	int res;
+
+	ep = get_ep_from_handle(pcd, ep_handle);
+	if (!ep) {
+		DWC_WARN("bad ep\n");
+		return -DWC_E_INVALID;
+	}
+
+	/* We support this extension only for DDMA mode */
+	if (ep->dwc_ep.type == DWC_OTG_EP_TYPE_ISOC)
+		if (!GET_CORE_IF(pcd)->dma_desc_enable)
+			return -DWC_E_INVALID;
+
+	/* Create a dwc_otg_pcd_request_t object */
+	if (atomic_alloc) {
+		req = DWC_ALLOC_ATOMIC(sizeof(*req));
+	} else {
+		req = DWC_ALLOC(sizeof(*req));
+	}
+
+	if (!req) {
+		return -DWC_E_NO_MEMORY;
+	}
+
+	/* Create the Isoc descs for this request which shall be the exact match
+	 * of the structure sent to us from the non-portable logic */
+	res =
+	    dwc_otg_pcd_xiso_create_pkt_descs(req, ereq_nonport, atomic_alloc);
+	if (res) {
+		DWC_WARN("Failed to init the Isoc descriptors");
+		DWC_FREE(req);
+		return res;
+	}
+
+	DWC_SPINLOCK_IRQSAVE(pcd->lock, &flags);
+
+	DWC_CIRCLEQ_INIT_ENTRY(req, queue_entry);
+	req->buf = buf;
+	req->dma = dma_buf;
+	req->length = buflen;
+	req->sent_zlp = zero;
+	req->priv = req_handle;
+
+	//DWC_SPINLOCK_IRQSAVE(pcd->lock, &flags);
+	ep->dwc_ep.dma_addr = dma_buf;
+	ep->dwc_ep.start_xfer_buff = buf;
+	ep->dwc_ep.xfer_buff = buf;
+	ep->dwc_ep.xfer_len = 0;
+	ep->dwc_ep.xfer_count = 0;
+	ep->dwc_ep.sent_zlp = 0;
+	ep->dwc_ep.total_len = buflen;
+
+	/* Add this request to the tail */
+	DWC_CIRCLEQ_INSERT_TAIL(&ep->queue, req, queue_entry);
+	ep->dwc_ep.xiso_queued_xfers++;
+
+//DWC_DEBUG("CP_0");
+//DWC_DEBUG("req->ext_req.tr_sub_flags=%d", req->ext_req.tr_sub_flags);
+//prn_ext_request((struct dwc_iso_xreq_port *) ereq_nonport);
+//prn_ext_request(&req->ext_req);
+
+	//DWC_SPINUNLOCK_IRQRESTORE(pcd->lock, flags);
+
+	/* If the req->status == ASAP  then check if there is any active transfer
+	 * for this endpoint. If no active transfers, then get the first entry
+	 * from the queue and start that transfer
+	 */
+	if (req->ext_req.tr_sub_flags == DWC_EREQ_TF_ASAP) {
+		res = dwc_otg_pcd_xiso_start_next_request(pcd, ep);
+		if (res) {
+			DWC_WARN("Failed to start the next Isoc transfer");
+			DWC_SPINUNLOCK_IRQRESTORE(pcd->lock, flags);
+			DWC_FREE(req);
+			return res;
+		}
+	}
+
+	DWC_SPINUNLOCK_IRQRESTORE(pcd->lock, flags);
+	return 0;
+}
+
+#endif
+/* END ifdef DWC_UTE_PER_IO ***************************************************/
+int dwc_otg_pcd_ep_queue(dwc_otg_pcd_t * pcd, void *ep_handle,
+			 uint8_t * buf, dwc_dma_t dma_buf, uint32_t buflen,
+			 int zero, void *req_handle, int atomic_alloc)
+{
+	dwc_irqflags_t flags;
+	dwc_otg_pcd_request_t *req;
+	dwc_otg_pcd_ep_t *ep;
+	uint32_t max_transfer;
+
+	ep = get_ep_from_handle(pcd, ep_handle);
+	if (!ep || (!ep->desc && ep->dwc_ep.num != 0)) {
+		DWC_WARN("bad ep\n");
+		return -DWC_E_INVALID;
+	}
+
+	if (atomic_alloc) {
+		req = DWC_ALLOC_ATOMIC(sizeof(*req));
+	} else {
+		req = DWC_ALLOC(sizeof(*req));
+	}
+
+	if (!req) {
+		return -DWC_E_NO_MEMORY;
+	}
+	DWC_CIRCLEQ_INIT_ENTRY(req, queue_entry);
+	if (!GET_CORE_IF(pcd)->core_params->opt) {
+		if (ep->dwc_ep.num != 0) {
+			DWC_ERROR("queue req %p, len %d buf %p\n",
+				  req_handle, buflen, buf);
+		}
+	}
+
+	req->buf = buf;
+	req->dma = dma_buf;
+	req->length = buflen;
+	req->sent_zlp = zero;
+	req->priv = req_handle;
+	req->dw_align_buf = NULL;
+	if ((dma_buf & 0x3) && GET_CORE_IF(pcd)->dma_enable
+			&& !GET_CORE_IF(pcd)->dma_desc_enable)
+		req->dw_align_buf = DWC_DMA_ALLOC(buflen,
+				 &req->dw_align_buf_dma);
+	DWC_SPINLOCK_IRQSAVE(pcd->lock, &flags);
+
+	/*
+	 * After adding request to the queue for IN ISOC wait for In Token Received
+	 * when TX FIFO is empty interrupt and for OUT ISOC wait for OUT Token
+	 * Received when EP is disabled interrupt to obtain starting microframe
+	 * (odd/even) start transfer
+	 */
+	if (ep->dwc_ep.type == DWC_OTG_EP_TYPE_ISOC)
+	{
+		if (req != 0) {
+			depctl_data_t depctl = {.d32 = DWC_READ_REG32(&pcd->core_if->dev_if->in_ep_regs[ep->dwc_ep.num]->diepctl)};
+			++pcd->request_pending;
+
+			DWC_CIRCLEQ_INSERT_TAIL(&ep->queue, req, queue_entry);
+			if (ep->dwc_ep.is_in)
+			{
+				depctl.b.cnak = 1;
+				DWC_WRITE_REG32(&pcd->core_if->dev_if->in_ep_regs[ep->dwc_ep.num]->diepctl, depctl.d32);
+			}
+
+			DWC_SPINUNLOCK_IRQRESTORE(pcd->lock, flags);
+		}
+		return 0;
+	}
+
+	/*
+	 * For EP0 IN without premature status, zlp is required?
+	 */
+	if (ep->dwc_ep.num == 0 && ep->dwc_ep.is_in) {
+		DWC_DEBUGPL(DBG_PCDV, "%d-OUT ZLP\n", ep->dwc_ep.num);
+		//_req->zero = 1;
+	}
+
+	/* Start the transfer */
+	if (DWC_CIRCLEQ_EMPTY(&ep->queue) && !ep->stopped) {
+		/* EP0 Transfer? */
+		if (ep->dwc_ep.num == 0) {
+			switch (pcd->ep0state) {
+			case EP0_IN_DATA_PHASE:
+				DWC_DEBUGPL(DBG_PCD,
+					    "%s ep0: EP0_IN_DATA_PHASE\n",
+					    __func__);
+				break;
+
+			case EP0_OUT_DATA_PHASE:
+				DWC_DEBUGPL(DBG_PCD,
+					    "%s ep0: EP0_OUT_DATA_PHASE\n",
+					    __func__);
+				if (pcd->request_config) {
+					/* Complete STATUS PHASE */
+					ep->dwc_ep.is_in = 1;
+					pcd->ep0state = EP0_IN_STATUS_PHASE;
+				}
+				break;
+
+			case EP0_IN_STATUS_PHASE:
+				DWC_DEBUGPL(DBG_PCD,
+					    "%s ep0: EP0_IN_STATUS_PHASE\n",
+					    __func__);
+				break;
+
+			default:
+				DWC_DEBUGPL(DBG_ANY, "ep0: odd state %d\n",
+					    pcd->ep0state);
+				DWC_SPINUNLOCK_IRQRESTORE(pcd->lock, flags);
+				return -DWC_E_SHUTDOWN;
+			}
+
+			ep->dwc_ep.dma_addr = dma_buf;
+			ep->dwc_ep.start_xfer_buff = buf;
+			ep->dwc_ep.xfer_buff = buf;
+			ep->dwc_ep.xfer_len = buflen;
+			ep->dwc_ep.xfer_count = 0;
+			ep->dwc_ep.sent_zlp = 0;
+			ep->dwc_ep.total_len = ep->dwc_ep.xfer_len;
+
+			if (zero) {
+				if ((ep->dwc_ep.xfer_len %
+				     ep->dwc_ep.maxpacket == 0)
+				    && (ep->dwc_ep.xfer_len != 0)) {
+					ep->dwc_ep.sent_zlp = 1;
+				}
+
+			}
+
+			dwc_otg_ep0_start_transfer(GET_CORE_IF(pcd),
+						   &ep->dwc_ep);
+		}		// non-ep0 endpoints
+		else {
+#ifdef DWC_UTE_CFI
+			if (ep->dwc_ep.buff_mode != BM_STANDARD) {
+				/* store the request length */
+				ep->dwc_ep.cfi_req_len = buflen;
+				pcd->cfi->ops.build_descriptors(pcd->cfi, pcd,
+								ep, req);
+			} else {
+#endif
+				max_transfer =
+				    GET_CORE_IF(ep->pcd)->
+				    core_params->max_transfer_size;
+
+				/* Setup and start the Transfer */
+				if (req->dw_align_buf){
+					if (ep->dwc_ep.is_in)
+						dwc_memcpy(req->dw_align_buf, buf, buflen);
+					ep->dwc_ep.dma_addr = req->dw_align_buf_dma;
+					ep->dwc_ep.start_xfer_buff = req->dw_align_buf;
+                                        ep->dwc_ep.xfer_buff = req->dw_align_buf;
+				} else {
+					ep->dwc_ep.dma_addr = dma_buf;
+					ep->dwc_ep.start_xfer_buff = buf;
+                                        ep->dwc_ep.xfer_buff = buf;
+				}
+				ep->dwc_ep.xfer_len = 0;
+				ep->dwc_ep.xfer_count = 0;
+				ep->dwc_ep.sent_zlp = 0;
+				ep->dwc_ep.total_len = buflen;
+
+				ep->dwc_ep.maxxfer = max_transfer;
+				if (GET_CORE_IF(pcd)->dma_desc_enable) {
+					uint32_t out_max_xfer =
+					    DDMA_MAX_TRANSFER_SIZE -
+					    (DDMA_MAX_TRANSFER_SIZE % 4);
+					if (ep->dwc_ep.is_in) {
+						if (ep->dwc_ep.maxxfer >
+						    DDMA_MAX_TRANSFER_SIZE) {
+							ep->dwc_ep.maxxfer =
+							    DDMA_MAX_TRANSFER_SIZE;
+						}
+					} else {
+						if (ep->dwc_ep.maxxfer >
+						    out_max_xfer) {
+							ep->dwc_ep.maxxfer =
+							    out_max_xfer;
+						}
+					}
+				}
+				if (ep->dwc_ep.maxxfer < ep->dwc_ep.total_len) {
+					ep->dwc_ep.maxxfer -=
+					    (ep->dwc_ep.maxxfer %
+					     ep->dwc_ep.maxpacket);
+				}
+
+				if (zero) {
+					if ((ep->dwc_ep.total_len %
+					     ep->dwc_ep.maxpacket == 0)
+					    && (ep->dwc_ep.total_len != 0)) {
+						ep->dwc_ep.sent_zlp = 1;
+					}
+				}
+#ifdef DWC_UTE_CFI
+			}
+#endif
+			dwc_otg_ep_start_transfer(GET_CORE_IF(pcd),
+						  &ep->dwc_ep);
+		}
+	}
+
+	if (req != 0) {
+		++pcd->request_pending;
+		DWC_CIRCLEQ_INSERT_TAIL(&ep->queue, req, queue_entry);
+		if (ep->dwc_ep.is_in && ep->stopped
+		    && !(GET_CORE_IF(pcd)->dma_enable)) {
+			/** @todo NGS Create a function for this. */
+			diepmsk_data_t diepmsk = {.d32 = 0 };
+			diepmsk.b.intktxfemp = 1;
+			if (GET_CORE_IF(pcd)->multiproc_int_enable) {
+				DWC_MODIFY_REG32(&GET_CORE_IF(pcd)->dev_if->
+						 dev_global_regs->
+						 diepeachintmsk[ep->dwc_ep.num],
+						 0, diepmsk.d32);
+			} else {
+				DWC_MODIFY_REG32(&GET_CORE_IF(pcd)->dev_if->
+						 dev_global_regs->diepmsk, 0,
+						 diepmsk.d32);
+			}
+
+		}
+	}
+	DWC_SPINUNLOCK_IRQRESTORE(pcd->lock, flags);
+
+	return 0;
+}
+
+int dwc_otg_pcd_ep_dequeue(dwc_otg_pcd_t * pcd, void *ep_handle,
+			   void *req_handle)
+{
+	dwc_irqflags_t flags;
+	dwc_otg_pcd_request_t *req;
+	dwc_otg_pcd_ep_t *ep;
+
+	ep = get_ep_from_handle(pcd, ep_handle);
+	if (!ep || (!ep->desc && ep->dwc_ep.num != 0)) {
+		DWC_WARN("bad argument\n");
+		return -DWC_E_INVALID;
+	}
+
+	DWC_SPINLOCK_IRQSAVE(pcd->lock, &flags);
+
+	/* make sure it's actually queued on this endpoint */
+	DWC_CIRCLEQ_FOREACH(req, &ep->queue, queue_entry) {
+		if (req->priv == (void *)req_handle) {
+			break;
+		}
+	}
+
+	if (req->priv != (void *)req_handle) {
+		DWC_SPINUNLOCK_IRQRESTORE(pcd->lock, flags);
+		return -DWC_E_INVALID;
+	}
+
+	if (!DWC_CIRCLEQ_EMPTY_ENTRY(req, queue_entry)) {
+		dwc_otg_request_done(ep, req, -DWC_E_RESTART);
+	} else {
+		req = NULL;
+	}
+
+	DWC_SPINUNLOCK_IRQRESTORE(pcd->lock, flags);
+
+	return req ? 0 : -DWC_E_SHUTDOWN;
+
+}
+
+int dwc_otg_pcd_ep_halt(dwc_otg_pcd_t * pcd, void *ep_handle, int value)
+{
+	dwc_otg_pcd_ep_t *ep;
+	dwc_irqflags_t flags;
+	int retval = 0;
+
+	ep = get_ep_from_handle(pcd, ep_handle);
+
+	if (!ep || (!ep->desc && ep != &pcd->ep0) ||
+	    (ep->desc && (ep->desc->bmAttributes == UE_ISOCHRONOUS))) {
+		DWC_WARN("%s, bad ep\n", __func__);
+		return -DWC_E_INVALID;
+	}
+
+	DWC_SPINLOCK_IRQSAVE(pcd->lock, &flags);
+	if (!DWC_CIRCLEQ_EMPTY(&ep->queue)) {
+		DWC_WARN("%d %s XFer In process\n", ep->dwc_ep.num,
+			 ep->dwc_ep.is_in ? "IN" : "OUT");
+		retval = -DWC_E_AGAIN;
+	} else if (value == 0) {
+		dwc_otg_ep_clear_stall(GET_CORE_IF(pcd), &ep->dwc_ep);
+	} else if (value == 1) {
+		if (ep->dwc_ep.is_in == 1 && GET_CORE_IF(pcd)->dma_desc_enable) {
+			dtxfsts_data_t txstatus;
+			fifosize_data_t txfifosize;
+
+			txfifosize.d32 =
+			    DWC_READ_REG32(&GET_CORE_IF(pcd)->core_global_regs->
+					   dtxfsiz[ep->dwc_ep.tx_fifo_num]);
+			txstatus.d32 =
+			    DWC_READ_REG32(&GET_CORE_IF(pcd)->dev_if->
+					   in_ep_regs[ep->dwc_ep.num]->dtxfsts);
+
+			if (txstatus.b.txfspcavail < txfifosize.b.depth) {
+				DWC_WARN("%s() Data In Tx Fifo\n", __func__);
+				retval = -DWC_E_AGAIN;
+			} else {
+				if (ep->dwc_ep.num == 0) {
+					pcd->ep0state = EP0_STALL;
+				}
+
+				ep->stopped = 1;
+				dwc_otg_ep_set_stall(GET_CORE_IF(pcd),
+						     &ep->dwc_ep);
+			}
+		} else {
+			if (ep->dwc_ep.num == 0) {
+				pcd->ep0state = EP0_STALL;
+			}
+
+			ep->stopped = 1;
+			dwc_otg_ep_set_stall(GET_CORE_IF(pcd), &ep->dwc_ep);
+		}
+	} else if (value == 2) {
+		ep->dwc_ep.stall_clear_flag = 0;
+	} else if (value == 3) {
+		ep->dwc_ep.stall_clear_flag = 1;
+	}
+
+	DWC_SPINUNLOCK_IRQRESTORE(pcd->lock, flags);
+
+	return retval;
+}
+
+/**
+ * This function initiates remote wakeup of the host from suspend state.
+ */
+void dwc_otg_pcd_rem_wkup_from_suspend(dwc_otg_pcd_t * pcd, int set)
+{
+	dctl_data_t dctl = { 0 };
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(pcd);
+	dsts_data_t dsts;
+
+	dsts.d32 = DWC_READ_REG32(&core_if->dev_if->dev_global_regs->dsts);
+	if (!dsts.b.suspsts) {
+		DWC_WARN("Remote wakeup while is not in suspend state\n");
+	}
+	/* Check if DEVICE_REMOTE_WAKEUP feature enabled */
+	if (pcd->remote_wakeup_enable) {
+		if (set) {
+
+			if (core_if->adp_enable) {
+				gpwrdn_data_t gpwrdn;
+
+				dwc_otg_adp_probe_stop(core_if);
+
+				/* Mask SRP detected interrupt from Power Down Logic */
+				gpwrdn.d32 = 0;
+				gpwrdn.b.srp_det_msk = 1;
+				DWC_MODIFY_REG32(&core_if->core_global_regs->
+						 gpwrdn, gpwrdn.d32, 0);
+
+				/* Disable Power Down Logic */
+				gpwrdn.d32 = 0;
+				gpwrdn.b.pmuactv = 1;
+				DWC_MODIFY_REG32(&core_if->core_global_regs->
+						 gpwrdn, gpwrdn.d32, 0);
+
+				/*
+				 * Initialize the Core for Device mode.
+				 */
+				core_if->op_state = B_PERIPHERAL;
+				dwc_otg_core_init(core_if);
+				dwc_otg_enable_global_interrupts(core_if);
+				cil_pcd_start(core_if);
+
+				dwc_otg_initiate_srp(core_if);
+			}
+
+			dctl.b.rmtwkupsig = 1;
+			DWC_MODIFY_REG32(&core_if->dev_if->
+					 dev_global_regs->dctl, 0, dctl.d32);
+			DWC_DEBUGPL(DBG_PCD, "Set Remote Wakeup\n");
+
+			dwc_mdelay(2);
+			DWC_MODIFY_REG32(&core_if->dev_if->
+					 dev_global_regs->dctl, dctl.d32, 0);
+			DWC_DEBUGPL(DBG_PCD, "Clear Remote Wakeup\n");
+		}
+	} else {
+		DWC_DEBUGPL(DBG_PCD, "Remote Wakeup is disabled\n");
+	}
+}
+
+#ifdef CONFIG_USB_DWC_OTG_LPM
+/**
+ * This function initiates remote wakeup of the host from L1 sleep state.
+ */
+void dwc_otg_pcd_rem_wkup_from_sleep(dwc_otg_pcd_t * pcd, int set)
+{
+	glpmcfg_data_t lpmcfg;
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(pcd);
+
+	lpmcfg.d32 = DWC_READ_REG32(&core_if->core_global_regs->glpmcfg);
+
+	/* Check if we are in L1 state */
+	if (!lpmcfg.b.prt_sleep_sts) {
+		DWC_DEBUGPL(DBG_PCD, "Device is not in sleep state\n");
+		return;
+	}
+
+	/* Check if host allows remote wakeup */
+	if (!lpmcfg.b.rem_wkup_en) {
+		DWC_DEBUGPL(DBG_PCD, "Host does not allow remote wakeup\n");
+		return;
+	}
+
+	/* Check if Resume OK */
+	if (!lpmcfg.b.sleep_state_resumeok) {
+		DWC_DEBUGPL(DBG_PCD, "Sleep state resume is not OK\n");
+		return;
+	}
+
+	lpmcfg.d32 = DWC_READ_REG32(&core_if->core_global_regs->glpmcfg);
+	lpmcfg.b.en_utmi_sleep = 0;
+	lpmcfg.b.hird_thres &= (~(1 << 4));
+	DWC_WRITE_REG32(&core_if->core_global_regs->glpmcfg, lpmcfg.d32);
+
+	if (set) {
+		dctl_data_t dctl = {.d32 = 0 };
+		dctl.b.rmtwkupsig = 1;
+		/* Set RmtWkUpSig bit to start remote wakup signaling.
+		 * Hardware will automatically clear this bit.
+		 */
+		DWC_MODIFY_REG32(&core_if->dev_if->dev_global_regs->dctl,
+				 0, dctl.d32);
+		DWC_DEBUGPL(DBG_PCD, "Set Remote Wakeup\n");
+	}
+
+}
+#endif
+
+/**
+ * Performs remote wakeup.
+ */
+void dwc_otg_pcd_remote_wakeup(dwc_otg_pcd_t * pcd, int set)
+{
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(pcd);
+	dwc_irqflags_t flags;
+	if (dwc_otg_is_device_mode(core_if)) {
+		DWC_SPINLOCK_IRQSAVE(pcd->lock, &flags);
+#ifdef CONFIG_USB_DWC_OTG_LPM
+		if (core_if->lx_state == DWC_OTG_L1) {
+			dwc_otg_pcd_rem_wkup_from_sleep(pcd, set);
+		} else {
+#endif
+			dwc_otg_pcd_rem_wkup_from_suspend(pcd, set);
+#ifdef CONFIG_USB_DWC_OTG_LPM
+		}
+#endif
+		DWC_SPINUNLOCK_IRQRESTORE(pcd->lock, flags);
+	}
+	return;
+}
+
+void dwc_otg_pcd_disconnect_us(dwc_otg_pcd_t * pcd, int no_of_usecs)
+{
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(pcd);
+	dctl_data_t dctl = { 0 };
+
+	if (dwc_otg_is_device_mode(core_if)) {
+		dctl.b.sftdiscon = 1;
+		DWC_PRINTF("Soft disconnect for %d useconds\n",no_of_usecs);
+		DWC_MODIFY_REG32(&core_if->dev_if->dev_global_regs->dctl, 0, dctl.d32);
+		dwc_udelay(no_of_usecs);
+		DWC_MODIFY_REG32(&core_if->dev_if->dev_global_regs->dctl, dctl.d32,0);
+
+	} else{
+		DWC_PRINTF("NOT SUPPORTED IN HOST MODE\n");
+	}
+	return;
+
+}
+
+int dwc_otg_pcd_wakeup(dwc_otg_pcd_t * pcd)
+{
+	dsts_data_t dsts;
+	gotgctl_data_t gotgctl;
+
+	/*
+	 * This function starts the Protocol if no session is in progress. If
+	 * a session is already in progress, but the device is suspended,
+	 * remote wakeup signaling is started.
+	 */
+
+	/* Check if valid session */
+	gotgctl.d32 =
+	    DWC_READ_REG32(&(GET_CORE_IF(pcd)->core_global_regs->gotgctl));
+	if (gotgctl.b.bsesvld) {
+		/* Check if suspend state */
+		dsts.d32 =
+		    DWC_READ_REG32(&
+				   (GET_CORE_IF(pcd)->dev_if->
+				    dev_global_regs->dsts));
+		if (dsts.b.suspsts) {
+			dwc_otg_pcd_remote_wakeup(pcd, 1);
+		}
+	} else {
+		dwc_otg_pcd_initiate_srp(pcd);
+	}
+
+	return 0;
+
+}
+
+/**
+ * Start the SRP timer to detect when the SRP does not complete within
+ * 6 seconds.
+ *
+ * @param pcd the pcd structure.
+ */
+void dwc_otg_pcd_initiate_srp(dwc_otg_pcd_t * pcd)
+{
+	dwc_irqflags_t flags;
+	DWC_SPINLOCK_IRQSAVE(pcd->lock, &flags);
+	dwc_otg_initiate_srp(GET_CORE_IF(pcd));
+	DWC_SPINUNLOCK_IRQRESTORE(pcd->lock, flags);
+}
+
+int dwc_otg_pcd_get_frame_number(dwc_otg_pcd_t * pcd)
+{
+	return dwc_otg_get_frame_number(GET_CORE_IF(pcd));
+}
+
+int dwc_otg_pcd_is_lpm_enabled(dwc_otg_pcd_t * pcd)
+{
+	return GET_CORE_IF(pcd)->core_params->lpm_enable;
+}
+
+uint32_t get_b_hnp_enable(dwc_otg_pcd_t * pcd)
+{
+	return pcd->b_hnp_enable;
+}
+
+uint32_t get_a_hnp_support(dwc_otg_pcd_t * pcd)
+{
+	return pcd->a_hnp_support;
+}
+
+uint32_t get_a_alt_hnp_support(dwc_otg_pcd_t * pcd)
+{
+	return pcd->a_alt_hnp_support;
+}
+
+int dwc_otg_pcd_get_rmwkup_enable(dwc_otg_pcd_t * pcd)
+{
+	return pcd->remote_wakeup_enable;
+}
+
+#endif /* DWC_HOST_ONLY */
diff --git a/drivers/usb/dwc_otg/dwc_otg_pcd.h b/drivers/usb/dwc_otg/dwc_otg_pcd.h
new file mode 100644
index 0000000..4b2dde9
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_otg_pcd.h
@@ -0,0 +1,262 @@
+/* ==========================================================================
+ * $File: //dwh/usb_iip/dev/software/otg/linux/drivers/dwc_otg_pcd.h $
+ * $Revision: #46 $
+ * $Date: 2011/10/20 $
+ * $Change: 1870124 $
+ *
+ * Synopsys HS OTG Linux Software Driver and documentation (hereinafter,
+ * "Software") is an Unsupported proprietary work of Synopsys, Inc. unless
+ * otherwise expressly agreed to in writing between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product under
+ * any End User Software License Agreement or Agreement for Licensed Product
+ * with Synopsys or any supplement thereto. You are permitted to use and
+ * redistribute this Software in source and binary forms, with or without
+ * modification, provided that redistributions of source code must retain this
+ * notice. You may not view, use, disclose, copy or distribute this file or
+ * any information contained herein except pursuant to this license grant from
+ * Synopsys. If you do not agree with this notice, including the disclaimer
+ * below, then you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS" BASIS
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE HEREBY DISCLAIMED. IN NO EVENT SHALL SYNOPSYS BE LIABLE FOR ANY DIRECT,
+ * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================== */
+#ifndef DWC_HOST_ONLY
+#if !defined(__DWC_PCD_H__)
+#define __DWC_PCD_H__
+
+#include "dwc_otg_os_dep.h"
+#include "usb.h"
+#include "dwc_otg_cil.h"
+#include "dwc_otg_pcd_if.h"
+struct cfiobject;
+
+/**
+ * @file
+ *
+ * This file contains the structures, constants, and interfaces for
+ * the Perpherial Contoller Driver (PCD).
+ *
+ * The Peripheral Controller Driver (PCD) for Linux will implement the
+ * Gadget API, so that the existing Gadget drivers can be used. For
+ * the Mass Storage Function driver the File-backed USB Storage Gadget
+ * (FBS) driver will be used.  The FBS driver supports the
+ * Control-Bulk (CB), Control-Bulk-Interrupt (CBI), and Bulk-Only
+ * transports.
+ *
+ */
+
+/** Invalid DMA Address */
+#define DWC_DMA_ADDR_INVALID	(~(dwc_dma_t)0)
+
+/** Max Transfer size for any EP */
+#define DDMA_MAX_TRANSFER_SIZE 65535
+
+/**
+ * Get the pointer to the core_if from the pcd pointer.
+ */
+#define GET_CORE_IF( _pcd ) (_pcd->core_if)
+
+/**
+ * States of EP0.
+ */
+typedef enum ep0_state {
+	EP0_DISCONNECT,		/* no host */
+	EP0_IDLE,
+	EP0_IN_DATA_PHASE,
+	EP0_OUT_DATA_PHASE,
+	EP0_IN_STATUS_PHASE,
+	EP0_OUT_STATUS_PHASE,
+	EP0_STALL,
+} ep0state_e;
+
+/** Fordward declaration.*/
+struct dwc_otg_pcd;
+
+/** DWC_otg iso request structure.
+ *
+ */
+typedef struct usb_iso_request dwc_otg_pcd_iso_request_t;
+
+#ifdef DWC_UTE_PER_IO
+
+/**
+ * This shall be the exact analogy of the same type structure defined in the
+ * usb_gadget.h. Each descriptor contains
+ */
+struct dwc_iso_pkt_desc_port {
+	uint32_t offset;
+	uint32_t length;	/* expected length */
+	uint32_t actual_length;
+	uint32_t status;
+};
+
+struct dwc_iso_xreq_port {
+	/** transfer/submission flag */
+	uint32_t tr_sub_flags;
+	/** Start the request ASAP */
+#define DWC_EREQ_TF_ASAP		0x00000002
+	/** Just enqueue the request w/o initiating a transfer */
+#define DWC_EREQ_TF_ENQUEUE		0x00000004
+
+	/**
+	* count of ISO packets attached to this request - shall
+	* not exceed the pio_alloc_pkt_count
+	*/
+	uint32_t pio_pkt_count;
+	/** count of ISO packets allocated for this request */
+	uint32_t pio_alloc_pkt_count;
+	/** number of ISO packet errors */
+	uint32_t error_count;
+	/** reserved for future extension */
+	uint32_t res;
+	/** Will be allocated and freed in the UTE gadget and based on the CFC value */
+	struct dwc_iso_pkt_desc_port *per_io_frame_descs;
+};
+#endif
+/** DWC_otg request structure.
+ * This structure is a list of requests.
+ */
+typedef struct dwc_otg_pcd_request {
+	void *priv;
+	void *buf;
+	dwc_dma_t dma;
+	uint32_t length;
+	uint32_t actual;
+	unsigned sent_zlp:1;
+    /**
+     * Used instead of original buffer if
+     * it(physical address) is not dword-aligned.
+     **/
+     uint8_t *dw_align_buf;
+     dwc_dma_t dw_align_buf_dma;
+
+	 DWC_CIRCLEQ_ENTRY(dwc_otg_pcd_request) queue_entry;
+#ifdef DWC_UTE_PER_IO
+	struct dwc_iso_xreq_port ext_req;
+	//void *priv_ereq_nport; /*  */
+#endif
+} dwc_otg_pcd_request_t;
+
+DWC_CIRCLEQ_HEAD(req_list, dwc_otg_pcd_request);
+
+/**	  PCD EP structure.
+ * This structure describes an EP, there is an array of EPs in the PCD
+ * structure.
+ */
+typedef struct dwc_otg_pcd_ep {
+	/** USB EP Descriptor */
+	const usb_endpoint_descriptor_t *desc;
+
+	/** queue of dwc_otg_pcd_requests. */
+	struct req_list queue;
+	unsigned stopped:1;
+	unsigned disabling:1;
+	unsigned dma:1;
+	unsigned queue_sof:1;
+
+#ifdef DWC_EN_ISOC
+	/** ISOC req handle passed */
+	void *iso_req_handle;
+#endif				//_EN_ISOC_
+
+	/** DWC_otg ep data. */
+	dwc_ep_t dwc_ep;
+
+	/** Pointer to PCD */
+	struct dwc_otg_pcd *pcd;
+
+	void *priv;
+} dwc_otg_pcd_ep_t;
+
+/** DWC_otg PCD Structure.
+ * This structure encapsulates the data for the dwc_otg PCD.
+ */
+struct dwc_otg_pcd {
+	const struct dwc_otg_pcd_function_ops *fops;
+	/** The DWC otg device pointer */
+	struct dwc_otg_device *otg_dev;
+	/** Core Interface */
+	dwc_otg_core_if_t *core_if;
+	/** State of EP0 */
+	ep0state_e ep0state;
+	/** EP0 Request is pending */
+	unsigned ep0_pending:1;
+	/** Indicates when SET CONFIGURATION Request is in process */
+	unsigned request_config:1;
+	/** The state of the Remote Wakeup Enable. */
+	unsigned remote_wakeup_enable:1;
+	/** The state of the B-Device HNP Enable. */
+	unsigned b_hnp_enable:1;
+	/** The state of A-Device HNP Support. */
+	unsigned a_hnp_support:1;
+	/** The state of the A-Device Alt HNP support. */
+	unsigned a_alt_hnp_support:1;
+	/** Count of pending Requests */
+	unsigned request_pending;
+
+	/** SETUP packet for EP0
+	 * This structure is allocated as a DMA buffer on PCD initialization
+	 * with enough space for up to 3 setup packets.
+	 */
+	union {
+		usb_device_request_t req;
+		uint32_t d32[2];
+	} *setup_pkt;
+
+	dwc_dma_t setup_pkt_dma_handle;
+
+	/** 2-byte dma buffer used to return status from GET_STATUS */
+	uint16_t *status_buf;
+	dwc_dma_t status_buf_dma_handle;
+
+	/** EP0 */
+	dwc_otg_pcd_ep_t ep0;
+
+	/** Array of IN EPs. */
+	dwc_otg_pcd_ep_t in_ep[MAX_EPS_CHANNELS - 1];
+	/** Array of OUT EPs. */
+	dwc_otg_pcd_ep_t out_ep[MAX_EPS_CHANNELS - 1];
+	/** number of valid EPs in the above array. */
+//        unsigned      num_eps : 4;
+	dwc_spinlock_t *lock;
+
+	/** Tasklet to defer starting of TEST mode transmissions until
+	 *	Status Phase has been completed.
+	 */
+	dwc_tasklet_t *test_mode_tasklet;
+
+	/** Tasklet to delay starting of xfer in DMA mode */
+	dwc_tasklet_t *start_xfer_tasklet;
+
+	/** The test mode to enter when the tasklet is executed. */
+	unsigned test_mode;
+	/** The cfi_api structure that implements most of the CFI API
+	 * and OTG specific core configuration functionality
+	 */
+#ifdef DWC_UTE_CFI
+	struct cfiobject *cfi;
+#endif
+
+};
+
+//FIXME this functions should be static, and this prototypes should be removed
+extern void dwc_otg_request_nuke(dwc_otg_pcd_ep_t * ep);
+extern void dwc_otg_request_done(dwc_otg_pcd_ep_t * ep,
+				 dwc_otg_pcd_request_t * req, int32_t status);
+
+void dwc_otg_iso_buffer_done(dwc_otg_pcd_t * pcd, dwc_otg_pcd_ep_t * ep,
+			     void *req_handle);
+
+extern void do_test_mode(void *data);
+#endif
+#endif /* DWC_HOST_ONLY */
diff --git a/drivers/usb/dwc_otg/dwc_otg_pcd_if.h b/drivers/usb/dwc_otg/dwc_otg_pcd_if.h
new file mode 100644
index 0000000..6283867
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_otg_pcd_if.h
@@ -0,0 +1,357 @@
+/* ==========================================================================
+ * $File: //dwh/usb_iip/dev/software/otg/linux/drivers/dwc_otg_pcd_if.h $
+ * $Revision: #11 $
+ * $Date: 2011/10/26 $
+ * $Change: 1873028 $
+ *
+ * Synopsys HS OTG Linux Software Driver and documentation (hereinafter,
+ * "Software") is an Unsupported proprietary work of Synopsys, Inc. unless
+ * otherwise expressly agreed to in writing between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product under
+ * any End User Software License Agreement or Agreement for Licensed Product
+ * with Synopsys or any supplement thereto. You are permitted to use and
+ * redistribute this Software in source and binary forms, with or without
+ * modification, provided that redistributions of source code must retain this
+ * notice. You may not view, use, disclose, copy or distribute this file or
+ * any information contained herein except pursuant to this license grant from
+ * Synopsys. If you do not agree with this notice, including the disclaimer
+ * below, then you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS" BASIS
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE HEREBY DISCLAIMED. IN NO EVENT SHALL SYNOPSYS BE LIABLE FOR ANY DIRECT,
+ * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================== */
+#ifndef DWC_HOST_ONLY
+
+#if !defined(__DWC_PCD_IF_H__)
+#define __DWC_PCD_IF_H__
+
+//#include "dwc_os.h"
+#include "dwc_otg_core_if.h"
+
+/** @file
+ * This file defines DWC_OTG PCD Core API.
+ */
+
+struct dwc_otg_pcd;
+typedef struct dwc_otg_pcd dwc_otg_pcd_t;
+
+/** Maxpacket size for EP0 */
+#define MAX_EP0_SIZE	64
+/** Maxpacket size for any EP */
+#define MAX_PACKET_SIZE 1024
+
+/** @name Function Driver Callbacks */
+/** @{ */
+
+/** This function will be called whenever a previously queued request has
+ * completed.  The status value will be set to -DWC_E_SHUTDOWN to indicated a
+ * failed or aborted transfer, or -DWC_E_RESTART to indicate the device was reset,
+ * or -DWC_E_TIMEOUT to indicate it timed out, or -DWC_E_INVALID to indicate invalid
+ * parameters. */
+typedef int (*dwc_completion_cb_t) (dwc_otg_pcd_t * pcd, void *ep_handle,
+				    void *req_handle, int32_t status,
+				    uint32_t actual);
+/**
+ * This function will be called whenever a previousle queued ISOC request has
+ * completed. Count of ISOC packets could be read using dwc_otg_pcd_get_iso_packet_count
+ * function.
+ * The status of each ISOC packet could be read using dwc_otg_pcd_get_iso_packet_*
+ * functions.
+ */
+typedef int (*dwc_isoc_completion_cb_t) (dwc_otg_pcd_t * pcd, void *ep_handle,
+					 void *req_handle, int proc_buf_num);
+/** This function should handle any SETUP request that cannot be handled by the
+ * PCD Core.  This includes most GET_DESCRIPTORs, SET_CONFIGS, Any
+ * class-specific requests, etc.  The function must non-blocking.
+ *
+ * Returns 0 on success.
+ * Returns -DWC_E_NOT_SUPPORTED if the request is not supported.
+ * Returns -DWC_E_INVALID if the setup request had invalid parameters or bytes.
+ * Returns -DWC_E_SHUTDOWN on any other error. */
+typedef int (*dwc_setup_cb_t) (dwc_otg_pcd_t * pcd, uint8_t * bytes);
+/** This is called whenever the device has been disconnected.  The function
+ * driver should take appropriate action to clean up all pending requests in the
+ * PCD Core, remove all endpoints (except ep0), and initialize back to reset
+ * state. */
+typedef int (*dwc_disconnect_cb_t) (dwc_otg_pcd_t * pcd);
+/** This function is called when device has been connected. */
+typedef int (*dwc_connect_cb_t) (dwc_otg_pcd_t * pcd, int speed);
+/** This function is called when device has been suspended */
+typedef int (*dwc_suspend_cb_t) (dwc_otg_pcd_t * pcd);
+/** This function is called when device has received LPM tokens, i.e.
+ * device has been sent to sleep state. */
+typedef int (*dwc_sleep_cb_t) (dwc_otg_pcd_t * pcd);
+/** This function is called when device has been resumed
+ * from suspend(L2) or L1 sleep state. */
+typedef int (*dwc_resume_cb_t) (dwc_otg_pcd_t * pcd);
+/** This function is called whenever hnp params has been changed.
+ * User can call get_b_hnp_enable, get_a_hnp_support, get_a_alt_hnp_support functions
+ * to get hnp parameters. */
+typedef int (*dwc_hnp_params_changed_cb_t) (dwc_otg_pcd_t * pcd);
+/** This function is called whenever USB RESET is detected. */
+typedef int (*dwc_reset_cb_t) (dwc_otg_pcd_t * pcd);
+
+typedef int (*cfi_setup_cb_t) (dwc_otg_pcd_t * pcd, void *ctrl_req_bytes);
+
+/**
+ *
+ * @param ep_handle	Void pointer to the usb_ep structure
+ * @param ereq_port Pointer to the extended request structure created in the
+ *					portable part.
+ */
+typedef int (*xiso_completion_cb_t) (dwc_otg_pcd_t * pcd, void *ep_handle,
+				     void *req_handle, int32_t status,
+				     void *ereq_port);
+/** Function Driver Ops Data Structure */
+struct dwc_otg_pcd_function_ops {
+	dwc_connect_cb_t connect;
+	dwc_disconnect_cb_t disconnect;
+	dwc_setup_cb_t setup;
+	dwc_completion_cb_t complete;
+	dwc_isoc_completion_cb_t isoc_complete;
+	dwc_suspend_cb_t suspend;
+	dwc_sleep_cb_t sleep;
+	dwc_resume_cb_t resume;
+	dwc_reset_cb_t reset;
+	dwc_hnp_params_changed_cb_t hnp_changed;
+	cfi_setup_cb_t cfi_setup;
+#ifdef DWC_UTE_PER_IO
+	xiso_completion_cb_t xisoc_complete;
+#endif
+};
+/** @} */
+
+/** @name Function Driver Functions */
+/** @{ */
+
+/** Call this function to get pointer on dwc_otg_pcd_t,
+ * this pointer will be used for all PCD API functions.
+ *
+ * @param core_if The DWC_OTG Core
+ */
+extern dwc_otg_pcd_t *dwc_otg_pcd_init(dwc_otg_core_if_t * core_if);
+
+/** Frees PCD allocated by dwc_otg_pcd_init
+ *
+ * @param pcd The PCD
+ */
+extern void dwc_otg_pcd_remove(dwc_otg_pcd_t * pcd);
+
+/** Call this to bind the function driver to the PCD Core.
+ *
+ * @param pcd Pointer on dwc_otg_pcd_t returned by dwc_otg_pcd_init function.
+ * @param fops The Function Driver Ops data structure containing pointers to all callbacks.
+ */
+extern void dwc_otg_pcd_start(dwc_otg_pcd_t * pcd,
+			      const struct dwc_otg_pcd_function_ops *fops);
+
+/** Enables an endpoint for use.  This function enables an endpoint in
+ * the PCD.  The endpoint is described by the ep_desc which has the
+ * same format as a USB ep descriptor.  The ep_handle parameter is used to refer
+ * to the endpoint from other API functions and in callbacks.  Normally this
+ * should be called after a SET_CONFIGURATION/SET_INTERFACE to configure the
+ * core for that interface.
+ *
+ * Returns -DWC_E_INVALID if invalid parameters were passed.
+ * Returns -DWC_E_SHUTDOWN if any other error ocurred.
+ * Returns 0 on success.
+ *
+ * @param pcd The PCD
+ * @param ep_desc Endpoint descriptor
+ * @param usb_ep Handle on endpoint, that will be used to identify endpoint.
+ */
+extern int dwc_otg_pcd_ep_enable(dwc_otg_pcd_t * pcd,
+				 const uint8_t * ep_desc, void *usb_ep);
+
+/** Disable the endpoint referenced by ep_handle.
+ *
+ * Returns -DWC_E_INVALID if invalid parameters were passed.
+ * Returns -DWC_E_SHUTDOWN if any other error occurred.
+ * Returns 0 on success. */
+extern int dwc_otg_pcd_ep_disable(dwc_otg_pcd_t * pcd, void *ep_handle);
+
+/** Queue a data transfer request on the endpoint referenced by ep_handle.
+ * After the transfer is completes, the complete callback will be called with
+ * the request status.
+ *
+ * @param pcd The PCD
+ * @param ep_handle The handle of the endpoint
+ * @param buf The buffer for the data
+ * @param dma_buf The DMA buffer for the data
+ * @param buflen The length of the data transfer
+ * @param zero Specifies whether to send zero length last packet.
+ * @param req_handle Set this handle to any value to use to reference this
+ * request in the ep_dequeue function or from the complete callback
+ * @param atomic_alloc If driver need to perform atomic allocations
+ * for internal data structures.
+ *
+ * Returns -DWC_E_INVALID if invalid parameters were passed.
+ * Returns -DWC_E_SHUTDOWN if any other error ocurred.
+ * Returns 0 on success. */
+extern int dwc_otg_pcd_ep_queue(dwc_otg_pcd_t * pcd, void *ep_handle,
+				uint8_t * buf, dwc_dma_t dma_buf,
+				uint32_t buflen, int zero, void *req_handle,
+				int atomic_alloc);
+#ifdef DWC_UTE_PER_IO
+/**
+ *
+ * @param ereq_nonport	Pointer to the extended request part of the
+ *						usb_request structure defined in usb_gadget.h file.
+ */
+extern int dwc_otg_pcd_xiso_ep_queue(dwc_otg_pcd_t * pcd, void *ep_handle,
+				     uint8_t * buf, dwc_dma_t dma_buf,
+				     uint32_t buflen, int zero,
+				     void *req_handle, int atomic_alloc,
+				     void *ereq_nonport);
+
+#endif
+
+/** De-queue the specified data transfer that has not yet completed.
+ *
+ * Returns -DWC_E_INVALID if invalid parameters were passed.
+ * Returns -DWC_E_SHUTDOWN if any other error ocurred.
+ * Returns 0 on success. */
+extern int dwc_otg_pcd_ep_dequeue(dwc_otg_pcd_t * pcd, void *ep_handle,
+				  void *req_handle);
+
+/** Halt (STALL) an endpoint or clear it.
+ *
+ * Returns -DWC_E_INVALID if invalid parameters were passed.
+ * Returns -DWC_E_SHUTDOWN if any other error ocurred.
+ * Returns -DWC_E_AGAIN if the STALL cannot be sent and must be tried again later
+ * Returns 0 on success. */
+extern int dwc_otg_pcd_ep_halt(dwc_otg_pcd_t * pcd, void *ep_handle, int value);
+
+/** This function should be called on every hardware interrupt */
+extern int32_t dwc_otg_pcd_handle_intr(dwc_otg_pcd_t * pcd);
+
+/** This function returns current frame number */
+extern int dwc_otg_pcd_get_frame_number(dwc_otg_pcd_t * pcd);
+
+/**
+ * Start isochronous transfers on the endpoint referenced by ep_handle.
+ * For isochronous transfers duble buffering is used.
+ * After processing each of buffers comlete callback will be called with
+ * status for each transaction.
+ *
+ * @param pcd The PCD
+ * @param ep_handle The handle of the endpoint
+ * @param buf0 The virtual address of first data buffer
+ * @param buf1 The virtual address of second data buffer
+ * @param dma0 The DMA address of first data buffer
+ * @param dma1 The DMA address of second data buffer
+ * @param sync_frame Data pattern frame number
+ * @param dp_frame Data size for pattern frame
+ * @param data_per_frame Data size for regular frame
+ * @param start_frame Frame number to start transfers, if -1 then start transfers ASAP.
+ * @param buf_proc_intrvl Interval of ISOC Buffer processing
+ * @param req_handle Handle of ISOC request
+ * @param atomic_alloc Specefies whether to perform atomic allocation for
+ * 			internal data structures.
+ *
+ * Returns -DWC_E_NO_MEMORY if there is no enough memory.
+ * Returns -DWC_E_INVALID if incorrect arguments are passed to the function.
+ * Returns -DW_E_SHUTDOWN for any other error.
+ * Returns 0 on success
+ */
+extern int dwc_otg_pcd_iso_ep_start(dwc_otg_pcd_t * pcd, void *ep_handle,
+				    uint8_t * buf0, uint8_t * buf1,
+				    dwc_dma_t dma0, dwc_dma_t dma1,
+				    int sync_frame, int dp_frame,
+				    int data_per_frame, int start_frame,
+				    int buf_proc_intrvl, void *req_handle,
+				    int atomic_alloc);
+
+/** Stop ISOC transfers on endpoint referenced by ep_handle.
+ *
+ * @param pcd The PCD
+ * @param ep_handle The handle of the endpoint
+ * @param req_handle Handle of ISOC request
+ *
+ * Returns -DWC_E_INVALID if incorrect arguments are passed to the function
+ * Returns 0 on success
+ */
+int dwc_otg_pcd_iso_ep_stop(dwc_otg_pcd_t * pcd, void *ep_handle,
+			    void *req_handle);
+
+/** Get ISOC packet status.
+ *
+ * @param pcd The PCD
+ * @param ep_handle The handle of the endpoint
+ * @param iso_req_handle Isochronoush request handle
+ * @param packet Number of packet
+ * @param status Out parameter for returning status
+ * @param actual Out parameter for returning actual length
+ * @param offset Out parameter for returning offset
+ *
+ */
+extern void dwc_otg_pcd_get_iso_packet_params(dwc_otg_pcd_t * pcd,
+					      void *ep_handle,
+					      void *iso_req_handle, int packet,
+					      int *status, int *actual,
+					      int *offset);
+
+/** Get ISOC packet count.
+ *
+ * @param pcd The PCD
+ * @param ep_handle The handle of the endpoint
+ * @param iso_req_handle
+ */
+extern int dwc_otg_pcd_get_iso_packet_count(dwc_otg_pcd_t * pcd,
+					    void *ep_handle,
+					    void *iso_req_handle);
+
+/** This function starts the SRP Protocol if no session is in progress. If
+ * a session is already in progress, but the device is suspended,
+ * remote wakeup signaling is started.
+ */
+extern int dwc_otg_pcd_wakeup(dwc_otg_pcd_t * pcd);
+
+/** This function returns 1 if LPM support is enabled, and 0 otherwise. */
+extern int dwc_otg_pcd_is_lpm_enabled(dwc_otg_pcd_t * pcd);
+
+/** This function returns 1 if remote wakeup is allowed and 0, otherwise. */
+extern int dwc_otg_pcd_get_rmwkup_enable(dwc_otg_pcd_t * pcd);
+
+/** Initiate SRP */
+extern void dwc_otg_pcd_initiate_srp(dwc_otg_pcd_t * pcd);
+
+/** Starts remote wakeup signaling. */
+extern void dwc_otg_pcd_remote_wakeup(dwc_otg_pcd_t * pcd, int set);
+
+/** Starts micorsecond soft disconnect. */
+extern void dwc_otg_pcd_disconnect_us(dwc_otg_pcd_t * pcd, int no_of_usecs);
+/** This function returns whether device is dualspeed.*/
+extern uint32_t dwc_otg_pcd_is_dualspeed(dwc_otg_pcd_t * pcd);
+
+/** This function returns whether device is otg. */
+extern uint32_t dwc_otg_pcd_is_otg(dwc_otg_pcd_t * pcd);
+
+/** These functions allow to get hnp parameters */
+extern uint32_t get_b_hnp_enable(dwc_otg_pcd_t * pcd);
+extern uint32_t get_a_hnp_support(dwc_otg_pcd_t * pcd);
+extern uint32_t get_a_alt_hnp_support(dwc_otg_pcd_t * pcd);
+
+/** CFI specific Interface functions */
+/** Allocate a cfi buffer */
+extern uint8_t *cfiw_ep_alloc_buffer(dwc_otg_pcd_t * pcd, void *pep,
+				     dwc_dma_t * addr, size_t buflen,
+				     int flags);
+
+/******************************************************************************/
+
+/** @} */
+
+#endif				/* __DWC_PCD_IF_H__ */
+
+#endif				/* DWC_HOST_ONLY */
diff --git a/drivers/usb/dwc_otg/dwc_otg_pcd_intr.c b/drivers/usb/dwc_otg/dwc_otg_pcd_intr.c
new file mode 100644
index 0000000..1d3455e
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_otg_pcd_intr.c
@@ -0,0 +1,4821 @@
+/* ==========================================================================
+ * $File: //dwh/usb_iip/dev/software/otg/linux/drivers/dwc_otg_pcd_intr.c $
+ * $Revision: #113 $
+ * $Date: 2011/10/24 $
+ * $Change: 1871160 $
+ *
+ * Synopsys HS OTG Linux Software Driver and documentation (hereinafter,
+ * "Software") is an Unsupported proprietary work of Synopsys, Inc. unless
+ * otherwise expressly agreed to in writing between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product under
+ * any End User Software License Agreement or Agreement for Licensed Product
+ * with Synopsys or any supplement thereto. You are permitted to use and
+ * redistribute this Software in source and binary forms, with or without
+ * modification, provided that redistributions of source code must retain this
+ * notice. You may not view, use, disclose, copy or distribute this file or
+ * any information contained herein except pursuant to this license grant from
+ * Synopsys. If you do not agree with this notice, including the disclaimer
+ * below, then you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS" BASIS
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE HEREBY DISCLAIMED. IN NO EVENT SHALL SYNOPSYS BE LIABLE FOR ANY DIRECT,
+ * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================== */
+#ifndef DWC_HOST_ONLY
+
+#include "dwc_otg_pcd.h"
+
+#ifdef DWC_UTE_CFI
+#include "dwc_otg_cfi.h"
+#endif
+
+#ifdef DWC_UTE_PER_IO
+extern void complete_xiso_ep(dwc_otg_pcd_ep_t * ep);
+#endif
+//#define PRINT_CFI_DMA_DESCS
+
+#define DEBUG_EP0
+
+/**
+ * This function updates OTG.
+ */
+static void dwc_otg_pcd_update_otg(dwc_otg_pcd_t * pcd, const unsigned reset)
+{
+
+	if (reset) {
+		pcd->b_hnp_enable = 0;
+		pcd->a_hnp_support = 0;
+		pcd->a_alt_hnp_support = 0;
+	}
+
+	if (pcd->fops->hnp_changed) {
+		pcd->fops->hnp_changed(pcd);
+	}
+}
+
+/** @file
+ * This file contains the implementation of the PCD Interrupt handlers.
+ *
+ * The PCD handles the device interrupts.  Many conditions can cause a
+ * device interrupt. When an interrupt occurs, the device interrupt
+ * service routine determines the cause of the interrupt and
+ * dispatches handling to the appropriate function. These interrupt
+ * handling functions are described below.
+ * All interrupt registers are processed from LSB to MSB.
+ */
+
+/**
+ * This function prints the ep0 state for debug purposes.
+ */
+static inline void print_ep0_state(dwc_otg_pcd_t * pcd)
+{
+#ifdef DEBUG
+	char str[40];
+
+	switch (pcd->ep0state) {
+	case EP0_DISCONNECT:
+		dwc_strcpy(str, "EP0_DISCONNECT");
+		break;
+	case EP0_IDLE:
+		dwc_strcpy(str, "EP0_IDLE");
+		break;
+	case EP0_IN_DATA_PHASE:
+		dwc_strcpy(str, "EP0_IN_DATA_PHASE");
+		break;
+	case EP0_OUT_DATA_PHASE:
+		dwc_strcpy(str, "EP0_OUT_DATA_PHASE");
+		break;
+	case EP0_IN_STATUS_PHASE:
+		dwc_strcpy(str, "EP0_IN_STATUS_PHASE");
+		break;
+	case EP0_OUT_STATUS_PHASE:
+		dwc_strcpy(str, "EP0_OUT_STATUS_PHASE");
+		break;
+	case EP0_STALL:
+		dwc_strcpy(str, "EP0_STALL");
+		break;
+	default:
+		dwc_strcpy(str, "EP0_INVALID");
+	}
+
+	DWC_DEBUGPL(DBG_ANY, "%s(%d)\n", str, pcd->ep0state);
+#endif
+}
+
+/**
+ * This function calculate the size of the payload in the memory
+ * for out endpoints and prints size for debug purposes(used in
+ * 2.93a DevOutNak feature).
+ */
+static inline void print_memory_payload(dwc_otg_pcd_t * pcd,  dwc_ep_t * ep)
+{
+#ifdef DEBUG
+	deptsiz_data_t deptsiz_init = {.d32 = 0 };
+	deptsiz_data_t deptsiz_updt = {.d32 = 0 };
+	int pack_num;
+	unsigned payload;
+
+	deptsiz_init.d32 = pcd->core_if->start_doeptsiz_val[ep->num];
+	deptsiz_updt.d32 =
+		DWC_READ_REG32(&pcd->core_if->dev_if->
+						out_ep_regs[ep->num]->doeptsiz);
+	/* Payload will be */
+	payload = deptsiz_init.b.xfersize - deptsiz_updt.b.xfersize;
+	/* Packet count is decremented every time a packet
+	 * is written to the RxFIFO not in to the external memory
+	 * So, if payload == 0, then it means no packet was sent to ext memory*/
+	pack_num = (!payload) ? 0 : (deptsiz_init.b.pktcnt - deptsiz_updt.b.pktcnt);
+	DWC_DEBUGPL(DBG_PCDV,
+		"Payload for EP%d-%s\n",
+		ep->num, (ep->is_in ? "IN" : "OUT"));
+	DWC_DEBUGPL(DBG_PCDV,
+		"Number of transfered bytes = 0x%08x\n", payload);
+	DWC_DEBUGPL(DBG_PCDV,
+		"Number of transfered packets = %d\n", pack_num);
+#endif
+}
+
+
+#ifdef DWC_UTE_CFI
+static inline void print_desc(struct dwc_otg_dma_desc *ddesc,
+			      const uint8_t * epname, int descnum)
+{
+	CFI_INFO
+	    ("%s DMA_DESC(%d) buf=0x%08x bytes=0x%04x; sp=0x%x; l=0x%x; sts=0x%02x; bs=0x%02x\n",
+	     epname, descnum, ddesc->buf, ddesc->status.b.bytes,
+	     ddesc->status.b.sp, ddesc->status.b.l, ddesc->status.b.sts,
+	     ddesc->status.b.bs);
+}
+#endif
+
+/**
+ * This function returns pointer to in ep struct with number ep_num
+ */
+static inline dwc_otg_pcd_ep_t *get_in_ep(dwc_otg_pcd_t * pcd, uint32_t ep_num)
+{
+	int i;
+	int num_in_eps = GET_CORE_IF(pcd)->dev_if->num_in_eps;
+	if (ep_num == 0) {
+		return &pcd->ep0;
+	} else {
+		for (i = 0; i < num_in_eps; ++i) {
+			if (pcd->in_ep[i].dwc_ep.num == ep_num)
+				return &pcd->in_ep[i];
+		}
+		return 0;
+	}
+}
+
+/**
+ * This function returns pointer to out ep struct with number ep_num
+ */
+static inline dwc_otg_pcd_ep_t *get_out_ep(dwc_otg_pcd_t * pcd, uint32_t ep_num)
+{
+	int i;
+	int num_out_eps = GET_CORE_IF(pcd)->dev_if->num_out_eps;
+	if (ep_num == 0) {
+		return &pcd->ep0;
+	} else {
+		for (i = 0; i < num_out_eps; ++i) {
+			if (pcd->out_ep[i].dwc_ep.num == ep_num)
+				return &pcd->out_ep[i];
+		}
+		return 0;
+	}
+}
+
+/**
+ * This functions gets a pointer to an EP from the wIndex address
+ * value of the control request.
+ */
+dwc_otg_pcd_ep_t *get_ep_by_addr(dwc_otg_pcd_t * pcd, u16 wIndex)
+{
+	dwc_otg_pcd_ep_t *ep;
+	uint32_t ep_num = UE_GET_ADDR(wIndex);
+
+	if (ep_num == 0) {
+		ep = &pcd->ep0;
+	} else if (UE_GET_DIR(wIndex) == UE_DIR_IN) {	/* in ep */
+		ep = &pcd->in_ep[ep_num - 1];
+	} else {
+		ep = &pcd->out_ep[ep_num - 1];
+	}
+
+	return ep;
+}
+
+/**
+ * This function checks the EP request queue, if the queue is not
+ * empty the next request is started.
+ */
+void start_next_request(dwc_otg_pcd_ep_t * ep)
+{
+	dwc_otg_pcd_request_t *req = 0;
+	uint32_t max_transfer =
+	    GET_CORE_IF(ep->pcd)->core_params->max_transfer_size;
+
+#ifdef DWC_UTE_CFI
+	struct dwc_otg_pcd *pcd;
+	pcd = ep->pcd;
+#endif
+
+	if (!DWC_CIRCLEQ_EMPTY(&ep->queue)) {
+		req = DWC_CIRCLEQ_FIRST(&ep->queue);
+
+#ifdef DWC_UTE_CFI
+		if (ep->dwc_ep.buff_mode != BM_STANDARD) {
+			ep->dwc_ep.cfi_req_len = req->length;
+			pcd->cfi->ops.build_descriptors(pcd->cfi, pcd, ep, req);
+		} else {
+#endif
+			/* Setup and start the Transfer */
+			if (req->dw_align_buf) {
+				ep->dwc_ep.dma_addr = req->dw_align_buf_dma;
+				ep->dwc_ep.start_xfer_buff = req->dw_align_buf;
+				ep->dwc_ep.xfer_buff = req->dw_align_buf;
+			} else {
+				ep->dwc_ep.dma_addr = req->dma;
+				ep->dwc_ep.start_xfer_buff = req->buf;
+				ep->dwc_ep.xfer_buff = req->buf;
+			}
+			ep->dwc_ep.sent_zlp = 0;
+			ep->dwc_ep.total_len = req->length;
+			ep->dwc_ep.xfer_len = 0;
+			ep->dwc_ep.xfer_count = 0;
+
+			ep->dwc_ep.maxxfer = max_transfer;
+			if (GET_CORE_IF(ep->pcd)->dma_desc_enable) {
+				uint32_t out_max_xfer = DDMA_MAX_TRANSFER_SIZE
+				    - (DDMA_MAX_TRANSFER_SIZE % 4);
+				if (ep->dwc_ep.is_in) {
+					if (ep->dwc_ep.maxxfer >
+					    DDMA_MAX_TRANSFER_SIZE) {
+						ep->dwc_ep.maxxfer =
+						    DDMA_MAX_TRANSFER_SIZE;
+					}
+				} else {
+					if (ep->dwc_ep.maxxfer > out_max_xfer) {
+						ep->dwc_ep.maxxfer =
+						    out_max_xfer;
+					}
+				}
+			}
+			if (ep->dwc_ep.maxxfer < ep->dwc_ep.total_len) {
+				ep->dwc_ep.maxxfer -=
+				    (ep->dwc_ep.maxxfer % ep->dwc_ep.maxpacket);
+			}
+			if (req->sent_zlp) {
+				if ((ep->dwc_ep.total_len %
+				     ep->dwc_ep.maxpacket == 0)
+				    && (ep->dwc_ep.total_len != 0)) {
+					ep->dwc_ep.sent_zlp = 1;
+				}
+
+			}
+#ifdef DWC_UTE_CFI
+		}
+#endif
+		dwc_otg_ep_start_transfer(GET_CORE_IF(ep->pcd), &ep->dwc_ep);
+	} else if (ep->dwc_ep.type == DWC_OTG_EP_TYPE_ISOC) {
+		DWC_PRINTF("There are no more ISOC requests \n");
+		ep->dwc_ep.frame_num = 0xFFFFFFFF;
+	}
+}
+
+/**
+ * This function handles the SOF Interrupts. At this time the SOF
+ * Interrupt is disabled.
+ */
+int32_t dwc_otg_pcd_handle_sof_intr(dwc_otg_pcd_t * pcd)
+{
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(pcd);
+
+	gintsts_data_t gintsts;
+
+	DWC_DEBUGPL(DBG_PCD, "SOF\n");
+
+	/* Clear interrupt */
+	gintsts.d32 = 0;
+	gintsts.b.sofintr = 1;
+	DWC_WRITE_REG32(&core_if->core_global_regs->gintsts, gintsts.d32);
+
+	return 1;
+}
+
+/**
+ * This function handles the Rx Status Queue Level Interrupt, which
+ * indicates that there is a least one packet in the Rx FIFO.  The
+ * packets are moved from the FIFO to memory, where they will be
+ * processed when the Endpoint Interrupt Register indicates Transfer
+ * Complete or SETUP Phase Done.
+ *
+ * Repeat the following until the Rx Status Queue is empty:
+ *	 -# Read the Receive Status Pop Register (GRXSTSP) to get Packet
+ *		info
+ *	 -# If Receive FIFO is empty then skip to step Clear the interrupt
+ *		and exit
+ *	 -# If SETUP Packet call dwc_otg_read_setup_packet to copy the
+ *		SETUP data to the buffer
+ *	 -# If OUT Data Packet call dwc_otg_read_packet to copy the data
+ *		to the destination buffer
+ */
+int32_t dwc_otg_pcd_handle_rx_status_q_level_intr(dwc_otg_pcd_t * pcd)
+{
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(pcd);
+	dwc_otg_core_global_regs_t *global_regs = core_if->core_global_regs;
+	gintmsk_data_t gintmask = {.d32 = 0 };
+	device_grxsts_data_t status;
+	dwc_otg_pcd_ep_t *ep;
+	gintsts_data_t gintsts;
+#ifdef DEBUG
+	static char *dpid_str[] = { "D0", "D2", "D1", "MDATA" };
+#endif
+
+	//DWC_DEBUGPL(DBG_PCDV, "%s(%p)\n", __func__, _pcd);
+	/* Disable the Rx Status Queue Level interrupt */
+	gintmask.b.rxstsqlvl = 1;
+	DWC_MODIFY_REG32(&global_regs->gintmsk, gintmask.d32, 0);
+
+	/* Get the Status from the top of the FIFO */
+	status.d32 = DWC_READ_REG32(&global_regs->grxstsp);
+
+	DWC_DEBUGPL(DBG_PCD, "EP:%d BCnt:%d DPID:%s "
+		    "pktsts:%x Frame:%d(0x%0x)\n",
+		    status.b.epnum, status.b.bcnt,
+		    dpid_str[status.b.dpid],
+		    status.b.pktsts, status.b.fn, status.b.fn);
+	/* Get pointer to EP structure */
+	ep = get_out_ep(pcd, status.b.epnum);
+
+	switch (status.b.pktsts) {
+	case DWC_DSTS_GOUT_NAK:
+		DWC_DEBUGPL(DBG_PCDV, "Global OUT NAK\n");
+		break;
+	case DWC_STS_DATA_UPDT:
+		DWC_DEBUGPL(DBG_PCDV, "OUT Data Packet\n");
+		if (status.b.bcnt && ep->dwc_ep.xfer_buff) {
+			/** @todo NGS Check for buffer overflow? */
+			dwc_otg_read_packet(core_if,
+					    ep->dwc_ep.xfer_buff,
+					    status.b.bcnt);
+			ep->dwc_ep.xfer_count += status.b.bcnt;
+			ep->dwc_ep.xfer_buff += status.b.bcnt;
+		}
+		break;
+	case DWC_STS_XFER_COMP:
+		DWC_DEBUGPL(DBG_PCDV, "OUT Complete\n");
+		break;
+	case DWC_DSTS_SETUP_COMP:
+#ifdef DEBUG_EP0
+		DWC_DEBUGPL(DBG_PCDV, "Setup Complete\n");
+#endif
+		break;
+	case DWC_DSTS_SETUP_UPDT:
+		dwc_otg_read_setup_packet(core_if, pcd->setup_pkt->d32);
+#ifdef DEBUG_EP0
+		DWC_DEBUGPL(DBG_PCD,
+			    "SETUP PKT: %02x.%02x v%04x i%04x l%04x\n",
+			    pcd->setup_pkt->req.bmRequestType,
+			    pcd->setup_pkt->req.bRequest,
+			    UGETW(pcd->setup_pkt->req.wValue),
+			    UGETW(pcd->setup_pkt->req.wIndex),
+			    UGETW(pcd->setup_pkt->req.wLength));
+#endif
+		ep->dwc_ep.xfer_count += status.b.bcnt;
+		break;
+	default:
+		DWC_DEBUGPL(DBG_PCDV, "Invalid Packet Status (0x%0x)\n",
+			    status.b.pktsts);
+		break;
+	}
+
+	/* Enable the Rx Status Queue Level interrupt */
+	DWC_MODIFY_REG32(&global_regs->gintmsk, 0, gintmask.d32);
+	/* Clear interrupt */
+	gintsts.d32 = 0;
+	gintsts.b.rxstsqlvl = 1;
+	DWC_WRITE_REG32(&global_regs->gintsts, gintsts.d32);
+
+	//DWC_DEBUGPL(DBG_PCDV, "EXIT: %s\n", __func__);
+	return 1;
+}
+
+/**
+ * This function examines the Device IN Token Learning Queue to
+ * determine the EP number of the last IN token received.  This
+ * implementation is for the Mass Storage device where there are only
+ * 2 IN EPs (Control-IN and BULK-IN).
+ *
+ * The EP numbers for the first six IN Tokens are in DTKNQR1 and there
+ * are 8 EP Numbers in each of the other possible DTKNQ Registers.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ *
+ */
+static inline int get_ep_of_last_in_token(dwc_otg_core_if_t * core_if)
+{
+	dwc_otg_device_global_regs_t *dev_global_regs =
+	    core_if->dev_if->dev_global_regs;
+	const uint32_t TOKEN_Q_DEPTH = core_if->hwcfg2.b.dev_token_q_depth;
+	/* Number of Token Queue Registers */
+	const int DTKNQ_REG_CNT = (TOKEN_Q_DEPTH + 7) / 8;
+	dtknq1_data_t dtknqr1;
+	uint32_t in_tkn_epnums[4];
+	int ndx = 0;
+	int i = 0;
+	volatile uint32_t *addr = &dev_global_regs->dtknqr1;
+	int epnum = 0;
+
+	//DWC_DEBUGPL(DBG_PCD,"dev_token_q_depth=%d\n",TOKEN_Q_DEPTH);
+
+	/* Read the DTKNQ Registers */
+	for (i = 0; i < DTKNQ_REG_CNT; i++) {
+		in_tkn_epnums[i] = DWC_READ_REG32(addr);
+		DWC_DEBUGPL(DBG_PCDV, "DTKNQR%d=0x%08x\n", i + 1,
+			    in_tkn_epnums[i]);
+		if (addr == &dev_global_regs->dvbusdis) {
+			addr = &dev_global_regs->dtknqr3_dthrctl;
+		} else {
+			++addr;
+		}
+
+	}
+
+	/* Copy the DTKNQR1 data to the bit field. */
+	dtknqr1.d32 = in_tkn_epnums[0];
+	/* Get the EP numbers */
+	in_tkn_epnums[0] = dtknqr1.b.epnums0_5;
+	ndx = dtknqr1.b.intknwptr - 1;
+
+	//DWC_DEBUGPL(DBG_PCDV,"ndx=%d\n",ndx);
+	if (ndx == -1) {
+		/** @todo Find a simpler way to calculate the max
+		 * queue position.*/
+		int cnt = TOKEN_Q_DEPTH;
+		if (TOKEN_Q_DEPTH <= 6) {
+			cnt = TOKEN_Q_DEPTH - 1;
+		} else if (TOKEN_Q_DEPTH <= 14) {
+			cnt = TOKEN_Q_DEPTH - 7;
+		} else if (TOKEN_Q_DEPTH <= 22) {
+			cnt = TOKEN_Q_DEPTH - 15;
+		} else {
+			cnt = TOKEN_Q_DEPTH - 23;
+		}
+		epnum = (in_tkn_epnums[DTKNQ_REG_CNT - 1] >> (cnt * 4)) & 0xF;
+	} else {
+		if (ndx <= 5) {
+			epnum = (in_tkn_epnums[0] >> (ndx * 4)) & 0xF;
+		} else if (ndx <= 13) {
+			ndx -= 6;
+			epnum = (in_tkn_epnums[1] >> (ndx * 4)) & 0xF;
+		} else if (ndx <= 21) {
+			ndx -= 14;
+			epnum = (in_tkn_epnums[2] >> (ndx * 4)) & 0xF;
+		} else if (ndx <= 29) {
+			ndx -= 22;
+			epnum = (in_tkn_epnums[3] >> (ndx * 4)) & 0xF;
+		}
+	}
+	//DWC_DEBUGPL(DBG_PCD,"epnum=%d\n",epnum);
+	return epnum;
+}
+
+/**
+ * This interrupt occurs when the non-periodic Tx FIFO is half-empty.
+ * The active request is checked for the next packet to be loaded into
+ * the non-periodic Tx FIFO.
+ */
+int32_t dwc_otg_pcd_handle_np_tx_fifo_empty_intr(dwc_otg_pcd_t * pcd)
+{
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(pcd);
+	dwc_otg_core_global_regs_t *global_regs = core_if->core_global_regs;
+	dwc_otg_dev_in_ep_regs_t *ep_regs;
+	gnptxsts_data_t txstatus = {.d32 = 0 };
+	gintsts_data_t gintsts;
+
+	int epnum = 0;
+	dwc_otg_pcd_ep_t *ep = 0;
+	uint32_t len = 0;
+	int dwords;
+
+	/* Get the epnum from the IN Token Learning Queue. */
+	epnum = get_ep_of_last_in_token(core_if);
+	ep = get_in_ep(pcd, epnum);
+
+	DWC_DEBUGPL(DBG_PCD, "NP TxFifo Empty: %d \n", epnum);
+
+	ep_regs = core_if->dev_if->in_ep_regs[epnum];
+
+	len = ep->dwc_ep.xfer_len - ep->dwc_ep.xfer_count;
+	if (len > ep->dwc_ep.maxpacket) {
+		len = ep->dwc_ep.maxpacket;
+	}
+	dwords = (len + 3) / 4;
+
+	/* While there is space in the queue and space in the FIFO and
+	 * More data to tranfer, Write packets to the Tx FIFO */
+	txstatus.d32 = DWC_READ_REG32(&global_regs->gnptxsts);
+	DWC_DEBUGPL(DBG_PCDV, "b4 GNPTXSTS=0x%08x\n", txstatus.d32);
+
+	while (txstatus.b.nptxqspcavail > 0 &&
+	       txstatus.b.nptxfspcavail > dwords &&
+	       ep->dwc_ep.xfer_count < ep->dwc_ep.xfer_len) {
+		/* Write the FIFO */
+		dwc_otg_ep_write_packet(core_if, &ep->dwc_ep, 0);
+		len = ep->dwc_ep.xfer_len - ep->dwc_ep.xfer_count;
+
+		if (len > ep->dwc_ep.maxpacket) {
+			len = ep->dwc_ep.maxpacket;
+		}
+
+		dwords = (len + 3) / 4;
+		txstatus.d32 = DWC_READ_REG32(&global_regs->gnptxsts);
+		DWC_DEBUGPL(DBG_PCDV, "GNPTXSTS=0x%08x\n", txstatus.d32);
+	}
+
+	DWC_DEBUGPL(DBG_PCDV, "GNPTXSTS=0x%08x\n",
+		    DWC_READ_REG32(&global_regs->gnptxsts));
+
+	/* Clear interrupt */
+	gintsts.d32 = 0;
+	gintsts.b.nptxfempty = 1;
+	DWC_WRITE_REG32(&global_regs->gintsts, gintsts.d32);
+
+	return 1;
+}
+
+/**
+ * This function is called when dedicated Tx FIFO Empty interrupt occurs.
+ * The active request is checked for the next packet to be loaded into
+ * apropriate Tx FIFO.
+ */
+static int32_t write_empty_tx_fifo(dwc_otg_pcd_t * pcd, uint32_t epnum)
+{
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(pcd);
+	dwc_otg_dev_if_t *dev_if = core_if->dev_if;
+	dwc_otg_dev_in_ep_regs_t *ep_regs;
+	dtxfsts_data_t txstatus = {.d32 = 0 };
+	dwc_otg_pcd_ep_t *ep = 0;
+	uint32_t len = 0;
+	int dwords;
+
+	ep = get_in_ep(pcd, epnum);
+
+	DWC_DEBUGPL(DBG_PCD, "Dedicated TxFifo Empty: %d \n", epnum);
+
+	ep_regs = core_if->dev_if->in_ep_regs[epnum];
+
+	len = ep->dwc_ep.xfer_len - ep->dwc_ep.xfer_count;
+
+	if (len > ep->dwc_ep.maxpacket) {
+		len = ep->dwc_ep.maxpacket;
+	}
+
+	dwords = (len + 3) / 4;
+
+	/* While there is space in the queue and space in the FIFO and
+	 * More data to tranfer, Write packets to the Tx FIFO */
+	txstatus.d32 = DWC_READ_REG32(&dev_if->in_ep_regs[epnum]->dtxfsts);
+	DWC_DEBUGPL(DBG_PCDV, "b4 dtxfsts[%d]=0x%08x\n", epnum, txstatus.d32);
+
+	while (txstatus.b.txfspcavail > dwords &&
+	       ep->dwc_ep.xfer_count < ep->dwc_ep.xfer_len &&
+	       ep->dwc_ep.xfer_len != 0) {
+		/* Write the FIFO */
+		dwc_otg_ep_write_packet(core_if, &ep->dwc_ep, 0);
+
+		len = ep->dwc_ep.xfer_len - ep->dwc_ep.xfer_count;
+		if (len > ep->dwc_ep.maxpacket) {
+			len = ep->dwc_ep.maxpacket;
+		}
+
+		dwords = (len + 3) / 4;
+		txstatus.d32 =
+		    DWC_READ_REG32(&dev_if->in_ep_regs[epnum]->dtxfsts);
+		DWC_DEBUGPL(DBG_PCDV, "dtxfsts[%d]=0x%08x\n", epnum,
+			    txstatus.d32);
+	}
+
+	DWC_DEBUGPL(DBG_PCDV, "b4 dtxfsts[%d]=0x%08x\n", epnum,
+		    DWC_READ_REG32(&dev_if->in_ep_regs[epnum]->dtxfsts));
+
+	return 1;
+}
+
+/**
+ * This function is called when the Device is disconnected. It stops
+ * any active requests and informs the Gadget driver of the
+ * disconnect.
+ */
+void dwc_otg_pcd_stop(dwc_otg_pcd_t * pcd)
+{
+	int i, num_in_eps, num_out_eps;
+	dwc_otg_pcd_ep_t *ep;
+
+	gintmsk_data_t intr_mask = {.d32 = 0 };
+
+	DWC_SPINLOCK(pcd->lock);
+
+	num_in_eps = GET_CORE_IF(pcd)->dev_if->num_in_eps;
+	num_out_eps = GET_CORE_IF(pcd)->dev_if->num_out_eps;
+
+	DWC_DEBUGPL(DBG_PCDV, "%s() \n", __func__);
+	/* don't disconnect drivers more than once */
+	if (pcd->ep0state == EP0_DISCONNECT) {
+		DWC_DEBUGPL(DBG_ANY, "%s() Already Disconnected\n", __func__);
+		DWC_SPINUNLOCK(pcd->lock);
+		return;
+	}
+	pcd->ep0state = EP0_DISCONNECT;
+
+	/* Reset the OTG state. */
+	dwc_otg_pcd_update_otg(pcd, 1);
+
+	/* Disable the NP Tx Fifo Empty Interrupt. */
+	intr_mask.b.nptxfempty = 1;
+	DWC_MODIFY_REG32(&GET_CORE_IF(pcd)->core_global_regs->gintmsk,
+			 intr_mask.d32, 0);
+
+	/* Flush the FIFOs */
+	/**@todo NGS Flush Periodic FIFOs */
+	dwc_otg_flush_tx_fifo(GET_CORE_IF(pcd), 0x10);
+	dwc_otg_flush_rx_fifo(GET_CORE_IF(pcd));
+
+	/* prevent new request submissions, kill any outstanding requests  */
+	ep = &pcd->ep0;
+	dwc_otg_request_nuke(ep);
+	/* prevent new request submissions, kill any outstanding requests  */
+	for (i = 0; i < num_in_eps; i++) {
+		dwc_otg_pcd_ep_t *ep = &pcd->in_ep[i];
+		dwc_otg_request_nuke(ep);
+	}
+	/* prevent new request submissions, kill any outstanding requests  */
+	for (i = 0; i < num_out_eps; i++) {
+		dwc_otg_pcd_ep_t *ep = &pcd->out_ep[i];
+		dwc_otg_request_nuke(ep);
+	}
+
+	/* report disconnect; the driver is already quiesced */
+	if (pcd->fops->disconnect) {
+		DWC_SPINUNLOCK(pcd->lock);
+		pcd->fops->disconnect(pcd);
+		DWC_SPINLOCK(pcd->lock);
+	}
+	DWC_SPINUNLOCK(pcd->lock);
+}
+
+/**
+ * This interrupt indicates that ...
+ */
+int32_t dwc_otg_pcd_handle_i2c_intr(dwc_otg_pcd_t * pcd)
+{
+	gintmsk_data_t intr_mask = {.d32 = 0 };
+	gintsts_data_t gintsts;
+
+	DWC_PRINTF("INTERRUPT Handler not implemented for %s\n", "i2cintr");
+	intr_mask.b.i2cintr = 1;
+	DWC_MODIFY_REG32(&GET_CORE_IF(pcd)->core_global_regs->gintmsk,
+			 intr_mask.d32, 0);
+
+	/* Clear interrupt */
+	gintsts.d32 = 0;
+	gintsts.b.i2cintr = 1;
+	DWC_WRITE_REG32(&GET_CORE_IF(pcd)->core_global_regs->gintsts,
+			gintsts.d32);
+	return 1;
+}
+
+/**
+ * This interrupt indicates that ...
+ */
+int32_t dwc_otg_pcd_handle_early_suspend_intr(dwc_otg_pcd_t * pcd)
+{
+	gintsts_data_t gintsts;
+#if defined(VERBOSE)
+	DWC_PRINTF("Early Suspend Detected\n");
+#endif
+
+	/* Clear interrupt */
+	gintsts.d32 = 0;
+	gintsts.b.erlysuspend = 1;
+	DWC_WRITE_REG32(&GET_CORE_IF(pcd)->core_global_regs->gintsts,
+			gintsts.d32);
+	return 1;
+}
+
+/**
+ * This function configures EPO to receive SETUP packets.
+ *
+ * @todo NGS: Update the comments from the HW FS.
+ *
+ *	-# Program the following fields in the endpoint specific registers
+ *	for Control OUT EP 0, in order to receive a setup packet
+ *	- DOEPTSIZ0.Packet Count = 3 (To receive up to 3 back to back
+ *	  setup packets)
+ *	- DOEPTSIZE0.Transfer Size = 24 Bytes (To receive up to 3 back
+ *	  to back setup packets)
+ *		- In DMA mode, DOEPDMA0 Register with a memory address to
+ *		  store any setup packets received
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param pcd	  Programming view of the PCD.
+ */
+static inline void ep0_out_start(dwc_otg_core_if_t * core_if,
+				 dwc_otg_pcd_t * pcd)
+{
+	dwc_otg_dev_if_t *dev_if = core_if->dev_if;
+	deptsiz0_data_t doeptsize0 = {.d32 = 0 };
+	dwc_otg_dev_dma_desc_t *dma_desc;
+	depctl_data_t doepctl = {.d32 = 0 };
+
+#ifdef VERBOSE
+	DWC_DEBUGPL(DBG_PCDV, "%s() doepctl0=%0x\n", __func__,
+		    DWC_READ_REG32(&dev_if->out_ep_regs[0]->doepctl));
+#endif
+
+	doeptsize0.b.supcnt = 3;
+	doeptsize0.b.pktcnt = 1;
+	doeptsize0.b.xfersize = 8 * 3;
+
+	if (core_if->dma_enable) {
+		if (!core_if->dma_desc_enable) {
+			/** put here as for Hermes mode deptisz register should not be written */
+			DWC_WRITE_REG32(&dev_if->out_ep_regs[0]->doeptsiz,
+					doeptsize0.d32);
+
+			/** @todo dma needs to handle multiple setup packets (up to 3) */
+			DWC_WRITE_REG32(&dev_if->out_ep_regs[0]->doepdma,
+					pcd->setup_pkt_dma_handle);
+		} else {
+			dev_if->setup_desc_index =
+			    (dev_if->setup_desc_index + 1) & 1;
+			dma_desc =
+			    dev_if->setup_desc_addr[dev_if->setup_desc_index];
+
+			/** DMA Descriptor Setup */
+			dma_desc->status.b.bs = BS_HOST_BUSY;
+			dma_desc->status.b.l = 1;
+			dma_desc->status.b.ioc = 1;
+			dma_desc->status.b.bytes = pcd->ep0.dwc_ep.maxpacket;
+			dma_desc->buf = pcd->setup_pkt_dma_handle;
+			dma_desc->status.b.sts = 0;
+			dma_desc->status.b.bs = BS_HOST_READY;
+
+			/** DOEPDMA0 Register write */
+			DWC_WRITE_REG32(&dev_if->out_ep_regs[0]->doepdma,
+					dev_if->
+					dma_setup_desc_addr
+					[dev_if->setup_desc_index]);
+		}
+
+	} else {
+		/** put here as for Hermes mode deptisz register should not be written */
+		DWC_WRITE_REG32(&dev_if->out_ep_regs[0]->doeptsiz,
+				doeptsize0.d32);
+	}
+
+	/** DOEPCTL0 Register write */
+	doepctl.b.epena = 1;
+	doepctl.b.cnak = 1;
+	DWC_WRITE_REG32(&dev_if->out_ep_regs[0]->doepctl, doepctl.d32);
+
+#ifdef VERBOSE
+	DWC_DEBUGPL(DBG_PCDV, "doepctl0=%0x\n",
+		    DWC_READ_REG32(&dev_if->out_ep_regs[0]->doepctl));
+	DWC_DEBUGPL(DBG_PCDV, "diepctl0=%0x\n",
+		    DWC_READ_REG32(&dev_if->in_ep_regs[0]->diepctl));
+#endif
+}
+
+/**
+ * This interrupt occurs when a USB Reset is detected. When the USB
+ * Reset Interrupt occurs the device state is set to DEFAULT and the
+ * EP0 state is set to IDLE.
+ *	-#	Set the NAK bit for all OUT endpoints (DOEPCTLn.SNAK = 1)
+ *	-#	Unmask the following interrupt bits
+ *		- DAINTMSK.INEP0 = 1 (Control 0 IN endpoint)
+ *	- DAINTMSK.OUTEP0 = 1 (Control 0 OUT endpoint)
+ *	- DOEPMSK.SETUP = 1
+ *	- DOEPMSK.XferCompl = 1
+ *	- DIEPMSK.XferCompl = 1
+ *	- DIEPMSK.TimeOut = 1
+ *	-# Program the following fields in the endpoint specific registers
+ *	for Control OUT EP 0, in order to receive a setup packet
+ *	- DOEPTSIZ0.Packet Count = 3 (To receive up to 3 back to back
+ *	  setup packets)
+ *	- DOEPTSIZE0.Transfer Size = 24 Bytes (To receive up to 3 back
+ *	  to back setup packets)
+ *		- In DMA mode, DOEPDMA0 Register with a memory address to
+ *		  store any setup packets received
+ * At this point, all the required initialization, except for enabling
+ * the control 0 OUT endpoint is done, for receiving SETUP packets.
+ */
+int32_t dwc_otg_pcd_handle_usb_reset_intr(dwc_otg_pcd_t * pcd)
+{
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(pcd);
+	dwc_otg_dev_if_t *dev_if = core_if->dev_if;
+	depctl_data_t doepctl = {.d32 = 0 };
+	depctl_data_t diepctl = {.d32 = 0 };
+	daint_data_t daintmsk = {.d32 = 0 };
+	doepmsk_data_t doepmsk = {.d32 = 0 };
+	diepmsk_data_t diepmsk = {.d32 = 0 };
+	dcfg_data_t dcfg = {.d32 = 0 };
+	grstctl_t resetctl = {.d32 = 0 };
+	dctl_data_t dctl = {.d32 = 0 };
+	int i = 0;
+	gintsts_data_t gintsts;
+	pcgcctl_data_t power = {.d32 = 0 };
+
+	power.d32 = DWC_READ_REG32(core_if->pcgcctl);
+	if (power.b.stoppclk) {
+		power.d32 = 0;
+		power.b.stoppclk = 1;
+		DWC_MODIFY_REG32(core_if->pcgcctl, power.d32, 0);
+
+		power.b.pwrclmp = 1;
+		DWC_MODIFY_REG32(core_if->pcgcctl, power.d32, 0);
+
+		power.b.rstpdwnmodule = 1;
+		DWC_MODIFY_REG32(core_if->pcgcctl, power.d32, 0);
+	}
+
+	core_if->lx_state = DWC_OTG_L0;
+
+	DWC_PRINTF("USB RESET\n");
+#ifdef DWC_EN_ISOC
+	for (i = 1; i < 16; ++i) {
+		dwc_otg_pcd_ep_t *ep;
+		dwc_ep_t *dwc_ep;
+		ep = get_in_ep(pcd, i);
+		if (ep != 0) {
+			dwc_ep = &ep->dwc_ep;
+			dwc_ep->next_frame = 0xffffffff;
+		}
+	}
+#endif /* DWC_EN_ISOC */
+
+	/* reset the HNP settings */
+	dwc_otg_pcd_update_otg(pcd, 1);
+
+	/* Clear the Remote Wakeup Signalling */
+	dctl.b.rmtwkupsig = 1;
+	DWC_MODIFY_REG32(&core_if->dev_if->dev_global_regs->dctl, dctl.d32, 0);
+
+	/* Set NAK for all OUT EPs */
+	doepctl.b.snak = 1;
+	for (i = 0; i <= dev_if->num_out_eps; i++) {
+		DWC_WRITE_REG32(&dev_if->out_ep_regs[i]->doepctl, doepctl.d32);
+	}
+
+	/* Flush the NP Tx FIFO */
+	dwc_otg_flush_tx_fifo(core_if, 0x10);
+	/* Flush the Learning Queue */
+	resetctl.b.intknqflsh = 1;
+	DWC_WRITE_REG32(&core_if->core_global_regs->grstctl, resetctl.d32);
+
+	if (!core_if->core_params->en_multiple_tx_fifo && core_if->dma_enable) {
+		core_if->start_predict = 0;
+		for (i = 0; i<= core_if->dev_if->num_in_eps; ++i) {
+			core_if->nextep_seq[i] = 0xff;	// 0xff - EP not active
+		}
+		core_if->nextep_seq[0] = 0;
+		core_if->first_in_nextep_seq = 0;
+		diepctl.d32 = DWC_READ_REG32(&dev_if->in_ep_regs[0]->diepctl);
+		diepctl.b.nextep = 0;
+		DWC_WRITE_REG32(&dev_if->in_ep_regs[0]->diepctl, diepctl.d32);
+
+		/* Update IN Endpoint Mismatch Count by active IN NP EP count + 1 */
+		dcfg.d32 = DWC_READ_REG32(&dev_if->dev_global_regs->dcfg);
+		dcfg.b.epmscnt = 2;
+		DWC_WRITE_REG32(&dev_if->dev_global_regs->dcfg, dcfg.d32);
+
+		DWC_DEBUGPL(DBG_PCDV,"%s first_in_nextep_seq= %2d; nextep_seq[]:\n",
+			__func__, core_if->first_in_nextep_seq);
+		for (i=0; i <= core_if->dev_if->num_in_eps; i++) {
+			DWC_DEBUGPL(DBG_PCDV, "%2d\n", core_if->nextep_seq[i]);
+		}
+	}
+
+	if (core_if->multiproc_int_enable) {
+		daintmsk.b.inep0 = 1;
+		daintmsk.b.outep0 = 1;
+		DWC_WRITE_REG32(&dev_if->dev_global_regs->deachintmsk,
+				daintmsk.d32);
+
+		doepmsk.b.setup = 1;
+		doepmsk.b.xfercompl = 1;
+		doepmsk.b.ahberr = 1;
+		doepmsk.b.epdisabled = 1;
+
+		if (core_if->dma_desc_enable) {
+			doepmsk.b.stsphsercvd = 1;
+			doepmsk.b.bna = 1;
+		}
+/*
+		doepmsk.b.babble = 1;
+		doepmsk.b.nyet = 1;
+
+		if (core_if->dma_enable) {
+			doepmsk.b.nak = 1;
+		}
+*/
+		DWC_WRITE_REG32(&dev_if->dev_global_regs->doepeachintmsk[0],
+				doepmsk.d32);
+
+		diepmsk.b.xfercompl = 1;
+		diepmsk.b.timeout = 1;
+		diepmsk.b.epdisabled = 1;
+		diepmsk.b.ahberr = 1;
+		diepmsk.b.intknepmis = 1;
+		if (!core_if->en_multiple_tx_fifo && core_if->dma_enable)
+			diepmsk.b.intknepmis = 0;
+
+/*		if (core_if->dma_desc_enable) {
+			diepmsk.b.bna = 1;
+		}
+*/
+/*
+		if (core_if->dma_enable) {
+			diepmsk.b.nak = 1;
+		}
+*/
+		DWC_WRITE_REG32(&dev_if->dev_global_regs->diepeachintmsk[0],
+				diepmsk.d32);
+	} else {
+		daintmsk.b.inep0 = 1;
+		daintmsk.b.outep0 = 1;
+		DWC_WRITE_REG32(&dev_if->dev_global_regs->daintmsk,
+				daintmsk.d32);
+
+		doepmsk.b.setup = 1;
+		doepmsk.b.xfercompl = 1;
+		doepmsk.b.ahberr = 1;
+		doepmsk.b.epdisabled = 1;
+
+		if (core_if->dma_desc_enable) {
+			doepmsk.b.stsphsercvd = 1;
+			doepmsk.b.bna = 1;
+		}
+		DWC_WRITE_REG32(&dev_if->dev_global_regs->doepmsk, doepmsk.d32);
+
+		diepmsk.b.xfercompl = 1;
+		diepmsk.b.timeout = 1;
+		diepmsk.b.epdisabled = 1;
+		diepmsk.b.ahberr = 1;
+		if (!core_if->en_multiple_tx_fifo && core_if->dma_enable)
+			diepmsk.b.intknepmis = 0;
+/*
+		if (core_if->dma_desc_enable) {
+			diepmsk.b.bna = 1;
+		}
+*/
+
+		DWC_WRITE_REG32(&dev_if->dev_global_regs->diepmsk, diepmsk.d32);
+	}
+
+	/* Reset Device Address */
+	dcfg.d32 = DWC_READ_REG32(&dev_if->dev_global_regs->dcfg);
+	dcfg.b.devaddr = 0;
+	DWC_WRITE_REG32(&dev_if->dev_global_regs->dcfg, dcfg.d32);
+
+	/* setup EP0 to receive SETUP packets */
+	ep0_out_start(core_if, pcd);
+
+	/* Clear interrupt */
+	gintsts.d32 = 0;
+	gintsts.b.usbreset = 1;
+	DWC_WRITE_REG32(&core_if->core_global_regs->gintsts, gintsts.d32);
+
+	return 1;
+}
+
+/**
+ * Get the device speed from the device status register and convert it
+ * to USB speed constant.
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ */
+static int get_device_speed(dwc_otg_core_if_t * core_if)
+{
+	dsts_data_t dsts;
+	int speed = 0;
+	dsts.d32 = DWC_READ_REG32(&core_if->dev_if->dev_global_regs->dsts);
+
+	switch (dsts.b.enumspd) {
+	case DWC_DSTS_ENUMSPD_HS_PHY_30MHZ_OR_60MHZ:
+		speed = USB_SPEED_HIGH;
+		break;
+	case DWC_DSTS_ENUMSPD_FS_PHY_30MHZ_OR_60MHZ:
+	case DWC_DSTS_ENUMSPD_FS_PHY_48MHZ:
+		speed = USB_SPEED_FULL;
+		break;
+
+	case DWC_DSTS_ENUMSPD_LS_PHY_6MHZ:
+		speed = USB_SPEED_LOW;
+		break;
+	}
+
+	return speed;
+}
+
+/**
+ * Read the device status register and set the device speed in the
+ * data structure.
+ * Set up EP0 to receive SETUP packets by calling dwc_ep0_activate.
+ */
+int32_t dwc_otg_pcd_handle_enum_done_intr(dwc_otg_pcd_t * pcd)
+{
+	dwc_otg_pcd_ep_t *ep0 = &pcd->ep0;
+	gintsts_data_t gintsts;
+	gusbcfg_data_t gusbcfg;
+	dwc_otg_core_global_regs_t *global_regs =
+	    GET_CORE_IF(pcd)->core_global_regs;
+	uint8_t utmi16b, utmi8b;
+	int speed;
+	DWC_DEBUGPL(DBG_PCD, "SPEED ENUM\n");
+
+	if (GET_CORE_IF(pcd)->snpsid >= OTG_CORE_REV_2_60a) {
+		utmi16b = 6;	//vahrama old value was 6;
+		utmi8b = 9;
+	} else {
+		utmi16b = 4;
+		utmi8b = 8;
+	}
+	dwc_otg_ep0_activate(GET_CORE_IF(pcd), &ep0->dwc_ep);
+
+#ifdef DEBUG_EP0
+	print_ep0_state(pcd);
+#endif
+
+	if (pcd->ep0state == EP0_DISCONNECT) {
+		pcd->ep0state = EP0_IDLE;
+	} else if (pcd->ep0state == EP0_STALL) {
+		pcd->ep0state = EP0_IDLE;
+	}
+
+	pcd->ep0state = EP0_IDLE;
+
+	ep0->stopped = 0;
+
+	speed = get_device_speed(GET_CORE_IF(pcd));
+	pcd->fops->connect(pcd, speed);
+
+	/* Set USB turnaround time based on device speed and PHY interface. */
+	gusbcfg.d32 = DWC_READ_REG32(&global_regs->gusbcfg);
+	if (speed == USB_SPEED_HIGH) {
+		if (GET_CORE_IF(pcd)->hwcfg2.b.hs_phy_type ==
+		    DWC_HWCFG2_HS_PHY_TYPE_ULPI) {
+			/* ULPI interface */
+			gusbcfg.b.usbtrdtim = 9;
+		}
+		if (GET_CORE_IF(pcd)->hwcfg2.b.hs_phy_type ==
+		    DWC_HWCFG2_HS_PHY_TYPE_UTMI) {
+			/* UTMI+ interface */
+			if (GET_CORE_IF(pcd)->hwcfg4.b.utmi_phy_data_width == 0) {
+				gusbcfg.b.usbtrdtim = utmi8b;
+			} else if (GET_CORE_IF(pcd)->hwcfg4.
+				   b.utmi_phy_data_width == 1) {
+				gusbcfg.b.usbtrdtim = utmi16b;
+			} else if (GET_CORE_IF(pcd)->
+				   core_params->phy_utmi_width == 8) {
+				gusbcfg.b.usbtrdtim = utmi8b;
+			} else {
+				gusbcfg.b.usbtrdtim = utmi16b;
+			}
+		}
+		if (GET_CORE_IF(pcd)->hwcfg2.b.hs_phy_type ==
+		    DWC_HWCFG2_HS_PHY_TYPE_UTMI_ULPI) {
+			/* UTMI+  OR  ULPI interface */
+			if (gusbcfg.b.ulpi_utmi_sel == 1) {
+				/* ULPI interface */
+				gusbcfg.b.usbtrdtim = 9;
+			} else {
+				/* UTMI+ interface */
+				if (GET_CORE_IF(pcd)->
+				    core_params->phy_utmi_width == 16) {
+					gusbcfg.b.usbtrdtim = utmi16b;
+				} else {
+					gusbcfg.b.usbtrdtim = utmi8b;
+				}
+			}
+		}
+	} else {
+		/* Full or low speed */
+		gusbcfg.b.usbtrdtim = 9;
+	}
+	DWC_WRITE_REG32(&global_regs->gusbcfg, gusbcfg.d32);
+
+	/* Clear interrupt */
+	gintsts.d32 = 0;
+	gintsts.b.enumdone = 1;
+	DWC_WRITE_REG32(&GET_CORE_IF(pcd)->core_global_regs->gintsts,
+			gintsts.d32);
+	return 1;
+}
+
+/**
+ * This interrupt indicates that the ISO OUT Packet was dropped due to
+ * Rx FIFO full or Rx Status Queue Full.  If this interrupt occurs
+ * read all the data from the Rx FIFO.
+ */
+int32_t dwc_otg_pcd_handle_isoc_out_packet_dropped_intr(dwc_otg_pcd_t * pcd)
+{
+	gintmsk_data_t intr_mask = {.d32 = 0 };
+	gintsts_data_t gintsts;
+
+	DWC_WARN("INTERRUPT Handler not implemented for %s\n",
+		 "ISOC Out Dropped");
+
+	intr_mask.b.isooutdrop = 1;
+	DWC_MODIFY_REG32(&GET_CORE_IF(pcd)->core_global_regs->gintmsk,
+			 intr_mask.d32, 0);
+
+	/* Clear interrupt */
+	gintsts.d32 = 0;
+	gintsts.b.isooutdrop = 1;
+	DWC_WRITE_REG32(&GET_CORE_IF(pcd)->core_global_regs->gintsts,
+			gintsts.d32);
+
+	return 1;
+}
+
+/**
+ * This interrupt indicates the end of the portion of the micro-frame
+ * for periodic transactions.  If there is a periodic transaction for
+ * the next frame, load the packets into the EP periodic Tx FIFO.
+ */
+int32_t dwc_otg_pcd_handle_end_periodic_frame_intr(dwc_otg_pcd_t * pcd)
+{
+	gintmsk_data_t intr_mask = {.d32 = 0 };
+	gintsts_data_t gintsts;
+	DWC_PRINTF("INTERRUPT Handler not implemented for %s\n", "EOP");
+
+	intr_mask.b.eopframe = 1;
+	DWC_MODIFY_REG32(&GET_CORE_IF(pcd)->core_global_regs->gintmsk,
+			 intr_mask.d32, 0);
+
+	/* Clear interrupt */
+	gintsts.d32 = 0;
+	gintsts.b.eopframe = 1;
+	DWC_WRITE_REG32(&GET_CORE_IF(pcd)->core_global_regs->gintsts,
+			gintsts.d32);
+
+	return 1;
+}
+
+/**
+ * This interrupt indicates that EP of the packet on the top of the
+ * non-periodic Tx FIFO does not match EP of the IN Token received.
+ *
+ * The "Device IN Token Queue" Registers are read to determine the
+ * order the IN Tokens have been received. The non-periodic Tx FIFO
+ * is flushed, so it can be reloaded in the order seen in the IN Token
+ * Queue.
+ */
+int32_t dwc_otg_pcd_handle_ep_mismatch_intr(dwc_otg_pcd_t * pcd)
+{
+	gintsts_data_t gintsts;
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(pcd);
+	dctl_data_t dctl;
+	gintmsk_data_t intr_mask = {.d32 = 0 };
+
+	if (!core_if->en_multiple_tx_fifo && core_if->dma_enable) {
+		core_if->start_predict = 1;
+
+		DWC_DEBUGPL(DBG_PCDV, "%s(%p)\n", __func__, core_if);
+
+		gintsts.d32 = DWC_READ_REG32(&core_if->core_global_regs->gintsts);
+		if (!gintsts.b.ginnakeff) {
+			/* Disable EP Mismatch interrupt */
+			intr_mask.d32 = 0;
+			intr_mask.b.epmismatch = 1;
+			DWC_MODIFY_REG32(&core_if->core_global_regs->gintmsk, intr_mask.d32, 0);
+			/* Enable the Global IN NAK Effective Interrupt */
+			intr_mask.d32 = 0;
+			intr_mask.b.ginnakeff = 1;
+			DWC_MODIFY_REG32(&core_if->core_global_regs->gintmsk, 0, intr_mask.d32);
+			/* Set the global non-periodic IN NAK handshake */
+			dctl.d32 = DWC_READ_REG32(&core_if->dev_if->dev_global_regs->dctl);
+			dctl.b.sgnpinnak = 1;
+			DWC_WRITE_REG32(&core_if->dev_if->dev_global_regs->dctl, dctl.d32);
+		} else {
+			DWC_PRINTF("gintsts.b.ginnakeff = 1! dctl.b.sgnpinnak not set\n");
+		}
+		/* Disabling of all EP's will be done in dwc_otg_pcd_handle_in_nak_effective()
+		 * handler after Global IN NAK Effective interrupt will be asserted */
+	}
+	/* Clear interrupt */
+	gintsts.d32 = 0;
+	gintsts.b.epmismatch = 1;
+	DWC_WRITE_REG32(&core_if->core_global_regs->gintsts, gintsts.d32);
+
+	return 1;
+}
+
+/**
+ * This interrupt is valid only in DMA mode. This interrupt indicates that the
+ * core has stopped fetching data for IN endpoints due to the unavailability of
+ * TxFIFO space or Request Queue space. This interrupt is used by the
+ * application for an endpoint mismatch algorithm.
+ *
+ * @param pcd The PCD
+ */
+int32_t dwc_otg_pcd_handle_ep_fetsusp_intr(dwc_otg_pcd_t * pcd)
+{
+	gintsts_data_t gintsts;
+	gintmsk_data_t gintmsk_data;
+	dctl_data_t dctl;
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(pcd);
+	DWC_DEBUGPL(DBG_PCDV, "%s(%p)\n", __func__, core_if);
+
+	/* Clear the global non-periodic IN NAK handshake */
+	dctl.d32 = 0;
+	dctl.b.cgnpinnak = 1;
+	DWC_MODIFY_REG32(&core_if->dev_if->dev_global_regs->dctl, dctl.d32, dctl.d32);
+
+	/* Mask GINTSTS.FETSUSP interrupt */
+	gintmsk_data.d32 = DWC_READ_REG32(&core_if->core_global_regs->gintmsk);
+	gintmsk_data.b.fetsusp = 0;
+	DWC_WRITE_REG32(&core_if->core_global_regs->gintmsk, gintmsk_data.d32);
+
+	/* Clear interrupt */
+	gintsts.d32 = 0;
+	gintsts.b.fetsusp = 1;
+	DWC_WRITE_REG32(&core_if->core_global_regs->gintsts, gintsts.d32);
+
+	return 1;
+}
+/**
+ * This funcion stalls EP0.
+ */
+static inline void ep0_do_stall(dwc_otg_pcd_t * pcd, const int err_val)
+{
+	dwc_otg_pcd_ep_t *ep0 = &pcd->ep0;
+	usb_device_request_t *ctrl = &pcd->setup_pkt->req;
+	DWC_WARN("req %02x.%02x protocol STALL; err %d\n",
+		 ctrl->bmRequestType, ctrl->bRequest, err_val);
+
+	ep0->dwc_ep.is_in = 1;
+	dwc_otg_ep_set_stall(GET_CORE_IF(pcd), &ep0->dwc_ep);
+	pcd->ep0.stopped = 1;
+	pcd->ep0state = EP0_IDLE;
+	ep0_out_start(GET_CORE_IF(pcd), pcd);
+}
+
+/**
+ * This functions delegates the setup command to the gadget driver.
+ */
+static inline void do_gadget_setup(dwc_otg_pcd_t * pcd,
+				   usb_device_request_t * ctrl)
+{
+	int ret = 0;
+	DWC_SPINUNLOCK(pcd->lock);
+	ret = pcd->fops->setup(pcd, (uint8_t *) ctrl);
+	DWC_SPINLOCK(pcd->lock);
+	if (ret < 0) {
+		ep0_do_stall(pcd, ret);
+	}
+
+	/** @todo This is a g_file_storage gadget driver specific
+	 * workaround: a DELAYED_STATUS result from the fsg_setup
+	 * routine will result in the gadget queueing a EP0 IN status
+	 * phase for a two-stage control transfer. Exactly the same as
+	 * a SET_CONFIGURATION/SET_INTERFACE except that this is a class
+	 * specific request.  Need a generic way to know when the gadget
+	 * driver will queue the status phase. Can we assume when we
+	 * call the gadget driver setup() function that it will always
+	 * queue and require the following flag? Need to look into
+	 * this.
+	 */
+
+	if (ret == 256 + 999) {
+		pcd->request_config = 1;
+	}
+}
+
+#ifdef DWC_UTE_CFI
+/**
+ * This functions delegates the CFI setup commands to the gadget driver.
+ * This function will return a negative value to indicate a failure.
+ */
+static inline int cfi_gadget_setup(dwc_otg_pcd_t * pcd,
+				   struct cfi_usb_ctrlrequest *ctrl_req)
+{
+	int ret = 0;
+
+	if (pcd->fops && pcd->fops->cfi_setup) {
+		DWC_SPINUNLOCK(pcd->lock);
+		ret = pcd->fops->cfi_setup(pcd, ctrl_req);
+		DWC_SPINLOCK(pcd->lock);
+		if (ret < 0) {
+			ep0_do_stall(pcd, ret);
+			return ret;
+		}
+	}
+
+	return ret;
+}
+#endif
+
+/**
+ * This function starts the Zero-Length Packet for the IN status phase
+ * of a 2 stage control transfer.
+ */
+static inline void do_setup_in_status_phase(dwc_otg_pcd_t * pcd)
+{
+	dwc_otg_pcd_ep_t *ep0 = &pcd->ep0;
+	if (pcd->ep0state == EP0_STALL) {
+		return;
+	}
+
+	pcd->ep0state = EP0_IN_STATUS_PHASE;
+
+	/* Prepare for more SETUP Packets */
+	DWC_DEBUGPL(DBG_PCD, "EP0 IN ZLP\n");
+	ep0->dwc_ep.xfer_len = 0;
+	ep0->dwc_ep.xfer_count = 0;
+	ep0->dwc_ep.is_in = 1;
+	ep0->dwc_ep.dma_addr = pcd->setup_pkt_dma_handle;
+	dwc_otg_ep0_start_transfer(GET_CORE_IF(pcd), &ep0->dwc_ep);
+
+	/* Prepare for more SETUP Packets */
+	//ep0_out_start(GET_CORE_IF(pcd), pcd);
+}
+
+/**
+ * This function starts the Zero-Length Packet for the OUT status phase
+ * of a 2 stage control transfer.
+ */
+static inline void do_setup_out_status_phase(dwc_otg_pcd_t * pcd)
+{
+	dwc_otg_pcd_ep_t *ep0 = &pcd->ep0;
+	if (pcd->ep0state == EP0_STALL) {
+		DWC_DEBUGPL(DBG_PCD, "EP0 STALLED\n");
+		return;
+	}
+	pcd->ep0state = EP0_OUT_STATUS_PHASE;
+
+	DWC_DEBUGPL(DBG_PCD, "EP0 OUT ZLP\n");
+	ep0->dwc_ep.xfer_len = 0;
+	ep0->dwc_ep.xfer_count = 0;
+	ep0->dwc_ep.is_in = 0;
+	ep0->dwc_ep.dma_addr = pcd->setup_pkt_dma_handle;
+	dwc_otg_ep0_start_transfer(GET_CORE_IF(pcd), &ep0->dwc_ep);
+
+	/* Prepare for more SETUP Packets */
+	if (GET_CORE_IF(pcd)->dma_enable == 0) {
+		ep0_out_start(GET_CORE_IF(pcd), pcd);
+	}
+}
+
+/**
+ * Clear the EP halt (STALL) and if pending requests start the
+ * transfer.
+ */
+static inline void pcd_clear_halt(dwc_otg_pcd_t * pcd, dwc_otg_pcd_ep_t * ep)
+{
+	if (ep->dwc_ep.stall_clear_flag == 0)
+		dwc_otg_ep_clear_stall(GET_CORE_IF(pcd), &ep->dwc_ep);
+
+	/* Reactive the EP */
+	dwc_otg_ep_activate(GET_CORE_IF(pcd), &ep->dwc_ep);
+	if (ep->stopped) {
+		ep->stopped = 0;
+		/* If there is a request in the EP queue start it */
+
+		/** @todo FIXME: this causes an EP mismatch in DMA mode.
+		 * epmismatch not yet implemented. */
+
+		/*
+		 * Above fixme is solved by implmenting a tasklet to call the
+		 * start_next_request(), outside of interrupt context at some
+		 * time after the current time, after a clear-halt setup packet.
+		 * Still need to implement ep mismatch in the future if a gadget
+		 * ever uses more than one endpoint at once
+		 */
+		ep->queue_sof = 1;
+		DWC_TASK_SCHEDULE(pcd->start_xfer_tasklet);
+	}
+	/* Start Control Status Phase */
+	do_setup_in_status_phase(pcd);
+}
+
+/**
+ * This function is called when the SET_FEATURE TEST_MODE Setup packet
+ * is sent from the host.  The Device Control register is written with
+ * the Test Mode bits set to the specified Test Mode.  This is done as
+ * a tasklet so that the "Status" phase of the control transfer
+ * completes before transmitting the TEST packets.
+ *
+ * @todo This has not been tested since the tasklet struct was put
+ * into the PCD struct!
+ *
+ */
+void do_test_mode(void *data)
+{
+	dctl_data_t dctl;
+	dwc_otg_pcd_t *pcd = (dwc_otg_pcd_t *) data;
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(pcd);
+	int test_mode = pcd->test_mode;
+
+//        DWC_WARN("%s() has not been tested since being rewritten!\n", __func__);
+
+	dctl.d32 = DWC_READ_REG32(&core_if->dev_if->dev_global_regs->dctl);
+	switch (test_mode) {
+	case 1:		// TEST_J
+		dctl.b.tstctl = 1;
+		break;
+
+	case 2:		// TEST_K
+		dctl.b.tstctl = 2;
+		break;
+
+	case 3:		// TEST_SE0_NAK
+		dctl.b.tstctl = 3;
+		break;
+
+	case 4:		// TEST_PACKET
+		dctl.b.tstctl = 4;
+		break;
+
+	case 5:		// TEST_FORCE_ENABLE
+		dctl.b.tstctl = 5;
+		break;
+	}
+	DWC_WRITE_REG32(&core_if->dev_if->dev_global_regs->dctl, dctl.d32);
+}
+
+/**
+ * This function process the GET_STATUS Setup Commands.
+ */
+static inline void do_get_status(dwc_otg_pcd_t * pcd)
+{
+	usb_device_request_t ctrl = pcd->setup_pkt->req;
+	dwc_otg_pcd_ep_t *ep;
+	dwc_otg_pcd_ep_t *ep0 = &pcd->ep0;
+	uint16_t *status = pcd->status_buf;
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(pcd);
+
+#ifdef DEBUG_EP0
+	DWC_DEBUGPL(DBG_PCD,
+		    "GET_STATUS %02x.%02x v%04x i%04x l%04x\n",
+		    ctrl.bmRequestType, ctrl.bRequest,
+		    UGETW(ctrl.wValue), UGETW(ctrl.wIndex),
+		    UGETW(ctrl.wLength));
+#endif
+
+	switch (UT_GET_RECIPIENT(ctrl.bmRequestType)) {
+	case UT_DEVICE:
+		if(UGETW(ctrl.wIndex) == 0xF000) { /* OTG Status selector */
+			DWC_PRINTF("wIndex - %d\n", UGETW(ctrl.wIndex));
+			DWC_PRINTF("OTG VERSION - %d\n", core_if->otg_ver);
+			DWC_PRINTF("OTG CAP - %d, %d\n", core_if->core_params->otg_cap,
+						DWC_OTG_CAP_PARAM_HNP_SRP_CAPABLE);
+			if(core_if->otg_ver == 1 &&
+			core_if->core_params->otg_cap == DWC_OTG_CAP_PARAM_HNP_SRP_CAPABLE) {
+				uint8_t *otgsts = (uint8_t*)pcd->status_buf;
+				*otgsts = (core_if->otg_sts & 0x1);
+				pcd->ep0_pending = 1;
+				ep0->dwc_ep.start_xfer_buff = (uint8_t *) otgsts;
+				ep0->dwc_ep.xfer_buff = (uint8_t *) otgsts;
+				ep0->dwc_ep.dma_addr = pcd->status_buf_dma_handle;
+				ep0->dwc_ep.xfer_len = 1;
+				ep0->dwc_ep.xfer_count = 0;
+				ep0->dwc_ep.total_len = ep0->dwc_ep.xfer_len;
+				dwc_otg_ep0_start_transfer(GET_CORE_IF(pcd), &ep0->dwc_ep);
+				return;
+			} else {
+				ep0_do_stall(pcd, -DWC_E_NOT_SUPPORTED);
+				return;
+			}
+			break;
+		} else {
+			*status = 0x1;	/* Self powered */
+			*status |= pcd->remote_wakeup_enable << 1;
+			break;
+		}
+	case UT_INTERFACE:
+		*status = 0;
+		break;
+
+	case UT_ENDPOINT:
+		ep = get_ep_by_addr(pcd, UGETW(ctrl.wIndex));
+		if (ep == 0 || UGETW(ctrl.wLength) > 2) {
+			ep0_do_stall(pcd, -DWC_E_NOT_SUPPORTED);
+			return;
+		}
+		/** @todo check for EP stall */
+		*status = ep->stopped;
+		break;
+	}
+	pcd->ep0_pending = 1;
+	ep0->dwc_ep.start_xfer_buff = (uint8_t *) status;
+	ep0->dwc_ep.xfer_buff = (uint8_t *) status;
+	ep0->dwc_ep.dma_addr = pcd->status_buf_dma_handle;
+	ep0->dwc_ep.xfer_len = 2;
+	ep0->dwc_ep.xfer_count = 0;
+	ep0->dwc_ep.total_len = ep0->dwc_ep.xfer_len;
+	dwc_otg_ep0_start_transfer(GET_CORE_IF(pcd), &ep0->dwc_ep);
+}
+
+/**
+ * This function process the SET_FEATURE Setup Commands.
+ */
+static inline void do_set_feature(dwc_otg_pcd_t * pcd)
+{
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(pcd);
+	dwc_otg_core_global_regs_t *global_regs = core_if->core_global_regs;
+	usb_device_request_t ctrl = pcd->setup_pkt->req;
+	dwc_otg_pcd_ep_t *ep = 0;
+	int32_t otg_cap_param = core_if->core_params->otg_cap;
+
+#ifndef CONFIG_MACH_M822XX
+    /* Base value of GOTGCTL, VBUS checking enabled */
+	gotgctl_data_t gotgctl = {.d32 = 0 };
+#else  /* CONFIG_MACH_M822XX */
+	/* Base value of GOTGCTL, VBUS checking disabled */
+    gotgctl_data_t gotgctl = {.d32 = ((1<<3) | (1<<2)) };
+#endif	/* CONFIG_MACH_M822XX */
+
+	DWC_DEBUGPL(DBG_PCD, "SET_FEATURE:%02x.%02x v%04x i%04x l%04x\n",
+		    ctrl.bmRequestType, ctrl.bRequest,
+		    UGETW(ctrl.wValue), UGETW(ctrl.wIndex),
+		    UGETW(ctrl.wLength));
+	DWC_DEBUGPL(DBG_PCD, "otg_cap=%d\n", otg_cap_param);
+
+	switch (UT_GET_RECIPIENT(ctrl.bmRequestType)) {
+	case UT_DEVICE:
+		switch (UGETW(ctrl.wValue)) {
+		case UF_DEVICE_REMOTE_WAKEUP:
+			pcd->remote_wakeup_enable = 1;
+			break;
+
+		case UF_TEST_MODE:
+			/* Setup the Test Mode tasklet to do the Test
+			 * Packet generation after the SETUP Status
+			 * phase has completed. */
+
+			/** @todo This has not been tested since the
+			 * tasklet struct was put into the PCD
+			 * struct! */
+			pcd->test_mode = UGETW(ctrl.wIndex) >> 8;
+			DWC_TASK_SCHEDULE(pcd->test_mode_tasklet);
+			break;
+
+		case UF_DEVICE_B_HNP_ENABLE:
+			DWC_DEBUGPL(DBG_PCDV,
+				    "SET_FEATURE: USB_DEVICE_B_HNP_ENABLE\n");
+
+			/* dev may initiate HNP */
+			if (otg_cap_param == DWC_OTG_CAP_PARAM_HNP_SRP_CAPABLE) {
+				pcd->b_hnp_enable = 1;
+				dwc_otg_pcd_update_otg(pcd, 0);
+				DWC_DEBUGPL(DBG_PCD, "Request B HNP\n");
+				/**@todo Is the gotgctl.devhnpen cleared
+				 * by a USB Reset? */
+				gotgctl.b.devhnpen = 1;
+				gotgctl.b.hnpreq = 1;
+				DWC_WRITE_REG32(&global_regs->gotgctl,
+						gotgctl.d32);
+			} else {
+				ep0_do_stall(pcd, -DWC_E_NOT_SUPPORTED);
+				return;
+			}
+			break;
+
+		case UF_DEVICE_A_HNP_SUPPORT:
+			/* RH port supports HNP */
+			DWC_DEBUGPL(DBG_PCDV,
+				    "SET_FEATURE: USB_DEVICE_A_HNP_SUPPORT\n");
+			if (otg_cap_param == DWC_OTG_CAP_PARAM_HNP_SRP_CAPABLE) {
+				pcd->a_hnp_support = 1;
+				dwc_otg_pcd_update_otg(pcd, 0);
+			} else {
+				ep0_do_stall(pcd, -DWC_E_NOT_SUPPORTED);
+				return;
+			}
+			break;
+
+		case UF_DEVICE_A_ALT_HNP_SUPPORT:
+			/* other RH port does */
+			DWC_DEBUGPL(DBG_PCDV,
+				    "SET_FEATURE: USB_DEVICE_A_ALT_HNP_SUPPORT\n");
+			if (otg_cap_param == DWC_OTG_CAP_PARAM_HNP_SRP_CAPABLE) {
+				pcd->a_alt_hnp_support = 1;
+				dwc_otg_pcd_update_otg(pcd, 0);
+			} else {
+				ep0_do_stall(pcd, -DWC_E_NOT_SUPPORTED);
+				return;
+			}
+			break;
+
+		default:
+			ep0_do_stall(pcd, -DWC_E_NOT_SUPPORTED);
+			return;
+
+		}
+		do_setup_in_status_phase(pcd);
+		break;
+
+	case UT_INTERFACE:
+		do_gadget_setup(pcd, &ctrl);
+		break;
+
+	case UT_ENDPOINT:
+		if (UGETW(ctrl.wValue) == UF_ENDPOINT_HALT) {
+			ep = get_ep_by_addr(pcd, UGETW(ctrl.wIndex));
+			if (ep == 0) {
+				ep0_do_stall(pcd, -DWC_E_NOT_SUPPORTED);
+				return;
+			}
+			ep->stopped = 1;
+			dwc_otg_ep_set_stall(core_if, &ep->dwc_ep);
+		}
+		do_setup_in_status_phase(pcd);
+		break;
+	}
+}
+
+/**
+ * This function process the CLEAR_FEATURE Setup Commands.
+ */
+static inline void do_clear_feature(dwc_otg_pcd_t * pcd)
+{
+	usb_device_request_t ctrl = pcd->setup_pkt->req;
+	dwc_otg_pcd_ep_t *ep = 0;
+
+	DWC_DEBUGPL(DBG_PCD,
+		    "CLEAR_FEATURE:%02x.%02x v%04x i%04x l%04x\n",
+		    ctrl.bmRequestType, ctrl.bRequest,
+		    UGETW(ctrl.wValue), UGETW(ctrl.wIndex),
+		    UGETW(ctrl.wLength));
+
+	switch (UT_GET_RECIPIENT(ctrl.bmRequestType)) {
+	case UT_DEVICE:
+		switch (UGETW(ctrl.wValue)) {
+		case UF_DEVICE_REMOTE_WAKEUP:
+			pcd->remote_wakeup_enable = 0;
+			break;
+
+		case UF_TEST_MODE:
+			/** @todo Add CLEAR_FEATURE for TEST modes. */
+			break;
+
+		default:
+			ep0_do_stall(pcd, -DWC_E_NOT_SUPPORTED);
+			return;
+		}
+		do_setup_in_status_phase(pcd);
+		break;
+
+	case UT_ENDPOINT:
+		ep = get_ep_by_addr(pcd, UGETW(ctrl.wIndex));
+		if (ep == 0) {
+			ep0_do_stall(pcd, -DWC_E_NOT_SUPPORTED);
+			return;
+		}
+
+		pcd_clear_halt(pcd, ep);
+
+		break;
+	}
+}
+
+/**
+ * This function process the SET_ADDRESS Setup Commands.
+ */
+static inline void do_set_address(dwc_otg_pcd_t * pcd)
+{
+	dwc_otg_dev_if_t *dev_if = GET_CORE_IF(pcd)->dev_if;
+	usb_device_request_t ctrl = pcd->setup_pkt->req;
+
+	if (ctrl.bmRequestType == UT_DEVICE) {
+		dcfg_data_t dcfg = {.d32 = 0 };
+
+#ifdef DEBUG_EP0
+//                      DWC_DEBUGPL(DBG_PCDV, "SET_ADDRESS:%d\n", ctrl.wValue);
+#endif
+		dcfg.b.devaddr = UGETW(ctrl.wValue);
+		DWC_MODIFY_REG32(&dev_if->dev_global_regs->dcfg, 0, dcfg.d32);
+		do_setup_in_status_phase(pcd);
+	}
+}
+
+/**
+ *	This function processes SETUP commands. In Linux, the USB Command
+ *	processing is done in two places - the first being the PCD and the
+ *	second in the Gadget Driver (for example, the File-Backed Storage
+ *	Gadget Driver).
+ *
+ * <table>
+ * <tr><td>Command	</td><td>Driver </td><td>Description</td></tr>
+ *
+ * <tr><td>GET_STATUS </td><td>PCD </td><td>Command is processed as
+ * defined in chapter 9 of the USB 2.0 Specification chapter 9
+ * </td></tr>
+ *
+ * <tr><td>CLEAR_FEATURE </td><td>PCD </td><td>The Device and Endpoint
+ * requests are the ENDPOINT_HALT feature is procesed, all others the
+ * interface requests are ignored.</td></tr>
+ *
+ * <tr><td>SET_FEATURE </td><td>PCD </td><td>The Device and Endpoint
+ * requests are processed by the PCD.  Interface requests are passed
+ * to the Gadget Driver.</td></tr>
+ *
+ * <tr><td>SET_ADDRESS </td><td>PCD </td><td>Program the DCFG reg,
+ * with device address received </td></tr>
+ *
+ * <tr><td>GET_DESCRIPTOR </td><td>Gadget Driver </td><td>Return the
+ * requested descriptor</td></tr>
+ *
+ * <tr><td>SET_DESCRIPTOR </td><td>Gadget Driver </td><td>Optional -
+ * not implemented by any of the existing Gadget Drivers.</td></tr>
+ *
+ * <tr><td>SET_CONFIGURATION </td><td>Gadget Driver </td><td>Disable
+ * all EPs and enable EPs for new configuration.</td></tr>
+ *
+ * <tr><td>GET_CONFIGURATION </td><td>Gadget Driver </td><td>Return
+ * the current configuration</td></tr>
+ *
+ * <tr><td>SET_INTERFACE </td><td>Gadget Driver </td><td>Disable all
+ * EPs and enable EPs for new configuration.</td></tr>
+ *
+ * <tr><td>GET_INTERFACE </td><td>Gadget Driver </td><td>Return the
+ * current interface.</td></tr>
+ *
+ * <tr><td>SYNC_FRAME </td><td>PCD </td><td>Display debug
+ * message.</td></tr>
+ * </table>
+ *
+ * When the SETUP Phase Done interrupt occurs, the PCD SETUP commands are
+ * processed by pcd_setup. Calling the Function Driver's setup function from
+ * pcd_setup processes the gadget SETUP commands.
+ */
+static inline void pcd_setup(dwc_otg_pcd_t * pcd)
+{
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(pcd);
+	dwc_otg_dev_if_t *dev_if = core_if->dev_if;
+	usb_device_request_t ctrl = pcd->setup_pkt->req;
+	dwc_otg_pcd_ep_t *ep0 = &pcd->ep0;
+
+	deptsiz0_data_t doeptsize0 = {.d32 = 0 };
+
+#ifdef DWC_UTE_CFI
+	int retval = 0;
+	struct cfi_usb_ctrlrequest cfi_req;
+#endif
+
+#ifdef DEBUG_EP0
+	DWC_DEBUGPL(DBG_PCD, "SETUP %02x.%02x v%04x i%04x l%04x\n",
+		    ctrl.bmRequestType, ctrl.bRequest,
+		    UGETW(ctrl.wValue), UGETW(ctrl.wIndex),
+		    UGETW(ctrl.wLength));
+#endif
+
+	doeptsize0.d32 = DWC_READ_REG32(&dev_if->out_ep_regs[0]->doeptsiz);
+
+	/** @todo handle > 1 setup packet , assert error for now */
+
+	if (core_if->dma_enable && core_if->dma_desc_enable == 0
+	    && (doeptsize0.b.supcnt < 2)) {
+		DWC_ERROR
+		    ("\n\n-----------	 CANNOT handle > 1 setup packet in DMA mode\n\n");
+	}
+
+	/* Clean up the request queue */
+	dwc_otg_request_nuke(ep0);
+	ep0->stopped = 0;
+
+	if (ctrl.bmRequestType & UE_DIR_IN) {
+		ep0->dwc_ep.is_in = 1;
+		pcd->ep0state = EP0_IN_DATA_PHASE;
+	} else {
+		ep0->dwc_ep.is_in = 0;
+		pcd->ep0state = EP0_OUT_DATA_PHASE;
+	}
+
+	if (UGETW(ctrl.wLength) == 0) {
+		ep0->dwc_ep.is_in = 1;
+		pcd->ep0state = EP0_IN_STATUS_PHASE;
+	}
+
+	if (UT_GET_TYPE(ctrl.bmRequestType) != UT_STANDARD) {
+
+#ifdef DWC_UTE_CFI
+		DWC_MEMCPY(&cfi_req, &ctrl, sizeof(usb_device_request_t));
+
+		//printk(KERN_ALERT "CFI: req_type=0x%02x; req=0x%02x\n", ctrl.bRequestType, ctrl.bRequest);
+		if (UT_GET_TYPE(cfi_req.bRequestType) == UT_VENDOR) {
+			if (cfi_req.bRequest > 0xB0 && cfi_req.bRequest < 0xBF) {
+				retval = cfi_setup(pcd, &cfi_req);
+				if (retval < 0) {
+					ep0_do_stall(pcd, retval);
+					pcd->ep0_pending = 0;
+					return;
+				}
+
+				/* if need gadget setup then call it and check the retval */
+				if (pcd->cfi->need_gadget_att) {
+					retval =
+					    cfi_gadget_setup(pcd,
+							     &pcd->
+							     cfi->ctrl_req);
+					if (retval < 0) {
+						pcd->ep0_pending = 0;
+						return;
+					}
+				}
+
+				if (pcd->cfi->need_status_in_complete) {
+					do_setup_in_status_phase(pcd);
+				}
+				return;
+			}
+		}
+#endif
+
+		/* handle non-standard (class/vendor) requests in the gadget driver */
+		do_gadget_setup(pcd, &ctrl);
+		return;
+	}
+
+	/** @todo NGS: Handle bad setup packet? */
+
+///////////////////////////////////////////
+//// --- Standard Request handling --- ////
+
+	switch (ctrl.bRequest) {
+	case UR_GET_STATUS:
+		do_get_status(pcd);
+		break;
+
+	case UR_CLEAR_FEATURE:
+		do_clear_feature(pcd);
+		break;
+
+	case UR_SET_FEATURE:
+		do_set_feature(pcd);
+		break;
+
+	case UR_SET_ADDRESS:
+		do_set_address(pcd);
+		break;
+
+	case UR_SET_INTERFACE:
+	case UR_SET_CONFIG:
+//              _pcd->request_config = 1;       /* Configuration changed */
+		do_gadget_setup(pcd, &ctrl);
+		break;
+
+	case UR_SYNCH_FRAME:
+		do_gadget_setup(pcd, &ctrl);
+		break;
+
+	default:
+		/* Call the Gadget Driver's setup functions */
+		do_gadget_setup(pcd, &ctrl);
+		break;
+	}
+}
+
+/**
+ * This function completes the ep0 control transfer.
+ */
+static int32_t ep0_complete_request(dwc_otg_pcd_ep_t * ep)
+{
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(ep->pcd);
+	dwc_otg_dev_if_t *dev_if = core_if->dev_if;
+	dwc_otg_dev_in_ep_regs_t *in_ep_regs =
+	    dev_if->in_ep_regs[ep->dwc_ep.num];
+#ifdef DEBUG_EP0
+	dwc_otg_dev_out_ep_regs_t *out_ep_regs =
+	    dev_if->out_ep_regs[ep->dwc_ep.num];
+#endif
+	deptsiz0_data_t deptsiz;
+	dev_dma_desc_sts_t desc_sts;
+	dwc_otg_pcd_request_t *req;
+	int is_last = 0;
+	dwc_otg_pcd_t *pcd = ep->pcd;
+
+#ifdef DWC_UTE_CFI
+	struct cfi_usb_ctrlrequest *ctrlreq;
+	int retval = -DWC_E_NOT_SUPPORTED;
+#endif
+
+	if (pcd->ep0_pending && DWC_CIRCLEQ_EMPTY(&ep->queue)) {
+		if (ep->dwc_ep.is_in) {
+#ifdef DEBUG_EP0
+			DWC_DEBUGPL(DBG_PCDV, "Do setup OUT status phase\n");
+#endif
+			do_setup_out_status_phase(pcd);
+		} else {
+#ifdef DEBUG_EP0
+			DWC_DEBUGPL(DBG_PCDV, "Do setup IN status phase\n");
+#endif
+
+#ifdef DWC_UTE_CFI
+			ctrlreq = &pcd->cfi->ctrl_req;
+
+			if (UT_GET_TYPE(ctrlreq->bRequestType) == UT_VENDOR) {
+				if (ctrlreq->bRequest > 0xB0
+				    && ctrlreq->bRequest < 0xBF) {
+
+					/* Return if the PCD failed to handle the request */
+					if ((retval =
+					     pcd->cfi->ops.
+					     ctrl_write_complete(pcd->cfi,
+								 pcd)) < 0) {
+						CFI_INFO
+						    ("ERROR setting a new value in the PCD(%d)\n",
+						     retval);
+						ep0_do_stall(pcd, retval);
+						pcd->ep0_pending = 0;
+						return 0;
+					}
+
+					/* If the gadget needs to be notified on the request */
+					if (pcd->cfi->need_gadget_att == 1) {
+						//retval = do_gadget_setup(pcd, &pcd->cfi->ctrl_req);
+						retval =
+						    cfi_gadget_setup(pcd,
+								     &pcd->cfi->
+								     ctrl_req);
+
+						/* Return from the function if the gadget failed to process
+						 * the request properly - this should never happen !!!
+						 */
+						if (retval < 0) {
+							CFI_INFO
+							    ("ERROR setting a new value in the gadget(%d)\n",
+							     retval);
+							pcd->ep0_pending = 0;
+							return 0;
+						}
+					}
+
+					CFI_INFO("%s: RETVAL=%d\n", __func__,
+						 retval);
+					/* If we hit here then the PCD and the gadget has properly
+					 * handled the request - so send the ZLP IN to the host.
+					 */
+					/* @todo: MAS - decide whether we need to start the setup
+					 * stage based on the need_setup value of the cfi object
+					 */
+					do_setup_in_status_phase(pcd);
+					pcd->ep0_pending = 0;
+					return 1;
+				}
+			}
+#endif
+
+			do_setup_in_status_phase(pcd);
+		}
+		pcd->ep0_pending = 0;
+		return 1;
+	}
+
+	if (DWC_CIRCLEQ_EMPTY(&ep->queue)) {
+		return 0;
+	}
+	req = DWC_CIRCLEQ_FIRST(&ep->queue);
+
+	if (pcd->ep0state == EP0_OUT_STATUS_PHASE
+	    || pcd->ep0state == EP0_IN_STATUS_PHASE) {
+		is_last = 1;
+	} else if (ep->dwc_ep.is_in) {
+		deptsiz.d32 = DWC_READ_REG32(&in_ep_regs->dieptsiz);
+		if (core_if->dma_desc_enable != 0)
+			desc_sts = dev_if->in_desc_addr->status;
+#ifdef DEBUG_EP0
+		DWC_DEBUGPL(DBG_PCDV, "%d len=%d  xfersize=%d pktcnt=%d\n",
+			    ep->dwc_ep.num, ep->dwc_ep.xfer_len,
+			    deptsiz.b.xfersize, deptsiz.b.pktcnt);
+#endif
+
+		if (((core_if->dma_desc_enable == 0)
+		     && (deptsiz.b.xfersize == 0))
+		    || ((core_if->dma_desc_enable != 0)
+			&& (desc_sts.b.bytes == 0))) {
+			req->actual = ep->dwc_ep.xfer_count;
+			/* Is a Zero Len Packet needed? */
+			if (req->sent_zlp) {
+#ifdef DEBUG_EP0
+				DWC_DEBUGPL(DBG_PCD, "Setup Rx ZLP\n");
+#endif
+				req->sent_zlp = 0;
+			}
+			do_setup_out_status_phase(pcd);
+		}
+	} else {
+		/* ep0-OUT */
+#ifdef DEBUG_EP0
+		deptsiz.d32 = DWC_READ_REG32(&out_ep_regs->doeptsiz);
+		DWC_DEBUGPL(DBG_PCDV, "%d len=%d xsize=%d pktcnt=%d\n",
+			    ep->dwc_ep.num, ep->dwc_ep.xfer_len,
+			    deptsiz.b.xfersize, deptsiz.b.pktcnt);
+#endif
+		req->actual = ep->dwc_ep.xfer_count;
+
+		/* Is a Zero Len Packet needed? */
+		if (req->sent_zlp) {
+#ifdef DEBUG_EP0
+			DWC_DEBUGPL(DBG_PCDV, "Setup Tx ZLP\n");
+#endif
+			req->sent_zlp = 0;
+		}
+		if (core_if->dma_desc_enable == 0)
+			do_setup_in_status_phase(pcd);
+	}
+
+	/* Complete the request */
+	if (is_last) {
+		dwc_otg_request_done(ep, req, 0);
+		ep->dwc_ep.start_xfer_buff = 0;
+		ep->dwc_ep.xfer_buff = 0;
+		ep->dwc_ep.xfer_len = 0;
+		return 1;
+	}
+	return 0;
+}
+
+#ifdef DWC_UTE_CFI
+/**
+ * This function calculates traverses all the CFI DMA descriptors and
+ * and accumulates the bytes that are left to be transfered.
+ *
+ * @return The total bytes left to transfered, or a negative value as failure
+ */
+static inline int cfi_calc_desc_residue(dwc_otg_pcd_ep_t * ep)
+{
+	int32_t ret = 0;
+	int i;
+	struct dwc_otg_dma_desc *ddesc = NULL;
+	struct cfi_ep *cfiep;
+
+	/* See if the pcd_ep has its respective cfi_ep mapped */
+	cfiep = get_cfi_ep_by_pcd_ep(ep->pcd->cfi, ep);
+	if (!cfiep) {
+		CFI_INFO("%s: Failed to find ep\n", __func__);
+		return -1;
+	}
+
+	ddesc = ep->dwc_ep.descs;
+
+	for (i = 0; (i < cfiep->desc_count) && (i < MAX_DMA_DESCS_PER_EP); i++) {
+
+#if defined(PRINT_CFI_DMA_DESCS)
+		print_desc(ddesc, ep->ep.name, i);
+#endif
+		ret += ddesc->status.b.bytes;
+		ddesc++;
+	}
+
+	if (ret)
+		CFI_INFO("!!!!!!!!!! WARNING (%s) - residue=%d\n", __func__,
+			 ret);
+
+	return ret;
+}
+#endif
+
+/**
+ * This function completes the request for the EP. If there are
+ * additional requests for the EP in the queue they will be started.
+ */
+static void complete_ep(dwc_otg_pcd_ep_t * ep)
+{
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(ep->pcd);
+	dwc_otg_dev_if_t *dev_if = core_if->dev_if;
+	dwc_otg_dev_in_ep_regs_t *in_ep_regs =
+	    dev_if->in_ep_regs[ep->dwc_ep.num];
+	deptsiz_data_t deptsiz;
+	dev_dma_desc_sts_t desc_sts;
+	dwc_otg_pcd_request_t *req = 0;
+	dwc_otg_dev_dma_desc_t *dma_desc;
+	uint32_t byte_count = 0;
+	int is_last = 0;
+	int i;
+
+	DWC_DEBUGPL(DBG_PCDV, "%s() %d-%s\n", __func__, ep->dwc_ep.num,
+		    (ep->dwc_ep.is_in ? "IN" : "OUT"));
+
+	/* Get any pending requests */
+	if (!DWC_CIRCLEQ_EMPTY(&ep->queue)) {
+		req = DWC_CIRCLEQ_FIRST(&ep->queue);
+		if (!req) {
+			DWC_PRINTF("complete_ep 0x%p, req = NULL!\n", ep);
+			return;
+		}
+	} else {
+		DWC_PRINTF("complete_ep 0x%p, ep->queue empty!\n", ep);
+		return;
+	}
+
+	DWC_DEBUGPL(DBG_PCD, "Requests %d\n", ep->pcd->request_pending);
+
+	if (ep->dwc_ep.is_in) {
+		deptsiz.d32 = DWC_READ_REG32(&in_ep_regs->dieptsiz);
+
+		if (core_if->dma_enable) {
+			if (core_if->dma_desc_enable == 0) {
+				if (deptsiz.b.xfersize == 0
+				    && deptsiz.b.pktcnt == 0) {
+					byte_count =
+					    ep->dwc_ep.xfer_len -
+					    ep->dwc_ep.xfer_count;
+
+					ep->dwc_ep.xfer_buff += byte_count;
+					ep->dwc_ep.dma_addr += byte_count;
+					ep->dwc_ep.xfer_count += byte_count;
+
+					DWC_DEBUGPL(DBG_PCDV,
+						    "%d-%s len=%d  xfersize=%d pktcnt=%d\n",
+						    ep->dwc_ep.num,
+						    (ep->dwc_ep.
+						     is_in ? "IN" : "OUT"),
+						    ep->dwc_ep.xfer_len,
+						    deptsiz.b.xfersize,
+						    deptsiz.b.pktcnt);
+
+					if (ep->dwc_ep.xfer_len <
+					    ep->dwc_ep.total_len) {
+						dwc_otg_ep_start_transfer
+						    (core_if, &ep->dwc_ep);
+					} else if (ep->dwc_ep.sent_zlp) {
+						/*
+						 * This fragment of code should initiate 0
+						 * length transfer in case if it is queued
+						 * a transfer with size divisible to EPs max
+						 * packet size and with usb_request zero field
+						 * is set, which means that after data is transfered,
+						 * it is also should be transfered
+						 * a 0 length packet at the end. For Slave and
+						 * Buffer DMA modes in this case SW has
+						 * to initiate 2 transfers one with transfer size,
+						 * and the second with 0 size. For Descriptor
+						 * DMA mode SW is able to initiate a transfer,
+						 * which will handle all the packets including
+						 * the last  0 length.
+						 */
+						ep->dwc_ep.sent_zlp = 0;
+						dwc_otg_ep_start_zl_transfer
+						    (core_if, &ep->dwc_ep);
+					} else {
+						is_last = 1;
+					}
+				} else {
+					if(ep->dwc_ep.type == DWC_OTG_EP_TYPE_ISOC)
+					{
+						req->actual = 0;
+						dwc_otg_request_done(ep, req, 0);
+
+						ep->dwc_ep.start_xfer_buff = 0;
+						ep->dwc_ep.xfer_buff = 0;
+						ep->dwc_ep.xfer_len = 0;
+
+						/* If there is a request in the queue start it. */
+						start_next_request(ep);
+					} else
+						DWC_WARN
+						("Incomplete transfer (%d - %s [siz=%d pkt=%d])\n",
+						ep->dwc_ep.num,
+						(ep->dwc_ep.is_in ? "IN" : "OUT"),
+						deptsiz.b.xfersize,
+						deptsiz.b.pktcnt);
+				}
+			} else {
+				dma_desc = ep->dwc_ep.desc_addr;
+				byte_count = 0;
+				ep->dwc_ep.sent_zlp = 0;
+
+#ifdef DWC_UTE_CFI
+				CFI_INFO("%s: BUFFER_MODE=%d\n", __func__,
+					 ep->dwc_ep.buff_mode);
+				if (ep->dwc_ep.buff_mode != BM_STANDARD) {
+					int residue;
+
+					residue = cfi_calc_desc_residue(ep);
+					if (residue < 0)
+						return;
+
+					byte_count = residue;
+				} else {
+#endif
+					for (i = 0; i < ep->dwc_ep.desc_cnt;
+					     ++i) {
+					desc_sts = dma_desc->status;
+					byte_count += desc_sts.b.bytes;
+					dma_desc++;
+				}
+#ifdef DWC_UTE_CFI
+				}
+#endif
+				if (byte_count == 0) {
+					ep->dwc_ep.xfer_count =
+					    ep->dwc_ep.total_len;
+					is_last = 1;
+				} else {
+					DWC_WARN("Incomplete transfer\n");
+				}
+			}
+		} else {
+			if (deptsiz.b.xfersize == 0 && deptsiz.b.pktcnt == 0) {
+				DWC_DEBUGPL(DBG_PCDV,
+					    "%d-%s len=%d  xfersize=%d pktcnt=%d\n",
+					    ep->dwc_ep.num,
+					    ep->dwc_ep.is_in ? "IN" : "OUT",
+					    ep->dwc_ep.xfer_len,
+					    deptsiz.b.xfersize,
+					    deptsiz.b.pktcnt);
+
+				/*      Check if the whole transfer was completed,
+				 *      if no, setup transfer for next portion of data
+				 */
+				if (ep->dwc_ep.xfer_len < ep->dwc_ep.total_len) {
+					dwc_otg_ep_start_transfer(core_if,
+								  &ep->dwc_ep);
+				} else if (ep->dwc_ep.sent_zlp) {
+					/*
+					 * This fragment of code should initiate 0
+					 * length trasfer in case if it is queued
+					 * a trasfer with size divisible to EPs max
+					 * packet size and with usb_request zero field
+					 * is set, which means that after data is transfered,
+					 * it is also should be transfered
+					 * a 0 length packet at the end. For Slave and
+					 * Buffer DMA modes in this case SW has
+					 * to initiate 2 transfers one with transfer size,
+					 * and the second with 0 size. For Desriptor
+					 * DMA mode SW is able to initiate a transfer,
+					 * which will handle all the packets including
+					 * the last  0 legth.
+					 */
+					ep->dwc_ep.sent_zlp = 0;
+					dwc_otg_ep_start_zl_transfer(core_if,
+								     &ep->dwc_ep);
+				} else {
+					is_last = 1;
+				}
+			} else {
+				DWC_WARN
+				    ("Incomplete transfer (%d-%s [siz=%d pkt=%d])\n",
+				     ep->dwc_ep.num,
+				     (ep->dwc_ep.is_in ? "IN" : "OUT"),
+				     deptsiz.b.xfersize, deptsiz.b.pktcnt);
+			}
+		}
+	} else {
+		dwc_otg_dev_out_ep_regs_t *out_ep_regs =
+		    dev_if->out_ep_regs[ep->dwc_ep.num];
+		desc_sts.d32 = 0;
+		if (core_if->dma_enable) {
+			if (core_if->dma_desc_enable) {
+				dma_desc = ep->dwc_ep.desc_addr;
+				byte_count = 0;
+				ep->dwc_ep.sent_zlp = 0;
+
+#ifdef DWC_UTE_CFI
+				CFI_INFO("%s: BUFFER_MODE=%d\n", __func__,
+					 ep->dwc_ep.buff_mode);
+				if (ep->dwc_ep.buff_mode != BM_STANDARD) {
+					int residue;
+					residue = cfi_calc_desc_residue(ep);
+					if (residue < 0)
+						return;
+					byte_count = residue;
+				} else {
+#endif
+
+					for (i = 0; i < ep->dwc_ep.desc_cnt;
+					     ++i) {
+						desc_sts = dma_desc->status;
+						byte_count += desc_sts.b.bytes;
+						dma_desc++;
+					}
+
+#ifdef DWC_UTE_CFI
+				}
+#endif
+				/* Checking for interrupt Out transfers with not
+				 * dword aligned mps sizes
+				 */
+				if (ep->dwc_ep.type == DWC_OTG_EP_TYPE_INTR &&
+							(ep->dwc_ep.maxpacket%4)) {
+					ep->dwc_ep.xfer_count = ep->dwc_ep.total_len - byte_count;
+					if ((ep->dwc_ep.xfer_len % ep->dwc_ep.maxpacket) &&
+						(ep->dwc_ep.xfer_len/ep->dwc_ep.maxpacket < MAX_DMA_DESC_CNT))
+						ep->dwc_ep.xfer_len -=
+							(ep->dwc_ep.desc_cnt - 1) * ep->dwc_ep.maxpacket +
+									ep->dwc_ep.xfer_len % ep->dwc_ep.maxpacket;
+					else
+						ep->dwc_ep.xfer_len -=
+									ep->dwc_ep.desc_cnt * ep->dwc_ep.maxpacket;
+					if (ep->dwc_ep.xfer_len > 0) {
+                                        	dwc_otg_ep_start_transfer(core_if,
+                                                                  &ep->dwc_ep);
+					} else {
+						is_last = 1;
+					}
+				} else {
+					ep->dwc_ep.xfer_count = ep->dwc_ep.total_len
+						- byte_count +
+						((4 - (ep->dwc_ep.total_len & 0x3)) & 0x3);
+					is_last = 1;
+				}
+			} else {
+				deptsiz.d32 = 0;
+				deptsiz.d32 =
+				    DWC_READ_REG32(&out_ep_regs->doeptsiz);
+
+				byte_count = (ep->dwc_ep.xfer_len -
+					      ep->dwc_ep.xfer_count -
+					      deptsiz.b.xfersize);
+				ep->dwc_ep.xfer_buff += byte_count;
+				ep->dwc_ep.dma_addr += byte_count;
+				ep->dwc_ep.xfer_count += byte_count;
+
+				/*      Check if the whole transfer was completed,
+				 *      if no, setup transfer for next portion of data
+				 */
+				if (ep->dwc_ep.xfer_len < ep->dwc_ep.total_len) {
+					dwc_otg_ep_start_transfer(core_if,
+								  &ep->dwc_ep);
+				} else if (ep->dwc_ep.sent_zlp) {
+					/*
+					 * This fragment of code should initiate 0
+					 * length trasfer in case if it is queued
+					 * a trasfer with size divisible to EPs max
+					 * packet size and with usb_request zero field
+					 * is set, which means that after data is transfered,
+					 * it is also should be transfered
+					 * a 0 length packet at the end. For Slave and
+					 * Buffer DMA modes in this case SW has
+					 * to initiate 2 transfers one with transfer size,
+					 * and the second with 0 size. For Desriptor
+					 * DMA mode SW is able to initiate a transfer,
+					 * which will handle all the packets including
+					 * the last  0 legth.
+					 */
+					ep->dwc_ep.sent_zlp = 0;
+					dwc_otg_ep_start_zl_transfer(core_if,
+								     &ep->dwc_ep);
+				} else {
+					is_last = 1;
+				}
+			}
+		} else {
+			/*      Check if the whole transfer was completed,
+			 *      if no, setup transfer for next portion of data
+			 */
+			if (ep->dwc_ep.xfer_len < ep->dwc_ep.total_len) {
+				dwc_otg_ep_start_transfer(core_if, &ep->dwc_ep);
+			} else if (ep->dwc_ep.sent_zlp) {
+				/*
+				 * This fragment of code should initiate 0
+				 * length transfer in case if it is queued
+				 * a transfer with size divisible to EPs max
+				 * packet size and with usb_request zero field
+				 * is set, which means that after data is transfered,
+				 * it is also should be transfered
+				 * a 0 length packet at the end. For Slave and
+				 * Buffer DMA modes in this case SW has
+				 * to initiate 2 transfers one with transfer size,
+				 * and the second with 0 size. For Descriptor
+				 * DMA mode SW is able to initiate a transfer,
+				 * which will handle all the packets including
+				 * the last  0 length.
+				 */
+				ep->dwc_ep.sent_zlp = 0;
+				dwc_otg_ep_start_zl_transfer(core_if,
+							     &ep->dwc_ep);
+			} else {
+				is_last = 1;
+			}
+		}
+
+		DWC_DEBUGPL(DBG_PCDV,
+			    "addr %p,	 %d-%s len=%d cnt=%d xsize=%d pktcnt=%d\n",
+			    &out_ep_regs->doeptsiz, ep->dwc_ep.num,
+			    ep->dwc_ep.is_in ? "IN" : "OUT",
+			    ep->dwc_ep.xfer_len, ep->dwc_ep.xfer_count,
+			    deptsiz.b.xfersize, deptsiz.b.pktcnt);
+	}
+
+	/* Complete the request */
+	if (is_last) {
+#ifdef DWC_UTE_CFI
+		if (ep->dwc_ep.buff_mode != BM_STANDARD) {
+			req->actual = ep->dwc_ep.cfi_req_len - byte_count;
+		} else {
+#endif
+			req->actual = ep->dwc_ep.xfer_count;
+#ifdef DWC_UTE_CFI
+		}
+#endif
+		if (req->dw_align_buf) {
+			if (!ep->dwc_ep.is_in) {
+				dwc_memcpy(req->buf, req->dw_align_buf, req->length);
+			}
+			DWC_DMA_FREE(req->length, req->dw_align_buf,
+				     req->dw_align_buf_dma);
+		}
+
+		dwc_otg_request_done(ep, req, 0);
+
+		ep->dwc_ep.start_xfer_buff = 0;
+		ep->dwc_ep.xfer_buff = 0;
+		ep->dwc_ep.xfer_len = 0;
+
+		/* If there is a request in the queue start it. */
+		start_next_request(ep);
+	}
+}
+
+#ifdef DWC_EN_ISOC
+
+/**
+ * This function BNA interrupt for Isochronous EPs
+ *
+ */
+static void dwc_otg_pcd_handle_iso_bna(dwc_otg_pcd_ep_t * ep)
+{
+	dwc_ep_t *dwc_ep = &ep->dwc_ep;
+	volatile uint32_t *addr;
+	depctl_data_t depctl = {.d32 = 0 };
+	dwc_otg_pcd_t *pcd = ep->pcd;
+	dwc_otg_dev_dma_desc_t *dma_desc;
+	int i;
+
+	dma_desc =
+	    dwc_ep->iso_desc_addr + dwc_ep->desc_cnt * (dwc_ep->proc_buf_num);
+
+	if (dwc_ep->is_in) {
+		dev_dma_desc_sts_t sts = {.d32 = 0 };
+		for (i = 0; i < dwc_ep->desc_cnt; ++i, ++dma_desc) {
+			sts.d32 = dma_desc->status.d32;
+			sts.b_iso_in.bs = BS_HOST_READY;
+			dma_desc->status.d32 = sts.d32;
+		}
+	} else {
+		dev_dma_desc_sts_t sts = {.d32 = 0 };
+		for (i = 0; i < dwc_ep->desc_cnt; ++i, ++dma_desc) {
+			sts.d32 = dma_desc->status.d32;
+			sts.b_iso_out.bs = BS_HOST_READY;
+			dma_desc->status.d32 = sts.d32;
+		}
+	}
+
+	if (dwc_ep->is_in == 0) {
+		addr =
+		    &GET_CORE_IF(pcd)->dev_if->out_ep_regs[dwc_ep->
+							   num]->doepctl;
+	} else {
+		addr =
+		    &GET_CORE_IF(pcd)->dev_if->in_ep_regs[dwc_ep->num]->diepctl;
+	}
+	depctl.b.epena = 1;
+	DWC_MODIFY_REG32(addr, depctl.d32, depctl.d32);
+}
+
+/**
+ * This function sets latest iso packet information(non-PTI mode)
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param ep The EP to start the transfer on.
+ *
+ */
+void set_current_pkt_info(dwc_otg_core_if_t * core_if, dwc_ep_t * ep)
+{
+	deptsiz_data_t deptsiz = {.d32 = 0 };
+	dma_addr_t dma_addr;
+	uint32_t offset;
+
+	if (ep->proc_buf_num)
+		dma_addr = ep->dma_addr1;
+	else
+		dma_addr = ep->dma_addr0;
+
+	if (ep->is_in) {
+		deptsiz.d32 =
+		    DWC_READ_REG32(&core_if->dev_if->
+				   in_ep_regs[ep->num]->dieptsiz);
+		offset = ep->data_per_frame;
+	} else {
+		deptsiz.d32 =
+		    DWC_READ_REG32(&core_if->dev_if->
+				   out_ep_regs[ep->num]->doeptsiz);
+		offset =
+		    ep->data_per_frame +
+		    (0x4 & (0x4 - (ep->data_per_frame & 0x3)));
+	}
+
+	if (!deptsiz.b.xfersize) {
+		ep->pkt_info[ep->cur_pkt].length = ep->data_per_frame;
+		ep->pkt_info[ep->cur_pkt].offset =
+		    ep->cur_pkt_dma_addr - dma_addr;
+		ep->pkt_info[ep->cur_pkt].status = 0;
+	} else {
+		ep->pkt_info[ep->cur_pkt].length = ep->data_per_frame;
+		ep->pkt_info[ep->cur_pkt].offset =
+		    ep->cur_pkt_dma_addr - dma_addr;
+		ep->pkt_info[ep->cur_pkt].status = -DWC_E_NO_DATA;
+	}
+	ep->cur_pkt_addr += offset;
+	ep->cur_pkt_dma_addr += offset;
+	ep->cur_pkt++;
+}
+
+/**
+ * This function sets latest iso packet information(DDMA mode)
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param dwc_ep The EP to start the transfer on.
+ *
+ */
+static void set_ddma_iso_pkts_info(dwc_otg_core_if_t * core_if,
+				   dwc_ep_t * dwc_ep)
+{
+	dwc_otg_dev_dma_desc_t *dma_desc;
+	dev_dma_desc_sts_t sts = {.d32 = 0 };
+	iso_pkt_info_t *iso_packet;
+	uint32_t data_per_desc;
+	uint32_t offset;
+	int i, j;
+
+	iso_packet = dwc_ep->pkt_info;
+
+	/** Reinit closed DMA Descriptors*/
+	/** ISO OUT EP */
+	if (dwc_ep->is_in == 0) {
+		dma_desc =
+		    dwc_ep->iso_desc_addr +
+		    dwc_ep->desc_cnt * dwc_ep->proc_buf_num;
+		offset = 0;
+
+		for (i = 0; i < dwc_ep->desc_cnt - dwc_ep->pkt_per_frm;
+		     i += dwc_ep->pkt_per_frm) {
+			for (j = 0; j < dwc_ep->pkt_per_frm; ++j) {
+				data_per_desc =
+				    ((j + 1) * dwc_ep->maxpacket >
+				     dwc_ep->
+				     data_per_frame) ? dwc_ep->data_per_frame -
+				    j * dwc_ep->maxpacket : dwc_ep->maxpacket;
+				data_per_desc +=
+				    (data_per_desc % 4) ? (4 -
+							   data_per_desc %
+							   4) : 0;
+
+				sts.d32 = dma_desc->status.d32;
+
+				/* Write status in iso_packet_decsriptor  */
+				iso_packet->status =
+				    sts.b_iso_out.rxsts +
+				    (sts.b_iso_out.bs ^ BS_DMA_DONE);
+				if (iso_packet->status) {
+					iso_packet->status = -DWC_E_NO_DATA;
+				}
+
+				/* Received data length */
+				if (!sts.b_iso_out.rxbytes) {
+					iso_packet->length =
+					    data_per_desc -
+					    sts.b_iso_out.rxbytes;
+				} else {
+					iso_packet->length =
+					    data_per_desc -
+					    sts.b_iso_out.rxbytes + (4 -
+								     dwc_ep->data_per_frame
+								     % 4);
+				}
+
+				iso_packet->offset = offset;
+
+				offset += data_per_desc;
+				dma_desc++;
+				iso_packet++;
+			}
+		}
+
+		for (j = 0; j < dwc_ep->pkt_per_frm - 1; ++j) {
+			data_per_desc =
+			    ((j + 1) * dwc_ep->maxpacket >
+			     dwc_ep->data_per_frame) ? dwc_ep->data_per_frame -
+			    j * dwc_ep->maxpacket : dwc_ep->maxpacket;
+			data_per_desc +=
+			    (data_per_desc % 4) ? (4 - data_per_desc % 4) : 0;
+
+			sts.d32 = dma_desc->status.d32;
+
+			/* Write status in iso_packet_decsriptor  */
+			iso_packet->status =
+			    sts.b_iso_out.rxsts +
+			    (sts.b_iso_out.bs ^ BS_DMA_DONE);
+			if (iso_packet->status) {
+				iso_packet->status = -DWC_E_NO_DATA;
+			}
+
+			/* Received data length */
+			iso_packet->length =
+			    dwc_ep->data_per_frame - sts.b_iso_out.rxbytes;
+
+			iso_packet->offset = offset;
+
+			offset += data_per_desc;
+			iso_packet++;
+			dma_desc++;
+		}
+
+		sts.d32 = dma_desc->status.d32;
+
+		/* Write status in iso_packet_decsriptor  */
+		iso_packet->status =
+		    sts.b_iso_out.rxsts + (sts.b_iso_out.bs ^ BS_DMA_DONE);
+		if (iso_packet->status) {
+			iso_packet->status = -DWC_E_NO_DATA;
+		}
+		/* Received data length */
+		if (!sts.b_iso_out.rxbytes) {
+			iso_packet->length =
+			    dwc_ep->data_per_frame - sts.b_iso_out.rxbytes;
+		} else {
+			iso_packet->length =
+			    dwc_ep->data_per_frame - sts.b_iso_out.rxbytes +
+			    (4 - dwc_ep->data_per_frame % 4);
+		}
+
+		iso_packet->offset = offset;
+	} else {
+/** ISO IN EP */
+
+		dma_desc =
+		    dwc_ep->iso_desc_addr +
+		    dwc_ep->desc_cnt * dwc_ep->proc_buf_num;
+
+		for (i = 0; i < dwc_ep->desc_cnt - 1; i++) {
+			sts.d32 = dma_desc->status.d32;
+
+			/* Write status in iso packet descriptor */
+			iso_packet->status =
+			    sts.b_iso_in.txsts +
+			    (sts.b_iso_in.bs ^ BS_DMA_DONE);
+			if (iso_packet->status != 0) {
+				iso_packet->status = -DWC_E_NO_DATA;
+
+			}
+			/* Bytes has been transfered */
+			iso_packet->length =
+			    dwc_ep->data_per_frame - sts.b_iso_in.txbytes;
+
+			dma_desc++;
+			iso_packet++;
+		}
+
+		sts.d32 = dma_desc->status.d32;
+		while (sts.b_iso_in.bs == BS_DMA_BUSY) {
+			sts.d32 = dma_desc->status.d32;
+		}
+
+		/* Write status in iso packet descriptor ??? do be done with ERROR codes */
+		iso_packet->status =
+		    sts.b_iso_in.txsts + (sts.b_iso_in.bs ^ BS_DMA_DONE);
+		if (iso_packet->status != 0) {
+			iso_packet->status = -DWC_E_NO_DATA;
+		}
+
+		/* Bytes has been transfered */
+		iso_packet->length =
+		    dwc_ep->data_per_frame - sts.b_iso_in.txbytes;
+	}
+}
+
+/**
+ * This function reinitialize DMA Descriptors for Isochronous transfer
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param dwc_ep The EP to start the transfer on.
+ *
+ */
+static void reinit_ddma_iso_xfer(dwc_otg_core_if_t * core_if, dwc_ep_t * dwc_ep)
+{
+	int i, j;
+	dwc_otg_dev_dma_desc_t *dma_desc;
+	dma_addr_t dma_ad;
+	volatile uint32_t *addr;
+	dev_dma_desc_sts_t sts = {.d32 = 0 };
+	uint32_t data_per_desc;
+
+	if (dwc_ep->is_in == 0) {
+		addr = &core_if->dev_if->out_ep_regs[dwc_ep->num]->doepctl;
+	} else {
+		addr = &core_if->dev_if->in_ep_regs[dwc_ep->num]->diepctl;
+	}
+
+	if (dwc_ep->proc_buf_num == 0) {
+		/** Buffer 0 descriptors setup */
+		dma_ad = dwc_ep->dma_addr0;
+	} else {
+		/** Buffer 1 descriptors setup */
+		dma_ad = dwc_ep->dma_addr1;
+	}
+
+	/** Reinit closed DMA Descriptors*/
+	/** ISO OUT EP */
+	if (dwc_ep->is_in == 0) {
+		dma_desc =
+		    dwc_ep->iso_desc_addr +
+		    dwc_ep->desc_cnt * dwc_ep->proc_buf_num;
+
+		sts.b_iso_out.bs = BS_HOST_READY;
+		sts.b_iso_out.rxsts = 0;
+		sts.b_iso_out.l = 0;
+		sts.b_iso_out.sp = 0;
+		sts.b_iso_out.ioc = 0;
+		sts.b_iso_out.pid = 0;
+		sts.b_iso_out.framenum = 0;
+
+		for (i = 0; i < dwc_ep->desc_cnt - dwc_ep->pkt_per_frm;
+		     i += dwc_ep->pkt_per_frm) {
+			for (j = 0; j < dwc_ep->pkt_per_frm; ++j) {
+				data_per_desc =
+				    ((j + 1) * dwc_ep->maxpacket >
+				     dwc_ep->
+				     data_per_frame) ? dwc_ep->data_per_frame -
+				    j * dwc_ep->maxpacket : dwc_ep->maxpacket;
+				data_per_desc +=
+				    (data_per_desc % 4) ? (4 -
+							   data_per_desc %
+							   4) : 0;
+				sts.b_iso_out.rxbytes = data_per_desc;
+				dma_desc->buf = dma_ad;
+				dma_desc->status.d32 = sts.d32;
+
+				dma_ad += data_per_desc;
+				dma_desc++;
+			}
+		}
+
+		for (j = 0; j < dwc_ep->pkt_per_frm - 1; ++j) {
+
+			data_per_desc =
+			    ((j + 1) * dwc_ep->maxpacket >
+			     dwc_ep->data_per_frame) ? dwc_ep->data_per_frame -
+			    j * dwc_ep->maxpacket : dwc_ep->maxpacket;
+			data_per_desc +=
+			    (data_per_desc % 4) ? (4 - data_per_desc % 4) : 0;
+			sts.b_iso_out.rxbytes = data_per_desc;
+
+			dma_desc->buf = dma_ad;
+			dma_desc->status.d32 = sts.d32;
+
+			dma_desc++;
+			dma_ad += data_per_desc;
+		}
+
+		sts.b_iso_out.ioc = 1;
+		sts.b_iso_out.l = dwc_ep->proc_buf_num;
+
+		data_per_desc =
+		    ((j + 1) * dwc_ep->maxpacket >
+		     dwc_ep->data_per_frame) ? dwc_ep->data_per_frame -
+		    j * dwc_ep->maxpacket : dwc_ep->maxpacket;
+		data_per_desc +=
+		    (data_per_desc % 4) ? (4 - data_per_desc % 4) : 0;
+		sts.b_iso_out.rxbytes = data_per_desc;
+
+		dma_desc->buf = dma_ad;
+		dma_desc->status.d32 = sts.d32;
+	} else {
+/** ISO IN EP */
+
+		dma_desc =
+		    dwc_ep->iso_desc_addr +
+		    dwc_ep->desc_cnt * dwc_ep->proc_buf_num;
+
+		sts.b_iso_in.bs = BS_HOST_READY;
+		sts.b_iso_in.txsts = 0;
+		sts.b_iso_in.sp = 0;
+		sts.b_iso_in.ioc = 0;
+		sts.b_iso_in.pid = dwc_ep->pkt_per_frm;
+		sts.b_iso_in.framenum = dwc_ep->next_frame;
+		sts.b_iso_in.txbytes = dwc_ep->data_per_frame;
+		sts.b_iso_in.l = 0;
+
+		for (i = 0; i < dwc_ep->desc_cnt - 1; i++) {
+			dma_desc->buf = dma_ad;
+			dma_desc->status.d32 = sts.d32;
+
+			sts.b_iso_in.framenum += dwc_ep->bInterval;
+			dma_ad += dwc_ep->data_per_frame;
+			dma_desc++;
+		}
+
+		sts.b_iso_in.ioc = 1;
+		sts.b_iso_in.l = dwc_ep->proc_buf_num;
+
+		dma_desc->buf = dma_ad;
+		dma_desc->status.d32 = sts.d32;
+
+		dwc_ep->next_frame =
+		    sts.b_iso_in.framenum + dwc_ep->bInterval * 1;
+	}
+	dwc_ep->proc_buf_num = (dwc_ep->proc_buf_num ^ 1) & 0x1;
+}
+
+/**
+ * This function is to handle Iso EP transfer complete interrupt
+ * in case Iso out packet was dropped
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param dwc_ep The EP for wihich transfer complete was asserted
+ *
+ */
+static uint32_t handle_iso_out_pkt_dropped(dwc_otg_core_if_t * core_if,
+					   dwc_ep_t * dwc_ep)
+{
+	uint32_t dma_addr;
+	uint32_t drp_pkt;
+	uint32_t drp_pkt_cnt;
+	deptsiz_data_t deptsiz = {.d32 = 0 };
+	depctl_data_t depctl = {.d32 = 0 };
+	int i;
+
+	deptsiz.d32 =
+	    DWC_READ_REG32(&core_if->dev_if->
+			   out_ep_regs[dwc_ep->num]->doeptsiz);
+
+	drp_pkt = dwc_ep->pkt_cnt - deptsiz.b.pktcnt;
+	drp_pkt_cnt = dwc_ep->pkt_per_frm - (drp_pkt % dwc_ep->pkt_per_frm);
+
+	/* Setting dropped packets status */
+	for (i = 0; i < drp_pkt_cnt; ++i) {
+		dwc_ep->pkt_info[drp_pkt].status = -DWC_E_NO_DATA;
+		drp_pkt++;
+		deptsiz.b.pktcnt--;
+	}
+
+	if (deptsiz.b.pktcnt > 0) {
+		deptsiz.b.xfersize =
+		    dwc_ep->xfer_len - (dwc_ep->pkt_cnt -
+					deptsiz.b.pktcnt) * dwc_ep->maxpacket;
+	} else {
+		deptsiz.b.xfersize = 0;
+		deptsiz.b.pktcnt = 0;
+	}
+
+	DWC_WRITE_REG32(&core_if->dev_if->out_ep_regs[dwc_ep->num]->doeptsiz,
+			deptsiz.d32);
+
+	if (deptsiz.b.pktcnt > 0) {
+		if (dwc_ep->proc_buf_num) {
+			dma_addr =
+			    dwc_ep->dma_addr1 + dwc_ep->xfer_len -
+			    deptsiz.b.xfersize;
+		} else {
+			dma_addr =
+			    dwc_ep->dma_addr0 + dwc_ep->xfer_len -
+			    deptsiz.b.xfersize;;
+		}
+
+		DWC_WRITE_REG32(&core_if->dev_if->
+				out_ep_regs[dwc_ep->num]->doepdma, dma_addr);
+
+		/** Re-enable endpoint, clear nak  */
+		depctl.d32 = 0;
+		depctl.b.epena = 1;
+		depctl.b.cnak = 1;
+
+		DWC_MODIFY_REG32(&core_if->dev_if->
+				 out_ep_regs[dwc_ep->num]->doepctl, depctl.d32,
+				 depctl.d32);
+		return 0;
+	} else {
+		return 1;
+	}
+}
+
+/**
+ * This function sets iso packets information(PTI mode)
+ *
+ * @param core_if Programming view of DWC_otg controller.
+ * @param ep The EP to start the transfer on.
+ *
+ */
+static uint32_t set_iso_pkts_info(dwc_otg_core_if_t * core_if, dwc_ep_t * ep)
+{
+	int i, j;
+	dma_addr_t dma_ad;
+	iso_pkt_info_t *packet_info = ep->pkt_info;
+	uint32_t offset;
+	uint32_t frame_data;
+	deptsiz_data_t deptsiz;
+
+	if (ep->proc_buf_num == 0) {
+		/** Buffer 0 descriptors setup */
+		dma_ad = ep->dma_addr0;
+	} else {
+		/** Buffer 1 descriptors setup */
+		dma_ad = ep->dma_addr1;
+	}
+
+	if (ep->is_in) {
+		deptsiz.d32 =
+		    DWC_READ_REG32(&core_if->dev_if->
+				   in_ep_regs[ep->num]->dieptsiz);
+	} else {
+		deptsiz.d32 =
+		    DWC_READ_REG32(&core_if->dev_if->
+				   out_ep_regs[ep->num]->doeptsiz);
+	}
+
+	if (!deptsiz.b.xfersize) {
+		offset = 0;
+		for (i = 0; i < ep->pkt_cnt; i += ep->pkt_per_frm) {
+			frame_data = ep->data_per_frame;
+			for (j = 0; j < ep->pkt_per_frm; ++j) {
+
+				/* Packet status - is not set as initially
+				 * it is set to 0 and if packet was sent
+				 successfully, status field will remain 0*/
+
+				/* Bytes has been transfered */
+				packet_info->length =
+				    (ep->maxpacket <
+				     frame_data) ? ep->maxpacket : frame_data;
+
+				/* Received packet offset */
+				packet_info->offset = offset;
+				offset += packet_info->length;
+				frame_data -= packet_info->length;
+
+				packet_info++;
+			}
+		}
+		return 1;
+	} else {
+		/* This is a workaround for in case of Transfer Complete with
+		 * PktDrpSts interrupts merging - in this case Transfer complete
+		 * interrupt for Isoc Out Endpoint is asserted without PktDrpSts
+		 * set and with DOEPTSIZ register non zero. Investigations showed,
+		 * that this happens when Out packet is dropped, but because of
+		 * interrupts merging during first interrupt handling PktDrpSts
+		 * bit is cleared and for next merged interrupts it is not reset.
+		 * In this case SW hadles the interrupt as if PktDrpSts bit is set.
+		 */
+		if (ep->is_in) {
+			return 1;
+		} else {
+			return handle_iso_out_pkt_dropped(core_if, ep);
+		}
+	}
+}
+
+/**
+ * This function is to handle Iso EP transfer complete interrupt
+ *
+ * @param pcd The PCD
+ * @param ep The EP for which transfer complete was asserted
+ *
+ */
+static void complete_iso_ep(dwc_otg_pcd_t * pcd, dwc_otg_pcd_ep_t * ep)
+{
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(ep->pcd);
+	dwc_ep_t *dwc_ep = &ep->dwc_ep;
+	uint8_t is_last = 0;
+
+	if (ep->dwc_ep.next_frame == 0xffffffff) {
+		DWC_WARN("Next frame is not set!\n");
+		return;
+	}
+
+	if (core_if->dma_enable) {
+		if (core_if->dma_desc_enable) {
+			set_ddma_iso_pkts_info(core_if, dwc_ep);
+			reinit_ddma_iso_xfer(core_if, dwc_ep);
+			is_last = 1;
+		} else {
+			if (core_if->pti_enh_enable) {
+				if (set_iso_pkts_info(core_if, dwc_ep)) {
+					dwc_ep->proc_buf_num =
+					    (dwc_ep->proc_buf_num ^ 1) & 0x1;
+					dwc_otg_iso_ep_start_buf_transfer
+					    (core_if, dwc_ep);
+					is_last = 1;
+				}
+			} else {
+				set_current_pkt_info(core_if, dwc_ep);
+				if (dwc_ep->cur_pkt >= dwc_ep->pkt_cnt) {
+					is_last = 1;
+					dwc_ep->cur_pkt = 0;
+					dwc_ep->proc_buf_num =
+					    (dwc_ep->proc_buf_num ^ 1) & 0x1;
+					if (dwc_ep->proc_buf_num) {
+						dwc_ep->cur_pkt_addr =
+						    dwc_ep->xfer_buff1;
+						dwc_ep->cur_pkt_dma_addr =
+						    dwc_ep->dma_addr1;
+					} else {
+						dwc_ep->cur_pkt_addr =
+						    dwc_ep->xfer_buff0;
+						dwc_ep->cur_pkt_dma_addr =
+						    dwc_ep->dma_addr0;
+					}
+
+				}
+				dwc_otg_iso_ep_start_frm_transfer(core_if,
+								  dwc_ep);
+			}
+		}
+	} else {
+		set_current_pkt_info(core_if, dwc_ep);
+		if (dwc_ep->cur_pkt >= dwc_ep->pkt_cnt) {
+			is_last = 1;
+			dwc_ep->cur_pkt = 0;
+			dwc_ep->proc_buf_num = (dwc_ep->proc_buf_num ^ 1) & 0x1;
+			if (dwc_ep->proc_buf_num) {
+				dwc_ep->cur_pkt_addr = dwc_ep->xfer_buff1;
+				dwc_ep->cur_pkt_dma_addr = dwc_ep->dma_addr1;
+			} else {
+				dwc_ep->cur_pkt_addr = dwc_ep->xfer_buff0;
+				dwc_ep->cur_pkt_dma_addr = dwc_ep->dma_addr0;
+			}
+
+		}
+		dwc_otg_iso_ep_start_frm_transfer(core_if, dwc_ep);
+	}
+	if (is_last)
+		dwc_otg_iso_buffer_done(pcd, ep, ep->iso_req_handle);
+}
+#endif /* DWC_EN_ISOC */
+
+/**
+ * This function handle BNA interrupt for Non Isochronous EPs
+ *
+ */
+static void dwc_otg_pcd_handle_noniso_bna(dwc_otg_pcd_ep_t * ep)
+{
+	dwc_ep_t *dwc_ep = &ep->dwc_ep;
+	volatile uint32_t *addr;
+	depctl_data_t depctl = {.d32 = 0 };
+	dwc_otg_pcd_t *pcd = ep->pcd;
+	dwc_otg_dev_dma_desc_t *dma_desc;
+	dev_dma_desc_sts_t sts = {.d32 = 0 };
+	dwc_otg_core_if_t *core_if = ep->pcd->core_if;
+	int i, start;
+
+	if (!dwc_ep->desc_cnt)
+		DWC_WARN("Descriptor count = %d\n", dwc_ep->desc_cnt);
+
+	if (core_if->core_params->cont_on_bna && !dwc_ep->is_in
+							&& dwc_ep->type != DWC_OTG_EP_TYPE_CONTROL) {
+		uint32_t doepdma;
+		dwc_otg_dev_out_ep_regs_t *out_regs =
+			core_if->dev_if->out_ep_regs[dwc_ep->num];
+		doepdma = DWC_READ_REG32(&(out_regs->doepdma));
+		start = (doepdma - dwc_ep->dma_desc_addr)/sizeof(dwc_otg_dev_dma_desc_t);
+		dma_desc = &(dwc_ep->desc_addr[start]);
+	} else {
+		start = 0;
+		dma_desc = dwc_ep->desc_addr;
+	}
+
+
+	for (i = start; i < dwc_ep->desc_cnt; ++i, ++dma_desc) {
+		sts.d32 = dma_desc->status.d32;
+		sts.b.bs = BS_HOST_READY;
+		dma_desc->status.d32 = sts.d32;
+	}
+
+	if (dwc_ep->is_in == 0) {
+		addr =
+		    &GET_CORE_IF(pcd)->dev_if->out_ep_regs[dwc_ep->
+							   num]->doepctl;
+	} else {
+		addr =
+		    &GET_CORE_IF(pcd)->dev_if->in_ep_regs[dwc_ep->num]->diepctl;
+	}
+	depctl.b.epena = 1;
+	depctl.b.cnak = 1;
+	DWC_MODIFY_REG32(addr, 0, depctl.d32);
+}
+
+/**
+ * This function handles EP0 Control transfers.
+ *
+ * The state of the control tranfers are tracked in
+ * <code>ep0state</code>.
+ */
+static void handle_ep0(dwc_otg_pcd_t * pcd)
+{
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(pcd);
+	dwc_otg_pcd_ep_t *ep0 = &pcd->ep0;
+	dev_dma_desc_sts_t desc_sts;
+	deptsiz0_data_t deptsiz;
+	uint32_t byte_count;
+
+#ifdef DEBUG_EP0
+	DWC_DEBUGPL(DBG_PCDV, "%s()\n", __func__);
+	print_ep0_state(pcd);
+#endif
+
+//      DWC_PRINTF("HANDLE EP0\n");
+
+	switch (pcd->ep0state) {
+	case EP0_DISCONNECT:
+		break;
+
+	case EP0_IDLE:
+		pcd->request_config = 0;
+
+		pcd_setup(pcd);
+		break;
+
+	case EP0_IN_DATA_PHASE:
+#ifdef DEBUG_EP0
+		DWC_DEBUGPL(DBG_PCD, "DATA_IN EP%d-%s: type=%d, mps=%d\n",
+			    ep0->dwc_ep.num, (ep0->dwc_ep.is_in ? "IN" : "OUT"),
+			    ep0->dwc_ep.type, ep0->dwc_ep.maxpacket);
+#endif
+
+		if (core_if->dma_enable != 0) {
+			/*
+			 * For EP0 we can only program 1 packet at a time so we
+			 * need to do the make calculations after each complete.
+			 * Call write_packet to make the calculations, as in
+			 * slave mode, and use those values to determine if we
+			 * can complete.
+			 */
+			if (core_if->dma_desc_enable == 0) {
+				deptsiz.d32 =
+				    DWC_READ_REG32(&core_if->
+						   dev_if->in_ep_regs[0]->
+						   dieptsiz);
+				byte_count =
+				    ep0->dwc_ep.xfer_len - deptsiz.b.xfersize;
+			} else {
+				desc_sts =
+				    core_if->dev_if->in_desc_addr->status;
+				byte_count =
+				    ep0->dwc_ep.xfer_len - desc_sts.b.bytes;
+			}
+			ep0->dwc_ep.xfer_count += byte_count;
+			ep0->dwc_ep.xfer_buff += byte_count;
+			ep0->dwc_ep.dma_addr += byte_count;
+		}
+		if (ep0->dwc_ep.xfer_count < ep0->dwc_ep.total_len) {
+			dwc_otg_ep0_continue_transfer(GET_CORE_IF(pcd),
+						      &ep0->dwc_ep);
+			DWC_DEBUGPL(DBG_PCD, "CONTINUE TRANSFER\n");
+		} else if (ep0->dwc_ep.sent_zlp) {
+			dwc_otg_ep0_continue_transfer(GET_CORE_IF(pcd),
+						      &ep0->dwc_ep);
+			ep0->dwc_ep.sent_zlp = 0;
+			DWC_DEBUGPL(DBG_PCD, "CONTINUE TRANSFER\n");
+		} else {
+			ep0_complete_request(ep0);
+			DWC_DEBUGPL(DBG_PCD, "COMPLETE TRANSFER\n");
+		}
+		break;
+	case EP0_OUT_DATA_PHASE:
+#ifdef DEBUG_EP0
+		DWC_DEBUGPL(DBG_PCD, "DATA_OUT EP%d-%s: type=%d, mps=%d\n",
+			    ep0->dwc_ep.num, (ep0->dwc_ep.is_in ? "IN" : "OUT"),
+			    ep0->dwc_ep.type, ep0->dwc_ep.maxpacket);
+#endif
+		if (core_if->dma_enable != 0) {
+			if (core_if->dma_desc_enable == 0) {
+				deptsiz.d32 =
+				    DWC_READ_REG32(&core_if->
+						   dev_if->out_ep_regs[0]->
+						   doeptsiz);
+				byte_count =
+				    ep0->dwc_ep.maxpacket - deptsiz.b.xfersize;
+			} else {
+				desc_sts =
+				    core_if->dev_if->out_desc_addr->status;
+				byte_count =
+				    ep0->dwc_ep.maxpacket - desc_sts.b.bytes;
+			}
+			ep0->dwc_ep.xfer_count += byte_count;
+			ep0->dwc_ep.xfer_buff += byte_count;
+			ep0->dwc_ep.dma_addr += byte_count;
+		}
+		if (ep0->dwc_ep.xfer_count < ep0->dwc_ep.total_len) {
+			dwc_otg_ep0_continue_transfer(GET_CORE_IF(pcd),
+						      &ep0->dwc_ep);
+			DWC_DEBUGPL(DBG_PCD, "CONTINUE TRANSFER\n");
+		} else if (ep0->dwc_ep.sent_zlp) {
+			dwc_otg_ep0_continue_transfer(GET_CORE_IF(pcd),
+						      &ep0->dwc_ep);
+			ep0->dwc_ep.sent_zlp = 0;
+			DWC_DEBUGPL(DBG_PCD, "CONTINUE TRANSFER\n");
+		} else {
+			ep0_complete_request(ep0);
+			DWC_DEBUGPL(DBG_PCD, "COMPLETE TRANSFER\n");
+		}
+		break;
+
+	case EP0_IN_STATUS_PHASE:
+	case EP0_OUT_STATUS_PHASE:
+		DWC_DEBUGPL(DBG_PCD, "CASE: EP0_STATUS\n");
+		ep0_complete_request(ep0);
+		pcd->ep0state = EP0_IDLE;
+		ep0->stopped = 1;
+		ep0->dwc_ep.is_in = 0;	/* OUT for next SETUP */
+
+		/* Prepare for more SETUP Packets */
+		if (core_if->dma_enable) {
+			ep0_out_start(core_if, pcd);
+		}
+		break;
+
+	case EP0_STALL:
+		DWC_ERROR("EP0 STALLed, should not get here pcd_setup()\n");
+		break;
+	}
+#ifdef DEBUG_EP0
+	print_ep0_state(pcd);
+#endif
+}
+
+/**
+ * Restart transfer
+ */
+static void restart_transfer(dwc_otg_pcd_t * pcd, const uint32_t epnum)
+{
+	dwc_otg_core_if_t *core_if;
+	dwc_otg_dev_if_t *dev_if;
+	deptsiz_data_t dieptsiz = {.d32 = 0 };
+	dwc_otg_pcd_ep_t *ep;
+
+	ep = get_in_ep(pcd, epnum);
+
+#ifdef DWC_EN_ISOC
+	if (ep->dwc_ep.type == DWC_OTG_EP_TYPE_ISOC) {
+		return;
+	}
+#endif /* DWC_EN_ISOC  */
+
+	core_if = GET_CORE_IF(pcd);
+	dev_if = core_if->dev_if;
+
+	dieptsiz.d32 = DWC_READ_REG32(&dev_if->in_ep_regs[epnum]->dieptsiz);
+
+	DWC_DEBUGPL(DBG_PCD, "xfer_buff=%p xfer_count=%0x xfer_len=%0x"
+		    " stopped=%d\n", ep->dwc_ep.xfer_buff,
+		    ep->dwc_ep.xfer_count, ep->dwc_ep.xfer_len, ep->stopped);
+	/*
+	 * If xfersize is 0 and pktcnt in not 0, resend the last packet.
+	 */
+	if (dieptsiz.b.pktcnt && dieptsiz.b.xfersize == 0 &&
+	    ep->dwc_ep.start_xfer_buff != 0) {
+		if (ep->dwc_ep.total_len <= ep->dwc_ep.maxpacket) {
+			ep->dwc_ep.xfer_count = 0;
+			ep->dwc_ep.xfer_buff = ep->dwc_ep.start_xfer_buff;
+			ep->dwc_ep.xfer_len = ep->dwc_ep.xfer_count;
+		} else {
+			ep->dwc_ep.xfer_count -= ep->dwc_ep.maxpacket;
+			/* convert packet size to dwords. */
+			ep->dwc_ep.xfer_buff -= ep->dwc_ep.maxpacket;
+			ep->dwc_ep.xfer_len = ep->dwc_ep.xfer_count;
+		}
+		ep->stopped = 0;
+		DWC_DEBUGPL(DBG_PCD, "xfer_buff=%p xfer_count=%0x "
+			    "xfer_len=%0x stopped=%d\n",
+			    ep->dwc_ep.xfer_buff,
+			    ep->dwc_ep.xfer_count, ep->dwc_ep.xfer_len,
+			    ep->stopped);
+		if (epnum == 0) {
+			dwc_otg_ep0_start_transfer(core_if, &ep->dwc_ep);
+		} else {
+			dwc_otg_ep_start_transfer(core_if, &ep->dwc_ep);
+		}
+	}
+}
+
+/*
+ * This function create new nextep sequnce based on Learn Queue.
+ *
+ * @param core_if Programming view of DWC_otg controller
+ */
+void predict_nextep_seq( dwc_otg_core_if_t * core_if)
+{
+	dwc_otg_device_global_regs_t *dev_global_regs =
+	    core_if->dev_if->dev_global_regs;
+	const uint32_t TOKEN_Q_DEPTH = core_if->hwcfg2.b.dev_token_q_depth;
+	/* Number of Token Queue Registers */
+	const int DTKNQ_REG_CNT = (TOKEN_Q_DEPTH + 7) / 8;
+	dtknq1_data_t dtknqr1;
+	uint32_t in_tkn_epnums[4];
+	uint8_t seqnum[MAX_EPS_CHANNELS];
+	uint8_t intkn_seq[TOKEN_Q_DEPTH];
+	grstctl_t resetctl = {.d32 = 0 };
+	uint8_t temp;
+	int ndx = 0;
+	int start = 0;
+	int end = 0;
+	int sort_done = 0;
+	int i = 0;
+	volatile uint32_t *addr = &dev_global_regs->dtknqr1;
+
+
+	DWC_DEBUGPL(DBG_PCD,"dev_token_q_depth=%d\n",TOKEN_Q_DEPTH);
+
+	/* Read the DTKNQ Registers */
+	for (i = 0; i < DTKNQ_REG_CNT; i++) {
+		in_tkn_epnums[i] = DWC_READ_REG32(addr);
+		DWC_DEBUGPL(DBG_PCDV, "DTKNQR%d=0x%08x\n", i + 1,
+			    in_tkn_epnums[i]);
+		if (addr == &dev_global_regs->dvbusdis) {
+			addr = &dev_global_regs->dtknqr3_dthrctl;
+		} else {
+			++addr;
+		}
+
+	}
+
+	/* Copy the DTKNQR1 data to the bit field. */
+	dtknqr1.d32 = in_tkn_epnums[0];
+	if (dtknqr1.b.wrap_bit) {
+		ndx = dtknqr1.b.intknwptr;
+		end = ndx -1;
+		if (end < 0)
+			end = TOKEN_Q_DEPTH -1;
+	} else {
+		ndx = 0;
+		end = dtknqr1.b.intknwptr -1;
+		if (end < 0)
+			end = 0;
+	}
+	start = ndx;
+
+	/* Fill seqnum[] by initial values: EP number + 31 */
+	for (i=0; i <= core_if->dev_if->num_in_eps; i++) {
+		seqnum[i] = i +31;
+	}
+
+	/* Fill intkn_seq[] from in_tkn_epnums[0] */
+	for (i=0; i < 6; i++)
+		intkn_seq[i] = (in_tkn_epnums[0] >> ((7-i) * 4)) & 0xf;
+
+	if (TOKEN_Q_DEPTH > 6) {
+		/* Fill intkn_seq[] from in_tkn_epnums[1] */
+		for (i=6; i < 14; i++)
+			intkn_seq[i] = (in_tkn_epnums[1] >> ((7-(i-6)) * 4)) & 0xf;
+	}
+
+	if (TOKEN_Q_DEPTH > 14) {
+		/* Fill intkn_seq[] from in_tkn_epnums[1] */
+		for (i=14; i < 22; i++)
+			intkn_seq[i] = (in_tkn_epnums[2] >> ((7-(i-14)) * 4)) & 0xf;
+	}
+
+	if (TOKEN_Q_DEPTH > 22) {
+		/* Fill intkn_seq[] from in_tkn_epnums[1] */
+		for (i=22; i < 30; i++)
+			intkn_seq[i] = (in_tkn_epnums[3] >> ((7-(i-22)) * 4)) & 0xf;
+	}
+
+	DWC_DEBUGPL(DBG_PCDV,"%s start=%d end=%d intkn_seq[]:\n", __func__, start, end);
+	for (i=0; i<TOKEN_Q_DEPTH; i++)
+		DWC_DEBUGPL(DBG_PCDV,"%d\n", intkn_seq[i]);
+
+	/* Update seqnum based on intkn_seq[] */
+	i = 0;
+	do {
+		seqnum[intkn_seq[ndx]] = i;
+		ndx++;
+		i++;
+		if (ndx == TOKEN_Q_DEPTH)
+			ndx = 0;
+	} while ( i < TOKEN_Q_DEPTH );
+
+	/* Mark non active EP's in seqnum[] by 0xff */
+	for (i=0; i<=core_if->dev_if->num_in_eps; i++) {
+		if (core_if->nextep_seq[i] == 0xff )
+			seqnum[i] = 0xff;
+	}
+
+	/* Sort seqnum[] */
+	sort_done = 0;
+	while (!sort_done) {
+		sort_done = 1;
+		for (i=0; i<core_if->dev_if->num_in_eps; i++) {
+			if (seqnum[i] > seqnum[i+1]) {
+				temp = seqnum[i];
+				seqnum[i] = seqnum[i+1];
+				seqnum[i+1] = temp;
+				sort_done = 0;
+			}
+		}
+	}
+
+	ndx = start + seqnum[0];
+	if (ndx >= TOKEN_Q_DEPTH)
+		ndx = ndx % TOKEN_Q_DEPTH;
+	core_if->first_in_nextep_seq = intkn_seq[ndx];
+
+	/* Update seqnum[] by EP numbers  */
+	for (i=0; i<=core_if->dev_if->num_in_eps; i++) {
+		ndx = start + i;
+		if (seqnum[i] < 31) {
+			ndx = start + seqnum[i];
+			if (ndx >= TOKEN_Q_DEPTH)
+				ndx = ndx % TOKEN_Q_DEPTH;
+			seqnum[i] = intkn_seq[ndx];
+		} else {
+			if (seqnum[i] < 0xff) {
+				seqnum[i] = seqnum[i] - 31;
+			} else {
+				break;
+			}
+		}
+	}
+
+	/* Update nextep_seq[] based on seqnum[] */
+	for (i=0; i<core_if->dev_if->num_in_eps; i++) {
+		if (seqnum[i] != 0xff) {
+			if (seqnum[i+1] != 0xff) {
+				core_if->nextep_seq[seqnum[i]] = seqnum[i+1];
+			} else {
+				core_if->nextep_seq[seqnum[i]] = core_if->first_in_nextep_seq;
+				break;
+			}
+		} else {
+			break;
+		}
+	}
+
+	DWC_DEBUGPL(DBG_PCDV, "%s first_in_nextep_seq= %2d; nextep_seq[]:\n",
+		__func__, core_if->first_in_nextep_seq);
+	for (i=0; i <= core_if->dev_if->num_in_eps; i++) {
+		DWC_DEBUGPL(DBG_PCDV,"%2d\n", core_if->nextep_seq[i]);
+	}
+
+	/* Flush the Learning Queue */
+	resetctl.d32 = DWC_READ_REG32(&core_if->core_global_regs->grstctl);
+	resetctl.b.intknqflsh = 1;
+	DWC_WRITE_REG32(&core_if->core_global_regs->grstctl, resetctl.d32);
+
+
+}
+
+/**
+ * handle the IN EP disable interrupt.
+ */
+static inline void handle_in_ep_disable_intr(dwc_otg_pcd_t * pcd,
+					     const uint32_t epnum)
+{
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(pcd);
+	dwc_otg_dev_if_t *dev_if = core_if->dev_if;
+	deptsiz_data_t dieptsiz = {.d32 = 0 };
+	dctl_data_t dctl = {.d32 = 0 };
+	dwc_otg_pcd_ep_t *ep;
+	dwc_ep_t *dwc_ep;
+	gintmsk_data_t gintmsk_data;
+	depctl_data_t depctl;
+	uint32_t diepdma;
+	uint32_t remain_to_transfer = 0;
+	uint8_t i;
+	uint32_t xfer_size;
+
+	ep = get_in_ep(pcd, epnum);
+	dwc_ep = &ep->dwc_ep;
+
+	if (dwc_ep->type == DWC_OTG_EP_TYPE_ISOC) {
+		dwc_otg_flush_tx_fifo(core_if, dwc_ep->tx_fifo_num);
+		complete_ep(ep);
+		return;
+	}
+
+	DWC_DEBUGPL(DBG_PCD, "diepctl%d=%0x\n", epnum,
+		    DWC_READ_REG32(&dev_if->in_ep_regs[epnum]->diepctl));
+	dieptsiz.d32 = DWC_READ_REG32(&dev_if->in_ep_regs[epnum]->dieptsiz);
+	depctl.d32 = DWC_READ_REG32(&dev_if->in_ep_regs[epnum]->diepctl);
+
+	DWC_DEBUGPL(DBG_ANY, "pktcnt=%d size=%d\n",
+		    dieptsiz.b.pktcnt, dieptsiz.b.xfersize);
+
+	if ((core_if->start_predict == 0) || (depctl.b.eptype & 1)) {
+		if (ep->stopped) {
+			if (core_if->en_multiple_tx_fifo)
+				/* Flush the Tx FIFO */
+				dwc_otg_flush_tx_fifo(core_if, dwc_ep->tx_fifo_num);
+			/* Clear the Global IN NP NAK */
+			dctl.d32 = 0;
+			dctl.b.cgnpinnak = 1;
+			DWC_MODIFY_REG32(&dev_if->dev_global_regs->dctl, dctl.d32, dctl.d32);
+			/* Restart the transaction */
+			if (dieptsiz.b.pktcnt != 0 || dieptsiz.b.xfersize != 0) {
+				restart_transfer(pcd, epnum);
+			}
+		} else {
+			/* Restart the transaction */
+			if (dieptsiz.b.pktcnt != 0 || dieptsiz.b.xfersize != 0) {
+				restart_transfer(pcd, epnum);
+			}
+			DWC_DEBUGPL(DBG_ANY, "STOPPED!!!\n");
+		}
+		return;
+	}
+
+	if (core_if->start_predict > 2) {	// NP IN EP
+		core_if->start_predict--;
+		return;
+	}
+
+	core_if->start_predict--;
+
+	if (core_if->start_predict == 1) {	// All NP IN Ep's disabled now
+
+		predict_nextep_seq(core_if);
+
+		/* Update all active IN EP's NextEP field based of nextep_seq[] */
+		for ( i = 0; i <= core_if->dev_if->num_in_eps; i++) {
+			depctl.d32 = DWC_READ_REG32(&dev_if->in_ep_regs[i]->diepctl);
+			if (core_if->nextep_seq[i] != 0xff) {	// Active NP IN EP
+				depctl.b.nextep = core_if->nextep_seq[i];
+				DWC_WRITE_REG32(&dev_if->in_ep_regs[i]->diepctl, depctl.d32);
+			}
+		}
+		/* Flush Shared NP TxFIFO */
+		dwc_otg_flush_tx_fifo(core_if, 0);
+		/* Rewind buffers */
+		if (!core_if->dma_desc_enable) {
+			i = core_if->first_in_nextep_seq;
+			do {
+				ep = get_in_ep(pcd, i);
+				dieptsiz.d32 = DWC_READ_REG32(&dev_if->in_ep_regs[i]->dieptsiz);
+				xfer_size = ep->dwc_ep.total_len - ep->dwc_ep.xfer_count;
+				if (xfer_size > ep->dwc_ep.maxxfer)
+					xfer_size = ep->dwc_ep.maxxfer;
+				depctl.d32 = DWC_READ_REG32(&dev_if->in_ep_regs[i]->diepctl);
+				if (dieptsiz.b.pktcnt != 0) {
+					if (xfer_size == 0) {
+						remain_to_transfer = 0;
+					} else {
+						if ((xfer_size % ep->dwc_ep.maxpacket) == 0) {
+							remain_to_transfer =
+								dieptsiz.b.pktcnt * ep->dwc_ep.maxpacket;
+						} else {
+							remain_to_transfer = ((dieptsiz.b.pktcnt -1) * ep->dwc_ep.maxpacket)
+								+ (xfer_size % ep->dwc_ep.maxpacket);
+						}
+					}
+					diepdma = DWC_READ_REG32(&dev_if->in_ep_regs[i]->diepdma);
+					dieptsiz.b.xfersize = remain_to_transfer;
+					DWC_WRITE_REG32(&dev_if->in_ep_regs[i]->dieptsiz, dieptsiz.d32);
+					diepdma = ep->dwc_ep.dma_addr + (xfer_size - remain_to_transfer);
+					DWC_WRITE_REG32(&dev_if->in_ep_regs[i]->diepdma, diepdma);
+				}
+				i = core_if->nextep_seq[i];
+			} while (i != core_if->first_in_nextep_seq);
+		} else { // dma_desc_enable
+				DWC_PRINTF("%s Learning Queue not supported in DDMA\n", __func__);
+		}
+
+		/* Restart transfers in predicted sequences */
+		i = core_if->first_in_nextep_seq;
+		do {
+			dieptsiz.d32 = DWC_READ_REG32(&dev_if->in_ep_regs[i]->dieptsiz);
+			depctl.d32 = DWC_READ_REG32(&dev_if->in_ep_regs[i]->diepctl);
+			if (dieptsiz.b.pktcnt != 0) {
+				depctl.d32 = DWC_READ_REG32(&dev_if->in_ep_regs[i]->diepctl);
+				depctl.b.epena = 1;
+				depctl.b.cnak = 1;
+				DWC_WRITE_REG32(&dev_if->in_ep_regs[i]->diepctl, depctl.d32);
+			}
+			i = core_if->nextep_seq[i];
+		} while (i != core_if->first_in_nextep_seq);
+
+		/* Clear the global non-periodic IN NAK handshake */
+		dctl.d32 = 0;
+		dctl.b.cgnpinnak = 1;
+		DWC_MODIFY_REG32(&dev_if->dev_global_regs->dctl, dctl.d32, dctl.d32);
+
+		/* Unmask EP Mismatch interrupt */
+		gintmsk_data.d32 = 0;
+		gintmsk_data.b.epmismatch = 1;
+		DWC_MODIFY_REG32(&core_if->core_global_regs->gintmsk, 0, gintmsk_data.d32);
+
+		core_if->start_predict = 0;
+
+	}
+}
+
+/**
+ * Handler for the IN EP timeout handshake interrupt.
+ */
+static inline void handle_in_ep_timeout_intr(dwc_otg_pcd_t * pcd,
+					     const uint32_t epnum)
+{
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(pcd);
+	dwc_otg_dev_if_t *dev_if = core_if->dev_if;
+
+#ifdef DEBUG
+	deptsiz_data_t dieptsiz = {.d32 = 0 };
+	uint32_t num = 0;
+#endif
+	dctl_data_t dctl = {.d32 = 0 };
+	dwc_otg_pcd_ep_t *ep;
+
+	gintmsk_data_t intr_mask = {.d32 = 0 };
+
+	ep = get_in_ep(pcd, epnum);
+
+	/* Disable the NP Tx Fifo Empty Interrrupt */
+	if (!core_if->dma_enable) {
+		intr_mask.b.nptxfempty = 1;
+		DWC_MODIFY_REG32(&core_if->core_global_regs->gintmsk,
+				 intr_mask.d32, 0);
+	}
+	/** @todo NGS Check EP type.
+	 * Implement for Periodic EPs */
+	/*
+	 * Non-periodic EP
+	 */
+	/* Enable the Global IN NAK Effective Interrupt */
+	intr_mask.b.ginnakeff = 1;
+	DWC_MODIFY_REG32(&core_if->core_global_regs->gintmsk, 0, intr_mask.d32);
+
+	/* Set Global IN NAK */
+	dctl.b.sgnpinnak = 1;
+	DWC_MODIFY_REG32(&dev_if->dev_global_regs->dctl, dctl.d32, dctl.d32);
+
+	ep->stopped = 1;
+
+#ifdef DEBUG
+	dieptsiz.d32 = DWC_READ_REG32(&dev_if->in_ep_regs[num]->dieptsiz);
+	DWC_DEBUGPL(DBG_ANY, "pktcnt=%d size=%d\n",
+		    dieptsiz.b.pktcnt, dieptsiz.b.xfersize);
+#endif
+
+#ifdef DISABLE_PERIODIC_EP
+	/*
+	 * Set the NAK bit for this EP to
+	 * start the disable process.
+	 */
+	diepctl.d32 = 0;
+	diepctl.b.snak = 1;
+	DWC_MODIFY_REG32(&dev_if->in_ep_regs[num]->diepctl, diepctl.d32,
+			 diepctl.d32);
+	ep->disabling = 1;
+	ep->stopped = 1;
+#endif
+}
+
+/**
+ * Handler for the IN EP NAK interrupt.
+ */
+static inline int32_t handle_in_ep_nak_intr(dwc_otg_pcd_t * pcd,
+					    const uint32_t epnum)
+{
+	/** @todo implement ISR */
+	dwc_otg_core_if_t *core_if;
+	diepmsk_data_t intr_mask = {.d32 = 0 };
+
+	DWC_PRINTF("INTERRUPT Handler not implemented for %s\n", "IN EP NAK");
+	core_if = GET_CORE_IF(pcd);
+	intr_mask.b.nak = 1;
+
+	if (core_if->multiproc_int_enable) {
+		DWC_MODIFY_REG32(&core_if->dev_if->dev_global_regs->
+				 diepeachintmsk[epnum], intr_mask.d32, 0);
+	} else {
+		DWC_MODIFY_REG32(&core_if->dev_if->dev_global_regs->diepmsk,
+				 intr_mask.d32, 0);
+	}
+
+	return 1;
+}
+
+/**
+ * Handler for the OUT EP Babble interrupt.
+ */
+static inline int32_t handle_out_ep_babble_intr(dwc_otg_pcd_t * pcd,
+						const uint32_t epnum)
+{
+	/** @todo implement ISR */
+	dwc_otg_core_if_t *core_if;
+	doepmsk_data_t intr_mask = {.d32 = 0 };
+
+	DWC_PRINTF("INTERRUPT Handler not implemented for %s\n",
+		   "OUT EP Babble");
+	core_if = GET_CORE_IF(pcd);
+	intr_mask.b.babble = 1;
+
+	if (core_if->multiproc_int_enable) {
+		DWC_MODIFY_REG32(&core_if->dev_if->dev_global_regs->
+				 doepeachintmsk[epnum], intr_mask.d32, 0);
+	} else {
+		DWC_MODIFY_REG32(&core_if->dev_if->dev_global_regs->doepmsk,
+				 intr_mask.d32, 0);
+	}
+
+	return 1;
+}
+
+/**
+ * Handler for the OUT EP NAK interrupt.
+ */
+static inline int32_t handle_out_ep_nak_intr(dwc_otg_pcd_t * pcd,
+					     const uint32_t epnum)
+{
+	/** @todo implement ISR */
+	dwc_otg_core_if_t *core_if;
+	doepmsk_data_t intr_mask = {.d32 = 0 };
+
+	DWC_PRINTF("INTERRUPT Handler not implemented for %s\n", "OUT EP NAK");
+	core_if = GET_CORE_IF(pcd);
+	intr_mask.b.nak = 1;
+
+	if (core_if->multiproc_int_enable) {
+		DWC_MODIFY_REG32(&core_if->dev_if->dev_global_regs->
+				 doepeachintmsk[epnum], intr_mask.d32, 0);
+	} else {
+		DWC_MODIFY_REG32(&core_if->dev_if->dev_global_regs->doepmsk,
+				 intr_mask.d32, 0);
+	}
+
+	return 1;
+}
+
+/**
+ * Handler for the OUT EP NYET interrupt.
+ */
+static inline int32_t handle_out_ep_nyet_intr(dwc_otg_pcd_t * pcd,
+					      const uint32_t epnum)
+{
+	/** @todo implement ISR */
+	dwc_otg_core_if_t *core_if;
+	doepmsk_data_t intr_mask = {.d32 = 0 };
+
+	DWC_PRINTF("INTERRUPT Handler not implemented for %s\n", "OUT EP NYET");
+	core_if = GET_CORE_IF(pcd);
+	intr_mask.b.nyet = 1;
+
+	if (core_if->multiproc_int_enable) {
+		DWC_MODIFY_REG32(&core_if->dev_if->dev_global_regs->
+				 doepeachintmsk[epnum], intr_mask.d32, 0);
+	} else {
+		DWC_MODIFY_REG32(&core_if->dev_if->dev_global_regs->doepmsk,
+				 intr_mask.d32, 0);
+	}
+
+	return 1;
+}
+
+/**
+ * This interrupt indicates that an IN EP has a pending Interrupt.
+ * The sequence for handling the IN EP interrupt is shown below:
+ * -#	Read the Device All Endpoint Interrupt register
+ * -#	Repeat the following for each IN EP interrupt bit set (from
+ *		LSB to MSB).
+ * -#	Read the Device Endpoint Interrupt (DIEPINTn) register
+ * -#	If "Transfer Complete" call the request complete function
+ * -#	If "Endpoint Disabled" complete the EP disable procedure.
+ * -#	If "AHB Error Interrupt" log error
+ * -#	If "Time-out Handshake" log error
+ * -#	If "IN Token Received when TxFIFO Empty" write packet to Tx
+ *		FIFO.
+ * -#	If "IN Token EP Mismatch" (disable, this is handled by EP
+ *		Mismatch Interrupt)
+ */
+static int32_t dwc_otg_pcd_handle_in_ep_intr(dwc_otg_pcd_t * pcd)
+{
+#define CLEAR_IN_EP_INTR(__core_if,__epnum,__intr) \
+do { \
+		diepint_data_t diepint = {.d32=0}; \
+		diepint.b.__intr = 1; \
+		DWC_WRITE_REG32(&__core_if->dev_if->in_ep_regs[__epnum]->diepint, \
+		diepint.d32); \
+} while (0)
+
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(pcd);
+	dwc_otg_dev_if_t *dev_if = core_if->dev_if;
+	diepint_data_t diepint = {.d32 = 0 };
+	depctl_data_t depctl = {.d32 = 0 };
+	uint32_t ep_intr;
+	uint32_t epnum = 0;
+	dwc_otg_pcd_ep_t *ep;
+	dwc_ep_t *dwc_ep;
+	gintmsk_data_t intr_mask = {.d32 = 0 };
+
+	DWC_DEBUGPL(DBG_PCDV, "%s(%p)\n", __func__, pcd);
+
+	/* Read in the device interrupt bits */
+	ep_intr = dwc_otg_read_dev_all_in_ep_intr(core_if);
+
+	/* Service the Device IN interrupts for each endpoint */
+	while (ep_intr) {
+		if (ep_intr & 0x1) {
+			uint32_t empty_msk;
+			/* Get EP pointer */
+			ep = get_in_ep(pcd, epnum);
+			dwc_ep = &ep->dwc_ep;
+
+			depctl.d32 =
+			    DWC_READ_REG32(&dev_if->in_ep_regs[epnum]->diepctl);
+			empty_msk =
+			    DWC_READ_REG32(&dev_if->
+					   dev_global_regs->dtknqr4_fifoemptymsk);
+
+			DWC_DEBUGPL(DBG_PCDV,
+				    "IN EP INTERRUPT - %d\nepmty_msk - %8x  diepctl - %8x\n",
+				    epnum, empty_msk, depctl.d32);
+
+			DWC_DEBUGPL(DBG_PCD,
+				    "EP%d-%s: type=%d, mps=%d\n",
+				    dwc_ep->num, (dwc_ep->is_in ? "IN" : "OUT"),
+				    dwc_ep->type, dwc_ep->maxpacket);
+
+			diepint.d32 =
+			    dwc_otg_read_dev_in_ep_intr(core_if, dwc_ep);
+
+			DWC_DEBUGPL(DBG_PCDV,
+				    "EP %d Interrupt Register - 0x%x\n", epnum,
+				    diepint.d32);
+			/* Transfer complete */
+			if (diepint.b.xfercompl) {
+				/* Disable the NP Tx FIFO Empty
+				 * Interrrupt */
+				if (core_if->en_multiple_tx_fifo == 0) {
+					intr_mask.b.nptxfempty = 1;
+					DWC_MODIFY_REG32
+					    (&core_if->core_global_regs->gintmsk,
+					     intr_mask.d32, 0);
+				} else {
+					/* Disable the Tx FIFO Empty Interrupt for this EP */
+					uint32_t fifoemptymsk =
+					    0x1 << dwc_ep->num;
+					DWC_MODIFY_REG32(&core_if->
+							 dev_if->dev_global_regs->dtknqr4_fifoemptymsk,
+							 fifoemptymsk, 0);
+				}
+				/* Clear the bit in DIEPINTn for this interrupt */
+				CLEAR_IN_EP_INTR(core_if, epnum, xfercompl);
+
+				/* Complete the transfer */
+				if (epnum == 0) {
+					handle_ep0(pcd);
+				}
+#ifdef DWC_EN_ISOC
+				else if (dwc_ep->type == DWC_OTG_EP_TYPE_ISOC) {
+					if (!ep->stopped)
+						complete_iso_ep(pcd, ep);
+				}
+#endif /* DWC_EN_ISOC */
+#ifdef DWC_UTE_PER_IO
+				else if (dwc_ep->type == DWC_OTG_EP_TYPE_ISOC) {
+					if (!ep->stopped)
+						complete_xiso_ep(ep);
+				}
+#endif /* DWC_UTE_PER_IO */
+				else {
+					if (dwc_ep->type == DWC_OTG_EP_TYPE_ISOC &&
+							dwc_ep->bInterval > 1) {
+						dwc_ep->frame_num += dwc_ep->bInterval;
+						if (dwc_ep->frame_num > 0x3FFF)
+						{
+							dwc_ep->frm_overrun = 1;
+							dwc_ep->frame_num &= 0x3FFF;
+						} else
+							dwc_ep->frm_overrun = 0;
+					}
+					complete_ep(ep);
+					if(diepint.b.nak)
+						CLEAR_IN_EP_INTR(core_if, epnum, nak);
+				}
+			}
+			/* Endpoint disable      */
+			if (diepint.b.epdisabled) {
+				DWC_DEBUGPL(DBG_ANY, "EP%d IN disabled\n",
+					    epnum);
+				handle_in_ep_disable_intr(pcd, epnum);
+
+				/* Clear the bit in DIEPINTn for this interrupt */
+				CLEAR_IN_EP_INTR(core_if, epnum, epdisabled);
+			}
+			/* AHB Error */
+			if (diepint.b.ahberr) {
+				DWC_ERROR("EP%d IN AHB Error\n", epnum);
+				/* Clear the bit in DIEPINTn for this interrupt */
+				CLEAR_IN_EP_INTR(core_if, epnum, ahberr);
+			}
+			/* TimeOUT Handshake (non-ISOC IN EPs) */
+			if (diepint.b.timeout) {
+				DWC_ERROR("EP%d IN Time-out\n", epnum);
+				handle_in_ep_timeout_intr(pcd, epnum);
+
+				CLEAR_IN_EP_INTR(core_if, epnum, timeout);
+			}
+			/** IN Token received with TxF Empty */
+			if (diepint.b.intktxfemp) {
+				DWC_DEBUGPL(DBG_ANY,
+					    "EP%d IN TKN TxFifo Empty\n",
+					    epnum);
+				if (!ep->stopped && epnum != 0) {
+
+					diepmsk_data_t diepmsk = {.d32 = 0 };
+					diepmsk.b.intktxfemp = 1;
+
+					if (core_if->multiproc_int_enable) {
+						DWC_MODIFY_REG32
+						    (&dev_if->dev_global_regs->diepeachintmsk
+						     [epnum], diepmsk.d32, 0);
+					} else {
+						DWC_MODIFY_REG32
+						    (&dev_if->dev_global_regs->diepmsk,
+						     diepmsk.d32, 0);
+					}
+				} else if (core_if->dma_desc_enable
+					   && epnum == 0
+					   && pcd->ep0state ==
+					   EP0_OUT_STATUS_PHASE) {
+					// EP0 IN set STALL
+					depctl.d32 =
+					    DWC_READ_REG32(&dev_if->in_ep_regs
+							   [epnum]->diepctl);
+
+					/* set the disable and stall bits */
+					if (depctl.b.epena) {
+						depctl.b.epdis = 1;
+					}
+					depctl.b.stall = 1;
+					DWC_WRITE_REG32(&dev_if->in_ep_regs
+							[epnum]->diepctl,
+							depctl.d32);
+				}
+				CLEAR_IN_EP_INTR(core_if, epnum, intktxfemp);
+			}
+			/** IN Token Received with EP mismatch */
+			if (diepint.b.intknepmis) {
+				DWC_DEBUGPL(DBG_ANY,
+					    "EP%d IN TKN EP Mismatch\n", epnum);
+				CLEAR_IN_EP_INTR(core_if, epnum, intknepmis);
+			}
+			/** IN Endpoint NAK Effective */
+			if (diepint.b.inepnakeff) {
+				DWC_DEBUGPL(DBG_ANY,
+					    "EP%d IN EP NAK Effective\n",
+					    epnum);
+				/* Periodic EP */
+				if (ep->disabling) {
+					depctl.d32 = 0;
+					depctl.b.snak = 1;
+					depctl.b.epdis = 1;
+					DWC_MODIFY_REG32(&dev_if->in_ep_regs
+							 [epnum]->diepctl,
+							 depctl.d32,
+							 depctl.d32);
+				}
+				CLEAR_IN_EP_INTR(core_if, epnum, inepnakeff);
+
+			}
+
+			/** IN EP Tx FIFO Empty Intr */
+			if (diepint.b.emptyintr) {
+				DWC_DEBUGPL(DBG_ANY,
+					    "EP%d Tx FIFO Empty Intr \n",
+					    epnum);
+				write_empty_tx_fifo(pcd, epnum);
+
+				CLEAR_IN_EP_INTR(core_if, epnum, emptyintr);
+
+			}
+
+			/** IN EP BNA Intr */
+			if (diepint.b.bna) {
+				CLEAR_IN_EP_INTR(core_if, epnum, bna);
+				if (core_if->dma_desc_enable) {
+#ifdef DWC_EN_ISOC
+					if (dwc_ep->type ==
+					    DWC_OTG_EP_TYPE_ISOC) {
+						/*
+						 * This checking is performed to prevent first "false" BNA
+						 * handling occuring right after reconnect
+						 */
+						if (dwc_ep->next_frame !=
+						    0xffffffff)
+							dwc_otg_pcd_handle_iso_bna(ep);
+					} else
+#endif				/* DWC_EN_ISOC */
+					{
+						dwc_otg_pcd_handle_noniso_bna(ep);
+					}
+				}
+			}
+			/* NAK Interrutp */
+			if (diepint.b.nak) {
+				DWC_DEBUGPL(DBG_ANY, "EP%d IN NAK Interrupt\n",
+					    epnum);
+				if (ep->dwc_ep.type == DWC_OTG_EP_TYPE_ISOC)
+				{
+					depctl_data_t depctl;
+					if (ep->dwc_ep.frame_num == 0xFFFFFFFF)
+					{
+						ep->dwc_ep.frame_num = core_if->frame_num;
+						if (ep->dwc_ep.bInterval > 1)
+						{
+							depctl.d32 = 0;
+							depctl.d32 = DWC_READ_REG32(&dev_if->in_ep_regs[epnum]->diepctl);
+							if (ep->dwc_ep.frame_num & 0x1) {
+								depctl.b.setd1pid = 1;
+								depctl.b.setd0pid = 0;
+							} else {
+								depctl.b.setd0pid = 1;
+								depctl.b.setd1pid = 0;
+							}
+							DWC_WRITE_REG32(&dev_if->in_ep_regs[epnum]->diepctl, depctl.d32);
+						}
+						start_next_request(ep);
+					}
+					ep->dwc_ep.frame_num += ep->dwc_ep.bInterval;
+					if (dwc_ep->frame_num > 0x3FFF)
+					{
+						dwc_ep->frm_overrun = 1;
+						dwc_ep->frame_num &= 0x3FFF;
+					} else
+						dwc_ep->frm_overrun = 0;
+				}
+
+				CLEAR_IN_EP_INTR(core_if, epnum, nak);
+			}
+		}
+		epnum++;
+		ep_intr >>= 1;
+	}
+
+	return 1;
+#undef CLEAR_IN_EP_INTR
+}
+
+/**
+ * This interrupt indicates that an OUT EP has a pending Interrupt.
+ * The sequence for handling the OUT EP interrupt is shown below:
+ * -#	Read the Device All Endpoint Interrupt register
+ * -#	Repeat the following for each OUT EP interrupt bit set (from
+ *		LSB to MSB).
+ * -#	Read the Device Endpoint Interrupt (DOEPINTn) register
+ * -#	If "Transfer Complete" call the request complete function
+ * -#	If "Endpoint Disabled" complete the EP disable procedure.
+ * -#	If "AHB Error Interrupt" log error
+ * -#	If "Setup Phase Done" process Setup Packet (See Standard USB
+ *		Command Processing)
+ */
+static int32_t dwc_otg_pcd_handle_out_ep_intr(dwc_otg_pcd_t * pcd)
+{
+#define CLEAR_OUT_EP_INTR(__core_if,__epnum,__intr) \
+do { \
+		doepint_data_t doepint = {.d32=0}; \
+		doepint.b.__intr = 1; \
+		DWC_WRITE_REG32(&__core_if->dev_if->out_ep_regs[__epnum]->doepint, \
+		doepint.d32); \
+} while (0)
+
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(pcd);
+	uint32_t ep_intr;
+	doepint_data_t doepint = {.d32 = 0 };
+	uint32_t epnum = 0;
+	dwc_otg_pcd_ep_t *ep;
+	dwc_ep_t *dwc_ep;
+	dctl_data_t dctl = {.d32 = 0 };
+	gintmsk_data_t gintmsk = {.d32 = 0 };
+
+
+	DWC_DEBUGPL(DBG_PCDV, "%s()\n", __func__);
+
+	/* Read in the device interrupt bits */
+	ep_intr = dwc_otg_read_dev_all_out_ep_intr(core_if);
+
+	while (ep_intr) {
+		if (ep_intr & 0x1) {
+			/* Get EP pointer */
+			ep = get_out_ep(pcd, epnum);
+			dwc_ep = &ep->dwc_ep;
+
+#ifdef VERBOSE
+			DWC_DEBUGPL(DBG_PCDV,
+				    "EP%d-%s: type=%d, mps=%d\n",
+				    dwc_ep->num, (dwc_ep->is_in ? "IN" : "OUT"),
+				    dwc_ep->type, dwc_ep->maxpacket);
+#endif
+			doepint.d32 =
+			    dwc_otg_read_dev_out_ep_intr(core_if, dwc_ep);
+
+			/* Transfer complete */
+			if (doepint.b.xfercompl) {
+
+				if (epnum == 0) {
+					/* Clear the bit in DOEPINTn for this interrupt */
+					CLEAR_OUT_EP_INTR(core_if, epnum,
+							  xfercompl);
+					if (core_if->dma_desc_enable == 0
+					    || pcd->ep0state != EP0_IDLE)
+						handle_ep0(pcd);
+#ifdef DWC_EN_ISOC
+				} else if (dwc_ep->type == DWC_OTG_EP_TYPE_ISOC) {
+					if (doepint.b.pktdrpsts == 0) {
+						/* Clear the bit in DOEPINTn for this interrupt */
+						CLEAR_OUT_EP_INTR(core_if,
+								  epnum,
+								  xfercompl);
+						complete_iso_ep(pcd, ep);
+					} else {
+
+						doepint_data_t doepint = {.d32 = 0 };
+						doepint.b.xfercompl = 1;
+						doepint.b.pktdrpsts = 1;
+						DWC_WRITE_REG32
+						    (&core_if->dev_if->out_ep_regs
+						     [epnum]->doepint,
+						     doepint.d32);
+						if (handle_iso_out_pkt_dropped
+						    (core_if, dwc_ep)) {
+							complete_iso_ep(pcd,
+									ep);
+						}
+					}
+#endif /* DWC_EN_ISOC */
+#ifdef DWC_UTE_PER_IO
+				} else if (dwc_ep->type == DWC_OTG_EP_TYPE_ISOC) {
+					CLEAR_OUT_EP_INTR(core_if, epnum, xfercompl);
+					if (!ep->stopped)
+						complete_xiso_ep(ep);
+#endif /* DWC_UTE_PER_IO */
+				} else {
+					/* Clear the bit in DOEPINTn for this interrupt */
+					CLEAR_OUT_EP_INTR(core_if, epnum,
+							  xfercompl);
+
+					if (core_if->core_params->dev_out_nak) {
+						DWC_TIMER_CANCEL(pcd->core_if->ep_xfer_timer[epnum]);
+						pcd->core_if->ep_xfer_info[epnum].state = 0;
+#ifdef DEBUG
+						print_memory_payload(pcd, dwc_ep);
+#endif
+					}
+					complete_ep(ep);
+				}
+
+			}
+
+			/* Endpoint disable      */
+			if (doepint.b.epdisabled) {
+
+				/* Clear the bit in DOEPINTn for this interrupt */
+				CLEAR_OUT_EP_INTR(core_if, epnum, epdisabled);
+				if (core_if->core_params->dev_out_nak) {
+#ifdef DEBUG
+					print_memory_payload(pcd, dwc_ep);
+#endif
+					/* In case of timeout condition */
+					if (core_if->ep_xfer_info[epnum].state == 2) {
+						dctl.d32 = DWC_READ_REG32(&core_if->dev_if->
+										dev_global_regs->dctl);
+						dctl.b.cgoutnak = 1;
+						DWC_WRITE_REG32(&core_if->dev_if->dev_global_regs->dctl,
+																dctl.d32);
+						/* Unmask goutnakeff interrupt which was masked
+						 * during handle nak out interrupt */
+						gintmsk.b.goutnakeff = 1;
+						DWC_MODIFY_REG32(&core_if->core_global_regs->gintmsk,
+																0, gintmsk.d32);
+
+						complete_ep(ep);
+					}
+				}
+				if (ep->dwc_ep.type == DWC_OTG_EP_TYPE_ISOC)
+				{
+					dctl_data_t dctl;
+					gintmsk_data_t intr_mask = {.d32 = 0};
+					dwc_otg_pcd_request_t *req = 0;
+
+					dctl.d32 = DWC_READ_REG32(&core_if->dev_if->
+						dev_global_regs->dctl);
+					dctl.b.cgoutnak = 1;
+					DWC_WRITE_REG32(&core_if->dev_if->dev_global_regs->dctl,
+						dctl.d32);
+
+					intr_mask.d32 = 0;
+					intr_mask.b.incomplisoout = 1;
+
+					/* Get any pending requests */
+					if (!DWC_CIRCLEQ_EMPTY(&ep->queue)) {
+						req = DWC_CIRCLEQ_FIRST(&ep->queue);
+						if (!req) {
+							DWC_PRINTF("complete_ep 0x%p, req = NULL!\n", ep);
+						} else {
+							dwc_otg_request_done(ep, req, 0);
+							start_next_request(ep);
+						}
+					} else {
+						DWC_PRINTF("complete_ep 0x%p, ep->queue empty!\n", ep);
+					}
+				}
+			}
+			/* AHB Error */
+			if (doepint.b.ahberr) {
+				DWC_ERROR("EP%d OUT AHB Error\n", epnum);
+				DWC_ERROR("EP%d DEPDMA=0x%08x \n",
+					  epnum, core_if->dev_if->out_ep_regs[epnum]->doepdma);
+				CLEAR_OUT_EP_INTR(core_if, epnum, ahberr);
+			}
+			/* Setup Phase Done (contorl EPs) */
+			if (doepint.b.setup) {
+#ifdef DEBUG_EP0
+				DWC_DEBUGPL(DBG_PCD, "EP%d SETUP Done\n",
+					    epnum);
+#endif
+				CLEAR_OUT_EP_INTR(core_if, epnum, setup);
+
+				handle_ep0(pcd);
+			}
+
+			/** OUT EP BNA Intr */
+			if (doepint.b.bna) {
+				CLEAR_OUT_EP_INTR(core_if, epnum, bna);
+				if (core_if->dma_desc_enable) {
+#ifdef DWC_EN_ISOC
+					if (dwc_ep->type ==
+					    DWC_OTG_EP_TYPE_ISOC) {
+						/*
+						 * This checking is performed to prevent first "false" BNA
+						 * handling occuring right after reconnect
+						 */
+						if (dwc_ep->next_frame !=
+						    0xffffffff)
+							dwc_otg_pcd_handle_iso_bna(ep);
+					} else
+#endif				/* DWC_EN_ISOC */
+					{
+						dwc_otg_pcd_handle_noniso_bna(ep);
+					}
+				}
+			}
+			if (doepint.b.stsphsercvd) {
+				CLEAR_OUT_EP_INTR(core_if, epnum, stsphsercvd);
+				if (core_if->dma_desc_enable) {
+					do_setup_in_status_phase(pcd);
+				}
+			}
+			/* Babble Interrutp */
+			if (doepint.b.babble) {
+				DWC_DEBUGPL(DBG_ANY, "EP%d OUT Babble\n",
+					    epnum);
+				handle_out_ep_babble_intr(pcd, epnum);
+
+				CLEAR_OUT_EP_INTR(core_if, epnum, babble);
+			}
+			if (doepint.b.outtknepdis)
+			{
+				DWC_DEBUGPL(DBG_ANY, "EP%d OUT Token received when EP is \
+					disabled\n",epnum);
+				if (ep->dwc_ep.type == DWC_OTG_EP_TYPE_ISOC)
+				{
+					doepmsk_data_t doepmsk = {.d32 = 0};
+					ep->dwc_ep.frame_num = core_if->frame_num;
+					if (ep->dwc_ep.bInterval > 1)
+					{
+						depctl_data_t depctl;
+						depctl.d32 = DWC_READ_REG32(&core_if->dev_if->
+													out_ep_regs[epnum]->doepctl);
+						if (ep->dwc_ep.frame_num & 0x1) {
+							depctl.b.setd1pid = 1;
+							depctl.b.setd0pid = 0;
+						} else {
+							depctl.b.setd0pid = 1;
+							depctl.b.setd1pid = 0;
+						}
+						DWC_WRITE_REG32(&core_if->dev_if->
+										out_ep_regs[epnum]->doepctl, depctl.d32);
+					}
+					start_next_request(ep);
+					doepmsk.b.outtknepdis = 1;
+					DWC_MODIFY_REG32(&core_if->dev_if->dev_global_regs->doepmsk,
+								 doepmsk.d32, 0);
+				}
+				CLEAR_OUT_EP_INTR(core_if, epnum, outtknepdis);
+			}
+
+			/* NAK Interrutp */
+			if (doepint.b.nak) {
+				DWC_DEBUGPL(DBG_ANY, "EP%d OUT NAK\n", epnum);
+				handle_out_ep_nak_intr(pcd, epnum);
+
+				CLEAR_OUT_EP_INTR(core_if, epnum, nak);
+			}
+			/* NYET Interrutp */
+			if (doepint.b.nyet) {
+				DWC_DEBUGPL(DBG_ANY, "EP%d OUT NYET\n", epnum);
+				handle_out_ep_nyet_intr(pcd, epnum);
+
+				CLEAR_OUT_EP_INTR(core_if, epnum, nyet);
+			}
+		}
+
+		epnum++;
+		ep_intr >>= 1;
+	}
+
+	return 1;
+
+#undef CLEAR_OUT_EP_INTR
+}
+static int drop_transfer(uint32_t trgt_fr, uint32_t curr_fr, uint8_t frm_overrun)
+{
+	int retval = 0;
+	if(!frm_overrun && curr_fr >= trgt_fr)
+		retval = 1;
+	else if (frm_overrun && (curr_fr >= trgt_fr && ((curr_fr - trgt_fr) < 0x3FFF/2)))
+		retval = 1;
+	return retval;
+}
+/**
+ * Incomplete ISO IN Transfer Interrupt.
+ * This interrupt indicates one of the following conditions occurred
+ * while transmitting an ISOC transaction.
+ * - Corrupted IN Token for ISOC EP.
+ * - Packet not complete in FIFO.
+ * The follow actions will be taken:
+ *	-#	Determine the EP
+ *	-#	Set incomplete flag in dwc_ep structure
+ *	-#	Disable EP; when "Endpoint Disabled" interrupt is received
+ *		Flush FIFO
+ */
+int32_t dwc_otg_pcd_handle_incomplete_isoc_in_intr(dwc_otg_pcd_t * pcd)
+{
+	gintsts_data_t gintsts;
+
+#ifdef DWC_EN_ISOC
+	dwc_otg_dev_if_t *dev_if;
+	deptsiz_data_t deptsiz = {.d32 = 0 };
+	depctl_data_t depctl = {.d32 = 0 };
+	dsts_data_t dsts = {.d32 = 0 };
+	dwc_ep_t *dwc_ep;
+	int i;
+
+	dev_if = GET_CORE_IF(pcd)->dev_if;
+
+	for (i = 1; i <= dev_if->num_in_eps; ++i) {
+		dwc_ep = &pcd->in_ep[i].dwc_ep;
+		if (dwc_ep->active && dwc_ep->type == DWC_OTG_EP_TYPE_ISOC) {
+			deptsiz.d32 =
+			    DWC_READ_REG32(&dev_if->in_ep_regs[i]->dieptsiz);
+			depctl.d32 =
+			    DWC_READ_REG32(&dev_if->in_ep_regs[i]->diepctl);
+
+			if (depctl.b.epdis && deptsiz.d32) {
+				set_current_pkt_info(GET_CORE_IF(pcd), dwc_ep);
+				if (dwc_ep->cur_pkt >= dwc_ep->pkt_cnt) {
+					dwc_ep->cur_pkt = 0;
+					dwc_ep->proc_buf_num =
+					    (dwc_ep->proc_buf_num ^ 1) & 0x1;
+
+					if (dwc_ep->proc_buf_num) {
+						dwc_ep->cur_pkt_addr =
+						    dwc_ep->xfer_buff1;
+						dwc_ep->cur_pkt_dma_addr =
+						    dwc_ep->dma_addr1;
+					} else {
+						dwc_ep->cur_pkt_addr =
+						    dwc_ep->xfer_buff0;
+						dwc_ep->cur_pkt_dma_addr =
+						    dwc_ep->dma_addr0;
+					}
+
+				}
+
+				dsts.d32 =
+				    DWC_READ_REG32(&GET_CORE_IF(pcd)->dev_if->
+						   dev_global_regs->dsts);
+				dwc_ep->next_frame = dsts.b.soffn;
+
+				dwc_otg_iso_ep_start_frm_transfer(GET_CORE_IF
+								  (pcd),
+								  dwc_ep);
+			}
+		}
+	}
+
+#else
+	depctl_data_t depctl = {.d32 = 0 };
+	dwc_ep_t *dwc_ep;
+	dwc_otg_dev_if_t *dev_if;
+	int i;
+	dev_if = GET_CORE_IF(pcd)->dev_if;
+
+	DWC_DEBUGPL(DBG_PCD,"Incomplete ISO IN \n");
+
+	for (i = 1; i <= dev_if->num_in_eps; ++i) {
+		dwc_ep = &pcd->in_ep[i-1].dwc_ep;
+		depctl.d32 =
+			DWC_READ_REG32(&dev_if->in_ep_regs[i]->diepctl);
+		if (depctl.b.epena && dwc_ep->type == DWC_OTG_EP_TYPE_ISOC) {
+			if (drop_transfer(dwc_ep->frame_num, GET_CORE_IF(pcd)->frame_num,
+							dwc_ep->frm_overrun))
+			{
+				depctl.d32 =
+					DWC_READ_REG32(&dev_if->in_ep_regs[i]->diepctl);
+				depctl.b.snak = 1;
+				depctl.b.epdis = 1;
+				DWC_MODIFY_REG32(&dev_if->in_ep_regs[i]->diepctl, depctl.d32, depctl.d32);
+			}
+		}
+	}
+
+	/*intr_mask.b.incomplisoin = 1;
+	DWC_MODIFY_REG32(&GET_CORE_IF(pcd)->core_global_regs->gintmsk,
+			 intr_mask.d32, 0);	 */
+#endif				//DWC_EN_ISOC
+
+	/* Clear interrupt */
+	gintsts.d32 = 0;
+	gintsts.b.incomplisoin = 1;
+	DWC_WRITE_REG32(&GET_CORE_IF(pcd)->core_global_regs->gintsts,
+			gintsts.d32);
+
+	return 1;
+}
+
+/**
+ * Incomplete ISO OUT Transfer Interrupt.
+ *
+ * This interrupt indicates that the core has dropped an ISO OUT
+ * packet. The following conditions can be the cause:
+ * - FIFO Full, the entire packet would not fit in the FIFO.
+ * - CRC Error
+ * - Corrupted Token
+ * The follow actions will be taken:
+ *	-#	Determine the EP
+ *	-#	Set incomplete flag in dwc_ep structure
+ *	-#	Read any data from the FIFO
+ *	-#	Disable EP. When "Endpoint Disabled" interrupt is received
+ *		re-enable EP.
+ */
+int32_t dwc_otg_pcd_handle_incomplete_isoc_out_intr(dwc_otg_pcd_t * pcd)
+{
+
+	gintsts_data_t gintsts;
+
+#ifdef DWC_EN_ISOC
+	dwc_otg_dev_if_t *dev_if;
+	deptsiz_data_t deptsiz = {.d32 = 0 };
+	depctl_data_t depctl = {.d32 = 0 };
+	dsts_data_t dsts = {.d32 = 0 };
+	dwc_ep_t *dwc_ep;
+	int i;
+
+	dev_if = GET_CORE_IF(pcd)->dev_if;
+
+	for (i = 1; i <= dev_if->num_out_eps; ++i) {
+		dwc_ep = &pcd->in_ep[i].dwc_ep;
+		if (pcd->out_ep[i].dwc_ep.active &&
+		    pcd->out_ep[i].dwc_ep.type == DWC_OTG_EP_TYPE_ISOC) {
+			deptsiz.d32 =
+			    DWC_READ_REG32(&dev_if->out_ep_regs[i]->doeptsiz);
+			depctl.d32 =
+			    DWC_READ_REG32(&dev_if->out_ep_regs[i]->doepctl);
+
+			if (depctl.b.epdis && deptsiz.d32) {
+				set_current_pkt_info(GET_CORE_IF(pcd),
+						     &pcd->out_ep[i].dwc_ep);
+				if (dwc_ep->cur_pkt >= dwc_ep->pkt_cnt) {
+					dwc_ep->cur_pkt = 0;
+					dwc_ep->proc_buf_num =
+					    (dwc_ep->proc_buf_num ^ 1) & 0x1;
+
+					if (dwc_ep->proc_buf_num) {
+						dwc_ep->cur_pkt_addr =
+						    dwc_ep->xfer_buff1;
+						dwc_ep->cur_pkt_dma_addr =
+						    dwc_ep->dma_addr1;
+					} else {
+						dwc_ep->cur_pkt_addr =
+						    dwc_ep->xfer_buff0;
+						dwc_ep->cur_pkt_dma_addr =
+						    dwc_ep->dma_addr0;
+					}
+
+				}
+
+				dsts.d32 =
+				    DWC_READ_REG32(&GET_CORE_IF(pcd)->dev_if->
+						   dev_global_regs->dsts);
+				dwc_ep->next_frame = dsts.b.soffn;
+
+				dwc_otg_iso_ep_start_frm_transfer(GET_CORE_IF
+								  (pcd),
+								  dwc_ep);
+			}
+		}
+	}
+#else
+	/** @todo implement ISR */
+	gintmsk_data_t intr_mask = {.d32 = 0 };
+	dwc_otg_core_if_t *core_if;
+	deptsiz_data_t deptsiz = {.d32 = 0 };
+	depctl_data_t depctl = {.d32 = 0 };
+	dctl_data_t dctl = {.d32 = 0 };
+	dwc_ep_t *dwc_ep = NULL;
+	int i;
+	core_if = GET_CORE_IF(pcd);
+
+	for (i = 0; i < core_if->dev_if->num_out_eps; ++i) {
+		dwc_ep = &pcd->out_ep[i].dwc_ep;
+		depctl.d32 =
+			DWC_READ_REG32(&core_if->dev_if->out_ep_regs[dwc_ep->num]->doepctl);
+		if (depctl.b.epena && depctl.b.dpid == (core_if->frame_num & 0x1)) {
+			core_if->dev_if->isoc_ep = dwc_ep;
+			deptsiz.d32 =
+					DWC_READ_REG32(&core_if->dev_if->out_ep_regs[dwc_ep->num]->doeptsiz);
+				break;
+		}
+	}
+	dctl.d32 = DWC_READ_REG32(&core_if->dev_if->dev_global_regs->dctl);
+	gintsts.d32 = DWC_READ_REG32(&core_if->core_global_regs->gintsts);
+	intr_mask.d32 = DWC_READ_REG32(&core_if->core_global_regs->gintmsk);
+
+	if (!intr_mask.b.goutnakeff) {
+		/* Unmask it */
+		intr_mask.b.goutnakeff = 1;
+		DWC_WRITE_REG32(&core_if->core_global_regs->gintmsk, intr_mask.d32);
+ 	}
+	if (!gintsts.b.goutnakeff) {
+		dctl.b.sgoutnak = 1;
+	}
+	DWC_WRITE_REG32(&core_if->dev_if->dev_global_regs->dctl, dctl.d32);
+
+	depctl.d32 = DWC_READ_REG32(&core_if->dev_if->out_ep_regs[dwc_ep->num]->doepctl);
+	if (depctl.b.epena) {
+		depctl.b.epdis = 1;
+		depctl.b.snak = 1;
+	}
+	DWC_WRITE_REG32(&core_if->dev_if->out_ep_regs[dwc_ep->num]->doepctl, depctl.d32);
+
+	intr_mask.d32 = 0;
+	intr_mask.b.incomplisoout = 1;
+
+#endif /* DWC_EN_ISOC */
+
+	/* Clear interrupt */
+	gintsts.d32 = 0;
+	gintsts.b.incomplisoout = 1;
+	DWC_WRITE_REG32(&GET_CORE_IF(pcd)->core_global_regs->gintsts,
+			gintsts.d32);
+
+	return 1;
+}
+
+/**
+ * This function handles the Global IN NAK Effective interrupt.
+ *
+ */
+int32_t dwc_otg_pcd_handle_in_nak_effective(dwc_otg_pcd_t * pcd)
+{
+	dwc_otg_dev_if_t *dev_if = GET_CORE_IF(pcd)->dev_if;
+	depctl_data_t diepctl = {.d32 = 0 };
+	gintmsk_data_t intr_mask = {.d32 = 0 };
+	gintsts_data_t gintsts;
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(pcd);
+	int i;
+
+	DWC_DEBUGPL(DBG_PCD, "Global IN NAK Effective\n");
+
+	/* Disable all active IN EPs */
+	for (i = 0; i <= dev_if->num_in_eps; i++) {
+		diepctl.d32 = DWC_READ_REG32(&dev_if->in_ep_regs[i]->diepctl);
+		if (!(diepctl.b.eptype & 1) && diepctl.b.epena) {
+			if (core_if->start_predict > 0)
+				core_if->start_predict++;
+			diepctl.b.epdis = 1;
+			diepctl.b.snak = 1;
+			DWC_WRITE_REG32(&dev_if->in_ep_regs[i]->diepctl, diepctl.d32);
+		}
+	}
+
+
+	/* Disable the Global IN NAK Effective Interrupt */
+	intr_mask.b.ginnakeff = 1;
+	DWC_MODIFY_REG32(&GET_CORE_IF(pcd)->core_global_regs->gintmsk,
+			 intr_mask.d32, 0);
+
+	/* Clear interrupt */
+	gintsts.d32 = 0;
+	gintsts.b.ginnakeff = 1;
+	DWC_WRITE_REG32(&GET_CORE_IF(pcd)->core_global_regs->gintsts,
+			gintsts.d32);
+
+	return 1;
+}
+
+/**
+ * OUT NAK Effective.
+ *
+ */
+int32_t dwc_otg_pcd_handle_out_nak_effective(dwc_otg_pcd_t * pcd)
+{
+	dwc_otg_dev_if_t *dev_if = GET_CORE_IF(pcd)->dev_if;
+	gintmsk_data_t intr_mask = {.d32 = 0 };
+	gintsts_data_t gintsts;
+	depctl_data_t doepctl;
+	int i;
+
+	/* Disable the Global OUT NAK Effective Interrupt */
+	intr_mask.b.goutnakeff = 1;
+	DWC_MODIFY_REG32(&GET_CORE_IF(pcd)->core_global_regs->gintmsk,
+		intr_mask.d32, 0);
+
+	/* If DEV OUT NAK enabled*/
+	if (pcd->core_if->core_params->dev_out_nak) {
+		/* Run over all out endpoints to determine the ep number on
+		 * which the timeout has happened
+		 */
+		for (i = 0; i <= dev_if->num_out_eps; i++) {
+			if ( pcd->core_if->ep_xfer_info[i].state == 2 )
+				break;
+		}
+		if (i > dev_if->num_out_eps) {
+			dctl_data_t dctl;
+			dctl.d32 = DWC_READ_REG32(&dev_if->
+				dev_global_regs->dctl);
+			dctl.b.cgoutnak = 1;
+			DWC_WRITE_REG32(&dev_if->dev_global_regs->dctl,
+				dctl.d32);
+			goto out;
+		}
+
+		/* Disable the endpoint */
+		doepctl.d32 = DWC_READ_REG32(&dev_if->
+										out_ep_regs[i]->doepctl);
+		if (doepctl.b.epena) {
+			doepctl.b.epdis = 1;
+			doepctl.b.snak = 1;
+		}
+		DWC_WRITE_REG32(&dev_if->out_ep_regs[i]->doepctl, doepctl.d32);
+		return 1;
+	}
+	/* We come here from Incomplete ISO OUT handler */
+	if(dev_if->isoc_ep)
+	{
+		dwc_ep_t *dwc_ep = (dwc_ep_t *)dev_if->isoc_ep;
+		uint32_t epnum = dwc_ep->num;
+		doepint_data_t doepint;
+		doepint.d32 = DWC_READ_REG32(&dev_if->out_ep_regs[dwc_ep->num]->doepint);
+		dev_if->isoc_ep = NULL;
+		doepctl.d32 = DWC_READ_REG32(&dev_if->out_ep_regs[epnum]->doepctl);
+		DWC_PRINTF("Before disable DOEPCTL = %08x\n", doepctl.d32);
+		if (doepctl.b.epena) {
+			doepctl.b.epdis = 1;
+			doepctl.b.snak = 1;
+		}
+		DWC_WRITE_REG32(&dev_if->out_ep_regs[epnum]->doepctl, doepctl.d32);
+		return 1;
+	} else
+		DWC_PRINTF("INTERRUPT Handler not implemented for %s\n",
+			   "Global OUT NAK Effective\n");
+
+out:
+	/* Clear interrupt */
+	gintsts.d32 = 0;
+	gintsts.b.goutnakeff = 1;
+	DWC_WRITE_REG32(&GET_CORE_IF(pcd)->core_global_regs->gintsts,
+			gintsts.d32);
+
+	return 1;
+}
+
+/**
+ * PCD interrupt handler.
+ *
+ * The PCD handles the device interrupts.  Many conditions can cause a
+ * device interrupt. When an interrupt occurs, the device interrupt
+ * service routine determines the cause of the interrupt and
+ * dispatches handling to the appropriate function. These interrupt
+ * handling functions are described below.
+ *
+ * All interrupt registers are processed from LSB to MSB.
+ *
+ */
+int32_t dwc_otg_pcd_handle_intr(dwc_otg_pcd_t * pcd)
+{
+	dwc_otg_core_if_t *core_if = GET_CORE_IF(pcd);
+#ifdef VERBOSE
+	dwc_otg_core_global_regs_t *global_regs = core_if->core_global_regs;
+#endif
+	gintsts_data_t gintr_status;
+	int32_t retval = 0;
+
+	/* Exit from ISR if core is hibernated */
+	if (core_if->hibernation_suspend == 1) {
+		return retval;
+	}
+#ifdef VERBOSE
+	DWC_DEBUGPL(DBG_ANY, "%s() gintsts=%08x	 gintmsk=%08x\n",
+		    __func__,
+		    DWC_READ_REG32(&global_regs->gintsts),
+		    DWC_READ_REG32(&global_regs->gintmsk));
+#endif
+
+	if (dwc_otg_is_device_mode(core_if)) {
+		DWC_SPINLOCK(pcd->lock);
+#ifdef VERBOSE
+		DWC_DEBUGPL(DBG_PCDV, "%s() gintsts=%08x  gintmsk=%08x\n",
+			    __func__,
+			    DWC_READ_REG32(&global_regs->gintsts),
+			    DWC_READ_REG32(&global_regs->gintmsk));
+#endif
+
+		gintr_status.d32 = dwc_otg_read_core_intr(core_if);
+
+		DWC_DEBUGPL(DBG_PCDV, "%s: gintsts&gintmsk=%08x\n",
+			    __func__, gintr_status.d32);
+
+		if (gintr_status.b.sofintr) {
+			retval |= dwc_otg_pcd_handle_sof_intr(pcd);
+		}
+		if (gintr_status.b.rxstsqlvl) {
+			retval |=
+			    dwc_otg_pcd_handle_rx_status_q_level_intr(pcd);
+		}
+		if (gintr_status.b.nptxfempty) {
+			retval |= dwc_otg_pcd_handle_np_tx_fifo_empty_intr(pcd);
+		}
+		if (gintr_status.b.goutnakeff) {
+			retval |= dwc_otg_pcd_handle_out_nak_effective(pcd);
+		}
+		if (gintr_status.b.i2cintr) {
+			retval |= dwc_otg_pcd_handle_i2c_intr(pcd);
+		}
+		if (gintr_status.b.erlysuspend) {
+			retval |= dwc_otg_pcd_handle_early_suspend_intr(pcd);
+		}
+		if (gintr_status.b.usbreset) {
+			retval |= dwc_otg_pcd_handle_usb_reset_intr(pcd);
+		}
+		if (gintr_status.b.enumdone) {
+			retval |= dwc_otg_pcd_handle_enum_done_intr(pcd);
+		}
+		if (gintr_status.b.isooutdrop) {
+			retval |=
+			    dwc_otg_pcd_handle_isoc_out_packet_dropped_intr
+			    (pcd);
+		}
+		if (gintr_status.b.eopframe) {
+			retval |=
+			    dwc_otg_pcd_handle_end_periodic_frame_intr(pcd);
+		}
+		if (gintr_status.b.inepint) {
+			if (!core_if->multiproc_int_enable) {
+				retval |= dwc_otg_pcd_handle_in_ep_intr(pcd);
+			}
+		}
+		if (gintr_status.b.outepintr) {
+			if (!core_if->multiproc_int_enable) {
+				retval |= dwc_otg_pcd_handle_out_ep_intr(pcd);
+			}
+		}
+		if (gintr_status.b.epmismatch) {
+			retval |= dwc_otg_pcd_handle_ep_mismatch_intr(pcd);
+		}
+		if (gintr_status.b.fetsusp) {
+			retval |= dwc_otg_pcd_handle_ep_fetsusp_intr(pcd);
+		}
+		if (gintr_status.b.ginnakeff) {
+			retval |= dwc_otg_pcd_handle_in_nak_effective(pcd);
+		}
+		if (gintr_status.b.incomplisoin) {
+			retval |=
+			    dwc_otg_pcd_handle_incomplete_isoc_in_intr(pcd);
+		}
+		if (gintr_status.b.incomplisoout) {
+			retval |=
+			    dwc_otg_pcd_handle_incomplete_isoc_out_intr(pcd);
+		}
+
+		/* In MPI mode Device Endpoints interrupts are asserted
+		 * without setting outepintr and inepint bits set, so these
+		 * Interrupt handlers are called without checking these bit-fields
+		 */
+		if (core_if->multiproc_int_enable) {
+			retval |= dwc_otg_pcd_handle_in_ep_intr(pcd);
+			retval |= dwc_otg_pcd_handle_out_ep_intr(pcd);
+		}
+#ifdef VERBOSE
+		DWC_DEBUGPL(DBG_PCDV, "%s() gintsts=%0x\n", __func__,
+			    DWC_READ_REG32(&global_regs->gintsts));
+#endif
+		DWC_SPINUNLOCK(pcd->lock);
+	}
+	return retval;
+}
+
+#endif /* DWC_HOST_ONLY */
diff --git a/drivers/usb/dwc_otg/dwc_otg_pcd_linux.c b/drivers/usb/dwc_otg/dwc_otg_pcd_linux.c
new file mode 100644
index 0000000..387d725
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_otg_pcd_linux.c
@@ -0,0 +1,1320 @@
+ /* ==========================================================================
+  * $File: //dwh/usb_iip/dev/software/otg/linux/drivers/dwc_otg_pcd_linux.c $
+  * $Revision: #19 $
+  * $Date: 2011/10/26 $
+  * $Change: 1873028 $
+  *
+  * Synopsys HS OTG Linux Software Driver and documentation (hereinafter,
+  * "Software") is an Unsupported proprietary work of Synopsys, Inc. unless
+  * otherwise expressly agreed to in writing between Synopsys and you.
+  *
+  * The Software IS NOT an item of Licensed Software or Licensed Product under
+  * any End User Software License Agreement or Agreement for Licensed Product
+  * with Synopsys or any supplement thereto. You are permitted to use and
+  * redistribute this Software in source and binary forms, with or without
+  * modification, provided that redistributions of source code must retain this
+  * notice. You may not view, use, disclose, copy or distribute this file or
+  * any information contained herein except pursuant to this license grant from
+  * Synopsys. If you do not agree with this notice, including the disclaimer
+  * below, then you are not authorized to use the Software.
+  *
+  * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS" BASIS
+  * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+  * ARE HEREBY DISCLAIMED. IN NO EVENT SHALL SYNOPSYS BE LIABLE FOR ANY DIRECT,
+  * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+  * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+  * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+  * DAMAGE.
+  * ========================================================================== */
+#ifndef DWC_HOST_ONLY
+
+/** @file
+ * This file implements the Peripheral Controller Driver.
+ *
+ * The Peripheral Controller Driver (PCD) is responsible for
+ * translating requests from the Function Driver into the appropriate
+ * actions on the DWC_otg controller. It isolates the Function Driver
+ * from the specifics of the controller by providing an API to the
+ * Function Driver.
+ *
+ * The Peripheral Controller Driver for Linux will implement the
+ * Gadget API, so that the existing Gadget drivers can be used.
+ * (Gadget Driver is the Linux terminology for a Function Driver.)
+ *
+ * The Linux Gadget API is defined in the header file
+ * <code><linux/usb_gadget.h></code>.  The USB EP operations API is
+ * defined in the structure <code>usb_ep_ops</code> and the USB
+ * Controller API is defined in the structure
+ * <code>usb_gadget_ops</code>.
+ *
+ */
+
+#include "dwc_otg_os_dep.h"
+#include "dwc_otg_pcd_if.h"
+#include "dwc_otg_pcd.h"
+#include "dwc_otg_driver.h"
+#include "dwc_otg_dbg.h"
+
+static struct gadget_wrapper {
+	dwc_otg_pcd_t *pcd;
+
+	struct usb_gadget gadget;
+	struct usb_gadget_driver *driver;
+
+	struct usb_ep ep0;
+	struct usb_ep in_ep[16];
+	struct usb_ep out_ep[16];
+
+} *gadget_wrapper;
+
+/* Display the contents of the buffer */
+extern void dump_msg(const u8 * buf, unsigned int length);
+/**
+ * Get the dwc_otg_pcd_ep_t* from usb_ep* pointer - NULL in case
+ * if the endpoint is not found
+ */
+static struct dwc_otg_pcd_ep *ep_from_handle(dwc_otg_pcd_t * pcd, void *handle)
+{
+	int i;
+	if (pcd->ep0.priv == handle) {
+		return &pcd->ep0;
+	}
+
+	for (i = 0; i < MAX_EPS_CHANNELS - 1; i++) {
+		if (pcd->in_ep[i].priv == handle)
+			return &pcd->in_ep[i];
+		if (pcd->out_ep[i].priv == handle)
+			return &pcd->out_ep[i];
+	}
+
+	return NULL;
+}
+
+/* USB Endpoint Operations */
+/*
+ * The following sections briefly describe the behavior of the Gadget
+ * API endpoint operations implemented in the DWC_otg driver
+ * software. Detailed descriptions of the generic behavior of each of
+ * these functions can be found in the Linux header file
+ * include/linux/usb_gadget.h.
+ *
+ * The Gadget API provides wrapper functions for each of the function
+ * pointers defined in usb_ep_ops. The Gadget Driver calls the wrapper
+ * function, which then calls the underlying PCD function. The
+ * following sections are named according to the wrapper
+ * functions. Within each section, the corresponding DWC_otg PCD
+ * function name is specified.
+ *
+ */
+
+/**
+ * This function is called by the Gadget Driver for each EP to be
+ * configured for the current configuration (SET_CONFIGURATION).
+ *
+ * This function initializes the dwc_otg_ep_t data structure, and then
+ * calls dwc_otg_ep_activate.
+ */
+static int ep_enable(struct usb_ep *usb_ep,
+		     const struct usb_endpoint_descriptor *ep_desc)
+{
+	int retval;
+
+	DWC_DEBUGPL(DBG_PCDV, "%s(%p,%p)\n", __func__, usb_ep, ep_desc);
+
+	if (!usb_ep || !ep_desc || ep_desc->bDescriptorType != USB_DT_ENDPOINT) {
+		DWC_WARN("%s, bad ep or descriptor\n", __func__);
+		return -EINVAL;
+	}
+	if (usb_ep == &gadget_wrapper->ep0) {
+		DWC_WARN("%s, bad ep(0)\n", __func__);
+		return -EINVAL;
+	}
+
+	/* Check FIFO size? */
+	if (!ep_desc->wMaxPacketSize) {
+		DWC_WARN("%s, bad %s maxpacket\n", __func__, usb_ep->name);
+		return -ERANGE;
+	}
+
+	if (!gadget_wrapper->driver ||
+	    gadget_wrapper->gadget.speed == USB_SPEED_UNKNOWN) {
+		DWC_WARN("%s, bogus device state\n", __func__);
+		return -ESHUTDOWN;
+	}
+
+	/* Delete after check - MAS */
+#if 0
+	nat = (uint32_t) ep_desc->wMaxPacketSize;
+	printk(KERN_ALERT "%s: nat (before) =%d\n", __func__, nat);
+	nat = (nat >> 11) & 0x03;
+	printk(KERN_ALERT "%s: nat (after) =%d\n", __func__, nat);
+#endif
+	retval = dwc_otg_pcd_ep_enable(gadget_wrapper->pcd,
+				       (const uint8_t *)ep_desc,
+				       (void *)usb_ep);
+	if (retval) {
+		DWC_WARN("dwc_otg_pcd_ep_enable failed\n");
+		return -EINVAL;
+	}
+
+	usb_ep->maxpacket = le16_to_cpu(ep_desc->wMaxPacketSize);
+
+	return 0;
+}
+
+/**
+ * This function is called when an EP is disabled due to disconnect or
+ * change in configuration. Any pending requests will terminate with a
+ * status of -ESHUTDOWN.
+ *
+ * This function modifies the dwc_otg_ep_t data structure for this EP,
+ * and then calls dwc_otg_ep_deactivate.
+ */
+static int ep_disable(struct usb_ep *usb_ep)
+{
+	int retval;
+
+	DWC_DEBUGPL(DBG_PCDV, "%s(%p)\n", __func__, usb_ep);
+	if (!usb_ep) {
+		DWC_DEBUGPL(DBG_PCD, "%s, %s not enabled\n", __func__,
+			    usb_ep ? usb_ep->name : NULL);
+		return -EINVAL;
+	}
+
+	retval = dwc_otg_pcd_ep_disable(gadget_wrapper->pcd, usb_ep);
+	if (retval) {
+		retval = -EINVAL;
+	}
+
+	return retval;
+}
+
+/**
+ * This function allocates a request object to use with the specified
+ * endpoint.
+ *
+ * @param ep The endpoint to be used with with the request
+ * @param gfp_flags the GFP_* flags to use.
+ */
+static struct usb_request *dwc_otg_pcd_alloc_request(struct usb_ep *ep,
+						     gfp_t gfp_flags)
+{
+	struct usb_request *usb_req;
+
+	DWC_DEBUGPL(DBG_PCDV, "%s(%p,%d)\n", __func__, ep, gfp_flags);
+	if (0 == ep) {
+		DWC_WARN("%s() %s\n", __func__, "Invalid EP!\n");
+		return 0;
+	}
+	usb_req = kmalloc(sizeof(*usb_req), gfp_flags);
+	if (0 == usb_req) {
+		DWC_WARN("%s() %s\n", __func__, "request allocation failed!\n");
+		return 0;
+	}
+	memset(usb_req, 0, sizeof(*usb_req));
+	usb_req->dma = DWC_DMA_ADDR_INVALID;
+
+	return usb_req;
+}
+
+/**
+ * This function frees a request object.
+ *
+ * @param ep The endpoint associated with the request
+ * @param req The request being freed
+ */
+static void dwc_otg_pcd_free_request(struct usb_ep *ep, struct usb_request *req)
+{
+	DWC_DEBUGPL(DBG_PCDV, "%s(%p,%p)\n", __func__, ep, req);
+
+	if (0 == ep || 0 == req) {
+		DWC_WARN("%s() %s\n", __func__,
+			 "Invalid ep or req argument!\n");
+		return;
+	}
+
+	kfree(req);
+}
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,28)
+/**
+ * This function allocates an I/O buffer to be used for a transfer
+ * to/from the specified endpoint.
+ *
+ * @param usb_ep The endpoint to be used with with the request
+ * @param bytes The desired number of bytes for the buffer
+ * @param dma Pointer to the buffer's DMA address; must be valid
+ * @param gfp_flags the GFP_* flags to use.
+ * @return address of a new buffer or null is buffer could not be allocated.
+ */
+static void *dwc_otg_pcd_alloc_buffer(struct usb_ep *usb_ep, unsigned bytes,
+				      dma_addr_t * dma, gfp_t gfp_flags)
+{
+	void *buf;
+	dwc_otg_pcd_t *pcd = 0;
+
+	pcd = gadget_wrapper->pcd;
+
+	DWC_DEBUGPL(DBG_PCDV, "%s(%p,%d,%p,%0x)\n", __func__, usb_ep, bytes,
+		    dma, gfp_flags);
+
+	/* Check dword alignment */
+	if ((bytes & 0x3UL) != 0) {
+		DWC_WARN("%s() Buffer size is not a multiple of"
+			 "DWORD size (%d)", __func__, bytes);
+	}
+
+	buf = dma_alloc_coherent(NULL, bytes, dma, gfp_flags);
+
+	/* Check dword alignment */
+	if (((int)buf & 0x3UL) != 0) {
+		DWC_WARN("%s() Buffer is not DWORD aligned (%p)",
+			 __func__, buf);
+	}
+
+	return buf;
+}
+
+/**
+ * This function frees an I/O buffer that was allocated by alloc_buffer.
+ *
+ * @param usb_ep the endpoint associated with the buffer
+ * @param buf address of the buffer
+ * @param dma The buffer's DMA address
+ * @param bytes The number of bytes of the buffer
+ */
+static void dwc_otg_pcd_free_buffer(struct usb_ep *usb_ep, void *buf,
+				    dma_addr_t dma, unsigned bytes)
+{
+	dwc_otg_pcd_t *pcd = 0;
+
+	pcd = gadget_wrapper->pcd;
+
+	DWC_DEBUGPL(DBG_PCDV, "%s(%p,%0x,%d)\n", __func__, buf, dma, bytes);
+
+	dma_free_coherent(NULL, bytes, buf, dma);
+}
+#endif
+
+/**
+ * This function is used to submit an I/O Request to an EP.
+ *
+ *	- When the request completes the request's completion callback
+ *	  is called to return the request to the driver.
+ *	- An EP, except control EPs, may have multiple requests
+ *	  pending.
+ *	- Once submitted the request cannot be examined or modified.
+ *	- Each request is turned into one or more packets.
+ *	- A BULK EP can queue any amount of data; the transfer is
+ *	  packetized.
+ *	- Zero length Packets are specified with the request 'zero'
+ *	  flag.
+ */
+static int ep_queue(struct usb_ep *usb_ep, struct usb_request *usb_req,
+		    gfp_t gfp_flags)
+{
+	dwc_otg_pcd_t *pcd;
+	struct dwc_otg_pcd_ep *ep = NULL;
+	int retval = 0, is_isoc_ep = 0;
+	dma_addr_t dma_addr = DWC_DMA_ADDR_INVALID;
+
+	DWC_DEBUGPL(DBG_PCDV, "%s(%p,%p,%d)\n",
+		    __func__, usb_ep, usb_req, gfp_flags);
+
+	if (!usb_req || !usb_req->complete || !usb_req->buf) {
+		DWC_WARN("bad params\n");
+		return -EINVAL;
+	}
+
+	if (!usb_ep) {
+		DWC_WARN("bad ep\n");
+		return -EINVAL;
+	}
+
+	pcd = gadget_wrapper->pcd;
+	if (!gadget_wrapper->driver ||
+	    gadget_wrapper->gadget.speed == USB_SPEED_UNKNOWN) {
+		DWC_DEBUGPL(DBG_PCDV, "gadget.speed=%d\n",
+			    gadget_wrapper->gadget.speed);
+		DWC_WARN("bogus device state\n");
+		return -ESHUTDOWN;
+	}
+
+	DWC_DEBUGPL(DBG_PCD, "%s queue req %p, len %d buf %p\n",
+		    usb_ep->name, usb_req, usb_req->length, usb_req->buf);
+
+	usb_req->status = -EINPROGRESS;
+	usb_req->actual = 0;
+
+	ep = ep_from_handle(pcd, usb_ep);
+	if (ep == NULL)
+		is_isoc_ep = 0;
+	else
+		is_isoc_ep = (ep->dwc_ep.type == DWC_OTG_EP_TYPE_ISOC) ? 1 : 0;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,28)
+	dma_addr = usb_req->dma;
+#else
+	if (GET_CORE_IF(pcd)->dma_enable) {
+		struct pci_dev *dev = gadget_wrapper->pcd->otg_dev->os_dep.pcidev;
+		if (usb_req->length != 0 && usb_req->dma == DWC_DMA_ADDR_INVALID) {
+			dma_addr = pci_map_single(dev, usb_req->buf, usb_req->length,
+					ep->dwc_ep.is_in ? PCI_DMA_TODEVICE : PCI_DMA_FROMDEVICE);
+		}
+	}
+#endif
+
+#ifdef DWC_UTE_PER_IO
+	if (is_isoc_ep == 1) {
+		retval = dwc_otg_pcd_xiso_ep_queue(pcd, usb_ep, usb_req->buf, dma_addr,
+			usb_req->length, usb_req->zero, usb_req,
+			gfp_flags == GFP_ATOMIC ? 1 : 0, &usb_req->ext_req);
+		if (retval)
+			return -EINVAL;
+
+		return 0;
+	}
+#endif
+	retval = dwc_otg_pcd_ep_queue(pcd, usb_ep, usb_req->buf, dma_addr,
+				      usb_req->length, usb_req->zero, usb_req,
+				      gfp_flags == GFP_ATOMIC ? 1 : 0);
+	if (retval) {
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+/**
+ * This function cancels an I/O request from an EP.
+ */
+static int ep_dequeue(struct usb_ep *usb_ep, struct usb_request *usb_req)
+{
+	DWC_DEBUGPL(DBG_PCDV, "%s(%p,%p)\n", __func__, usb_ep, usb_req);
+
+	if (!usb_ep || !usb_req) {
+		DWC_WARN("bad argument\n");
+		return -EINVAL;
+	}
+	if (!gadget_wrapper->driver ||
+	    gadget_wrapper->gadget.speed == USB_SPEED_UNKNOWN) {
+		DWC_WARN("bogus device state\n");
+		return -ESHUTDOWN;
+	}
+	if (dwc_otg_pcd_ep_dequeue(gadget_wrapper->pcd, usb_ep, usb_req)) {
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+/**
+ * usb_ep_set_halt stalls an endpoint.
+ *
+ * usb_ep_clear_halt clears an endpoint halt and resets its data
+ * toggle.
+ *
+ * Both of these functions are implemented with the same underlying
+ * function. The behavior depends on the value argument.
+ *
+ * @param[in] usb_ep the Endpoint to halt or clear halt.
+ * @param[in] value
+ *	- 0 means clear_halt.
+ *	- 1 means set_halt,
+ *	- 2 means clear stall lock flag.
+ *	- 3 means set  stall lock flag.
+ */
+static int ep_halt(struct usb_ep *usb_ep, int value)
+{
+	int retval = 0;
+
+	DWC_DEBUGPL(DBG_PCD, "HALT %s %d\n", usb_ep->name, value);
+
+	if (!usb_ep) {
+		DWC_WARN("bad ep\n");
+		return -EINVAL;
+	}
+
+	retval = dwc_otg_pcd_ep_halt(gadget_wrapper->pcd, usb_ep, value);
+	if (retval == -DWC_E_AGAIN) {
+		return -EAGAIN;
+	} else if (retval) {
+		retval = -EINVAL;
+	}
+
+	return retval;
+}
+
+#ifdef DWC_EN_ISOC
+/**
+ * This function is used to submit an ISOC Transfer Request to an EP.
+ *
+ *	- Every time a sync period completes the request's completion callback
+ *	  is called to provide data to the gadget driver.
+ *	- Once submitted the request cannot be modified.
+ *	- Each request is turned into periodic data packets untill ISO
+ *	  Transfer is stopped..
+ */
+static int iso_ep_start(struct usb_ep *usb_ep, struct usb_iso_request *req,
+			gfp_t gfp_flags)
+{
+	int retval = 0;
+
+	if (!req || !req->process_buffer || !req->buf0 || !req->buf1) {
+		DWC_WARN("bad params\n");
+		return -EINVAL;
+	}
+
+	if (!usb_ep) {
+		DWC_PRINTF("bad params\n");
+		return -EINVAL;
+	}
+
+	req->status = -EINPROGRESS;
+
+	retval =
+	    dwc_otg_pcd_iso_ep_start(gadget_wrapper->pcd, usb_ep, req->buf0,
+				     req->buf1, req->dma0, req->dma1,
+				     req->sync_frame, req->data_pattern_frame,
+				     req->data_per_frame,
+				     req->flags & USB_REQ_ISO_ASAP ? -1 : req->
+				     start_frame, req->buf_proc_intrvl, req,
+				     gfp_flags == GFP_ATOMIC ? 1 : 0);
+
+	if (retval) {
+		return -EINVAL;
+	}
+
+	return retval;
+}
+
+/**
+ * This function stops ISO EP Periodic Data Transfer.
+ */
+static int iso_ep_stop(struct usb_ep *usb_ep, struct usb_iso_request *req)
+{
+	int retval = 0;
+	if (!usb_ep) {
+		DWC_WARN("bad ep\n");
+	}
+
+	if (!gadget_wrapper->driver ||
+	    gadget_wrapper->gadget.speed == USB_SPEED_UNKNOWN) {
+		DWC_DEBUGPL(DBG_PCDV, "gadget.speed=%d\n",
+			    gadget_wrapper->gadget.speed);
+		DWC_WARN("bogus device state\n");
+	}
+
+	dwc_otg_pcd_iso_ep_stop(gadget_wrapper->pcd, usb_ep, req);
+	if (retval) {
+		retval = -EINVAL;
+	}
+
+	return retval;
+}
+
+static struct usb_iso_request *alloc_iso_request(struct usb_ep *ep,
+						 int packets, gfp_t gfp_flags)
+{
+	struct usb_iso_request *pReq = NULL;
+	uint32_t req_size;
+
+	req_size = sizeof(struct usb_iso_request);
+	req_size +=
+	    (2 * packets * (sizeof(struct usb_gadget_iso_packet_descriptor)));
+
+	pReq = kmalloc(req_size, gfp_flags);
+	if (!pReq) {
+		DWC_WARN("Can't allocate Iso Request\n");
+		return 0;
+	}
+	pReq->iso_packet_desc0 = (void *)(pReq + 1);
+
+	pReq->iso_packet_desc1 = pReq->iso_packet_desc0 + packets;
+
+	return pReq;
+}
+
+static void free_iso_request(struct usb_ep *ep, struct usb_iso_request *req)
+{
+	kfree(req);
+}
+
+static struct usb_isoc_ep_ops dwc_otg_pcd_ep_ops = {
+	.ep_ops = {
+		   .enable = ep_enable,
+		   .disable = ep_disable,
+
+		   .alloc_request = dwc_otg_pcd_alloc_request,
+		   .free_request = dwc_otg_pcd_free_request,
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,28)
+		   .alloc_buffer = dwc_otg_pcd_alloc_buffer,
+		   .free_buffer = dwc_otg_pcd_free_buffer,
+#endif
+
+		   .queue = ep_queue,
+		   .dequeue = ep_dequeue,
+
+		   .set_halt = ep_halt,
+		   .fifo_status = 0,
+		   .fifo_flush = 0,
+		   },
+	.iso_ep_start = iso_ep_start,
+	.iso_ep_stop = iso_ep_stop,
+	.alloc_iso_request = alloc_iso_request,
+	.free_iso_request = free_iso_request,
+};
+
+#else
+
+static struct usb_ep_ops dwc_otg_pcd_ep_ops = {
+	.enable = ep_enable,
+	.disable = ep_disable,
+
+	.alloc_request = dwc_otg_pcd_alloc_request,
+	.free_request = dwc_otg_pcd_free_request,
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,28)
+	.alloc_buffer = dwc_otg_pcd_alloc_buffer,
+	.free_buffer = dwc_otg_pcd_free_buffer,
+#endif
+
+	.queue = ep_queue,
+	.dequeue = ep_dequeue,
+
+	.set_halt = ep_halt,
+	.fifo_status = 0,
+	.fifo_flush = 0,
+
+};
+
+#endif /* _EN_ISOC_ */
+/*	Gadget Operations */
+/**
+ * The following gadget operations will be implemented in the DWC_otg
+ * PCD. Functions in the API that are not described below are not
+ * implemented.
+ *
+ * The Gadget API provides wrapper functions for each of the function
+ * pointers defined in usb_gadget_ops. The Gadget Driver calls the
+ * wrapper function, which then calls the underlying PCD function. The
+ * following sections are named according to the wrapper functions
+ * (except for ioctl, which doesn't have a wrapper function). Within
+ * each section, the corresponding DWC_otg PCD function name is
+ * specified.
+ *
+ */
+
+/**
+ *Gets the USB Frame number of the last SOF.
+ */
+static int get_frame_number(struct usb_gadget *gadget)
+{
+	struct gadget_wrapper *d;
+
+	DWC_DEBUGPL(DBG_PCDV, "%s(%p)\n", __func__, gadget);
+
+	if (gadget == 0) {
+		return -ENODEV;
+	}
+
+	d = container_of(gadget, struct gadget_wrapper, gadget);
+	return dwc_otg_pcd_get_frame_number(d->pcd);
+}
+
+#ifdef CONFIG_USB_DWC_OTG_LPM
+static int test_lpm_enabled(struct usb_gadget *gadget)
+{
+	struct gadget_wrapper *d;
+
+	d = container_of(gadget, struct gadget_wrapper, gadget);
+
+	return dwc_otg_pcd_is_lpm_enabled(d->pcd);
+}
+#endif
+
+/**
+ * Initiates Session Request Protocol (SRP) to wakeup the host if no
+ * session is in progress. If a session is already in progress, but
+ * the device is suspended, remote wakeup signaling is started.
+ *
+ */
+static int wakeup(struct usb_gadget *gadget)
+{
+	struct gadget_wrapper *d;
+
+	DWC_DEBUGPL(DBG_PCDV, "%s(%p)\n", __func__, gadget);
+
+	if (gadget == 0) {
+		return -ENODEV;
+	} else {
+		d = container_of(gadget, struct gadget_wrapper, gadget);
+	}
+	dwc_otg_pcd_wakeup(d->pcd);
+	return 0;
+}
+
+static const struct usb_gadget_ops dwc_otg_pcd_ops = {
+	.get_frame = get_frame_number,
+	.wakeup = wakeup,
+#ifdef CONFIG_USB_DWC_OTG_LPM
+	.lpm_support = test_lpm_enabled,
+#endif
+	// current versions must always be self-powered
+};
+
+static int _setup(dwc_otg_pcd_t * pcd, uint8_t * bytes)
+{
+	int retval = -DWC_E_NOT_SUPPORTED;
+	if (gadget_wrapper->driver && gadget_wrapper->driver->setup) {
+		retval = gadget_wrapper->driver->setup(&gadget_wrapper->gadget,
+						       (struct usb_ctrlrequest
+							*)bytes);
+	}
+
+	if (retval == -ENOTSUPP) {
+		retval = -DWC_E_NOT_SUPPORTED;
+	} else if (retval < 0) {
+		retval = -DWC_E_INVALID;
+	}
+
+	return retval;
+}
+
+#ifdef DWC_EN_ISOC
+static int _isoc_complete(dwc_otg_pcd_t * pcd, void *ep_handle,
+			  void *req_handle, int proc_buf_num)
+{
+	int i, packet_count;
+	struct usb_gadget_iso_packet_descriptor *iso_packet = 0;
+	struct usb_iso_request *iso_req = req_handle;
+
+	if (proc_buf_num) {
+		iso_packet = iso_req->iso_packet_desc1;
+	} else {
+		iso_packet = iso_req->iso_packet_desc0;
+	}
+	packet_count =
+	    dwc_otg_pcd_get_iso_packet_count(pcd, ep_handle, req_handle);
+	for (i = 0; i < packet_count; ++i) {
+		int status;
+		int actual;
+		int offset;
+		dwc_otg_pcd_get_iso_packet_params(pcd, ep_handle, req_handle,
+						  i, &status, &actual, &offset);
+		switch (status) {
+		case -DWC_E_NO_DATA:
+			status = -ENODATA;
+			break;
+		default:
+			if (status) {
+				DWC_PRINTF("unknown status in isoc packet\n");
+			}
+
+		}
+		iso_packet[i].status = status;
+		iso_packet[i].offset = offset;
+		iso_packet[i].actual_length = actual;
+	}
+
+	iso_req->status = 0;
+	iso_req->process_buffer(ep_handle, iso_req);
+
+	return 0;
+}
+#endif /* DWC_EN_ISOC */
+
+#ifdef DWC_UTE_PER_IO
+/**
+ * Copy the contents of the extended request to the Linux usb_request's
+ * extended part and call the gadget's completion.
+ *
+ * @param pcd			Pointer to the pcd structure
+ * @param ep_handle		Void pointer to the usb_ep structure
+ * @param req_handle	Void pointer to the usb_request structure
+ * @param status		Request status returned from the portable logic
+ * @param ereq_port		Void pointer to the extended request structure
+ *						created in the the portable part that contains the
+ *						results of the processed iso packets.
+ */
+static int _xisoc_complete(dwc_otg_pcd_t * pcd, void *ep_handle,
+			   void *req_handle, int32_t status, void *ereq_port)
+{
+	struct dwc_ute_iso_req_ext *ereqorg = NULL;
+	struct dwc_iso_xreq_port *ereqport = NULL;
+	struct dwc_ute_iso_packet_descriptor *desc_org = NULL;
+	int i;
+	struct usb_request *req;
+	//struct dwc_ute_iso_packet_descriptor *
+	//int status = 0;
+
+	req = (struct usb_request *)req_handle;
+	ereqorg = &req->ext_req;
+	ereqport = (struct dwc_iso_xreq_port *)ereq_port;
+	desc_org = ereqorg->per_io_frame_descs;
+
+	if (req && req->complete) {
+		/* Copy the request data from the portable logic to our request */
+		for (i = 0; i < ereqport->pio_pkt_count; i++) {
+			desc_org[i].actual_length =
+			    ereqport->per_io_frame_descs[i].actual_length;
+			desc_org[i].status =
+			    ereqport->per_io_frame_descs[i].status;
+		}
+
+		switch (status) {
+		case -DWC_E_SHUTDOWN:
+			req->status = -ESHUTDOWN;
+			break;
+		case -DWC_E_RESTART:
+			req->status = -ECONNRESET;
+			break;
+		case -DWC_E_INVALID:
+			req->status = -EINVAL;
+			break;
+		case -DWC_E_TIMEOUT:
+			req->status = -ETIMEDOUT;
+			break;
+		default:
+			req->status = status;
+		}
+
+		/* And call the gadget's completion */
+		req->complete(ep_handle, req);
+	}
+
+	return 0;
+}
+#endif /* DWC_UTE_PER_IO */
+static int _complete(dwc_otg_pcd_t * pcd, void *ep_handle,
+		     void *req_handle, int32_t status, uint32_t actual)
+{
+	struct usb_request *req = (struct usb_request *)req_handle;
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2,6,27)
+	struct dwc_otg_pcd_ep *ep = NULL;
+#endif
+#ifdef PCI_INTERFACE
+	struct pci_dev *dev = NULL;
+#endif
+
+	if (req && req->complete) {
+		switch (status) {
+		case -DWC_E_SHUTDOWN:
+			req->status = -ESHUTDOWN;
+			break;
+		case -DWC_E_RESTART:
+			req->status = -ECONNRESET;
+			break;
+		case -DWC_E_INVALID:
+			req->status = -EINVAL;
+			break;
+		case -DWC_E_TIMEOUT:
+			req->status = -ETIMEDOUT;
+			break;
+		default:
+			req->status = status;
+
+		}
+
+		req->actual = actual;
+		DWC_SPINUNLOCK(pcd->lock);
+		req->complete(ep_handle, req);
+		DWC_SPINLOCK(pcd->lock);
+	}
+#ifdef PCI_INTERFACE
+	dev = gadget_wrapper->pcd->otg_dev->os_dep.pcidev;
+	ep = ep_from_handle(pcd, ep_handle);
+	if (GET_CORE_IF(pcd)->dma_enable) {
+		if (req->length != 0)
+			pci_unmap_single(dev, req->dma, req->length,
+					 ep->dwc_ep.
+					 is_in ? PCI_DMA_TODEVICE :
+					 PCI_DMA_FROMDEVICE);
+	}
+#endif
+
+	return 0;
+}
+
+static int _connect(dwc_otg_pcd_t * pcd, int speed)
+{
+	gadget_wrapper->gadget.speed = speed;
+	return 0;
+}
+
+static int _disconnect(dwc_otg_pcd_t * pcd)
+{
+	if (gadget_wrapper->driver && gadget_wrapper->driver->disconnect) {
+		gadget_wrapper->driver->disconnect(&gadget_wrapper->gadget);
+	}
+	return 0;
+}
+
+static int _resume(dwc_otg_pcd_t * pcd)
+{
+	if (gadget_wrapper->driver && gadget_wrapper->driver->resume) {
+		gadget_wrapper->driver->resume(&gadget_wrapper->gadget);
+	}
+
+	return 0;
+}
+
+static int _suspend(dwc_otg_pcd_t * pcd)
+{
+	if (gadget_wrapper->driver && gadget_wrapper->driver->suspend) {
+		gadget_wrapper->driver->suspend(&gadget_wrapper->gadget);
+	}
+	return 0;
+}
+
+/**
+ * This function updates the otg values in the gadget structure.
+ */
+static int _hnp_changed(dwc_otg_pcd_t * pcd)
+{
+
+	if (!gadget_wrapper->gadget.is_otg)
+		return 0;
+
+	gadget_wrapper->gadget.b_hnp_enable = get_b_hnp_enable(pcd);
+	gadget_wrapper->gadget.a_hnp_support = get_a_hnp_support(pcd);
+	gadget_wrapper->gadget.a_alt_hnp_support = get_a_alt_hnp_support(pcd);
+	return 0;
+}
+
+static int _reset(dwc_otg_pcd_t * pcd)
+{
+	return 0;
+}
+
+#ifdef DWC_UTE_CFI
+static int _cfi_setup(dwc_otg_pcd_t * pcd, void *cfi_req)
+{
+	int retval = -DWC_E_INVALID;
+	if (gadget_wrapper->driver->cfi_feature_setup) {
+		retval =
+		    gadget_wrapper->driver->
+		    cfi_feature_setup(&gadget_wrapper->gadget,
+				      (struct cfi_usb_ctrlrequest *)cfi_req);
+	}
+
+	return retval;
+}
+#endif
+
+static const struct dwc_otg_pcd_function_ops fops = {
+	.complete = _complete,
+#ifdef DWC_EN_ISOC
+	.isoc_complete = _isoc_complete,
+#endif
+	.setup = _setup,
+	.disconnect = _disconnect,
+	.connect = _connect,
+	.resume = _resume,
+	.suspend = _suspend,
+	.hnp_changed = _hnp_changed,
+	.reset = _reset,
+#ifdef DWC_UTE_CFI
+	.cfi_setup = _cfi_setup,
+#endif
+#ifdef DWC_UTE_PER_IO
+	.xisoc_complete = _xisoc_complete,
+#endif
+};
+
+/**
+ * This function is the top level PCD interrupt handler.
+ */
+static irqreturn_t dwc_otg_pcd_irq(int irq, void *dev)
+{
+	dwc_otg_pcd_t *pcd = dev;
+	int32_t retval = IRQ_NONE;
+
+	retval = dwc_otg_pcd_handle_intr(pcd);
+	if (retval != 0) {
+		S3C2410X_CLEAR_EINTPEND();
+	}
+	return IRQ_RETVAL(retval);
+}
+
+/**
+ * This function initialized the usb_ep structures to there default
+ * state.
+ *
+ * @param d Pointer on gadget_wrapper.
+ */
+void gadget_add_eps(struct gadget_wrapper *d)
+{
+	static const char *names[] = {
+
+		"ep0",
+		"ep1in",
+		"ep2in",
+		"ep3in",
+		"ep4in",
+		"ep5in",
+		"ep6in",
+		"ep7in",
+		"ep8in",
+		"ep9in",
+		"ep10in",
+		"ep11in",
+		"ep12in",
+		"ep13in",
+		"ep14in",
+		"ep15in",
+		"ep1out",
+		"ep2out",
+		"ep3out",
+		"ep4out",
+		"ep5out",
+		"ep6out",
+		"ep7out",
+		"ep8out",
+		"ep9out",
+		"ep10out",
+		"ep11out",
+		"ep12out",
+		"ep13out",
+		"ep14out",
+		"ep15out"
+	};
+
+	int i;
+	struct usb_ep *ep;
+	int8_t dev_endpoints;
+
+	DWC_DEBUGPL(DBG_PCDV, "%s\n", __func__);
+
+	INIT_LIST_HEAD(&d->gadget.ep_list);
+	d->gadget.ep0 = &d->ep0;
+	d->gadget.speed = USB_SPEED_UNKNOWN;
+
+	INIT_LIST_HEAD(&d->gadget.ep0->ep_list);
+
+	/**
+	 * Initialize the EP0 structure.
+	 */
+	ep = &d->ep0;
+
+	/* Init the usb_ep structure. */
+	ep->name = names[0];
+	ep->ops = (struct usb_ep_ops *)&dwc_otg_pcd_ep_ops;
+
+	/**
+	 * @todo NGS: What should the max packet size be set to
+	 * here?  Before EP type is set?
+	 */
+	ep->maxpacket = MAX_PACKET_SIZE;
+	dwc_otg_pcd_ep_enable(d->pcd, NULL, ep);
+
+	list_add_tail(&ep->ep_list, &d->gadget.ep_list);
+
+	/**
+	 * Initialize the EP structures.
+	 */
+	dev_endpoints = d->pcd->core_if->dev_if->num_in_eps;
+
+	for (i = 0; i < dev_endpoints; i++) {
+		ep = &d->in_ep[i];
+
+		/* Init the usb_ep structure. */
+		ep->name = names[d->pcd->in_ep[i].dwc_ep.num];
+		ep->ops = (struct usb_ep_ops *)&dwc_otg_pcd_ep_ops;
+
+		/**
+		 * @todo NGS: What should the max packet size be set to
+		 * here?  Before EP type is set?
+		 */
+		ep->maxpacket = MAX_PACKET_SIZE;
+		list_add_tail(&ep->ep_list, &d->gadget.ep_list);
+	}
+
+	dev_endpoints = d->pcd->core_if->dev_if->num_out_eps;
+
+	for (i = 0; i < dev_endpoints; i++) {
+		ep = &d->out_ep[i];
+
+		/* Init the usb_ep structure. */
+		ep->name = names[15 + d->pcd->out_ep[i].dwc_ep.num];
+		ep->ops = (struct usb_ep_ops *)&dwc_otg_pcd_ep_ops;
+
+		/**
+		 * @todo NGS: What should the max packet size be set to
+		 * here?  Before EP type is set?
+		 */
+		ep->maxpacket = MAX_PACKET_SIZE;
+
+		list_add_tail(&ep->ep_list, &d->gadget.ep_list);
+	}
+
+	/* remove ep0 from the list.  There is a ep0 pointer. */
+	list_del_init(&d->ep0.ep_list);
+
+	d->ep0.maxpacket = MAX_EP0_SIZE;
+}
+
+/**
+ * This function releases the Gadget device.
+ * required by device_unregister().
+ *
+ * @todo Should this do something?	Should it free the PCD?
+ */
+static void dwc_otg_pcd_gadget_release(struct device *dev)
+{
+	DWC_DEBUGPL(DBG_PCDV, "%s(%p)\n", __func__, dev);
+}
+
+static struct gadget_wrapper *alloc_wrapper(
+#ifdef LM_INTERFACE
+	struct lm_device *_dev
+#elif  defined(PCI_INTERFACE)
+	struct pci_dev *_dev
+#endif
+    )
+{
+	static char pcd_name[] = "dwc_otg_pcd";
+#ifdef LM_INTERFACE
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(_dev);
+#elif defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = pci_get_drvdata(_dev);
+#endif
+
+	struct gadget_wrapper *d;
+	int retval;
+
+	d = DWC_ALLOC(sizeof(*d));
+	if (d == NULL) {
+		return NULL;
+	}
+
+	memset(d, 0, sizeof(*d));
+
+	d->gadget.name = pcd_name;
+	d->pcd = otg_dev->pcd;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,30)
+	strcpy(d->gadget.dev.bus_id, "gadget");
+#else
+	dev_set_name(&d->gadget.dev, "%s", "gadget");
+#endif
+
+	d->gadget.dev.parent = &_dev->dev;
+	d->gadget.dev.release = dwc_otg_pcd_gadget_release;
+	d->gadget.ops = &dwc_otg_pcd_ops;
+	d->gadget.is_dualspeed = dwc_otg_pcd_is_dualspeed(otg_dev->pcd);
+	d->gadget.is_otg = dwc_otg_pcd_is_otg(otg_dev->pcd);
+
+	d->driver = 0;
+	/* Register the gadget device */
+	retval = device_register(&d->gadget.dev);
+	if (retval != 0) {
+		DWC_ERROR("device_register failed\n");
+		DWC_FREE(d);
+		return NULL;
+	}
+
+	return d;
+}
+
+static void free_wrapper(struct gadget_wrapper *d)
+{
+	if (d->driver) {
+		/* should have been done already by driver model core */
+		DWC_WARN("driver '%s' is still registered\n",
+			 d->driver->driver.name);
+		usb_gadget_unregister_driver(d->driver);
+	}
+
+	device_unregister(&d->gadget.dev);
+	DWC_FREE(d);
+}
+
+/**
+ * This function initialized the PCD portion of the driver.
+ *
+ */
+int pcd_init(
+#ifdef LM_INTERFACE
+	struct lm_device *_dev
+#elif  defined(PCI_INTERFACE)
+	struct pci_dev *_dev
+#else
+	struct platform_device *_dev
+#endif
+    )
+{
+#ifdef LM_INTERFACE
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(_dev);
+#elif  defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = pci_get_drvdata(_dev);
+#endif
+
+	int retval = 0;
+
+	DWC_DEBUGPL(DBG_PCDV, "%s(%p)\n", __func__, _dev);
+
+	otg_dev->pcd = dwc_otg_pcd_init(otg_dev->core_if);
+
+	if (!otg_dev->pcd) {
+		DWC_ERROR("dwc_otg_pcd_init failed\n");
+		return -ENOMEM;
+	}
+
+	otg_dev->pcd->otg_dev = otg_dev;
+	gadget_wrapper = alloc_wrapper(_dev);
+
+	/*
+	 * Initialize EP structures
+	 */
+	gadget_add_eps(gadget_wrapper);
+	/*
+	 * Setup interupt handler
+	 */
+	DWC_DEBUGPL(DBG_ANY, "registering handler for irq%d\n", _dev->irq);
+	retval = request_irq(_dev->irq, dwc_otg_pcd_irq,
+			     IRQF_SHARED | IRQF_DISABLED,
+			     gadget_wrapper->gadget.name, otg_dev->pcd);
+	if (retval != 0) {
+		DWC_ERROR("request of irq%d failed\n", _dev->irq);
+		free_wrapper(gadget_wrapper);
+		return -EBUSY;
+	}
+
+	dwc_otg_pcd_start(gadget_wrapper->pcd, &fops);
+
+	return retval;
+}
+
+/**
+ * Cleanup the PCD.
+ */
+void pcd_remove(
+#ifdef LM_INTERFACE
+	struct lm_device *_dev
+#elif  defined(PCI_INTERFACE)
+	struct pci_dev *_dev
+#endif
+    )
+{
+#ifdef LM_INTERFACE
+	dwc_otg_device_t *otg_dev = lm_get_drvdata(_dev);
+#elif  defined(PCI_INTERFACE)
+	dwc_otg_device_t *otg_dev = pci_get_drvdata(_dev);
+#endif
+	dwc_otg_pcd_t *pcd = otg_dev->pcd;
+
+	DWC_DEBUGPL(DBG_PCDV, "%s(%p)\n", __func__, _dev);
+
+	/*
+	 * Free the IRQ
+	 */
+	free_irq(_dev->irq, pcd);
+	dwc_otg_pcd_remove(otg_dev->pcd);
+	free_wrapper(gadget_wrapper);
+	otg_dev->pcd = 0;
+}
+
+/**
+ * This function registers a gadget driver with the PCD.
+ *
+ * When a driver is successfully registered, it will receive control
+ * requests including set_configuration(), which enables non-control
+ * requests.  then usb traffic follows until a disconnect is reported.
+ * then a host may connect again, or the driver might get unbound.
+ *
+ * @param driver The driver being registered
+ * @param bind The bind function of gadget driver
+ */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,37)
+int usb_gadget_register_driver(struct usb_gadget_driver *driver)
+#else
+int usb_gadget_probe_driver(struct usb_gadget_driver *driver,
+		int (*bind)(struct usb_gadget *))
+#endif
+{
+	int retval;
+
+	DWC_DEBUGPL(DBG_PCD, "registering gadget driver '%s'\n",
+		    driver->driver.name);
+
+	if (!driver || driver->speed == USB_SPEED_UNKNOWN ||
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,37)
+	    !driver->bind ||
+#else
+		!bind ||
+#endif
+	    !driver->unbind || !driver->disconnect || !driver->setup) {
+		DWC_DEBUGPL(DBG_PCDV, "EINVAL\n");
+		return -EINVAL;
+	}
+	if (gadget_wrapper == 0) {
+		DWC_DEBUGPL(DBG_PCDV, "ENODEV\n");
+		return -ENODEV;
+	}
+	if (gadget_wrapper->driver != 0) {
+		DWC_DEBUGPL(DBG_PCDV, "EBUSY (%p)\n", gadget_wrapper->driver);
+		return -EBUSY;
+	}
+
+	/* hook up the driver */
+	gadget_wrapper->driver = driver;
+	gadget_wrapper->gadget.dev.driver = &driver->driver;
+
+	DWC_DEBUGPL(DBG_PCD, "bind to driver %s\n", driver->driver.name);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,37)
+	retval = driver->bind(&gadget_wrapper->gadget);
+#else
+	retval = bind(&gadget_wrapper->gadget);
+#endif
+	if (retval) {
+		DWC_ERROR("bind to driver %s --> error %d\n",
+			  driver->driver.name, retval);
+		gadget_wrapper->driver = 0;
+		gadget_wrapper->gadget.dev.driver = 0;
+		return retval;
+	}
+	DWC_DEBUGPL(DBG_ANY, "registered gadget driver '%s'\n",
+		    driver->driver.name);
+	return 0;
+}
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,37)
+EXPORT_SYMBOL(usb_gadget_register_driver);
+#else
+EXPORT_SYMBOL(usb_gadget_probe_driver);
+#endif
+
+/**
+ * This function unregisters a gadget driver
+ *
+ * @param driver The driver being unregistered
+ */
+int usb_gadget_unregister_driver(struct usb_gadget_driver *driver)
+{
+	//DWC_DEBUGPL(DBG_PCDV,"%s(%p)\n", __func__, _driver);
+
+	if (gadget_wrapper == 0) {
+		DWC_DEBUGPL(DBG_ANY, "%s Return(%d): s_pcd==0\n", __func__,
+			    -ENODEV);
+		return -ENODEV;
+	}
+	if (driver == 0 || driver != gadget_wrapper->driver) {
+		DWC_DEBUGPL(DBG_ANY, "%s Return(%d): driver?\n", __func__,
+			    -EINVAL);
+		return -EINVAL;
+	}
+
+	driver->unbind(&gadget_wrapper->gadget);
+	gadget_wrapper->driver = 0;
+
+	DWC_DEBUGPL(DBG_ANY, "unregistered driver '%s'\n", driver->driver.name);
+	return 0;
+}
+
+EXPORT_SYMBOL(usb_gadget_unregister_driver);
+
+#endif /* DWC_HOST_ONLY */
diff --git a/drivers/usb/dwc_otg/dwc_otg_regs.h b/drivers/usb/dwc_otg/dwc_otg_regs.h
new file mode 100644
index 0000000..8dc648b
--- /dev/null
+++ b/drivers/usb/dwc_otg/dwc_otg_regs.h
@@ -0,0 +1,2545 @@
+/* ==========================================================================
+ * $File: //dwh/usb_iip/dev/software/otg/linux/drivers/dwc_otg_regs.h $
+ * $Revision: #97 $
+ * $Date: 2011/10/24 $
+ * $Change: 1871160 $
+ *
+ * Synopsys HS OTG Linux Software Driver and documentation (hereinafter,
+ * "Software") is an Unsupported proprietary work of Synopsys, Inc. unless
+ * otherwise expressly agreed to in writing between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product under
+ * any End User Software License Agreement or Agreement for Licensed Product
+ * with Synopsys or any supplement thereto. You are permitted to use and
+ * redistribute this Software in source and binary forms, with or without
+ * modification, provided that redistributions of source code must retain this
+ * notice. You may not view, use, disclose, copy or distribute this file or
+ * any information contained herein except pursuant to this license grant from
+ * Synopsys. If you do not agree with this notice, including the disclaimer
+ * below, then you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS" BASIS
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE HEREBY DISCLAIMED. IN NO EVENT SHALL SYNOPSYS BE LIABLE FOR ANY DIRECT,
+ * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================== */
+
+#ifndef __DWC_OTG_REGS_H__
+#define __DWC_OTG_REGS_H__
+
+#include "dwc_otg_core_if.h"
+
+/**
+ * @file
+ *
+ * This file contains the data structures for accessing the DWC_otg core registers.
+ *
+ * The application interfaces with the HS OTG core by reading from and
+ * writing to the Control and Status Register (CSR) space through the
+ * AHB Slave interface. These registers are 32 bits wide, and the
+ * addresses are 32-bit-block aligned.
+ * CSRs are classified as follows:
+ * - Core Global Registers
+ * - Device Mode Registers
+ * - Device Global Registers
+ * - Device Endpoint Specific Registers
+ * - Host Mode Registers
+ * - Host Global Registers
+ * - Host Port CSRs
+ * - Host Channel Specific Registers
+ *
+ * Only the Core Global registers can be accessed in both Device and
+ * Host modes. When the HS OTG core is operating in one mode, either
+ * Device or Host, the application must not access registers from the
+ * other mode. When the core switches from one mode to another, the
+ * registers in the new mode of operation must be reprogrammed as they
+ * would be after a power-on reset.
+ */
+
+/****************************************************************************/
+/** DWC_otg Core registers . 
+ * The dwc_otg_core_global_regs structure defines the size
+ * and relative field offsets for the Core Global registers.
+ */
+typedef struct dwc_otg_core_global_regs {
+	/** OTG Control and Status Register.  <i>Offset: 000h</i> */
+	volatile uint32_t gotgctl;
+	/** OTG Interrupt Register.	 <i>Offset: 004h</i> */
+	volatile uint32_t gotgint;
+	/**Core AHB Configuration Register.	 <i>Offset: 008h</i> */
+	volatile uint32_t gahbcfg;
+
+#define DWC_GLBINTRMASK		0x0001
+#define DWC_DMAENABLE		0x0020
+#define DWC_NPTXEMPTYLVL_EMPTY	0x0080
+#define DWC_NPTXEMPTYLVL_HALFEMPTY	0x0000
+#define DWC_PTXEMPTYLVL_EMPTY	0x0100
+#define DWC_PTXEMPTYLVL_HALFEMPTY	0x0000
+
+	/**Core USB Configuration Register.	 <i>Offset: 00Ch</i> */
+	volatile uint32_t gusbcfg;
+	/**Core Reset Register.	 <i>Offset: 010h</i> */
+	volatile uint32_t grstctl;
+	/**Core Interrupt Register.	 <i>Offset: 014h</i> */
+	volatile uint32_t gintsts;
+	/**Core Interrupt Mask Register.  <i>Offset: 018h</i> */
+	volatile uint32_t gintmsk;
+	/**Receive Status Queue Read Register (Read Only).	<i>Offset: 01Ch</i> */
+	volatile uint32_t grxstsr;
+	/**Receive Status Queue Read & POP Register (Read Only).  <i>Offset: 020h</i>*/
+	volatile uint32_t grxstsp;
+	/**Receive FIFO Size Register.	<i>Offset: 024h</i> */
+	volatile uint32_t grxfsiz;
+	/**Non Periodic Transmit FIFO Size Register.  <i>Offset: 028h</i> */
+	volatile uint32_t gnptxfsiz;
+	/**Non Periodic Transmit FIFO/Queue Status Register (Read
+	 * Only). <i>Offset: 02Ch</i> */
+	volatile uint32_t gnptxsts;
+	/**I2C Access Register.	 <i>Offset: 030h</i> */
+	volatile uint32_t gi2cctl;
+	/**PHY Vendor Control Register.	 <i>Offset: 034h</i> */
+	volatile uint32_t gpvndctl;
+	/**General Purpose Input/Output Register.  <i>Offset: 038h</i> */
+	volatile uint32_t ggpio;
+	/**User ID Register.  <i>Offset: 03Ch</i> */
+	volatile uint32_t guid;
+	/**Synopsys ID Register (Read Only).  <i>Offset: 040h</i> */
+	volatile uint32_t gsnpsid;
+	/**User HW Config1 Register (Read Only).  <i>Offset: 044h</i> */
+	volatile uint32_t ghwcfg1;
+	/**User HW Config2 Register (Read Only).  <i>Offset: 048h</i> */
+	volatile uint32_t ghwcfg2;
+#define DWC_SLAVE_ONLY_ARCH 0
+#define DWC_EXT_DMA_ARCH 1
+#define DWC_INT_DMA_ARCH 2
+
+#define DWC_MODE_HNP_SRP_CAPABLE	0
+#define DWC_MODE_SRP_ONLY_CAPABLE	1
+#define DWC_MODE_NO_HNP_SRP_CAPABLE		2
+#define DWC_MODE_SRP_CAPABLE_DEVICE		3
+#define DWC_MODE_NO_SRP_CAPABLE_DEVICE	4
+#define DWC_MODE_SRP_CAPABLE_HOST	5
+#define DWC_MODE_NO_SRP_CAPABLE_HOST	6
+
+	/**User HW Config3 Register (Read Only).  <i>Offset: 04Ch</i> */
+	volatile uint32_t ghwcfg3;
+	/**User HW Config4 Register (Read Only).  <i>Offset: 050h</i>*/
+	volatile uint32_t ghwcfg4;
+	/** Core LPM Configuration register <i>Offset: 054h</i>*/
+	volatile uint32_t glpmcfg;
+	/** Global PowerDn Register <i>Offset: 058h</i> */
+	volatile uint32_t gpwrdn;
+	/** Global DFIFO SW Config Register  <i>Offset: 05Ch</i> */
+	volatile uint32_t gdfifocfg;
+	/** ADP Control Register  <i>Offset: 060h</i> */
+	volatile uint32_t adpctl;
+	/** Reserved  <i>Offset: 064h-0FFh</i> */
+	volatile uint32_t reserved39[39];
+	/** Host Periodic Transmit FIFO Size Register. <i>Offset: 100h</i> */
+	volatile uint32_t hptxfsiz;
+	/** Device Periodic Transmit FIFO#n Register if dedicated fifos are disabled,
+		otherwise Device Transmit FIFO#n Register.
+	 * <i>Offset: 104h + (FIFO_Number-1)*04h, 1 <= FIFO Number <= 15 (1<=n<=15).</i> */
+	volatile uint32_t dtxfsiz[15];
+} dwc_otg_core_global_regs_t;
+
+/**
+ * This union represents the bit fields of the Core OTG Control
+ * and Status Register (GOTGCTL).  Set the bits using the bit
+ * fields then write the <i>d32</i> value to the register.
+ */
+typedef union gotgctl_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		unsigned sesreqscs:1;
+		unsigned sesreq:1;
+		unsigned vbvalidoven:1;
+		unsigned vbvalidovval:1;
+		unsigned avalidoven:1;
+		unsigned avalidovval:1;
+		unsigned bvalidoven:1;
+		unsigned bvalidovval:1;
+		unsigned hstnegscs:1;
+		unsigned hnpreq:1;
+		unsigned hstsethnpen:1;
+		unsigned devhnpen:1;
+		unsigned reserved12_15:4;
+		unsigned conidsts:1;
+		unsigned dbnctime:1;
+		unsigned asesvld:1;
+		unsigned bsesvld:1;
+		unsigned otgver:1;
+		unsigned reserved1:1;
+		unsigned multvalidbc:5;
+		unsigned chirpen:1;
+		unsigned reserved28_31:4;
+	} b;
+} gotgctl_data_t;
+
+/**
+ * This union represents the bit fields of the Core OTG Interrupt Register
+ * (GOTGINT).  Set/clear the bits using the bit fields then write the <i>d32</i>
+ * value to the register.
+ */
+typedef union gotgint_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		/** Current Mode */
+		unsigned reserved0_1:2;
+
+		/** Session End Detected */
+		unsigned sesenddet:1;
+
+		unsigned reserved3_7:5;
+
+		/** Session Request Success Status Change */
+		unsigned sesreqsucstschng:1;
+		/** Host Negotiation Success Status Change */
+		unsigned hstnegsucstschng:1;
+
+		unsigned reserved10_16:7;
+
+		/** Host Negotiation Detected */
+		unsigned hstnegdet:1;
+		/** A-Device Timeout Change */
+		unsigned adevtoutchng:1;
+		/** Debounce Done */
+		unsigned debdone:1;
+		/** Multi-Valued input changed */
+		unsigned mvic:1;
+
+		unsigned reserved31_21:11;
+
+	} b;
+} gotgint_data_t;
+
+/**
+ * This union represents the bit fields of the Core AHB Configuration
+ * Register (GAHBCFG). Set/clear the bits using the bit fields then
+ * write the <i>d32</i> value to the register.
+ */
+typedef union gahbcfg_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		unsigned glblintrmsk:1;
+#define DWC_GAHBCFG_GLBINT_ENABLE		1
+
+		unsigned hburstlen:4;
+#define DWC_GAHBCFG_INT_DMA_BURST_SINGLE	0
+#define DWC_GAHBCFG_INT_DMA_BURST_INCR		1
+#define DWC_GAHBCFG_INT_DMA_BURST_INCR4		3
+#define DWC_GAHBCFG_INT_DMA_BURST_INCR8		5
+#define DWC_GAHBCFG_INT_DMA_BURST_INCR16	7
+
+		unsigned dmaenable:1;
+#define DWC_GAHBCFG_DMAENABLE			1
+		unsigned reserved:1;
+		unsigned nptxfemplvl_txfemplvl:1;
+		unsigned ptxfemplvl:1;
+#define DWC_GAHBCFG_TXFEMPTYLVL_EMPTY		1
+#define DWC_GAHBCFG_TXFEMPTYLVL_HALFEMPTY	0
+		unsigned reserved9_20:12;
+		unsigned remmemsupp:1;
+		unsigned notialldmawrit:1;
+		unsigned ahbsingle:1;
+		unsigned reserved24_31:8;
+	} b;
+} gahbcfg_data_t;
+
+/**
+ * This union represents the bit fields of the Core USB Configuration
+ * Register (GUSBCFG). Set the bits using the bit fields then write
+ * the <i>d32</i> value to the register.
+ */
+typedef union gusbcfg_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		unsigned toutcal:3;
+		unsigned phyif:1;
+		unsigned ulpi_utmi_sel:1;
+		unsigned fsintf:1;
+		unsigned physel:1;
+		unsigned ddrsel:1;
+		unsigned srpcap:1;
+		unsigned hnpcap:1;
+		unsigned usbtrdtim:4;
+		unsigned reserved1:1;
+		unsigned phylpwrclksel:1;
+		unsigned otgutmifssel:1;
+		unsigned ulpi_fsls:1;
+		unsigned ulpi_auto_res:1;
+		unsigned ulpi_clk_sus_m:1;
+		unsigned ulpi_ext_vbus_drv:1;
+		unsigned ulpi_int_vbus_indicator:1;
+		unsigned term_sel_dl_pulse:1;
+		unsigned indicator_complement:1;
+		unsigned indicator_pass_through:1;
+		unsigned ulpi_int_prot_dis:1;
+		unsigned ic_usb_cap:1;
+		unsigned ic_traffic_pull_remove:1;
+		unsigned tx_end_delay:1;
+		unsigned force_host_mode:1;
+		unsigned force_dev_mode:1;
+		unsigned reserved31:1;
+	} b;
+} gusbcfg_data_t;
+
+/**
+ * This union represents the bit fields of the Core Reset Register
+ * (GRSTCTL).  Set/clear the bits using the bit fields then write the
+ * <i>d32</i> value to the register.
+ */
+typedef union grstctl_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		/** Core Soft Reset (CSftRst) (Device and Host)
+		 *
+		 * The application can flush the control logic in the
+		 * entire core using this bit. This bit resets the
+		 * pipelines in the AHB Clock domain as well as the
+		 * PHY Clock domain.
+		 *
+		 * The state machines are reset to an IDLE state, the
+		 * control bits in the CSRs are cleared, all the
+		 * transmit FIFOs and the receive FIFO are flushed.
+		 *
+		 * The status mask bits that control the generation of
+		 * the interrupt, are cleared, to clear the
+		 * interrupt. The interrupt status bits are not
+		 * cleared, so the application can get the status of
+		 * any events that occurred in the core after it has
+		 * set this bit.
+		 *
+		 * Any transactions on the AHB are terminated as soon
+		 * as possible following the protocol. Any
+		 * transactions on the USB are terminated immediately.
+		 *
+		 * The configuration settings in the CSRs are
+		 * unchanged, so the software doesn't have to
+		 * reprogram these registers (Device
+		 * Configuration/Host Configuration/Core System
+		 * Configuration/Core PHY Configuration).
+		 *
+		 * The application can write to this bit, any time it
+		 * wants to reset the core. This is a self clearing
+		 * bit and the core clears this bit after all the
+		 * necessary logic is reset in the core, which may
+		 * take several clocks, depending on the current state
+		 * of the core.
+		 */
+		unsigned csftrst:1;
+		/** Hclk Soft Reset
+		 *
+		 * The application uses this bit to reset the control logic in
+		 * the AHB clock domain. Only AHB clock domain pipelines are
+		 * reset.
+		 */
+		unsigned hsftrst:1;
+		/** Host Frame Counter Reset (Host Only)<br>
+		 *
+		 * The application can reset the (micro)frame number
+		 * counter inside the core, using this bit. When the
+		 * (micro)frame counter is reset, the subsequent SOF
+		 * sent out by the core, will have a (micro)frame
+		 * number of 0.
+		 */
+		unsigned hstfrm:1;
+		/** In Token Sequence Learning Queue Flush
+		 * (INTknQFlsh) (Device Only)
+		 */
+		unsigned intknqflsh:1;
+		/** RxFIFO Flush (RxFFlsh) (Device and Host)
+		 *
+		 * The application can flush the entire Receive FIFO
+		 * using this bit. The application must first
+		 * ensure that the core is not in the middle of a
+		 * transaction. The application should write into
+		 * this bit, only after making sure that neither the
+		 * DMA engine is reading from the RxFIFO nor the MAC
+		 * is writing the data in to the FIFO. The
+		 * application should wait until the bit is cleared
+		 * before performing any other operations. This bit
+		 * will takes 8 clocks (slowest of PHY or AHB clock)
+		 * to clear.
+		 */
+		unsigned rxfflsh:1;
+		/** TxFIFO Flush (TxFFlsh) (Device and Host). 
+		 *
+		 * This bit is used to selectively flush a single or
+		 * all transmit FIFOs. The application must first
+		 * ensure that the core is not in the middle of a
+		 * transaction. The application should write into
+		 * this bit, only after making sure that neither the
+		 * DMA engine is writing into the TxFIFO nor the MAC
+		 * is reading the data out of the FIFO. The
+		 * application should wait until the core clears this
+		 * bit, before performing any operations. This bit
+		 * will takes 8 clocks (slowest of PHY or AHB clock)
+		 * to clear.
+		 */
+		unsigned txfflsh:1;
+
+		/** TxFIFO Number (TxFNum) (Device and Host).
+		 *
+		 * This is the FIFO number which needs to be flushed,
+		 * using the TxFIFO Flush bit. This field should not
+		 * be changed until the TxFIFO Flush bit is cleared by
+		 * the core.
+		 *	 - 0x0 : Non Periodic TxFIFO Flush
+		 *	 - 0x1 : Periodic TxFIFO #1 Flush in device mode
+		 *	   or Periodic TxFIFO in host mode
+		 *	 - 0x2 : Periodic TxFIFO #2 Flush in device mode.
+		 *	 - ...
+		 *	 - 0xF : Periodic TxFIFO #15 Flush in device mode
+		 *	 - 0x10: Flush all the Transmit NonPeriodic and
+		 *	   Transmit Periodic FIFOs in the core
+		 */
+		unsigned txfnum:5;
+		/** Reserved */
+		unsigned reserved11_29:19;
+		/** DMA Request Signal.	 Indicated DMA request is in
+		 * probress. Used for debug purpose. */
+		unsigned dmareq:1;
+		/** AHB Master Idle.  Indicates the AHB Master State
+		 * Machine is in IDLE condition. */
+		unsigned ahbidle:1;
+	} b;
+} grstctl_t;
+
+/**
+ * This union represents the bit fields of the Core Interrupt Mask
+ * Register (GINTMSK). Set/clear the bits using the bit fields then
+ * write the <i>d32</i> value to the register.
+ */
+typedef union gintmsk_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		unsigned reserved0:1;
+		unsigned modemismatch:1;
+		unsigned otgintr:1;
+		unsigned sofintr:1;
+		unsigned rxstsqlvl:1;
+		unsigned nptxfempty:1;
+		unsigned ginnakeff:1;
+		unsigned goutnakeff:1;
+		unsigned ulpickint:1;
+		unsigned i2cintr:1;
+		unsigned erlysuspend:1;
+		unsigned usbsuspend:1;
+		unsigned usbreset:1;
+		unsigned enumdone:1;
+		unsigned isooutdrop:1;
+		unsigned eopframe:1;
+		unsigned restoredone:1;
+		unsigned epmismatch:1;
+		unsigned inepintr:1;
+		unsigned outepintr:1;
+		unsigned incomplisoin:1;
+		unsigned incomplisoout:1;
+		unsigned fetsusp:1;
+		unsigned resetdet:1;
+		unsigned portintr:1;
+		unsigned hcintr:1;
+		unsigned ptxfempty:1;
+		unsigned lpmtranrcvd:1;
+		unsigned conidstschng:1;
+		unsigned disconnect:1;
+		unsigned sessreqintr:1;
+		unsigned wkupintr:1;
+	} b;
+} gintmsk_data_t;
+/**
+ * This union represents the bit fields of the Core Interrupt Register
+ * (GINTSTS).  Set/clear the bits using the bit fields then write the
+ * <i>d32</i> value to the register.
+ */
+typedef union gintsts_data {
+	/** raw register data */
+	uint32_t d32;
+#define DWC_SOF_INTR_MASK 0x0008
+	/** register bits */
+	struct {
+#define DWC_HOST_MODE 1
+		unsigned curmode:1;
+		unsigned modemismatch:1;
+		unsigned otgintr:1;
+		unsigned sofintr:1;
+		unsigned rxstsqlvl:1;
+		unsigned nptxfempty:1;
+		unsigned ginnakeff:1;
+		unsigned goutnakeff:1;
+		unsigned ulpickint:1;
+		unsigned i2cintr:1;
+		unsigned erlysuspend:1;
+		unsigned usbsuspend:1;
+		unsigned usbreset:1;
+		unsigned enumdone:1;
+		unsigned isooutdrop:1;
+		unsigned eopframe:1;
+		unsigned restoredone:1;
+		unsigned epmismatch:1;
+		unsigned inepint:1;
+		unsigned outepintr:1;
+		unsigned incomplisoin:1;
+		unsigned incomplisoout:1;
+		unsigned fetsusp:1;
+		unsigned resetdet:1;
+		unsigned portintr:1;
+		unsigned hcintr:1;
+		unsigned ptxfempty:1;
+		unsigned lpmtranrcvd:1;
+		unsigned conidstschng:1;
+		unsigned disconnect:1;
+		unsigned sessreqintr:1;
+		unsigned wkupintr:1;
+	} b;
+} gintsts_data_t;
+
+/**
+ * This union represents the bit fields in the Device Receive Status Read and
+ * Pop Registers (GRXSTSR, GRXSTSP) Read the register into the <i>d32</i>
+ * element then read out the bits using the <i>b</i>it elements.
+ */
+typedef union device_grxsts_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		unsigned epnum:4;
+		unsigned bcnt:11;
+		unsigned dpid:2;
+
+#define DWC_STS_DATA_UPDT		0x2	// OUT Data Packet
+#define DWC_STS_XFER_COMP		0x3	// OUT Data Transfer Complete
+
+#define DWC_DSTS_GOUT_NAK		0x1	// Global OUT NAK
+#define DWC_DSTS_SETUP_COMP		0x4	// Setup Phase Complete
+#define DWC_DSTS_SETUP_UPDT 0x6	// SETUP Packet
+		unsigned pktsts:4;
+		unsigned fn:4;
+		unsigned reserved25_31:7;
+	} b;
+} device_grxsts_data_t;
+
+/**
+ * This union represents the bit fields in the Host Receive Status Read and
+ * Pop Registers (GRXSTSR, GRXSTSP) Read the register into the <i>d32</i>
+ * element then read out the bits using the <i>b</i>it elements.
+ */
+typedef union host_grxsts_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		unsigned chnum:4;
+		unsigned bcnt:11;
+		unsigned dpid:2;
+
+		unsigned pktsts:4;
+#define DWC_GRXSTS_PKTSTS_IN			  0x2
+#define DWC_GRXSTS_PKTSTS_IN_XFER_COMP	  0x3
+#define DWC_GRXSTS_PKTSTS_DATA_TOGGLE_ERR 0x5
+#define DWC_GRXSTS_PKTSTS_CH_HALTED		  0x7
+
+		unsigned reserved21_31:11;
+	} b;
+} host_grxsts_data_t;
+
+/**
+ * This union represents the bit fields in the FIFO Size Registers (HPTXFSIZ,
+ * GNPTXFSIZ, DPTXFSIZn, DIEPTXFn). Read the register into the <i>d32</i> element 
+ * then read out the bits using the <i>b</i>it elements.
+ */
+typedef union fifosize_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		unsigned startaddr:16;
+		unsigned depth:16;
+	} b;
+} fifosize_data_t;
+
+/**
+ * This union represents the bit fields in the Non-Periodic Transmit
+ * FIFO/Queue Status Register (GNPTXSTS). Read the register into the
+ * <i>d32</i> element then read out the bits using the <i>b</i>it
+ * elements.
+ */
+typedef union gnptxsts_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		unsigned nptxfspcavail:16;
+		unsigned nptxqspcavail:8;
+		/** Top of the Non-Periodic Transmit Request Queue
+		 *	- bit 24 - Terminate (Last entry for the selected
+		 *	  channel/EP)
+		 *	- bits 26:25 - Token Type
+		 *	  - 2'b00 - IN/OUT
+		 *	  - 2'b01 - Zero Length OUT
+		 *	  - 2'b10 - PING/Complete Split
+		 *	  - 2'b11 - Channel Halt
+		 *	- bits 30:27 - Channel/EP Number
+		 */
+		unsigned nptxqtop_terminate:1;
+		unsigned nptxqtop_token:2;
+		unsigned nptxqtop_chnep:4;
+		unsigned reserved:1;
+	} b;
+} gnptxsts_data_t;
+
+/**
+ * This union represents the bit fields in the Transmit
+ * FIFO Status Register (DTXFSTS). Read the register into the
+ * <i>d32</i> element then read out the bits using the <i>b</i>it
+ * elements.
+ */
+typedef union dtxfsts_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		unsigned txfspcavail:16;
+		unsigned reserved:16;
+	} b;
+} dtxfsts_data_t;
+
+/**
+ * This union represents the bit fields in the I2C Control Register
+ * (I2CCTL). Read the register into the <i>d32</i> element then read out the
+ * bits using the <i>b</i>it elements.
+ */
+typedef union gi2cctl_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		unsigned rwdata:8;
+		unsigned regaddr:8;
+		unsigned addr:7;
+		unsigned i2cen:1;
+		unsigned ack:1;
+		unsigned i2csuspctl:1;
+		unsigned i2cdevaddr:2;
+		unsigned i2cdatse0:1;
+		unsigned reserved:1;
+		unsigned rw:1;
+		unsigned bsydne:1;
+	} b;
+} gi2cctl_data_t;
+
+/**
+ * This union represents the bit fields in the PHY Vendor Control Register
+ * (GPVNDCTL). Read the register into the <i>d32</i> element then read out the
+ * bits using the <i>b</i>it elements.
+ */
+typedef union gpvndctl_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		unsigned regdata:8;
+		unsigned vctrl:8;
+		unsigned regaddr16_21:6;
+		unsigned regwr:1;
+		unsigned reserved23_24:2;
+		unsigned newregreq:1;
+		unsigned vstsbsy:1;
+		unsigned vstsdone:1;
+		unsigned reserved28_30:3;
+		unsigned disulpidrvr:1;
+	} b;
+} gpvndctl_data_t;
+
+/**
+ * This union represents the bit fields in the General Purpose 
+ * Input/Output Register (GGPIO).
+ * Read the register into the <i>d32</i> element then read out the
+ * bits using the <i>b</i>it elements.
+ */
+typedef union ggpio_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		unsigned gpi:16;
+		unsigned gpo:16;
+	} b;
+} ggpio_data_t;
+
+/**
+ * This union represents the bit fields in the User ID Register
+ * (GUID). Read the register into the <i>d32</i> element then read out the
+ * bits using the <i>b</i>it elements.
+ */
+typedef union guid_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		unsigned rwdata:32;
+	} b;
+} guid_data_t;
+
+/**
+ * This union represents the bit fields in the Synopsys ID Register
+ * (GSNPSID). Read the register into the <i>d32</i> element then read out the
+ * bits using the <i>b</i>it elements.
+ */
+typedef union gsnpsid_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		unsigned rwdata:32;
+	} b;
+} gsnpsid_data_t;
+
+/**
+ * This union represents the bit fields in the User HW Config1
+ * Register.  Read the register into the <i>d32</i> element then read
+ * out the bits using the <i>b</i>it elements.
+ */
+typedef union hwcfg1_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		unsigned ep_dir0:2;
+		unsigned ep_dir1:2;
+		unsigned ep_dir2:2;
+		unsigned ep_dir3:2;
+		unsigned ep_dir4:2;
+		unsigned ep_dir5:2;
+		unsigned ep_dir6:2;
+		unsigned ep_dir7:2;
+		unsigned ep_dir8:2;
+		unsigned ep_dir9:2;
+		unsigned ep_dir10:2;
+		unsigned ep_dir11:2;
+		unsigned ep_dir12:2;
+		unsigned ep_dir13:2;
+		unsigned ep_dir14:2;
+		unsigned ep_dir15:2;
+	} b;
+} hwcfg1_data_t;
+
+/**
+ * This union represents the bit fields in the User HW Config2
+ * Register.  Read the register into the <i>d32</i> element then read
+ * out the bits using the <i>b</i>it elements.
+ */
+typedef union hwcfg2_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		/* GHWCFG2 */
+		unsigned op_mode:3;
+#define DWC_HWCFG2_OP_MODE_HNP_SRP_CAPABLE_OTG 0
+#define DWC_HWCFG2_OP_MODE_SRP_ONLY_CAPABLE_OTG 1
+#define DWC_HWCFG2_OP_MODE_NO_HNP_SRP_CAPABLE_OTG 2
+#define DWC_HWCFG2_OP_MODE_SRP_CAPABLE_DEVICE 3
+#define DWC_HWCFG2_OP_MODE_NO_SRP_CAPABLE_DEVICE 4
+#define DWC_HWCFG2_OP_MODE_SRP_CAPABLE_HOST 5
+#define DWC_HWCFG2_OP_MODE_NO_SRP_CAPABLE_HOST 6
+
+		unsigned architecture:2;
+		unsigned point2point:1;
+		unsigned hs_phy_type:2;
+#define DWC_HWCFG2_HS_PHY_TYPE_NOT_SUPPORTED 0
+#define DWC_HWCFG2_HS_PHY_TYPE_UTMI 1
+#define DWC_HWCFG2_HS_PHY_TYPE_ULPI 2
+#define DWC_HWCFG2_HS_PHY_TYPE_UTMI_ULPI 3
+
+		unsigned fs_phy_type:2;
+		unsigned num_dev_ep:4;
+		unsigned num_host_chan:4;
+		unsigned perio_ep_supported:1;
+		unsigned dynamic_fifo:1;
+		unsigned multi_proc_int:1;
+		unsigned reserved21:1;
+		unsigned nonperio_tx_q_depth:2;
+		unsigned host_perio_tx_q_depth:2;
+		unsigned dev_token_q_depth:5;
+		unsigned otg_enable_ic_usb:1;
+	} b;
+} hwcfg2_data_t;
+
+/**
+ * This union represents the bit fields in the User HW Config3
+ * Register.  Read the register into the <i>d32</i> element then read
+ * out the bits using the <i>b</i>it elements.
+ */
+typedef union hwcfg3_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		/* GHWCFG3 */
+		unsigned xfer_size_cntr_width:4;
+		unsigned packet_size_cntr_width:3;
+		unsigned otg_func:1;
+		unsigned i2c:1;
+		unsigned vendor_ctrl_if:1;
+		unsigned optional_features:1;
+		unsigned synch_reset_type:1;
+		unsigned adp_supp:1;
+		unsigned otg_enable_hsic:1;
+		unsigned bc_support:1;
+		unsigned otg_lpm_en:1;
+		unsigned dfifo_depth:16;
+	} b;
+} hwcfg3_data_t;
+
+/**
+ * This union represents the bit fields in the User HW Config4
+ * Register.  Read the register into the <i>d32</i> element then read
+ * out the bits using the <i>b</i>it elements.
+ */
+typedef union hwcfg4_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		unsigned num_dev_perio_in_ep:4;
+		unsigned power_optimiz:1;
+		unsigned min_ahb_freq:1;
+		unsigned part_power_down:1;
+		unsigned reserved:7;
+		unsigned utmi_phy_data_width:2;
+		unsigned num_dev_mode_ctrl_ep:4;
+		unsigned iddig_filt_en:1;
+		unsigned vbus_valid_filt_en:1;
+		unsigned a_valid_filt_en:1;
+		unsigned b_valid_filt_en:1;
+		unsigned session_end_filt_en:1;
+		unsigned ded_fifo_en:1;
+		unsigned num_in_eps:4;
+		unsigned desc_dma:1;
+		unsigned desc_dma_dyn:1;
+	} b;
+} hwcfg4_data_t;
+
+/**
+ * This union represents the bit fields of the Core LPM Configuration
+ * Register (GLPMCFG). Set the bits using bit fields then write
+ * the <i>d32</i> value to the register.
+ */
+typedef union glpmctl_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		/** LPM-Capable (LPMCap) (Device and Host)
+		 * The application uses this bit to control
+		 * the DWC_otg core LPM capabilities.
+		 */
+		unsigned lpm_cap_en:1;
+		/** LPM response programmed by application (AppL1Res) (Device)
+		 * Handshake response to LPM token pre-programmed
+		 * by device application software.
+		 */
+		unsigned appl_resp:1;
+		/** Host Initiated Resume Duration (HIRD) (Device and Host)
+		 * In Host mode this field indicates the value of HIRD
+		 * to be sent in an LPM transaction.
+		 * In Device mode this field is updated with the
+		 * Received LPM Token HIRD bmAttribute
+		 * when an ACK/NYET/STALL response is sent
+		 * to an LPM transaction.
+		 */
+		unsigned hird:4;
+		/** RemoteWakeEnable (bRemoteWake) (Device and Host)
+		 * In Host mode this bit indicates the value of remote
+		 * wake up to be sent in wIndex field of LPM transaction.
+		 * In Device mode this field is updated with the
+		 * Received LPM Token bRemoteWake bmAttribute
+		 * when an ACK/NYET/STALL response is sent
+		 * to an LPM transaction.
+		 */
+		unsigned rem_wkup_en:1;
+		/** Enable utmi_sleep_n (EnblSlpM) (Device and Host)
+		 * The application uses this bit to control
+		 * the utmi_sleep_n assertion to the PHY when in L1 state.
+		 */
+		unsigned en_utmi_sleep:1;
+		/** HIRD Threshold (HIRD_Thres) (Device and Host)
+		 */
+		unsigned hird_thres:5;
+		/** LPM Response (CoreL1Res) (Device and Host)
+		 * In Host mode this bit contains handsake response to
+		 * LPM transaction.
+		 * In Device mode the response of the core to
+		 * LPM transaction received is reflected in these two bits.
+		 	- 0x0 : ERROR (No handshake response)
+			- 0x1 : STALL
+			- 0x2 : NYET
+			- 0x3 : ACK			
+		 */
+		unsigned lpm_resp:2;
+		/** Port Sleep Status (SlpSts) (Device and Host)
+		 * This bit is set as long as a Sleep condition
+		 * is present on the USB bus.
+		 */
+		unsigned prt_sleep_sts:1;
+		/** Sleep State Resume OK (L1ResumeOK) (Device and Host)
+		 * Indicates that the application or host
+		 * can start resume from Sleep state.
+		 */
+		unsigned sleep_state_resumeok:1;
+		/** LPM channel Index (LPM_Chnl_Indx) (Host)
+		 * The channel number on which the LPM transaction
+		 * has to be applied while sending
+		 * an LPM transaction to the local device.
+		 */
+		unsigned lpm_chan_index:4;
+		/** LPM Retry Count (LPM_Retry_Cnt) (Host)
+		 * Number host retries that would be performed
+		 * if the device response was not valid response.
+		 */
+		unsigned retry_count:3;
+		/** Send LPM Transaction (SndLPM) (Host)
+		 * When set by application software,
+		 * an LPM transaction containing two tokens
+		 * is sent.
+		 */
+		unsigned send_lpm:1;
+		/** LPM Retry status (LPM_RetryCnt_Sts) (Host)
+		 * Number of LPM Host Retries still remaining
+		 * to be transmitted for the current LPM sequence
+		 */
+		unsigned retry_count_sts:3;
+		unsigned reserved28_29:2;
+		/** In host mode once this bit is set, the host
+		 * configures to drive the HSIC Idle state on the bus.
+		 * It then waits for the  device to initiate the Connect sequence.
+		 * In device mode once this bit is set, the device waits for
+		 * the HSIC Idle line state on the bus. Upon receving the Idle
+		 * line state, it initiates the HSIC Connect sequence.
+		 */
+		unsigned hsic_connect:1;
+		/** This bit overrides and functionally inverts
+		 * the if_select_hsic input port signal.
+		 */
+		unsigned inv_sel_hsic:1;
+	} b;
+} glpmcfg_data_t;
+
+/**
+ * This union represents the bit fields of the Core ADP Timer, Control and
+ * Status Register (ADPTIMCTLSTS). Set the bits using bit fields then write
+ * the <i>d32</i> value to the register.
+ */
+typedef union adpctl_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		/** Probe Discharge (PRB_DSCHG)
+		 *  These bits set the times for TADP_DSCHG. 
+		 *  These bits are defined as follows:
+		 *  2'b00 - 4 msec
+		 *  2'b01 - 8 msec
+		 *  2'b10 - 16 msec
+		 *  2'b11 - 32 msec
+		 */
+		unsigned prb_dschg:2;
+		/** Probe Delta (PRB_DELTA)
+		 *  These bits set the resolution for RTIM   value.
+		 *  The bits are defined in units of 32 kHz clock cycles as follows:
+		 *  2'b00  -  1 cycles
+		 *  2'b01  -  2 cycles
+		 *  2'b10 -  3 cycles
+		 *  2'b11 - 4 cycles
+		 *  For example if this value is chosen to 2'b01, it means that RTIM
+		 *  increments for every 3(three) 32Khz clock cycles.
+		 */
+		unsigned prb_delta:2;
+		/** Probe Period (PRB_PER)
+		 *  These bits sets the TADP_PRD as shown in Figure 4 as follows:
+		 *  2'b00  -  0.625 to 0.925 sec (typical 0.775 sec)
+		 *  2'b01  -  1.25 to 1.85 sec (typical 1.55 sec)
+		 *  2'b10  -  1.9 to 2.6 sec (typical 2.275 sec)
+		 *  2'b11  -  Reserved
+		 */
+		unsigned prb_per:2;
+		/** These bits capture the latest time it took for VBUS to ramp from 
+		 *  VADP_SINK to VADP_PRB. 
+		 *  0x000  -  1 cycles
+		 *  0x001  -  2 cycles
+		 *  0x002  -  3 cycles
+		 *  etc
+		 *  0x7FF  -  2048 cycles
+		 *  A time of 1024 cycles at 32 kHz corresponds to a time of 32 msec.
+		*/
+		unsigned rtim:11;
+		/** Enable Probe (EnaPrb)
+		 *  When programmed to 1'b1, the core performs a probe operation.
+		 *  This bit is valid only if OTG_Ver = 1'b1.
+		 */
+		unsigned enaprb:1;
+		/** Enable Sense (EnaSns)
+		 *  When programmed to 1'b1, the core performs a Sense operation.
+		 *  This bit is valid only if OTG_Ver = 1'b1.
+		 */
+		unsigned enasns:1;
+		/** ADP Reset (ADPRes)
+		 *  When set, ADP controller is reset.
+		 *  This bit is valid only if OTG_Ver = 1'b1.
+ 		 */
+		unsigned adpres:1;
+		/** ADP Enable (ADPEn)
+		 *  When set, the core performs either ADP probing or sensing
+		 *  based on EnaPrb or EnaSns.
+		 *  This bit is valid only if OTG_Ver = 1'b1.
+		 */
+		unsigned adpen:1;
+		/** ADP Probe Interrupt (ADP_PRB_INT)
+		 *  When this bit is set, it means that the VBUS
+		 *  voltage is greater than VADP_PRB or VADP_PRB is reached.
+		 *  This bit is valid only if OTG_Ver = 1'b1.
+		 */
+		unsigned adp_prb_int:1;
+		/**
+		 *  ADP Sense Interrupt (ADP_SNS_INT)
+		 *  When this bit is set, it means that the VBUS voltage is greater than 
+		 *  VADP_SNS value or VADP_SNS is reached.
+		 *  This bit is valid only if OTG_Ver = 1'b1.
+		 */
+		unsigned adp_sns_int:1;
+		/** ADP Tomeout Interrupt (ADP_TMOUT_INT)
+		 *  This bit is relevant only for an ADP probe.
+		 *  When this bit is set, it means that the ramp time has
+		 *  completed ie ADPCTL.RTIM has reached its terminal value
+		 *  of 0x7FF.  This is a debug feature that allows software
+		 *  to read the ramp time after each cycle.
+		 *  This bit is valid only if OTG_Ver = 1'b1.
+		 */
+		unsigned adp_tmout_int:1;
+		/** ADP Probe Interrupt Mask (ADP_PRB_INT_MSK)
+		 *  When this bit is set, it unmasks the interrupt due to ADP_PRB_INT.
+		 *  This bit is valid only if OTG_Ver = 1'b1.
+		 */
+		unsigned adp_prb_int_msk:1;
+		/** ADP Sense Interrupt Mask (ADP_SNS_INT_MSK)
+		 *  When this bit is set, it unmasks the interrupt due to ADP_SNS_INT.
+		 *  This bit is valid only if OTG_Ver = 1'b1.
+		 */
+		unsigned adp_sns_int_msk:1;
+		/** ADP Timoeout Interrupt Mask (ADP_TMOUT_MSK)
+		 *  When this bit is set, it unmasks the interrupt due to ADP_TMOUT_INT.
+		 *  This bit is valid only if OTG_Ver = 1'b1.
+		 */
+		unsigned adp_tmout_int_msk:1;
+		/** Access Request
+		 * 2'b00 - Read/Write Valid (updated by the core) 
+		 * 2'b01 - Read
+		 * 2'b00 - Write
+		 * 2'b00 - Reserved
+		 */
+		unsigned ar:2;
+		 /** Reserved */
+		unsigned reserved29_31:3;
+	} b;
+} adpctl_data_t;
+
+////////////////////////////////////////////
+// Device Registers
+/**
+ * Device Global Registers. <i>Offsets 800h-BFFh</i>
+ *
+ * The following structures define the size and relative field offsets
+ * for the Device Mode Registers.
+ *
+ * <i>These registers are visible only in Device mode and must not be
+ * accessed in Host mode, as the results are unknown.</i>
+ */
+typedef struct dwc_otg_dev_global_regs {
+	/** Device Configuration Register. <i>Offset 800h</i> */
+	volatile uint32_t dcfg;
+	/** Device Control Register. <i>Offset: 804h</i> */
+	volatile uint32_t dctl;
+	/** Device Status Register (Read Only). <i>Offset: 808h</i> */
+	volatile uint32_t dsts;
+	/** Reserved. <i>Offset: 80Ch</i> */
+	uint32_t unused;
+	/** Device IN Endpoint Common Interrupt Mask
+	 * Register. <i>Offset: 810h</i> */
+	volatile uint32_t diepmsk;
+	/** Device OUT Endpoint Common Interrupt Mask
+	 * Register. <i>Offset: 814h</i> */
+	volatile uint32_t doepmsk;
+	/** Device All Endpoints Interrupt Register.  <i>Offset: 818h</i> */
+	volatile uint32_t daint;
+	/** Device All Endpoints Interrupt Mask Register.  <i>Offset:
+	 * 81Ch</i> */
+	volatile uint32_t daintmsk;
+	/** Device IN Token Queue Read Register-1 (Read Only).
+	 * <i>Offset: 820h</i> */
+	volatile uint32_t dtknqr1;
+	/** Device IN Token Queue Read Register-2 (Read Only).
+	 * <i>Offset: 824h</i> */
+	volatile uint32_t dtknqr2;
+	/** Device VBUS	 discharge Register.  <i>Offset: 828h</i> */
+	volatile uint32_t dvbusdis;
+	/** Device VBUS Pulse Register.	 <i>Offset: 82Ch</i> */
+	volatile uint32_t dvbuspulse;
+	/** Device IN Token Queue Read Register-3 (Read Only). /
+	 *	Device Thresholding control register (Read/Write)
+	 * <i>Offset: 830h</i> */
+	volatile uint32_t dtknqr3_dthrctl;
+	/** Device IN Token Queue Read Register-4 (Read Only). /
+	 *	Device IN EPs empty Inr. Mask Register (Read/Write)
+	 * <i>Offset: 834h</i> */
+	volatile uint32_t dtknqr4_fifoemptymsk;
+	/** Device Each Endpoint Interrupt Register (Read Only). /
+	 * <i>Offset: 838h</i> */
+	volatile uint32_t deachint;
+	/** Device Each Endpoint Interrupt mask Register (Read/Write). /
+	 * <i>Offset: 83Ch</i> */
+	volatile uint32_t deachintmsk;
+	/** Device Each In Endpoint Interrupt mask Register (Read/Write). /
+	 * <i>Offset: 840h</i> */
+	volatile uint32_t diepeachintmsk[MAX_EPS_CHANNELS];
+	/** Device Each Out Endpoint Interrupt mask Register (Read/Write). /
+	 * <i>Offset: 880h</i> */
+	volatile uint32_t doepeachintmsk[MAX_EPS_CHANNELS];
+} dwc_otg_device_global_regs_t;
+
+/**
+ * This union represents the bit fields in the Device Configuration
+ * Register.  Read the register into the <i>d32</i> member then
+ * set/clear the bits using the <i>b</i>it elements.  Write the
+ * <i>d32</i> member to the dcfg register.
+ */
+typedef union dcfg_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		/** Device Speed */
+		unsigned devspd:2;
+		/** Non Zero Length Status OUT Handshake */
+		unsigned nzstsouthshk:1;
+#define DWC_DCFG_SEND_STALL 1
+
+		unsigned ena32khzs:1;
+		/** Device Addresses */
+		unsigned devaddr:7;
+		/** Periodic Frame Interval */
+		unsigned perfrint:2;
+#define DWC_DCFG_FRAME_INTERVAL_80 0
+#define DWC_DCFG_FRAME_INTERVAL_85 1
+#define DWC_DCFG_FRAME_INTERVAL_90 2
+#define DWC_DCFG_FRAME_INTERVAL_95 3
+		
+		/** Enable Device OUT NAK for bulk in DDMA mode */
+		unsigned endevoutnak:1;
+
+		unsigned reserved14_17:4;
+		/** In Endpoint Mis-match count */
+		unsigned epmscnt:5;
+		/** Enable Descriptor DMA in Device mode */
+		unsigned descdma:1;
+		unsigned perschintvl:2;
+		unsigned resvalid:6;
+	} b;
+} dcfg_data_t;
+
+/**
+ * This union represents the bit fields in the Device Control
+ * Register.  Read the register into the <i>d32</i> member then
+ * set/clear the bits using the <i>b</i>it elements.
+ */
+typedef union dctl_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		/** Remote Wakeup */
+		unsigned rmtwkupsig:1;
+		/** Soft Disconnect */
+		unsigned sftdiscon:1;
+		/** Global Non-Periodic IN NAK Status */
+		unsigned gnpinnaksts:1;
+		/** Global OUT NAK Status */
+		unsigned goutnaksts:1;
+		/** Test Control */
+		unsigned tstctl:3;
+		/** Set Global Non-Periodic IN NAK */
+		unsigned sgnpinnak:1;
+		/** Clear Global Non-Periodic IN NAK */
+		unsigned cgnpinnak:1;
+		/** Set Global OUT NAK */
+		unsigned sgoutnak:1;
+		/** Clear Global OUT NAK */
+		unsigned cgoutnak:1;
+		/** Power-On Programming Done */
+		unsigned pwronprgdone:1;
+		/** Reserved */
+		unsigned reserved:1;
+		/** Global Multi Count */
+		unsigned gmc:2;
+		/** Ignore Frame Number for ISOC EPs */
+		unsigned ifrmnum:1;
+		/** NAK on Babble */
+		unsigned nakonbble:1;
+		/** Enable Continue on BNA */
+		unsigned encontonbna:1;
+
+		unsigned reserved18_31:14;
+	} b;
+} dctl_data_t;
+
+/**
+ * This union represents the bit fields in the Device Status
+ * Register.  Read the register into the <i>d32</i> member then
+ * set/clear the bits using the <i>b</i>it elements.
+ */
+typedef union dsts_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		/** Suspend Status */
+		unsigned suspsts:1;
+		/** Enumerated Speed */
+		unsigned enumspd:2;
+#define DWC_DSTS_ENUMSPD_HS_PHY_30MHZ_OR_60MHZ 0
+#define DWC_DSTS_ENUMSPD_FS_PHY_30MHZ_OR_60MHZ 1
+#define DWC_DSTS_ENUMSPD_LS_PHY_6MHZ		   2
+#define DWC_DSTS_ENUMSPD_FS_PHY_48MHZ		   3
+		/** Erratic Error */
+		unsigned errticerr:1;
+		unsigned reserved4_7:4;
+		/** Frame or Microframe Number of the received SOF */
+		unsigned soffn:14;
+		unsigned reserved22_31:10;
+	} b;
+} dsts_data_t;
+
+/**
+ * This union represents the bit fields in the Device IN EP Interrupt
+ * Register and the Device IN EP Common Mask Register.
+ *
+ * - Read the register into the <i>d32</i> member then set/clear the
+ *	 bits using the <i>b</i>it elements.
+ */
+typedef union diepint_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		/** Transfer complete mask */
+		unsigned xfercompl:1;
+		/** Endpoint disable mask */
+		unsigned epdisabled:1;
+		/** AHB Error mask */
+		unsigned ahberr:1;
+		/** TimeOUT Handshake mask (non-ISOC EPs) */
+		unsigned timeout:1;
+		/** IN Token received with TxF Empty mask */
+		unsigned intktxfemp:1;
+		/** IN Token Received with EP mismatch mask */
+		unsigned intknepmis:1;
+		/** IN Endpoint NAK Effective mask */
+		unsigned inepnakeff:1;
+		/** Reserved */
+		unsigned emptyintr:1;
+
+		unsigned txfifoundrn:1;
+
+		/** BNA Interrupt mask */
+		unsigned bna:1;
+
+		unsigned reserved10_12:3;
+		/** BNA Interrupt mask */
+		unsigned nak:1;
+
+		unsigned reserved14_31:18;
+	} b;
+} diepint_data_t;
+
+/**
+ * This union represents the bit fields in the Device IN EP
+ * Common/Dedicated Interrupt Mask Register.
+ */
+typedef union diepint_data diepmsk_data_t;
+
+/**
+ * This union represents the bit fields in the Device OUT EP Interrupt
+ * Registerand Device OUT EP Common Interrupt Mask Register.
+ *
+ * - Read the register into the <i>d32</i> member then set/clear the
+ *	 bits using the <i>b</i>it elements.
+ */
+typedef union doepint_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		/** Transfer complete */
+		unsigned xfercompl:1;
+		/** Endpoint disable  */
+		unsigned epdisabled:1;
+		/** AHB Error */
+		unsigned ahberr:1;
+		/** Setup Phase Done (contorl EPs) */
+		unsigned setup:1;
+		/** OUT Token Received when Endpoint Disabled */
+		unsigned outtknepdis:1;
+
+		unsigned stsphsercvd:1;
+		/** Back-to-Back SETUP Packets Received */
+		unsigned back2backsetup:1;
+
+		unsigned reserved7:1;
+		/** OUT packet Error */
+		unsigned outpkterr:1;
+		/** BNA Interrupt */
+		unsigned bna:1;
+
+		unsigned reserved10:1;
+		/** Packet Drop Status */
+		unsigned pktdrpsts:1;
+		/** Babble Interrupt */
+		unsigned babble:1;
+		/** NAK Interrupt */
+		unsigned nak:1;
+		/** NYET Interrupt */
+		unsigned nyet:1;
+
+		unsigned reserved15_31:17;
+	} b;
+} doepint_data_t;
+
+/**
+ * This union represents the bit fields in the Device OUT EP
+ * Common/Dedicated Interrupt Mask Register.
+ */
+typedef union doepint_data doepmsk_data_t;
+
+/**
+ * This union represents the bit fields in the Device All EP Interrupt
+ * and Mask Registers.
+ * - Read the register into the <i>d32</i> member then set/clear the
+ *	 bits using the <i>b</i>it elements.
+ */
+typedef union daint_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		/** IN Endpoint bits */
+		unsigned in:16;
+		/** OUT Endpoint bits */
+		unsigned out:16;
+	} ep;
+	struct {
+		/** IN Endpoint bits */
+		unsigned inep0:1;
+		unsigned inep1:1;
+		unsigned inep2:1;
+		unsigned inep3:1;
+		unsigned inep4:1;
+		unsigned inep5:1;
+		unsigned inep6:1;
+		unsigned inep7:1;
+		unsigned inep8:1;
+		unsigned inep9:1;
+		unsigned inep10:1;
+		unsigned inep11:1;
+		unsigned inep12:1;
+		unsigned inep13:1;
+		unsigned inep14:1;
+		unsigned inep15:1;
+		/** OUT Endpoint bits */
+		unsigned outep0:1;
+		unsigned outep1:1;
+		unsigned outep2:1;
+		unsigned outep3:1;
+		unsigned outep4:1;
+		unsigned outep5:1;
+		unsigned outep6:1;
+		unsigned outep7:1;
+		unsigned outep8:1;
+		unsigned outep9:1;
+		unsigned outep10:1;
+		unsigned outep11:1;
+		unsigned outep12:1;
+		unsigned outep13:1;
+		unsigned outep14:1;
+		unsigned outep15:1;
+	} b;
+} daint_data_t;
+
+/**
+ * This union represents the bit fields in the Device IN Token Queue
+ * Read Registers.
+ * - Read the register into the <i>d32</i> member.
+ * - READ-ONLY Register
+ */
+typedef union dtknq1_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		/** In Token Queue Write Pointer */
+		unsigned intknwptr:5;
+		/** Reserved */
+		unsigned reserved05_06:2;
+		/** write pointer has wrapped. */
+		unsigned wrap_bit:1;
+		/** EP Numbers of IN Tokens 0 ... 4 */
+		unsigned epnums0_5:24;
+	} b;
+} dtknq1_data_t;
+
+/**
+ * This union represents Threshold control Register
+ * - Read and write the register into the <i>d32</i> member.
+ * - READ-WRITABLE Register
+ */
+typedef union dthrctl_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		/** non ISO Tx Thr. Enable */
+		unsigned non_iso_thr_en:1;
+		/** ISO Tx Thr. Enable */
+		unsigned iso_thr_en:1;
+		/** Tx Thr. Length */
+		unsigned tx_thr_len:9;
+		/** AHB Threshold ratio */
+		unsigned ahb_thr_ratio:2;
+		/** Reserved */
+		unsigned reserved13_15:3;
+		/** Rx Thr. Enable */
+		unsigned rx_thr_en:1;
+		/** Rx Thr. Length */
+		unsigned rx_thr_len:9;
+		unsigned reserved26:1;
+		/** Arbiter Parking Enable*/
+		unsigned arbprken:1;
+		/** Reserved */
+		unsigned reserved28_31:4;
+	} b;
+} dthrctl_data_t;
+
+/**
+ * Device Logical IN Endpoint-Specific Registers. <i>Offsets
+ * 900h-AFCh</i>
+ *
+ * There will be one set of endpoint registers per logical endpoint
+ * implemented.
+ *
+ * <i>These registers are visible only in Device mode and must not be
+ * accessed in Host mode, as the results are unknown.</i>
+ */
+typedef struct dwc_otg_dev_in_ep_regs {
+	/** Device IN Endpoint Control Register. <i>Offset:900h +
+	 * (ep_num * 20h) + 00h</i> */
+	volatile uint32_t diepctl;
+	/** Reserved. <i>Offset:900h + (ep_num * 20h) + 04h</i> */
+	uint32_t reserved04;
+	/** Device IN Endpoint Interrupt Register. <i>Offset:900h +
+	 * (ep_num * 20h) + 08h</i> */
+	volatile uint32_t diepint;
+	/** Reserved. <i>Offset:900h + (ep_num * 20h) + 0Ch</i> */
+	uint32_t reserved0C;
+	/** Device IN Endpoint Transfer Size
+	 * Register. <i>Offset:900h + (ep_num * 20h) + 10h</i> */
+	volatile uint32_t dieptsiz;
+	/** Device IN Endpoint DMA Address Register. <i>Offset:900h +
+	 * (ep_num * 20h) + 14h</i> */
+	volatile uint32_t diepdma;
+	/** Device IN Endpoint Transmit FIFO Status Register. <i>Offset:900h +
+	 * (ep_num * 20h) + 18h</i> */
+	volatile uint32_t dtxfsts;
+	/** Device IN Endpoint DMA Buffer Register. <i>Offset:900h +
+	 * (ep_num * 20h) + 1Ch</i> */
+	volatile uint32_t diepdmab;
+} dwc_otg_dev_in_ep_regs_t;
+
+/**
+ * Device Logical OUT Endpoint-Specific Registers. <i>Offsets:
+ * B00h-CFCh</i>
+ *
+ * There will be one set of endpoint registers per logical endpoint
+ * implemented.
+ *
+ * <i>These registers are visible only in Device mode and must not be
+ * accessed in Host mode, as the results are unknown.</i>
+ */
+typedef struct dwc_otg_dev_out_ep_regs {
+	/** Device OUT Endpoint Control Register. <i>Offset:B00h +
+	 * (ep_num * 20h) + 00h</i> */
+	volatile uint32_t doepctl;
+	/** Reserved. <i>Offset:B00h + (ep_num * 20h) + 04h</i> */
+	uint32_t reserved04;
+	/** Device OUT Endpoint Interrupt Register. <i>Offset:B00h +
+	 * (ep_num * 20h) + 08h</i> */
+	volatile uint32_t doepint;
+	/** Reserved. <i>Offset:B00h + (ep_num * 20h) + 0Ch</i> */
+	uint32_t reserved0C;
+	/** Device OUT Endpoint Transfer Size Register. <i>Offset:
+	 * B00h + (ep_num * 20h) + 10h</i> */
+	volatile uint32_t doeptsiz;
+	/** Device OUT Endpoint DMA Address Register. <i>Offset:B00h
+	 * + (ep_num * 20h) + 14h</i> */
+	volatile uint32_t doepdma;
+	/** Reserved. <i>Offset:B00h + 	 * (ep_num * 20h) + 18h</i> */
+	uint32_t unused;
+	/** Device OUT Endpoint DMA Buffer Register. <i>Offset:B00h
+	 * + (ep_num * 20h) + 1Ch</i> */
+	uint32_t doepdmab;
+} dwc_otg_dev_out_ep_regs_t;
+
+/**
+ * This union represents the bit fields in the Device EP Control
+ * Register.  Read the register into the <i>d32</i> member then
+ * set/clear the bits using the <i>b</i>it elements.
+ */
+typedef union depctl_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		/** Maximum Packet Size
+		 * IN/OUT EPn
+		 * IN/OUT EP0 - 2 bits
+		 *	 2'b00: 64 Bytes
+		 *	 2'b01: 32
+		 *	 2'b10: 16
+		 *	 2'b11: 8 */
+		unsigned mps:11;
+#define DWC_DEP0CTL_MPS_64	 0
+#define DWC_DEP0CTL_MPS_32	 1
+#define DWC_DEP0CTL_MPS_16	 2
+#define DWC_DEP0CTL_MPS_8	 3
+
+		/** Next Endpoint
+		 * IN EPn/IN EP0
+		 * OUT EPn/OUT EP0 - reserved */
+		unsigned nextep:4;
+
+		/** USB Active Endpoint */
+		unsigned usbactep:1;
+
+		/** Endpoint DPID (INTR/Bulk IN and OUT endpoints)
+		 * This field contains the PID of the packet going to
+		 * be received or transmitted on this endpoint. The
+		 * application should program the PID of the first
+		 * packet going to be received or transmitted on this
+		 * endpoint , after the endpoint is
+		 * activated. Application use the SetD1PID and
+		 * SetD0PID fields of this register to program either
+		 * D0 or D1 PID.
+		 *
+		 * The encoding for this field is
+		 *	 - 0: D0
+		 *	 - 1: D1
+		 */
+		unsigned dpid:1;
+
+		/** NAK Status */
+		unsigned naksts:1;
+
+		/** Endpoint Type
+		 *	2'b00: Control
+		 *	2'b01: Isochronous
+		 *	2'b10: Bulk
+		 *	2'b11: Interrupt */
+		unsigned eptype:2;
+
+		/** Snoop Mode
+		 * OUT EPn/OUT EP0
+		 * IN EPn/IN EP0 - reserved */
+		unsigned snp:1;
+
+		/** Stall Handshake */
+		unsigned stall:1;
+
+		/** Tx Fifo Number
+		 * IN EPn/IN EP0
+		 * OUT EPn/OUT EP0 - reserved */
+		unsigned txfnum:4;
+
+		/** Clear NAK */
+		unsigned cnak:1;
+		/** Set NAK */
+		unsigned snak:1;
+		/** Set DATA0 PID (INTR/Bulk IN and OUT endpoints)
+		 * Writing to this field sets the Endpoint DPID (DPID)
+		 * field in this register to DATA0. Set Even
+		 * (micro)frame (SetEvenFr) (ISO IN and OUT Endpoints)
+		 * Writing to this field sets the Even/Odd
+		 * (micro)frame (EO_FrNum) field to even (micro)
+		 * frame.
+		 */
+		unsigned setd0pid:1;
+		/** Set DATA1 PID (INTR/Bulk IN and OUT endpoints)
+		 * Writing to this field sets the Endpoint DPID (DPID)
+		 * field in this register to DATA1 Set Odd
+		 * (micro)frame (SetOddFr) (ISO IN and OUT Endpoints)
+		 * Writing to this field sets the Even/Odd
+		 * (micro)frame (EO_FrNum) field to odd (micro) frame.
+		 */
+		unsigned setd1pid:1;
+
+		/** Endpoint Disable */
+		unsigned epdis:1;
+		/** Endpoint Enable */
+		unsigned epena:1;
+	} b;
+} depctl_data_t;
+
+/**
+ * This union represents the bit fields in the Device EP Transfer
+ * Size Register.  Read the register into the <i>d32</i> member then
+ * set/clear the bits using the <i>b</i>it elements.
+ */
+typedef union deptsiz_data {
+		/** raw register data */
+	uint32_t d32;
+		/** register bits */
+	struct {
+		/** Transfer size */
+		unsigned xfersize:19;
+/** Max packet count for EP (pow(2,10)-1) */
+#define MAX_PKT_CNT 1023
+		/** Packet Count */
+		unsigned pktcnt:10;
+		/** Multi Count - Periodic IN endpoints */
+		unsigned mc:2;
+		unsigned reserved:1;
+	} b;
+} deptsiz_data_t;
+
+/**
+ * This union represents the bit fields in the Device EP 0 Transfer
+ * Size Register.  Read the register into the <i>d32</i> member then
+ * set/clear the bits using the <i>b</i>it elements.
+ */
+typedef union deptsiz0_data {
+		/** raw register data */
+	uint32_t d32;
+		/** register bits */
+	struct {
+		/** Transfer size */
+		unsigned xfersize:7;
+				/** Reserved */
+		unsigned reserved7_18:12;
+		/** Packet Count */
+		unsigned pktcnt:2;
+				/** Reserved */
+		unsigned reserved21_28:8;
+				/**Setup Packet Count (DOEPTSIZ0 Only) */
+		unsigned supcnt:2;
+		unsigned reserved31;
+	} b;
+} deptsiz0_data_t;
+
+/////////////////////////////////////////////////
+// DMA Descriptor Specific Structures
+//
+
+/** Buffer status definitions */
+
+#define BS_HOST_READY	0x0
+#define BS_DMA_BUSY		0x1
+#define BS_DMA_DONE		0x2
+#define BS_HOST_BUSY	0x3
+
+/** Receive/Transmit status definitions */
+
+#define RTS_SUCCESS		0x0
+#define RTS_BUFFLUSH	0x1
+#define RTS_RESERVED	0x2
+#define RTS_BUFERR		0x3
+
+/**
+ * This union represents the bit fields in the DMA Descriptor
+ * status quadlet. Read the quadlet into the <i>d32</i> member then
+ * set/clear the bits using the <i>b</i>it, <i>b_iso_out</i> and
+ * <i>b_iso_in</i> elements.
+ */
+typedef union dev_dma_desc_sts {
+		/** raw register data */
+	uint32_t d32;
+		/** quadlet bits */
+	struct {
+		/** Received number of bytes */
+		unsigned bytes:16;
+		/** NAK bit - only for OUT EPs */
+		unsigned nak:1;
+		unsigned reserved17_22:6;
+		/** Multiple Transfer - only for OUT EPs */
+		unsigned mtrf:1;
+		/** Setup Packet received - only for OUT EPs */
+		unsigned sr:1;
+		/** Interrupt On Complete */
+		unsigned ioc:1;
+		/** Short Packet */
+		unsigned sp:1;
+		/** Last */
+		unsigned l:1;
+		/** Receive Status */
+		unsigned sts:2;
+		/** Buffer Status */
+		unsigned bs:2;
+	} b;
+
+//#ifdef DWC_EN_ISOC
+		/** iso out quadlet bits */
+	struct {
+		/** Received number of bytes */
+		unsigned rxbytes:11;
+
+		unsigned reserved11:1;
+		/** Frame Number */
+		unsigned framenum:11;
+		/** Received ISO Data PID */
+		unsigned pid:2;
+		/** Interrupt On Complete */
+		unsigned ioc:1;
+		/** Short Packet */
+		unsigned sp:1;
+		/** Last */
+		unsigned l:1;
+		/** Receive Status */
+		unsigned rxsts:2;
+		/** Buffer Status */
+		unsigned bs:2;
+	} b_iso_out;
+
+		/** iso in quadlet bits */
+	struct {
+		/** Transmited number of bytes */
+		unsigned txbytes:12;
+		/** Frame Number */
+		unsigned framenum:11;
+		/** Transmited ISO Data PID */
+		unsigned pid:2;
+		/** Interrupt On Complete */
+		unsigned ioc:1;
+		/** Short Packet */
+		unsigned sp:1;
+		/** Last */
+		unsigned l:1;
+		/** Transmit Status */
+		unsigned txsts:2;
+		/** Buffer Status */
+		unsigned bs:2;
+	} b_iso_in;
+//#endif                                /* DWC_EN_ISOC */
+} dev_dma_desc_sts_t;
+
+/**
+ * DMA Descriptor structure
+ *
+ * DMA Descriptor structure contains two quadlets:
+ * Status quadlet and Data buffer pointer.
+ */
+typedef struct dwc_otg_dev_dma_desc {
+	/** DMA Descriptor status quadlet */
+	dev_dma_desc_sts_t status;
+	/** DMA Descriptor data buffer pointer */
+	uint32_t buf;
+} dwc_otg_dev_dma_desc_t;
+
+/**
+ * The dwc_otg_dev_if structure contains information needed to manage
+ * the DWC_otg controller acting in device mode. It represents the
+ * programming view of the device-specific aspects of the controller.
+ */
+typedef struct dwc_otg_dev_if {
+	/** Pointer to device Global registers.
+	 * Device Global Registers starting at offset 800h
+	 */
+	dwc_otg_device_global_regs_t *dev_global_regs;
+#define DWC_DEV_GLOBAL_REG_OFFSET 0x800
+
+	/**
+	 * Device Logical IN Endpoint-Specific Registers 900h-AFCh
+	 */
+	dwc_otg_dev_in_ep_regs_t *in_ep_regs[MAX_EPS_CHANNELS];
+#define DWC_DEV_IN_EP_REG_OFFSET 0x900
+#define DWC_EP_REG_OFFSET 0x20
+
+	/** Device Logical OUT Endpoint-Specific Registers B00h-CFCh */
+	dwc_otg_dev_out_ep_regs_t *out_ep_regs[MAX_EPS_CHANNELS];
+#define DWC_DEV_OUT_EP_REG_OFFSET 0xB00
+
+	/* Device configuration information */
+	uint8_t speed;				 /**< Device Speed	0: Unknown, 1: LS, 2:FS, 3: HS */
+	uint8_t num_in_eps;		 /**< Number # of Tx EP range: 0-15 exept ep0 */
+	uint8_t num_out_eps;		 /**< Number # of Rx EP range: 0-15 exept ep 0*/
+
+	/** Size of periodic FIFOs (Bytes) */
+	uint16_t perio_tx_fifo_size[MAX_PERIO_FIFOS];
+
+	/** Size of Tx FIFOs (Bytes) */
+	uint16_t tx_fifo_size[MAX_TX_FIFOS];
+
+	/** Thresholding enable flags and length varaiables **/
+	uint16_t rx_thr_en;
+	uint16_t iso_tx_thr_en;
+	uint16_t non_iso_tx_thr_en;
+
+	uint16_t rx_thr_length;
+	uint16_t tx_thr_length;
+
+	/**
+	 * Pointers to the DMA Descriptors for EP0 Control
+	 * transfers (virtual and physical)
+	 */
+
+	/** 2 descriptors for SETUP packets */
+	dwc_dma_t dma_setup_desc_addr[2];
+	dwc_otg_dev_dma_desc_t *setup_desc_addr[2];
+
+	/** Pointer to Descriptor with latest SETUP packet */
+	dwc_otg_dev_dma_desc_t *psetup;
+
+	/** Index of current SETUP handler descriptor */
+	uint32_t setup_desc_index;
+
+	/** Descriptor for Data In or Status In phases */
+	dwc_dma_t dma_in_desc_addr;
+	dwc_otg_dev_dma_desc_t *in_desc_addr;
+
+	/** Descriptor for Data Out or Status Out phases */
+	dwc_dma_t dma_out_desc_addr;
+	dwc_otg_dev_dma_desc_t *out_desc_addr;
+
+	/** Setup Packet Detected - if set clear NAK when queueing */
+	uint32_t spd;
+	/** Isoc ep pointer on which incomplete happens */
+	void *isoc_ep;
+
+} dwc_otg_dev_if_t;
+
+/////////////////////////////////////////////////
+// Host Mode Register Structures
+//
+/**
+ * The Host Global Registers structure defines the size and relative
+ * field offsets for the Host Mode Global Registers.  Host Global
+ * Registers offsets 400h-7FFh.
+*/
+typedef struct dwc_otg_host_global_regs {
+	/** Host Configuration Register.   <i>Offset: 400h</i> */
+	volatile uint32_t hcfg;
+	/** Host Frame Interval Register.	<i>Offset: 404h</i> */
+	volatile uint32_t hfir;
+	/** Host Frame Number / Frame Remaining Register. <i>Offset: 408h</i> */
+	volatile uint32_t hfnum;
+	/** Reserved.	<i>Offset: 40Ch</i> */
+	uint32_t reserved40C;
+	/** Host Periodic Transmit FIFO/ Queue Status Register. <i>Offset: 410h</i> */
+	volatile uint32_t hptxsts;
+	/** Host All Channels Interrupt Register. <i>Offset: 414h</i> */
+	volatile uint32_t haint;
+	/** Host All Channels Interrupt Mask Register. <i>Offset: 418h</i> */
+	volatile uint32_t haintmsk;
+	/** Host Frame List Base Address Register . <i>Offset: 41Ch</i> */
+	volatile uint32_t hflbaddr;
+} dwc_otg_host_global_regs_t;
+
+/**
+ * This union represents the bit fields in the Host Configuration Register.
+ * Read the register into the <i>d32</i> member then set/clear the bits using
+ * the <i>b</i>it elements. Write the <i>d32</i> member to the hcfg register.
+ */
+typedef union hcfg_data {
+	/** raw register data */
+	uint32_t d32;
+
+	/** register bits */
+	struct {
+		/** FS/LS Phy Clock Select */
+		unsigned fslspclksel:2;
+#define DWC_HCFG_30_60_MHZ 0
+#define DWC_HCFG_48_MHZ	   1
+#define DWC_HCFG_6_MHZ	   2
+
+		/** FS/LS Only Support */
+		unsigned fslssupp:1;
+		unsigned reserved3_6:4;
+		/** Enable 32-KHz Suspend Mode */
+		unsigned ena32khzs:1;
+		/** Resume Validation Periiod */
+		unsigned resvalid:8;
+		unsigned reserved16_22:7;
+		/** Enable Scatter/gather DMA in Host mode */
+		unsigned descdma:1;
+		/** Frame List Entries */
+		unsigned frlisten:2;
+		/** Enable Periodic Scheduling */
+		unsigned perschedena:1;
+		unsigned reserved27_30:4;
+		unsigned modechtimen:1;
+	} b;
+} hcfg_data_t;
+
+/**
+ * This union represents the bit fields in the Host Frame Remaing/Number
+ * Register. 
+ */
+typedef union hfir_data {
+	/** raw register data */
+	uint32_t d32;
+
+	/** register bits */
+	struct {
+		unsigned frint:16;
+		unsigned hfirrldctrl:1;
+		unsigned reserved:15;
+	} b;
+} hfir_data_t;
+
+/**
+ * This union represents the bit fields in the Host Frame Remaing/Number
+ * Register. 
+ */
+typedef union hfnum_data {
+	/** raw register data */
+	uint32_t d32;
+
+	/** register bits */
+	struct {
+		unsigned frnum:16;
+#define DWC_HFNUM_MAX_FRNUM 0x3FFF
+		unsigned frrem:16;
+	} b;
+} hfnum_data_t;
+
+typedef union hptxsts_data {
+	/** raw register data */
+	uint32_t d32;
+
+	/** register bits */
+	struct {
+		unsigned ptxfspcavail:16;
+		unsigned ptxqspcavail:8;
+		/** Top of the Periodic Transmit Request Queue
+		 *	- bit 24 - Terminate (last entry for the selected channel)
+		 *	- bits 26:25 - Token Type
+		 *	  - 2'b00 - Zero length
+		 *	  - 2'b01 - Ping
+		 *	  - 2'b10 - Disable
+		 *	- bits 30:27 - Channel Number
+		 *	- bit 31 - Odd/even microframe
+		 */
+		unsigned ptxqtop_terminate:1;
+		unsigned ptxqtop_token:2;
+		unsigned ptxqtop_chnum:4;
+		unsigned ptxqtop_odd:1;
+	} b;
+} hptxsts_data_t;
+
+/**
+ * This union represents the bit fields in the Host Port Control and Status
+ * Register. Read the register into the <i>d32</i> member then set/clear the
+ * bits using the <i>b</i>it elements. Write the <i>d32</i> member to the
+ * hprt0 register.
+ */
+typedef union hprt0_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		unsigned prtconnsts:1;
+		unsigned prtconndet:1;
+		unsigned prtena:1;
+		unsigned prtenchng:1;
+		unsigned prtovrcurract:1;
+		unsigned prtovrcurrchng:1;
+		unsigned prtres:1;
+		unsigned prtsusp:1;
+		unsigned prtrst:1;
+		unsigned reserved9:1;
+		unsigned prtlnsts:2;
+		unsigned prtpwr:1;
+		unsigned prttstctl:4;
+		unsigned prtspd:2;
+#define DWC_HPRT0_PRTSPD_HIGH_SPEED 0
+#define DWC_HPRT0_PRTSPD_FULL_SPEED 1
+#define DWC_HPRT0_PRTSPD_LOW_SPEED	2
+		unsigned reserved19_31:13;
+	} b;
+} hprt0_data_t;
+
+/**
+ * This union represents the bit fields in the Host All Interrupt
+ * Register. 
+ */
+typedef union haint_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		unsigned ch0:1;
+		unsigned ch1:1;
+		unsigned ch2:1;
+		unsigned ch3:1;
+		unsigned ch4:1;
+		unsigned ch5:1;
+		unsigned ch6:1;
+		unsigned ch7:1;
+		unsigned ch8:1;
+		unsigned ch9:1;
+		unsigned ch10:1;
+		unsigned ch11:1;
+		unsigned ch12:1;
+		unsigned ch13:1;
+		unsigned ch14:1;
+		unsigned ch15:1;
+		unsigned reserved:16;
+	} b;
+
+	struct {
+		unsigned chint:16;
+		unsigned reserved:16;
+	} b2;
+} haint_data_t;
+
+/**
+ * This union represents the bit fields in the Host All Interrupt
+ * Register. 
+ */
+typedef union haintmsk_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		unsigned ch0:1;
+		unsigned ch1:1;
+		unsigned ch2:1;
+		unsigned ch3:1;
+		unsigned ch4:1;
+		unsigned ch5:1;
+		unsigned ch6:1;
+		unsigned ch7:1;
+		unsigned ch8:1;
+		unsigned ch9:1;
+		unsigned ch10:1;
+		unsigned ch11:1;
+		unsigned ch12:1;
+		unsigned ch13:1;
+		unsigned ch14:1;
+		unsigned ch15:1;
+		unsigned reserved:16;
+	} b;
+
+	struct {
+		unsigned chint:16;
+		unsigned reserved:16;
+	} b2;
+} haintmsk_data_t;
+
+/**
+ * Host Channel Specific Registers. <i>500h-5FCh</i>
+ */
+typedef struct dwc_otg_hc_regs {
+	/** Host Channel 0 Characteristic Register. <i>Offset: 500h + (chan_num * 20h) + 00h</i> */
+	volatile uint32_t hcchar;
+	/** Host Channel 0 Split Control Register. <i>Offset: 500h + (chan_num * 20h) + 04h</i> */
+	volatile uint32_t hcsplt;
+	/** Host Channel 0 Interrupt Register. <i>Offset: 500h + (chan_num * 20h) + 08h</i> */
+	volatile uint32_t hcint;
+	/** Host Channel 0 Interrupt Mask Register. <i>Offset: 500h + (chan_num * 20h) + 0Ch</i> */
+	volatile uint32_t hcintmsk;
+	/** Host Channel 0 Transfer Size Register. <i>Offset: 500h + (chan_num * 20h) + 10h</i> */
+	volatile uint32_t hctsiz;
+	/** Host Channel 0 DMA Address Register. <i>Offset: 500h + (chan_num * 20h) + 14h</i> */
+	volatile uint32_t hcdma;
+	volatile uint32_t reserved;
+	/** Host Channel 0 DMA Buffer Address Register. <i>Offset: 500h + (chan_num * 20h) + 1Ch</i> */
+	volatile uint32_t hcdmab;
+} dwc_otg_hc_regs_t;
+
+/**
+ * This union represents the bit fields in the Host Channel Characteristics
+ * Register. Read the register into the <i>d32</i> member then set/clear the
+ * bits using the <i>b</i>it elements. Write the <i>d32</i> member to the
+ * hcchar register.
+ */
+typedef union hcchar_data {
+	/** raw register data */
+	uint32_t d32;
+
+	/** register bits */
+	struct {
+		/** Maximum packet size in bytes */
+		unsigned mps:11;
+
+		/** Endpoint number */
+		unsigned epnum:4;
+
+		/** 0: OUT, 1: IN */
+		unsigned epdir:1;
+
+		unsigned reserved:1;
+
+		/** 0: Full/high speed device, 1: Low speed device */
+		unsigned lspddev:1;
+
+		/** 0: Control, 1: Isoc, 2: Bulk, 3: Intr */
+		unsigned eptype:2;
+
+		/** Packets per frame for periodic transfers. 0 is reserved. */
+		unsigned multicnt:2;
+
+		/** Device address */
+		unsigned devaddr:7;
+
+		/**
+		 * Frame to transmit periodic transaction.
+		 * 0: even, 1: odd
+		 */
+		unsigned oddfrm:1;
+
+		/** Channel disable */
+		unsigned chdis:1;
+
+		/** Channel enable */
+		unsigned chen:1;
+	} b;
+} hcchar_data_t;
+
+typedef union hcsplt_data {
+	/** raw register data */
+	uint32_t d32;
+
+	/** register bits */
+	struct {
+		/** Port Address */
+		unsigned prtaddr:7;
+
+		/** Hub Address */
+		unsigned hubaddr:7;
+
+		/** Transaction Position */
+		unsigned xactpos:2;
+#define DWC_HCSPLIT_XACTPOS_MID 0
+#define DWC_HCSPLIT_XACTPOS_END 1
+#define DWC_HCSPLIT_XACTPOS_BEGIN 2
+#define DWC_HCSPLIT_XACTPOS_ALL 3
+
+		/** Do Complete Split */
+		unsigned compsplt:1;
+
+		/** Reserved */
+		unsigned reserved:14;
+
+		/** Split Enble */
+		unsigned spltena:1;
+	} b;
+} hcsplt_data_t;
+
+/**
+ * This union represents the bit fields in the Host All Interrupt
+ * Register. 
+ */
+typedef union hcint_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		/** Transfer Complete */
+		unsigned xfercomp:1;
+		/** Channel Halted */
+		unsigned chhltd:1;
+		/** AHB Error */
+		unsigned ahberr:1;
+		/** STALL Response Received */
+		unsigned stall:1;
+		/** NAK Response Received */
+		unsigned nak:1;
+		/** ACK Response Received */
+		unsigned ack:1;
+		/** NYET Response Received */
+		unsigned nyet:1;
+		/** Transaction Err */
+		unsigned xacterr:1;
+		/** Babble Error */
+		unsigned bblerr:1;
+		/** Frame Overrun */
+		unsigned frmovrun:1;
+		/** Data Toggle Error */
+		unsigned datatglerr:1;
+		/** Buffer Not Available (only for DDMA mode) */
+		unsigned bna:1;
+		/** Exessive transaction error (only for DDMA mode) */
+		unsigned xcs_xact:1;
+		/** Frame List Rollover interrupt */
+		unsigned frm_list_roll:1;
+		/** Reserved */
+		unsigned reserved14_31:18;
+	} b;
+} hcint_data_t;
+
+/**
+ * This union represents the bit fields in the Host Channel Interrupt Mask
+ * Register. Read the register into the <i>d32</i> member then set/clear the
+ * bits using the <i>b</i>it elements. Write the <i>d32</i> member to the
+ * hcintmsk register.
+ */
+typedef union hcintmsk_data {
+	/** raw register data */
+	uint32_t d32;
+
+	/** register bits */
+	struct {
+		unsigned xfercompl:1;
+		unsigned chhltd:1;
+		unsigned ahberr:1;
+		unsigned stall:1;
+		unsigned nak:1;
+		unsigned ack:1;
+		unsigned nyet:1;
+		unsigned xacterr:1;
+		unsigned bblerr:1;
+		unsigned frmovrun:1;
+		unsigned datatglerr:1;
+		unsigned bna:1;
+		unsigned xcs_xact:1;
+		unsigned frm_list_roll:1;
+		unsigned reserved14_31:18;
+	} b;
+} hcintmsk_data_t;
+
+/**
+ * This union represents the bit fields in the Host Channel Transfer Size
+ * Register. Read the register into the <i>d32</i> member then set/clear the
+ * bits using the <i>b</i>it elements. Write the <i>d32</i> member to the
+ * hcchar register.
+ */
+
+typedef union hctsiz_data {
+	/** raw register data */
+	uint32_t d32;
+
+	/** register bits */
+	struct {
+		/** Total transfer size in bytes */
+		unsigned xfersize:19;
+
+		/** Data packets to transfer */
+		unsigned pktcnt:10;
+
+		/**
+		 * Packet ID for next data packet
+		 * 0: DATA0
+		 * 1: DATA2
+		 * 2: DATA1
+		 * 3: MDATA (non-Control), SETUP (Control)
+		 */
+		unsigned pid:2;
+#define DWC_HCTSIZ_DATA0 0
+#define DWC_HCTSIZ_DATA1 2
+#define DWC_HCTSIZ_DATA2 1
+#define DWC_HCTSIZ_MDATA 3
+#define DWC_HCTSIZ_SETUP 3
+
+		/** Do PING protocol when 1 */
+		unsigned dopng:1;
+	} b;
+
+	/** register bits */
+	struct {
+		/** Scheduling information */
+		unsigned schinfo:8;
+
+		/** Number of transfer descriptors.
+		 * Max value:
+		 * 64 in general,
+		 * 256 only for HS isochronous endpoint.
+		 */
+		unsigned ntd:8;
+
+		/** Data packets to transfer */
+		unsigned reserved16_28:13;
+
+		/**
+		 * Packet ID for next data packet
+		 * 0: DATA0
+		 * 1: DATA2
+		 * 2: DATA1
+		 * 3: MDATA (non-Control)
+		 */
+		unsigned pid:2;
+
+		/** Do PING protocol when 1 */
+		unsigned dopng:1;
+	} b_ddma;
+} hctsiz_data_t;
+
+/**
+ * This union represents the bit fields in the Host DMA Address 
+ * Register used in Descriptor DMA mode.
+ */
+typedef union hcdma_data {
+	/** raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		unsigned reserved0_2:3;
+		/** Current Transfer Descriptor. Not used for ISOC */
+		unsigned ctd:8;
+		/** Start Address of Descriptor List */
+		unsigned dma_addr:21;
+	} b;
+} hcdma_data_t;
+
+/**
+ * This union represents the bit fields in the DMA Descriptor
+ * status quadlet for host mode. Read the quadlet into the <i>d32</i> member then
+ * set/clear the bits using the <i>b</i>it elements.
+ */
+typedef union host_dma_desc_sts {
+	/** raw register data */
+	uint32_t d32;
+	/** quadlet bits */
+
+	/* for non-isochronous  */
+	struct {
+		/** Number of bytes */
+		unsigned n_bytes:17;
+		/** QTD offset to jump when Short Packet received - only for IN EPs */
+		unsigned qtd_offset:6;
+		/**
+		 * Set to request the core to jump to alternate QTD if
+		 * Short Packet received - only for IN EPs
+		 */
+		unsigned a_qtd:1;
+		 /**
+		  * Setup Packet bit. When set indicates that buffer contains
+		  * setup packet.
+		  */
+		unsigned sup:1;
+		/** Interrupt On Complete */
+		unsigned ioc:1;
+		/** End of List */
+		unsigned eol:1;
+		unsigned reserved27:1;
+		/** Rx/Tx Status */
+		unsigned sts:2;
+#define DMA_DESC_STS_PKTERR	1
+		unsigned reserved30:1;
+		/** Active Bit */
+		unsigned a:1;
+	} b;
+	/* for isochronous */
+	struct {
+		/** Number of bytes */
+		unsigned n_bytes:12;
+		unsigned reserved12_24:13;
+		/** Interrupt On Complete */
+		unsigned ioc:1;
+		unsigned reserved26_27:2;
+		/** Rx/Tx Status */
+		unsigned sts:2;
+		unsigned reserved30:1;
+		/** Active Bit */
+		unsigned a:1;
+	} b_isoc;
+} host_dma_desc_sts_t;
+
+#define	MAX_DMA_DESC_SIZE		131071
+#define MAX_DMA_DESC_NUM_GENERIC	64
+#define MAX_DMA_DESC_NUM_HS_ISOC	256
+#define MAX_FRLIST_EN_NUM		64
+/**
+ * Host-mode DMA Descriptor structure
+ *
+ * DMA Descriptor structure contains two quadlets:
+ * Status quadlet and Data buffer pointer.
+ */
+typedef struct dwc_otg_host_dma_desc {
+	/** DMA Descriptor status quadlet */
+	host_dma_desc_sts_t status;
+	/** DMA Descriptor data buffer pointer */
+	uint32_t buf;
+} dwc_otg_host_dma_desc_t;
+
+/** OTG Host Interface Structure.
+ *
+ * The OTG Host Interface Structure structure contains information
+ * needed to manage the DWC_otg controller acting in host mode. It
+ * represents the programming view of the host-specific aspects of the
+ * controller.
+ */
+typedef struct dwc_otg_host_if {
+	/** Host Global Registers starting at offset 400h.*/
+	dwc_otg_host_global_regs_t *host_global_regs;
+#define DWC_OTG_HOST_GLOBAL_REG_OFFSET 0x400
+
+	/** Host Port 0 Control and Status Register */
+	volatile uint32_t *hprt0;
+#define DWC_OTG_HOST_PORT_REGS_OFFSET 0x440
+
+	/** Host Channel Specific Registers at offsets 500h-5FCh. */
+	dwc_otg_hc_regs_t *hc_regs[MAX_EPS_CHANNELS];
+#define DWC_OTG_HOST_CHAN_REGS_OFFSET 0x500
+#define DWC_OTG_CHAN_REGS_OFFSET 0x20
+
+	/* Host configuration information */
+	/** Number of Host Channels (range: 1-16) */
+	uint8_t num_host_channels;
+	/** Periodic EPs supported (0: no, 1: yes) */
+	uint8_t perio_eps_supported;
+	/** Periodic Tx FIFO Size (Only 1 host periodic Tx FIFO) */
+	uint16_t perio_tx_fifo_size;
+
+} dwc_otg_host_if_t;
+
+/**
+ * This union represents the bit fields in the Power and Clock Gating Control
+ * Register. Read the register into the <i>d32</i> member then set/clear the
+ * bits using the <i>b</i>it elements.
+ */
+typedef union pcgcctl_data {
+	/** raw register data */
+	uint32_t d32;
+
+	/** register bits */
+	struct {
+		/** Stop Pclk */
+		unsigned stoppclk:1;
+		/** Gate Hclk */
+		unsigned gatehclk:1;
+		/** Power Clamp */
+		unsigned pwrclmp:1;
+		/** Reset Power Down Modules */
+		unsigned rstpdwnmodule:1;
+		/** Reserved */
+		unsigned reserved:1;
+		/** Enable Sleep Clock Gating (Enbl_L1Gating) */
+		unsigned enbl_sleep_gating:1;
+		/** PHY In Sleep (PhySleep) */
+		unsigned phy_in_sleep:1;
+		/** Deep Sleep*/
+		unsigned deep_sleep:1;
+		unsigned resetaftsusp:1;
+		unsigned restoremode:1;
+		unsigned reserved10_12:3;
+		unsigned ess_reg_restored:1;
+		unsigned prt_clk_sel:2;
+		unsigned port_power:1;
+		unsigned max_xcvrselect:2;
+		unsigned max_termsel:1;
+		unsigned mac_dev_addr:7;
+		unsigned p2hd_dev_enum_spd:2;
+		unsigned p2hd_prt_spd:2;
+		unsigned if_dev_mode:1;
+	} b;
+} pcgcctl_data_t;
+
+/**
+ * This union represents the bit fields in the Global Data FIFO Software
+ * Configuration Register. Read the register into the <i>d32</i> member then
+ * set/clear the bits using the <i>b</i>it elements.
+ */
+typedef union gdfifocfg_data {
+	/* raw register data */
+	uint32_t d32;
+	/** register bits */
+	struct {
+		/** OTG Data FIFO depth */
+		unsigned gdfifocfg:16;
+		/** Start address of EP info controller */
+		unsigned epinfobase:16;
+	} b;
+} gdfifocfg_data_t;
+
+/**
+ * This union represents the bit fields in the Global Power Down Register
+ * Register. Read the register into the <i>d32</i> member then set/clear the
+ * bits using the <i>b</i>it elements.
+ */
+typedef union gpwrdn_data {
+	/* raw register data */
+	uint32_t d32;
+
+	/** register bits */
+	struct {
+		/** PMU Interrupt Select */
+		unsigned pmuintsel:1;
+		/** PMU Active */
+		unsigned pmuactv:1;
+		/** Restore */
+		unsigned restore:1;
+		/** Power Down Clamp */
+		unsigned pwrdnclmp:1;
+		/** Power Down Reset */
+		unsigned pwrdnrstn:1;
+		/** Power Down Switch */
+		unsigned pwrdnswtch:1;
+		/** Disable VBUS */
+		unsigned dis_vbus:1;
+		/** Line State Change */
+		unsigned lnstschng:1;
+		/** Line state change mask */
+		unsigned lnstchng_msk:1;
+		/** Reset Detected */
+		unsigned rst_det:1;
+		/** Reset Detect mask */
+		unsigned rst_det_msk:1;
+		/** Disconnect Detected */
+		unsigned disconn_det:1;
+		/** Disconnect Detect mask */
+		unsigned disconn_det_msk:1;
+		/** Connect Detected*/
+		unsigned connect_det:1;
+		/** Connect Detected Mask*/
+		unsigned connect_det_msk:1;
+		/** SRP Detected */
+		unsigned srp_det:1;
+		/** SRP Detect mask */
+		unsigned srp_det_msk:1;
+		/** Status Change Interrupt */
+		unsigned sts_chngint:1;
+		/** Status Change Interrupt Mask */
+		unsigned sts_chngint_msk:1;
+		/** Line State */
+		unsigned linestate:2;
+		/** Indicates current mode(status of IDDIG signal) */
+		unsigned idsts:1;
+		/** B Session Valid signal status*/
+		unsigned bsessvld:1;
+		/** ADP Event Detected */
+		unsigned adp_int:1;
+		/** Multi Valued ID pin */
+		unsigned mult_val_id_bc:5;
+		/** Reserved 24_31 */
+		unsigned reserved29_31:3;
+	} b;
+} gpwrdn_data_t;
+
+#endif
diff --git a/drivers/usb/dwc_otg/linux/dwc_otg_plat.h b/drivers/usb/dwc_otg/linux/dwc_otg_plat.h
new file mode 100644
index 0000000..aab09bf
--- /dev/null
+++ b/drivers/usb/dwc_otg/linux/dwc_otg_plat.h
@@ -0,0 +1,263 @@
+/* ==========================================================================
+ * $File: //dwh/usb_iip/dev/software/otg/linux/platform/dwc_otg_plat.h $
+ * $Revision: 1.2 $
+ * $Date: 2008-11-21 05:39:16 $
+ * $Change: 1064915 $
+ *
+ * Synopsys HS OTG Linux Software Driver and documentation (hereinafter,
+ * "Software") is an Unsupported proprietary work of Synopsys, Inc. unless
+ * otherwise expressly agreed to in writing between Synopsys and you.
+ *
+ * The Software IS NOT an item of Licensed Software or Licensed Product under
+ * any End User Software License Agreement or Agreement for Licensed Product
+ * with Synopsys or any supplement thereto. You are permitted to use and
+ * redistribute this Software in source and binary forms, with or without
+ * modification, provided that redistributions of source code must retain this
+ * notice. You may not view, use, disclose, copy or distribute this file or
+ * any information contained herein except pursuant to this license grant from
+ * Synopsys. If you do not agree with this notice, including the disclaimer
+ * below, then you are not authorized to use the Software.
+ *
+ * THIS SOFTWARE IS BEING DISTRIBUTED BY SYNOPSYS SOLELY ON AN "AS IS" BASIS
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE HEREBY DISCLAIMED. IN NO EVENT SHALL SYNOPSYS BE LIABLE FOR ANY DIRECT,
+ * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
+ * DAMAGE.
+ * ========================================================================== */
+
+#if !defined(__DWC_OTG_PLAT_H__)
+#define __DWC_OTG_PLAT_H__
+
+#include <linux/types.h>
+#include <linux/slab.h>
+#include <linux/list.h>
+#include <linux/delay.h>
+#include <asm/io.h>
+
+
+/**
+ * @file
+ *
+ * This file contains the Platform Specific constants, interfaces
+ * (functions and macros) for Linux.
+ *
+ */
+//#if !defined(__LINUX_ARM_ARCH__)
+//#error "The contents of this file is Linux specific!!!"
+//#endif
+
+/**
+ * Reads the content of a register.
+ *
+ * @param reg address of register to read.
+ * @return contents of the register.
+ *
+
+ * Usage:<br>
+ * <code>uint32_t dev_ctl = dwc_read_reg32(&dev_regs->dctl);</code>
+ */
+static __inline__ uint32_t dwc_read_reg32( volatile uint32_t *reg)
+{
+        return readl(reg);
+};
+
+/**
+ * Writes a register with a 32 bit value.
+ *
+ * @param reg address of register to read.
+ * @param value to write to _reg.
+ *
+ * Usage:<br>
+ * <code>dwc_write_reg32(&dev_regs->dctl, 0); </code>
+ */
+static __inline__ void dwc_write_reg32( volatile uint32_t *reg, const uint32_t value)
+{
+        writel( value, reg );
+};
+
+/**
+ * This function modifies bit values in a register.  Using the
+ * algorithm: (reg_contents & ~clear_mask) | set_mask.
+ *
+ * @param reg address of register to read.
+ * @param clear_mask bit mask to be cleared.
+ * @param set_mask bit mask to be set.
+ *
+ * Usage:<br>
+ * <code> // Clear the SOF Interrupt Mask bit and <br>
+ * // set the OTG Interrupt mask bit, leaving all others as they were.
+ *    dwc_modify_reg32(&dev_regs->gintmsk, DWC_SOF_INT, DWC_OTG_INT);</code>
+ */
+static __inline__
+ void dwc_modify_reg32( volatile uint32_t *reg, const uint32_t clear_mask, const uint32_t set_mask)
+{
+        writel( (readl(reg) & ~clear_mask) | set_mask, reg );
+};
+
+
+/**
+ * Wrapper for the OS micro-second delay function.
+ * @param[in] usecs Microseconds of delay
+ */
+static __inline__ void UDELAY( const uint32_t usecs )
+{
+        udelay( usecs );
+}
+
+/**
+ * Wrapper for the OS milli-second delay function.
+ * @param[in] msecs milliseconds of delay
+ */
+static __inline__ void MDELAY( const uint32_t msecs )
+{
+        mdelay( msecs );
+}
+
+/**
+ * Wrapper for the Linux spin_lock.  On the ARM (Integrator)
+ * spin_lock() is a nop.
+ *
+ * @param lock Pointer to the spinlock.
+ */
+static __inline__ void SPIN_LOCK( spinlock_t *lock )
+{
+        spin_lock(lock);
+}
+
+/**
+ * Wrapper for the Linux spin_unlock.  On the ARM (Integrator)
+ * spin_lock() is a nop.
+ *
+ * @param lock Pointer to the spinlock.
+ */
+static __inline__ void SPIN_UNLOCK( spinlock_t *lock )
+{
+        spin_unlock(lock);
+}
+
+/**
+ * Wrapper (macro) for the Linux spin_lock_irqsave.  On the ARM
+ * (Integrator) spin_lock() is a nop.
+ *
+ * @param l Pointer to the spinlock.
+ * @param f unsigned long for irq flags storage.
+ */
+#define SPIN_LOCK_IRQSAVE( l, f )  spin_lock_irqsave(l,f);
+//#define SPIN_LOCK_IRQSAVE( l, f )  local_irq_save (f);
+
+/**
+ * Wrapper (macro) for the Linux spin_unlock_irqrestore.  On the ARM
+ * (Integrator) spin_lock() is a nop.
+ *
+ * @param l Pointer to the spinlock.
+ * @param f unsigned long for irq flags storage.
+ */
+#define SPIN_UNLOCK_IRQRESTORE( l,f ) spin_unlock_irqrestore(l,f);
+//#define SPIN_UNLOCK_IRQRESTORE( l,f ) local_irq_restore (f);
+
+/*
+ * Debugging support vanishes in non-debug builds.
+ */
+
+
+/**
+ * The Debug Level bit-mask variable.
+ */
+extern uint32_t g_dbg_lvl;
+/**
+ * Set the Debug Level variable.
+ */
+static inline uint32_t SET_DEBUG_LEVEL( const uint32_t new )
+{
+        uint32_t old = g_dbg_lvl;
+        g_dbg_lvl = new;
+        return old;
+}
+
+/** When debug level has the DBG_CIL bit set, display CIL Debug messages. */
+#define DBG_CIL		(0x2)
+/** When debug level has the DBG_CILV bit set, display CIL Verbose debug
+ * messages */
+#define DBG_CILV	(0x20)
+/**  When debug level has the DBG_PCD bit set, display PCD (Device) debug
+ *  messages */
+#define DBG_PCD		(0x4)
+/** When debug level has the DBG_PCDV set, display PCD (Device) Verbose debug
+ * messages */
+#define DBG_PCDV	(0x40)
+/** When debug level has the DBG_HCD bit set, display Host debug messages */
+#define DBG_HCD		(0x8)
+/** When debug level has the DBG_HCDV bit set, display Verbose Host debug
+ * messages */
+#define DBG_HCDV	(0x80)
+/** When debug level has the DBG_HCD_URB bit set, display enqueued URBs in host
+ *  mode. */
+#define DBG_HCD_URB	(0x800)
+
+/** When debug level has any bit set, display debug messages */
+#define DBG_ANY		(0xFF)
+
+/** All debug messages off */
+#define DBG_OFF		0
+
+/** Prefix string for DWC_DEBUG print macros. */
+#define USB_DWC "dwc_otg: "
+
+/**
+ * Print a debug message when the Global debug level variable contains
+ * the bit defined in <code>lvl</code>.
+ *
+ * @param[in] lvl - Debug level, use one of the DBG_ constants above.
+ * @param[in] x - like printf
+ *
+ *    Example:<p>
+ * <code>
+ *      DWC_DEBUGPL( DBG_ANY, "%s(%p)\n", __func__, _reg_base_addr);
+ * </code>
+ * <br>
+ * results in:<br>
+ * <code>
+ * usb-DWC_otg: dwc_otg_cil_init(ca867000)
+ * </code>
+ */
+#ifdef DEBUG
+
+# define DWC_DEBUGPL(lvl, x...) do{ if ((lvl)&g_dbg_lvl)printk( KERN_DEBUG USB_DWC x ); }while(0)
+# define DWC_DEBUGP(x...)	DWC_DEBUGPL(DBG_ANY, x )
+# define CHK_DEBUG_LEVEL(level) ((level) & g_dbg_lvl)
+
+#else
+
+# define DWC_DEBUGPL(lvl, x...) do{}while(0)
+# define DWC_DEBUGP(x...)
+
+# define CHK_DEBUG_LEVEL(level) (0)
+
+#endif /*DEBUG*/
+
+/**
+ * Print an Error message.
+ */
+#define DWC_ERROR(x...) printk( KERN_ERR USB_DWC x )
+//#define DWC_ERROR(x...) printk( x )
+
+/**
+ * Print a Warning message.
+ */
+#define DWC_WARN(x...) printk( KERN_WARNING USB_DWC x )
+/**
+ * Print a notice (normal but significant message).
+ */
+#define DWC_NOTICE(x...) printk( KERN_NOTICE USB_DWC x )
+/**
+ *  Basic message printing.
+ */
+#define DWC_PRINT(x...) printk( KERN_INFO USB_DWC x )
+
+#endif
diff --git a/drivers/usb/dwc_otg/usb.h b/drivers/usb/dwc_otg/usb.h
new file mode 100644
index 0000000..27bda82
--- /dev/null
+++ b/drivers/usb/dwc_otg/usb.h
@@ -0,0 +1,946 @@
+/*
+ * Copyright (c) 1998 The NetBSD Foundation, Inc.
+ * All rights reserved.
+ *
+ * This code is derived from software contributed to The NetBSD Foundation
+ * by Lennart Augustsson (lennart@augustsson.net) at
+ * Carlstedt Research & Technology.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. All advertising materials mentioning features or use of this software
+ *    must display the following acknowledgement:
+ *        This product includes software developed by the NetBSD
+ *        Foundation, Inc. and its contributors.
+ * 4. Neither the name of The NetBSD Foundation nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
+ * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
+ * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
+ * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/* Modified by Synopsys, Inc, 12/12/2007 */
+
+
+#ifndef _USB_H_
+#define _USB_H_
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/*
+ * The USB records contain some unaligned little-endian word
+ * components.  The U[SG]ETW macros take care of both the alignment
+ * and endian problem and should always be used to access non-byte
+ * values.
+ */
+typedef u_int8_t uByte;
+typedef u_int8_t uWord[2];
+typedef u_int8_t uDWord[4];
+
+#define USETW2(w,h,l) ((w)[0] = (u_int8_t)(l), (w)[1] = (u_int8_t)(h))
+#define UCONSTW(x)	{ (x) & 0xff, ((x) >> 8) & 0xff }
+#define UCONSTDW(x)	{ (x) & 0xff, ((x) >> 8) & 0xff, \
+			  ((x) >> 16) & 0xff, ((x) >> 24) & 0xff }
+
+#if 1
+#define UGETW(w) ((w)[0] | ((w)[1] << 8))
+#define USETW(w,v) ((w)[0] = (u_int8_t)(v), (w)[1] = (u_int8_t)((v) >> 8))
+#define UGETDW(w) ((w)[0] | ((w)[1] << 8) | ((w)[2] << 16) | ((w)[3] << 24))
+#define USETDW(w,v) ((w)[0] = (u_int8_t)(v), \
+		     (w)[1] = (u_int8_t)((v) >> 8), \
+		     (w)[2] = (u_int8_t)((v) >> 16), \
+		     (w)[3] = (u_int8_t)((v) >> 24))
+#else
+/*
+ * On little-endian machines that can handle unanliged accesses
+ * (e.g. i386) these macros can be replaced by the following.
+ */
+#define UGETW(w) (*(u_int16_t *)(w))
+#define USETW(w,v) (*(u_int16_t *)(w) = (v))
+#define UGETDW(w) (*(u_int32_t *)(w))
+#define USETDW(w,v) (*(u_int32_t *)(w) = (v))
+#endif
+
+/*
+ * Macros for accessing UAS IU fields, which are big-endian
+ */
+#define IUSETW2(w,h,l) ((w)[0] = (u_int8_t)(h), (w)[1] = (u_int8_t)(l))
+#define IUCONSTW(x)	{ ((x) >> 8) & 0xff, (x) & 0xff }
+#define IUCONSTDW(x)	{ ((x) >> 24) & 0xff, ((x) >> 16) & 0xff, \
+			((x) >> 8) & 0xff, (x) & 0xff }
+#define IUGETW(w) (((w)[0] << 8) | (w)[1])
+#define IUSETW(w,v) ((w)[0] = (u_int8_t)((v) >> 8), (w)[1] = (u_int8_t)(v))
+#define IUGETDW(w) (((w)[0] << 24) | ((w)[1] << 16) | ((w)[2] << 8) | (w)[3])
+#define IUSETDW(w,v) ((w)[0] = (u_int8_t)((v) >> 24), \
+		      (w)[1] = (u_int8_t)((v) >> 16), \
+		      (w)[2] = (u_int8_t)((v) >> 8), \
+		      (w)[3] = (u_int8_t)(v))
+
+#define UPACKED __attribute__((__packed__))
+
+typedef struct {
+	uByte		bmRequestType;
+	uByte		bRequest;
+	uWord		wValue;
+	uWord		wIndex;
+	uWord		wLength;
+} UPACKED usb_device_request_t;
+
+#define UT_GET_DIR(a) ((a) & 0x80)
+#define UT_WRITE		0x00
+#define UT_READ			0x80
+
+#define UT_GET_TYPE(a) ((a) & 0x60)
+#define UT_STANDARD		0x00
+#define UT_CLASS		0x20
+#define UT_VENDOR		0x40
+
+#define UT_GET_RECIPIENT(a) ((a) & 0x1f)
+#define UT_DEVICE		0x00
+#define UT_INTERFACE		0x01
+#define UT_ENDPOINT		0x02
+#define UT_OTHER		0x03
+
+#define UT_READ_DEVICE		(UT_READ  | UT_STANDARD | UT_DEVICE)
+#define UT_READ_INTERFACE	(UT_READ  | UT_STANDARD | UT_INTERFACE)
+#define UT_READ_ENDPOINT	(UT_READ  | UT_STANDARD | UT_ENDPOINT)
+#define UT_WRITE_DEVICE		(UT_WRITE | UT_STANDARD | UT_DEVICE)
+#define UT_WRITE_INTERFACE	(UT_WRITE | UT_STANDARD | UT_INTERFACE)
+#define UT_WRITE_ENDPOINT	(UT_WRITE | UT_STANDARD | UT_ENDPOINT)
+#define UT_READ_CLASS_DEVICE	(UT_READ  | UT_CLASS | UT_DEVICE)
+#define UT_READ_CLASS_INTERFACE	(UT_READ  | UT_CLASS | UT_INTERFACE)
+#define UT_READ_CLASS_OTHER	(UT_READ  | UT_CLASS | UT_OTHER)
+#define UT_READ_CLASS_ENDPOINT	(UT_READ  | UT_CLASS | UT_ENDPOINT)
+#define UT_WRITE_CLASS_DEVICE	(UT_WRITE | UT_CLASS | UT_DEVICE)
+#define UT_WRITE_CLASS_INTERFACE (UT_WRITE | UT_CLASS | UT_INTERFACE)
+#define UT_WRITE_CLASS_OTHER	(UT_WRITE | UT_CLASS | UT_OTHER)
+#define UT_WRITE_CLASS_ENDPOINT	(UT_WRITE | UT_CLASS | UT_ENDPOINT)
+#define UT_READ_VENDOR_DEVICE	(UT_READ  | UT_VENDOR | UT_DEVICE)
+#define UT_READ_VENDOR_INTERFACE (UT_READ  | UT_VENDOR | UT_INTERFACE)
+#define UT_READ_VENDOR_OTHER	(UT_READ  | UT_VENDOR | UT_OTHER)
+#define UT_READ_VENDOR_ENDPOINT	(UT_READ  | UT_VENDOR | UT_ENDPOINT)
+#define UT_WRITE_VENDOR_DEVICE	(UT_WRITE | UT_VENDOR | UT_DEVICE)
+#define UT_WRITE_VENDOR_INTERFACE (UT_WRITE | UT_VENDOR | UT_INTERFACE)
+#define UT_WRITE_VENDOR_OTHER	(UT_WRITE | UT_VENDOR | UT_OTHER)
+#define UT_WRITE_VENDOR_ENDPOINT (UT_WRITE | UT_VENDOR | UT_ENDPOINT)
+
+/* Requests */
+#define UR_GET_STATUS		0x00
+#define  USTAT_STANDARD_STATUS  0x00
+#define  WUSTAT_WUSB_FEATURE    0x01
+#define  WUSTAT_CHANNEL_INFO    0x02
+#define  WUSTAT_RECEIVED_DATA   0x03
+#define  WUSTAT_MAS_AVAILABILITY 0x04
+#define  WUSTAT_CURRENT_TRANSMIT_POWER 0x05
+#define UR_CLEAR_FEATURE	0x01
+#define UR_SET_FEATURE		0x03
+#define UR_SET_AND_TEST_FEATURE 0x0c
+#define UR_SET_ADDRESS		0x05
+#define UR_GET_DESCRIPTOR	0x06
+#define  UDESC_DEVICE		0x01
+#define  UDESC_CONFIG		0x02
+#define  UDESC_STRING		0x03
+#define  UDESC_INTERFACE	0x04
+#define  UDESC_ENDPOINT		0x05
+#define  UDESC_SS_USB_COMPANION	0x30
+#define  UDESC_DEVICE_QUALIFIER	0x06
+#define  UDESC_OTHER_SPEED_CONFIGURATION 0x07
+#define  UDESC_INTERFACE_POWER	0x08
+#define  UDESC_OTG		0x09
+#define  WUDESC_SECURITY	0x0c
+#define  WUDESC_KEY		0x0d
+#define   WUD_GET_KEY_INDEX(_wValue_) ((_wValue_) & 0xf)
+#define   WUD_GET_KEY_TYPE(_wValue_) (((_wValue_) & 0x30) >> 4)
+#define    WUD_KEY_TYPE_ASSOC    0x01
+#define    WUD_KEY_TYPE_GTK      0x02
+#define   WUD_GET_KEY_ORIGIN(_wValue_) (((_wValue_) & 0x40) >> 6)
+#define    WUD_KEY_ORIGIN_HOST   0x00
+#define    WUD_KEY_ORIGIN_DEVICE 0x01
+#define  WUDESC_ENCRYPTION_TYPE	0x0e
+#define  WUDESC_BOS		0x0f
+#define  WUDESC_DEVICE_CAPABILITY 0x10
+#define  WUDESC_WIRELESS_ENDPOINT_COMPANION 0x11
+#define  UDESC_BOS		0x0f
+#define  UDESC_DEVICE_CAPABILITY 0x10
+#define  UDESC_CS_DEVICE	0x21	/* class specific */
+#define  UDESC_CS_CONFIG	0x22
+#define  UDESC_CS_STRING	0x23
+#define  UDESC_CS_INTERFACE	0x24
+#define  UDESC_CS_ENDPOINT	0x25
+#define  UDESC_HUB		0x29
+#define UR_SET_DESCRIPTOR	0x07
+#define UR_GET_CONFIG		0x08
+#define UR_SET_CONFIG		0x09
+#define UR_GET_INTERFACE	0x0a
+#define UR_SET_INTERFACE	0x0b
+#define UR_SYNCH_FRAME		0x0c
+#define WUR_SET_ENCRYPTION      0x0d
+#define WUR_GET_ENCRYPTION	0x0e
+#define WUR_SET_HANDSHAKE	0x0f
+#define WUR_GET_HANDSHAKE	0x10
+#define WUR_SET_CONNECTION	0x11
+#define WUR_SET_SECURITY_DATA	0x12
+#define WUR_GET_SECURITY_DATA	0x13
+#define WUR_SET_WUSB_DATA	0x14
+#define  WUDATA_DRPIE_INFO	0x01
+#define  WUDATA_TRANSMIT_DATA	0x02
+#define  WUDATA_TRANSMIT_PARAMS	0x03
+#define  WUDATA_RECEIVE_PARAMS	0x04
+#define  WUDATA_TRANSMIT_POWER	0x05
+#define WUR_LOOPBACK_DATA_WRITE	0x15
+#define WUR_LOOPBACK_DATA_READ	0x16
+#define WUR_SET_INTERFACE_DS	0x17
+
+/* Feature numbers */
+#define UF_ENDPOINT_HALT	0
+#define UF_DEVICE_REMOTE_WAKEUP	1
+#define UF_TEST_MODE		2
+#define UF_DEVICE_B_HNP_ENABLE	3
+#define UF_DEVICE_A_HNP_SUPPORT	4
+#define UF_DEVICE_A_ALT_HNP_SUPPORT 5
+#define WUF_WUSB		3
+#define  WUF_TX_DRPIE		0x0
+#define  WUF_DEV_XMIT_PACKET	0x1
+#define  WUF_COUNT_PACKETS	0x2
+#define  WUF_CAPTURE_PACKETS	0x3
+#define UF_FUNCTION_SUSPEND	0
+#define UF_U1_ENABLE		48
+#define UF_U2_ENABLE		49
+#define UF_LTM_ENABLE		50
+
+/* Class requests from the USB 2.0 hub spec, table 11-15 */
+#define UCR_CLEAR_HUB_FEATURE		(0x2000 | UR_CLEAR_FEATURE)
+#define UCR_CLEAR_PORT_FEATURE		(0x2300 | UR_CLEAR_FEATURE)
+#define UCR_GET_HUB_DESCRIPTOR		(0xa000 | UR_GET_DESCRIPTOR)
+#define UCR_GET_HUB_STATUS		(0xa000 | UR_GET_STATUS)
+#define UCR_GET_PORT_STATUS		(0xa300 | UR_GET_STATUS)
+#define UCR_SET_HUB_FEATURE		(0x2000 | UR_SET_FEATURE)
+#define UCR_SET_PORT_FEATURE		(0x2300 | UR_SET_FEATURE)
+#define UCR_SET_AND_TEST_PORT_FEATURE	(0xa300 | UR_SET_AND_TEST_FEATURE)
+
+#ifdef _MSC_VER
+#include <pshpack1.h>
+#endif
+
+typedef struct {
+	uByte		bLength;
+	uByte		bDescriptorType;
+	uByte		bDescriptorSubtype;
+} UPACKED usb_descriptor_t;
+
+typedef struct {
+	uByte		bLength;
+	uByte		bDescriptorType;
+} UPACKED usb_descriptor_header_t;
+
+typedef struct {
+	uByte		bLength;
+	uByte		bDescriptorType;
+	uWord		bcdUSB;
+#define UD_USB_2_0		0x0200
+#define UD_IS_USB2(d) (UGETW((d)->bcdUSB) >= UD_USB_2_0)
+	uByte		bDeviceClass;
+	uByte		bDeviceSubClass;
+	uByte		bDeviceProtocol;
+	uByte		bMaxPacketSize;
+	/* The fields below are not part of the initial descriptor. */
+	uWord		idVendor;
+	uWord		idProduct;
+	uWord		bcdDevice;
+	uByte		iManufacturer;
+	uByte		iProduct;
+	uByte		iSerialNumber;
+	uByte		bNumConfigurations;
+} UPACKED usb_device_descriptor_t;
+#define USB_DEVICE_DESCRIPTOR_SIZE 18
+
+typedef struct {
+	uByte		bLength;
+	uByte		bDescriptorType;
+	uWord		wTotalLength;
+	uByte		bNumInterface;
+	uByte		bConfigurationValue;
+	uByte		iConfiguration;
+#define UC_ATT_ONE		(1 << 7)	/* must be set */
+#define UC_ATT_SELFPOWER	(1 << 6)	/* self powered */
+#define UC_ATT_WAKEUP		(1 << 5)	/* can wakeup */
+#define UC_ATT_BATTERY		(1 << 4)	/* battery powered */
+	uByte		bmAttributes;
+#define UC_BUS_POWERED		0x80
+#define UC_SELF_POWERED		0x40
+#define UC_REMOTE_WAKEUP	0x20
+	uByte		bMaxPower; /* max current in 2 mA units */
+#define UC_POWER_FACTOR 2
+} UPACKED usb_config_descriptor_t;
+#define USB_CONFIG_DESCRIPTOR_SIZE 9
+
+typedef struct {
+	uByte		bLength;
+	uByte		bDescriptorType;
+	uByte		bInterfaceNumber;
+	uByte		bAlternateSetting;
+	uByte		bNumEndpoints;
+	uByte		bInterfaceClass;
+	uByte		bInterfaceSubClass;
+	uByte		bInterfaceProtocol;
+	uByte		iInterface;
+} UPACKED usb_interface_descriptor_t;
+#define USB_INTERFACE_DESCRIPTOR_SIZE 9
+
+typedef struct {
+	uByte		bLength;
+	uByte		bDescriptorType;
+	uByte		bEndpointAddress;
+#define UE_GET_DIR(a)	((a) & 0x80)
+#define UE_SET_DIR(a,d)	((a) | (((d)&1) << 7))
+#define UE_DIR_IN	0x80
+#define UE_DIR_OUT	0x00
+#define UE_ADDR		0x0f
+#define UE_GET_ADDR(a)	((a) & UE_ADDR)
+	uByte		bmAttributes;
+#define UE_XFERTYPE	0x03
+#define  UE_CONTROL	0x00
+#define  UE_ISOCHRONOUS	0x01
+#define  UE_BULK	0x02
+#define  UE_INTERRUPT	0x03
+#define UE_GET_XFERTYPE(a)	((a) & UE_XFERTYPE)
+#define UE_ISO_TYPE	0x0c
+#define  UE_ISO_ASYNC	0x04
+#define  UE_ISO_ADAPT	0x08
+#define  UE_ISO_SYNC	0x0c
+#define UE_GET_ISO_TYPE(a)	((a) & UE_ISO_TYPE)
+	uWord		wMaxPacketSize;
+	uByte		bInterval;
+} UPACKED usb_endpoint_descriptor_t;
+#define USB_ENDPOINT_DESCRIPTOR_SIZE 7
+
+typedef struct ss_endpoint_companion_descriptor {
+	uByte bLength;
+	uByte bDescriptorType;
+	uByte bMaxBurst;
+#define USSE_GET_MAX_STREAMS(a)		((a) & 0x1f)
+#define USSE_SET_MAX_STREAMS(a, b)	((a) | ((b) & 0x1f))
+#define USSE_GET_MAX_PACKET_NUM(a)	((a) & 0x03)
+#define USSE_SET_MAX_PACKET_NUM(a, b)	((a) | ((b) & 0x03))
+	uByte bmAttributes;
+	uWord wBytesPerInterval;
+} UPACKED ss_endpoint_companion_descriptor_t;
+#define USB_SS_ENDPOINT_COMPANION_DESCRIPTOR_SIZE 6
+
+typedef struct {
+	uByte		bLength;
+	uByte		bDescriptorType;
+	uWord		bString[127];
+} UPACKED usb_string_descriptor_t;
+#define USB_MAX_STRING_LEN 128
+#define USB_LANGUAGE_TABLE 0	/* # of the string language id table */
+
+/* Hub specific request */
+#define UR_GET_BUS_STATE	0x02
+#define UR_CLEAR_TT_BUFFER	0x08
+#define UR_RESET_TT		0x09
+#define UR_GET_TT_STATE		0x0a
+#define UR_STOP_TT		0x0b
+
+/* Hub features */
+#define UHF_C_HUB_LOCAL_POWER	0
+#define UHF_C_HUB_OVER_CURRENT	1
+#define UHF_PORT_CONNECTION	0
+#define UHF_PORT_ENABLE		1
+#define UHF_PORT_SUSPEND	2
+#define UHF_PORT_OVER_CURRENT	3
+#define UHF_PORT_RESET		4
+#define UHF_PORT_L1		5
+#define UHF_PORT_POWER		8
+#define UHF_PORT_LOW_SPEED	9
+#define UHF_PORT_HIGH_SPEED	10
+#define UHF_C_PORT_CONNECTION	16
+#define UHF_C_PORT_ENABLE	17
+#define UHF_C_PORT_SUSPEND	18
+#define UHF_C_PORT_OVER_CURRENT	19
+#define UHF_C_PORT_RESET	20
+#define UHF_C_PORT_L1		23
+#define UHF_PORT_TEST		21
+#define UHF_PORT_INDICATOR	22
+
+typedef struct {
+	uByte		bDescLength;
+	uByte		bDescriptorType;
+	uByte		bNbrPorts;
+	uWord		wHubCharacteristics;
+#define UHD_PWR			0x0003
+#define  UHD_PWR_GANGED		0x0000
+#define  UHD_PWR_INDIVIDUAL	0x0001
+#define  UHD_PWR_NO_SWITCH	0x0002
+#define UHD_COMPOUND		0x0004
+#define UHD_OC			0x0018
+#define  UHD_OC_GLOBAL		0x0000
+#define  UHD_OC_INDIVIDUAL	0x0008
+#define  UHD_OC_NONE		0x0010
+#define UHD_TT_THINK		0x0060
+#define  UHD_TT_THINK_8		0x0000
+#define  UHD_TT_THINK_16	0x0020
+#define  UHD_TT_THINK_24	0x0040
+#define  UHD_TT_THINK_32	0x0060
+#define UHD_PORT_IND		0x0080
+	uByte		bPwrOn2PwrGood;	/* delay in 2 ms units */
+#define UHD_PWRON_FACTOR 2
+	uByte		bHubContrCurrent;
+	uByte		DeviceRemovable[32]; /* max 255 ports */
+#define UHD_NOT_REMOV(desc, i) \
+    (((desc)->DeviceRemovable[(i)/8] >> ((i) % 8)) & 1)
+	/* deprecated */ uByte		PortPowerCtrlMask[1];
+} UPACKED usb_hub_descriptor_t;
+#define USB_HUB_DESCRIPTOR_SIZE 9 /* includes deprecated PortPowerCtrlMask */
+
+typedef struct {
+	uByte		bLength;
+	uByte		bDescriptorType;
+	uWord		bcdUSB;
+	uByte		bDeviceClass;
+	uByte		bDeviceSubClass;
+	uByte		bDeviceProtocol;
+	uByte		bMaxPacketSize0;
+	uByte		bNumConfigurations;
+	uByte		bReserved;
+} UPACKED usb_device_qualifier_t;
+#define USB_DEVICE_QUALIFIER_SIZE 10
+
+typedef struct {
+	uByte		bLength;
+	uByte		bDescriptorType;
+	uByte		bmAttributes;
+#define UOTG_SRP	0x01
+#define UOTG_HNP	0x02
+} UPACKED usb_otg_descriptor_t;
+
+/* OTG feature selectors */
+#define UOTG_B_HNP_ENABLE	3
+#define UOTG_A_HNP_SUPPORT	4
+#define UOTG_A_ALT_HNP_SUPPORT	5
+
+typedef struct {
+	uWord		wStatus;
+/* Device status flags */
+#define UDS_SELF_POWERED		0x0001
+#define UDS_REMOTE_WAKEUP		0x0002
+/* Endpoint status flags */
+#define UES_HALT			0x0001
+} UPACKED usb_status_t;
+
+typedef struct {
+	uWord		wHubStatus;
+#define UHS_LOCAL_POWER			0x0001
+#define UHS_OVER_CURRENT		0x0002
+	uWord		wHubChange;
+} UPACKED usb_hub_status_t;
+
+typedef struct {
+	uWord		wPortStatus;
+#define UPS_CURRENT_CONNECT_STATUS	0x0001
+#define UPS_PORT_ENABLED		0x0002
+#define UPS_SUSPEND			0x0004
+#define UPS_OVERCURRENT_INDICATOR	0x0008
+#define UPS_RESET			0x0010
+#define UPS_PORT_POWER			0x0100
+#define UPS_LOW_SPEED			0x0200
+#define UPS_HIGH_SPEED			0x0400
+#define UPS_PORT_TEST			0x0800
+#define UPS_PORT_INDICATOR		0x1000
+	uWord		wPortChange;
+#define UPS_C_CONNECT_STATUS		0x0001
+#define UPS_C_PORT_ENABLED		0x0002
+#define UPS_C_SUSPEND			0x0004
+#define UPS_C_OVERCURRENT_INDICATOR	0x0008
+#define UPS_C_PORT_RESET		0x0010
+} UPACKED usb_port_status_t;
+
+#ifdef _MSC_VER
+#include <poppack.h>
+#endif
+
+/* Device class codes */
+#define UDCLASS_IN_INTERFACE	0x00
+#define UDCLASS_COMM		0x02
+#define UDCLASS_HUB		0x09
+#define  UDSUBCLASS_HUB		0x00
+#define  UDPROTO_FSHUB		0x00
+#define  UDPROTO_HSHUBSTT	0x01
+#define  UDPROTO_HSHUBMTT	0x02
+#define UDCLASS_DIAGNOSTIC	0xdc
+#define UDCLASS_WIRELESS	0xe0
+#define  UDSUBCLASS_RF		0x01
+#define   UDPROTO_BLUETOOTH	0x01
+#define UDCLASS_VENDOR		0xff
+
+/* Interface class codes */
+#define UICLASS_UNSPEC		0x00
+
+#define UICLASS_AUDIO		0x01
+#define  UISUBCLASS_AUDIOCONTROL	1
+#define  UISUBCLASS_AUDIOSTREAM		2
+#define  UISUBCLASS_MIDISTREAM		3
+
+#define UICLASS_CDC		0x02 /* communication */
+#define  UISUBCLASS_DIRECT_LINE_CONTROL_MODEL	1
+#define  UISUBCLASS_ABSTRACT_CONTROL_MODEL	2
+#define  UISUBCLASS_TELEPHONE_CONTROL_MODEL	3
+#define  UISUBCLASS_MULTICHANNEL_CONTROL_MODEL	4
+#define  UISUBCLASS_CAPI_CONTROLMODEL		5
+#define  UISUBCLASS_ETHERNET_NETWORKING_CONTROL_MODEL 6
+#define  UISUBCLASS_ATM_NETWORKING_CONTROL_MODEL 7
+#define   UIPROTO_CDC_AT			1
+
+#define UICLASS_HID		0x03
+#define  UISUBCLASS_BOOT	1
+#define  UIPROTO_BOOT_KEYBOARD	1
+
+#define UICLASS_PHYSICAL	0x05
+
+#define UICLASS_IMAGE		0x06
+
+#define UICLASS_PRINTER		0x07
+#define  UISUBCLASS_PRINTER	1
+#define  UIPROTO_PRINTER_UNI	1
+#define  UIPROTO_PRINTER_BI	2
+#define  UIPROTO_PRINTER_1284	3
+
+#define UICLASS_MASS		0x08
+#define  UISUBCLASS_RBC		1
+#define  UISUBCLASS_SFF8020I	2
+#define  UISUBCLASS_QIC157	3
+#define  UISUBCLASS_UFI		4
+#define  UISUBCLASS_SFF8070I	5
+#define  UISUBCLASS_SCSI	6
+#define  UIPROTO_MASS_CBI_I	0
+#define  UIPROTO_MASS_CBI	1
+#define  UIPROTO_MASS_BBB_OLD	2	/* Not in the spec anymore */
+#define  UIPROTO_MASS_BBB	80	/* 'P' for the Iomega Zip drive */
+
+#define UICLASS_HUB		0x09
+#define  UISUBCLASS_HUB		0
+#define  UIPROTO_FSHUB		0
+#define  UIPROTO_HSHUBSTT	0 /* Yes, same as previous */
+#define  UIPROTO_HSHUBMTT	1
+
+#define UICLASS_CDC_DATA	0x0a
+#define  UISUBCLASS_DATA		0
+#define   UIPROTO_DATA_ISDNBRI		0x30    /* Physical iface */
+#define   UIPROTO_DATA_HDLC		0x31    /* HDLC */
+#define   UIPROTO_DATA_TRANSPARENT	0x32    /* Transparent */
+#define   UIPROTO_DATA_Q921M		0x50    /* Management for Q921 */
+#define   UIPROTO_DATA_Q921		0x51    /* Data for Q921 */
+#define   UIPROTO_DATA_Q921TM		0x52    /* TEI multiplexer for Q921 */
+#define   UIPROTO_DATA_V42BIS		0x90    /* Data compression */
+#define   UIPROTO_DATA_Q931		0x91    /* Euro-ISDN */
+#define   UIPROTO_DATA_V120		0x92    /* V.24 rate adaption */
+#define   UIPROTO_DATA_CAPI		0x93    /* CAPI 2.0 commands */
+#define   UIPROTO_DATA_HOST_BASED	0xfd    /* Host based driver */
+#define   UIPROTO_DATA_PUF		0xfe    /* see Prot. Unit Func. Desc.*/
+#define   UIPROTO_DATA_VENDOR		0xff    /* Vendor specific */
+
+#define UICLASS_SMARTCARD	0x0b
+
+/*#define UICLASS_FIRM_UPD	0x0c*/
+
+#define UICLASS_SECURITY	0x0d
+
+#define UICLASS_DIAGNOSTIC	0xdc
+
+#define UICLASS_WIRELESS	0xe0
+#define  UISUBCLASS_RF			0x01
+#define   UIPROTO_BLUETOOTH		0x01
+
+#define UICLASS_APPL_SPEC	0xfe
+#define  UISUBCLASS_FIRMWARE_DOWNLOAD	1
+#define  UISUBCLASS_IRDA		2
+#define  UIPROTO_IRDA			0
+
+#define UICLASS_VENDOR		0xff
+
+#define USB_HUB_MAX_DEPTH 5
+
+/*
+ * Minimum time a device needs to be powered down to go through
+ * a power cycle.  XXX Are these time in the spec?
+ */
+#define USB_POWER_DOWN_TIME	200 /* ms */
+#define USB_PORT_POWER_DOWN_TIME	100 /* ms */
+
+#if 0
+/* These are the values from the spec. */
+#define USB_PORT_RESET_DELAY	10  /* ms */
+#define USB_PORT_ROOT_RESET_DELAY 50  /* ms */
+#define USB_PORT_RESET_RECOVERY	10  /* ms */
+#define USB_PORT_POWERUP_DELAY	100 /* ms */
+#define USB_SET_ADDRESS_SETTLE	2   /* ms */
+#define USB_RESUME_DELAY	(20*5)  /* ms */
+#define USB_RESUME_WAIT		10  /* ms */
+#define USB_RESUME_RECOVERY	10  /* ms */
+#define USB_EXTRA_POWER_UP_TIME	0   /* ms */
+#else
+/* Allow for marginal (i.e. non-conforming) devices. */
+#define USB_PORT_RESET_DELAY	50  /* ms */
+#define USB_PORT_ROOT_RESET_DELAY 250  /* ms */
+#define USB_PORT_RESET_RECOVERY	250  /* ms */
+#define USB_PORT_POWERUP_DELAY	300 /* ms */
+#define USB_SET_ADDRESS_SETTLE	10  /* ms */
+#define USB_RESUME_DELAY	(50*5)  /* ms */
+#define USB_RESUME_WAIT		50  /* ms */
+#define USB_RESUME_RECOVERY	50  /* ms */
+#define USB_EXTRA_POWER_UP_TIME	20  /* ms */
+#endif
+
+#define USB_MIN_POWER		100 /* mA */
+#define USB_MAX_POWER		500 /* mA */
+
+#define USB_BUS_RESET_DELAY	100 /* ms XXX?*/
+
+#define USB_UNCONFIG_NO 0
+#define USB_UNCONFIG_INDEX (-1)
+
+/*** ioctl() related stuff ***/
+
+struct usb_ctl_request {
+	int	ucr_addr;
+	usb_device_request_t ucr_request;
+	void	*ucr_data;
+	int	ucr_flags;
+#define USBD_SHORT_XFER_OK	0x04	/* allow short reads */
+	int	ucr_actlen;		/* actual length transferred */
+};
+
+struct usb_alt_interface {
+	int	uai_config_index;
+	int	uai_interface_index;
+	int	uai_alt_no;
+};
+
+#define USB_CURRENT_CONFIG_INDEX (-1)
+#define USB_CURRENT_ALT_INDEX (-1)
+
+struct usb_config_desc {
+	int	ucd_config_index;
+	usb_config_descriptor_t ucd_desc;
+};
+
+struct usb_interface_desc {
+	int	uid_config_index;
+	int	uid_interface_index;
+	int	uid_alt_index;
+	usb_interface_descriptor_t uid_desc;
+};
+
+struct usb_endpoint_desc {
+	int	ued_config_index;
+	int	ued_interface_index;
+	int	ued_alt_index;
+	int	ued_endpoint_index;
+	usb_endpoint_descriptor_t ued_desc;
+};
+
+struct usb_full_desc {
+	int	ufd_config_index;
+	u_int	ufd_size;
+	u_char	*ufd_data;
+};
+
+struct usb_string_desc {
+	int	usd_string_index;
+	int	usd_language_id;
+	usb_string_descriptor_t usd_desc;
+};
+
+struct usb_ctl_report_desc {
+	int	ucrd_size;
+	u_char	ucrd_data[1024];	/* filled data size will vary */
+};
+
+typedef struct { u_int32_t cookie; } usb_event_cookie_t;
+
+#define USB_MAX_DEVNAMES 4
+#define USB_MAX_DEVNAMELEN 16
+struct usb_device_info {
+	u_int8_t	udi_bus;
+	u_int8_t	udi_addr;	/* device address */
+	usb_event_cookie_t udi_cookie;
+	char		udi_product[USB_MAX_STRING_LEN];
+	char		udi_vendor[USB_MAX_STRING_LEN];
+	char		udi_release[8];
+	u_int16_t	udi_productNo;
+	u_int16_t	udi_vendorNo;
+	u_int16_t	udi_releaseNo;
+	u_int8_t	udi_class;
+	u_int8_t	udi_subclass;
+	u_int8_t	udi_protocol;
+	u_int8_t	udi_config;
+	u_int8_t	udi_speed;
+#define USB_SPEED_UNKNOWN	0
+#define USB_SPEED_LOW		1
+#define USB_SPEED_FULL		2
+#define USB_SPEED_HIGH		3
+#define USB_SPEED_VARIABLE	4
+#define USB_SPEED_SUPER		5
+	int		udi_power;	/* power consumption in mA, 0 if selfpowered */
+	int		udi_nports;
+	char		udi_devnames[USB_MAX_DEVNAMES][USB_MAX_DEVNAMELEN];
+	u_int8_t	udi_ports[16];/* hub only: addresses of devices on ports */
+#define USB_PORT_ENABLED 0xff
+#define USB_PORT_SUSPENDED 0xfe
+#define USB_PORT_POWERED 0xfd
+#define USB_PORT_DISABLED 0xfc
+};
+
+struct usb_ctl_report {
+	int	ucr_report;
+	u_char	ucr_data[1024];	/* filled data size will vary */
+};
+
+struct usb_device_stats {
+	u_long	uds_requests[4];	/* indexed by transfer type UE_* */
+};
+
+#define WUSB_MIN_IE			0x80
+#define WUSB_WCTA_IE			0x80
+#define WUSB_WCONNECTACK_IE		0x81
+#define WUSB_WHOSTINFO_IE		0x82
+#define  WUHI_GET_CA(_bmAttributes_) ((_bmAttributes_) & 0x3)
+#define   WUHI_CA_RECONN		0x00
+#define   WUHI_CA_LIMITED		0x01
+#define   WUHI_CA_ALL			0x03
+#define  WUHI_GET_MLSI(_bmAttributes_) (((_bmAttributes_) & 0x38) >> 3)
+#define WUSB_WCHCHANGEANNOUNCE_IE	0x83
+#define WUSB_WDEV_DISCONNECT_IE		0x84
+#define WUSB_WHOST_DISCONNECT_IE	0x85
+#define WUSB_WRELEASE_CHANNEL_IE	0x86
+#define WUSB_WWORK_IE			0x87
+#define WUSB_WCHANNEL_STOP_IE		0x88
+#define WUSB_WDEV_KEEPALIVE_IE		0x89
+#define WUSB_WISOCH_DISCARD_IE		0x8A
+#define WUSB_WRESETDEVICE_IE		0x8B
+#define WUSB_WXMIT_PACKET_ADJUST_IE	0x8C
+#define WUSB_MAX_IE			0x8C
+
+/* Device Notification Types */
+
+#define WUSB_DN_MIN			0x01
+#define WUSB_DN_CONNECT			0x01
+# define WUSB_DA_OLDCONN	0x00
+# define WUSB_DA_NEWCONN	0x01
+# define WUSB_DA_SELF_BEACON	0x02
+# define WUSB_DA_DIR_BEACON	0x04
+# define WUSB_DA_NO_BEACON	0x06
+#define WUSB_DN_DISCONNECT		0x02
+#define WUSB_DN_EPRDY			0x03
+#define WUSB_DN_MASAVAILCHANGED		0x04
+#define WUSB_DN_REMOTEWAKEUP		0x05
+#define WUSB_DN_SLEEP			0x06
+#define WUSB_DN_ALIVE			0x07
+#define WUSB_DN_MAX			0x07
+
+#ifdef _MSC_VER
+#include <pshpack1.h>
+#endif
+
+/* WUSB Handshake Data.  Used during the SET/GET HANDSHAKE requests */
+typedef struct wusb_hndshk_data {
+	uByte bMessageNumber;
+	uByte bStatus;
+	uByte tTKID[3];
+	uByte bReserved;
+	uByte CDID[16];
+	uByte Nonce[16];
+	uByte MIC[8];
+} UPACKED wusb_hndshk_data_t;
+#define WUSB_HANDSHAKE_LEN_FOR_MIC	38
+
+/* WUSB Connection Context */
+typedef struct wusb_conn_context {
+	uByte CHID [16];
+	uByte CDID [16];
+	uByte CK [16];
+} UPACKED wusb_conn_context_t;
+
+/* WUSB Security Descriptor */
+typedef struct wusb_security_desc {
+	uByte bLength;
+	uByte bDescriptorType;
+	uWord wTotalLength;
+	uByte bNumEncryptionTypes;
+} UPACKED wusb_security_desc_t;
+
+/* WUSB Encryption Type Descriptor */
+typedef struct wusb_encrypt_type_desc {
+	uByte bLength;
+	uByte bDescriptorType;
+
+	uByte bEncryptionType;
+#define WUETD_UNSECURE		0
+#define WUETD_WIRED		1
+#define WUETD_CCM_1		2
+#define WUETD_RSA_1		3
+
+	uByte bEncryptionValue;
+	uByte bAuthKeyIndex;
+} UPACKED wusb_encrypt_type_desc_t;
+
+/* WUSB Key Descriptor */
+typedef struct wusb_key_desc {
+	uByte bLength;
+	uByte bDescriptorType;
+	uByte tTKID[3];
+	uByte bReserved;
+	uByte KeyData[1];	/* variable length */
+} UPACKED wusb_key_desc_t;
+
+/* WUSB BOS Descriptor (Binary device Object Store) */
+typedef struct wusb_bos_desc {
+	uByte bLength;
+	uByte bDescriptorType;
+	uWord wTotalLength;
+	uByte bNumDeviceCaps;
+} UPACKED wusb_bos_desc_t;
+
+#define USB_DEVICE_CAPABILITY_20_EXTENSION	0x02
+typedef struct usb_dev_cap_20_ext_desc {
+	uByte bLength;
+	uByte bDescriptorType;
+	uByte bDevCapabilityType;
+#define USB_20_EXT_LPM				0x02
+	uDWord bmAttributes;
+} UPACKED usb_dev_cap_20_ext_desc_t;
+
+#define USB_DEVICE_CAPABILITY_SS_USB		0x03
+typedef struct usb_dev_cap_ss_usb {
+	uByte bLength;
+	uByte bDescriptorType;
+	uByte bDevCapabilityType;
+#define USB_DC_SS_USB_LTM_CAPABLE		0x02
+	uByte bmAttributes;
+#define USB_DC_SS_USB_SPEED_SUPPORT_LOW		0x01
+#define USB_DC_SS_USB_SPEED_SUPPORT_FULL	0x02
+#define USB_DC_SS_USB_SPEED_SUPPORT_HIGH	0x04
+#define USB_DC_SS_USB_SPEED_SUPPORT_SS		0x08
+	uWord wSpeedsSupported;
+	uByte bFunctionalitySupport;
+	uByte bU1DevExitLat;
+	uWord wU2DevExitLat;
+} UPACKED usb_dev_cap_ss_usb_t;
+
+#define USB_DEVICE_CAPABILITY_CONTAINER_ID	0x04
+typedef struct usb_dev_cap_container_id {
+	uByte bLength;
+	uByte bDescriptorType;
+	uByte bDevCapabilityType;
+	uByte bReserved;
+	uByte containerID[16];
+} UPACKED usb_dev_cap_container_id_t;
+
+/* Device Capability Type Codes */
+#define WUSB_DEVICE_CAPABILITY_WIRELESS_USB 0x01
+
+/* Device Capability Descriptor */
+typedef struct wusb_dev_cap_desc {
+	uByte bLength;
+	uByte bDescriptorType;
+	uByte bDevCapabilityType;
+	uByte caps[1];	/* Variable length */
+} UPACKED wusb_dev_cap_desc_t;
+
+/* Device Capability Descriptor */
+typedef struct wusb_dev_cap_uwb_desc {
+	uByte bLength;
+	uByte bDescriptorType;
+	uByte bDevCapabilityType;
+	uByte bmAttributes;
+	uWord wPHYRates;	/* Bitmap */
+	uByte bmTFITXPowerInfo;
+	uByte bmFFITXPowerInfo;
+	uWord bmBandGroup;
+	uByte bReserved;
+} UPACKED wusb_dev_cap_uwb_desc_t;
+
+/* Wireless USB Endpoint Companion Descriptor */
+typedef struct wusb_endpoint_companion_desc {
+	uByte bLength;
+	uByte bDescriptorType;
+	uByte bMaxBurst;
+	uByte bMaxSequence;
+	uWord wMaxStreamDelay;
+	uWord wOverTheAirPacketSize;
+	uByte bOverTheAirInterval;
+	uByte bmCompAttributes;
+} UPACKED wusb_endpoint_companion_desc_t;
+
+/* Wireless USB Numeric Association M1 Data Structure */
+typedef struct wusb_m1_data {
+	uByte version;
+	uWord langId;
+	uByte deviceFriendlyNameLength;
+	uByte sha_256_m3[32];
+	uByte deviceFriendlyName[256];
+} UPACKED wusb_m1_data_t;
+
+typedef struct wusb_m2_data {
+	uByte version;
+	uWord langId;
+	uByte hostFriendlyNameLength;
+	uByte pkh[384];
+	uByte hostFriendlyName[256];
+} UPACKED wusb_m2_data_t;
+
+typedef struct wusb_m3_data {
+	uByte pkd[384];
+	uByte nd;
+} UPACKED wusb_m3_data_t;
+
+typedef struct wusb_m4_data {
+	uDWord _attributeTypeIdAndLength_1;
+	uWord  associationTypeId;
+
+	uDWord _attributeTypeIdAndLength_2;
+	uWord  associationSubTypeId;
+
+	uDWord _attributeTypeIdAndLength_3;
+	uDWord length;
+
+	uDWord _attributeTypeIdAndLength_4;
+	uDWord associationStatus;
+
+	uDWord _attributeTypeIdAndLength_5;
+	uByte  chid[16];
+
+	uDWord _attributeTypeIdAndLength_6;
+	uByte  cdid[16];
+
+	uDWord _attributeTypeIdAndLength_7;
+	uByte  bandGroups[2];
+} UPACKED wusb_m4_data_t;
+
+#ifdef _MSC_VER
+#include <poppack.h>
+#endif
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* _USB_H_ */
diff --git a/drivers/watchdog/mpcore_wdt.c b/drivers/watchdog/mpcore_wdt.c
index 2b4af22..258b7db 100644
--- a/drivers/watchdog/mpcore_wdt.c
+++ b/drivers/watchdog/mpcore_wdt.c
@@ -32,20 +32,27 @@
 #include <linux/uaccess.h>
 #include <linux/slab.h>
 #include <linux/io.h>
+#include <linux/reboot.h>
 
 #include <asm/smp_twd.h>
 
 struct mpcore_wdt {
-	unsigned long	timer_alive;
+	unsigned long	timer_alive[NR_CPUS];
 	struct device	*dev;
 	void __iomem	*base;
 	int		irq;
-	unsigned int	perturb;
-	char		expect_close;
+	unsigned int	perturb[NR_CPUS];
+	char		expect_close[NR_CPUS];
+	unsigned long	count[NR_CPUS]; /* holds the counter value */
+	unsigned long	ts[NR_CPUS];	/* holds timestamp when the counter value has been recoreded */
 };
 
 static struct platform_device *mpcore_wdt_dev;
 static DEFINE_SPINLOCK(wdt_lock);
+#ifdef CONFIG_LOCAL_TIMERS
+extern void __iomem *twd_base;
+#endif
+extern unsigned int mpcore_timer_rate;
 
 #define TIMER_MARGIN	60
 static int mpcore_margin = TIMER_MARGIN;
@@ -77,8 +84,13 @@ static irqreturn_t mpcore_wdt_fire(int irq, void *arg)
 
 	/* Check it really was our interrupt */
 	if (readl(wdt->base + TWD_WDOG_INTSTAT)) {
+#ifdef CONFIG_ARCH_TRANSCEDE
+		/* there is an issue with hardware reset, so we are using software one */
+		emergency_restart();
+#else
 		dev_printk(KERN_CRIT, wdt->dev,
 					"Triggered - Reboot ignored.\n");
+#endif
 		/* Clear the interrupt on the watchdog */
 		writel(1, wdt->base + TWD_WDOG_INTSTAT);
 		return IRQ_HANDLED;
@@ -95,26 +107,68 @@ static irqreturn_t mpcore_wdt_fire(int irq, void *arg)
  */
 static void mpcore_wdt_keepalive(struct mpcore_wdt *wdt)
 {
-	unsigned long count;
-
 	spin_lock(&wdt_lock);
-	/* Assume prescale is set to 256 */
-	count =  __raw_readl(wdt->base + TWD_WDOG_COUNTER);
-	count = (0xFFFFFFFFU - count) * (HZ / 5);
-	count = (count / 256) * mpcore_margin;
-
-	/* Reload the counter */
-	writel(count + wdt->perturb, wdt->base + TWD_WDOG_LOAD);
-	wdt->perturb = wdt->perturb ? 0 : 1;
+	if (wdt->count[smp_processor_id()] && wdt->ts[smp_processor_id()]) {
+		/* The counter is set and it's value is known at some specific timestamp.
+		 * So now we can get current counter value, current timestamp and calculate the actual 'counter decrement speed'.
+		 * And basing on it we can calculate the counter value which is needed to be set to get configured margin,
+		 * 'Margin' is the period in seconds which should elapse for watchdog to trigger reboot if no 'keepalive' update is done
+		 * during this period.
+		 */
+		const unsigned long jiffies_delta = (long)jiffies - (long)wdt->ts[smp_processor_id()]; /* the jiffies are incrementing */
+
+		if (jiffies_delta >= (HZ * 5)) {
+			/* Not less than 5 seconds have elapsed from last known counter value,
+			 * so it's safe to calculate speed.
+			 */
+			const unsigned long count = __raw_readl(wdt->base + TWD_WDOG_COUNTER);
+			const unsigned long count_delta = wdt->count[smp_processor_id()] - count; /* the counter is decrementing */
+			const unsigned long count_per_jiffie = count_delta / jiffies_delta;
+			const unsigned long count_per_second = count_per_jiffie * HZ;
+			const unsigned long count_per_margin = count_per_second * mpcore_margin + wdt->perturb[smp_processor_id()];
+
+			/* Reload the counter.
+			 * Counter will hold the value which matches configured margin.
+			 */
+			writel(count_per_margin, wdt->base + TWD_WDOG_LOAD);
+			wdt->perturb[smp_processor_id()] = wdt->perturb[smp_processor_id()] ? 0 : 1;
+
+			/* Record the value we have configured and
+			 * timestamp when this has been done.
+			 */
+			wdt->count[smp_processor_id()] = count_per_margin;
+			wdt->ts[smp_processor_id()] = jiffies;
+		}
+	} else {
+		/* Counter hasn't been set yet.
+		 * Have to setup initial value.
+		 * Setup initially maximum value.
+		 */
+		const unsigned long count = 0xFFFFFFFFU;
+		writel(count, wdt->base + TWD_WDOG_LOAD);
+
+		/* Record the value we have configured and
+		 * timestamp when this has been done.
+		 * Next keepalive() invocation should trigger 'counter decrement speed' calculation
+		 * and configuring the right counter value for the specified margin.
+		 */
+		wdt->count[smp_processor_id()] = count;
+		wdt->ts[smp_processor_id()] = jiffies;
+	}
 	spin_unlock(&wdt_lock);
 }
 
 static void mpcore_wdt_stop(struct mpcore_wdt *wdt)
 {
+	uint load;
 	spin_lock(&wdt_lock);
 	writel(0x12345678, wdt->base + TWD_WDOG_DISABLE);
 	writel(0x87654321, wdt->base + TWD_WDOG_DISABLE);
 	writel(0x0, wdt->base + TWD_WDOG_CONTROL);
+	load = readl (wdt->base + TWD_WDOG_LOAD);
+	writel(load, wdt->base + TWD_WDOG_COUNTER);
+	wdt->count[smp_processor_id()] = 0;
+	wdt->ts[smp_processor_id()] = 0;
 	spin_unlock(&wdt_lock);
 }
 
@@ -125,13 +179,23 @@ static void mpcore_wdt_start(struct mpcore_wdt *wdt)
 	/* This loads the count register but does NOT start the count yet */
 	mpcore_wdt_keepalive(wdt);
 
+	disable_irq(wdt->irq);
+	enable_irq(wdt->irq);
+
+	spin_lock(&wdt_lock);
 	if (mpcore_noboot) {
 		/* Enable watchdog - prescale=256, watchdog mode=0, enable=1 */
 		writel(0x0000FF01, wdt->base + TWD_WDOG_CONTROL);
 	} else {
 		/* Enable watchdog - prescale=256, watchdog mode=1, enable=1 */
+#ifdef	CONFIG_ARCH_TRANSCEDE
+		/* enable interrupt instead of cluster reset */
+		writel(0x0000FF05, wdt->base + TWD_WDOG_CONTROL);
+#else
 		writel(0x0000FF09, wdt->base + TWD_WDOG_CONTROL);
+#endif
 	}
+	spin_unlock(&wdt_lock);
 }
 
 static int mpcore_wdt_set_heartbeat(int t)
@@ -150,7 +214,7 @@ static int mpcore_wdt_open(struct inode *inode, struct file *file)
 {
 	struct mpcore_wdt *wdt = platform_get_drvdata(mpcore_wdt_dev);
 
-	if (test_and_set_bit(0, &wdt->timer_alive))
+	if (test_and_set_bit(0, &wdt->timer_alive[smp_processor_id()]))
 		return -EBUSY;
 
 	if (nowayout)
@@ -174,15 +238,15 @@ static int mpcore_wdt_release(struct inode *inode, struct file *file)
 	 *	Shut off the timer.
 	 *	Lock it in if it's a module and we set nowayout
 	 */
-	if (wdt->expect_close == 42)
+	if (wdt->expect_close[smp_processor_id()] == 42)
 		mpcore_wdt_stop(wdt);
 	else {
 		dev_printk(KERN_CRIT, wdt->dev,
 				"unexpected close, not stopping watchdog!\n");
 		mpcore_wdt_keepalive(wdt);
 	}
-	clear_bit(0, &wdt->timer_alive);
-	wdt->expect_close = 0;
+	clear_bit(0, &wdt->timer_alive[smp_processor_id()]);
+	wdt->expect_close[smp_processor_id()] = 0;
 	return 0;
 }
 
@@ -199,7 +263,7 @@ static ssize_t mpcore_wdt_write(struct file *file, const char *data,
 			size_t i;
 
 			/* In case it was set long ago */
-			wdt->expect_close = 0;
+			wdt->expect_close[smp_processor_id()] = 0;
 
 			for (i = 0; i != len; i++) {
 				char c;
@@ -207,7 +271,7 @@ static ssize_t mpcore_wdt_write(struct file *file, const char *data,
 				if (get_user(c, data + i))
 					return -EFAULT;
 				if (c == 'V')
-					wdt->expect_close = 42;
+					wdt->expect_close[smp_processor_id()] = 42;
 			}
 		}
 		mpcore_wdt_keepalive(wdt);
@@ -327,18 +391,22 @@ static struct miscdevice mpcore_wdt_miscdev = {
 static int __devinit mpcore_wdt_probe(struct platform_device *dev)
 {
 	struct mpcore_wdt *wdt;
+#ifndef CONFIG_LOCAL_TIMERS
 	struct resource *res;
+#endif
 	int ret;
 
 	/* We only accept one device, and it must have an id of -1 */
 	if (dev->id != -1)
 		return -ENODEV;
 
+#ifndef CONFIG_LOCAL_TIMERS
 	res = platform_get_resource(dev, IORESOURCE_MEM, 0);
 	if (!res) {
 		ret = -ENODEV;
 		goto err_out;
 	}
+#endif
 
 	wdt = kzalloc(sizeof(struct mpcore_wdt), GFP_KERNEL);
 	if (!wdt) {
@@ -352,7 +420,11 @@ static int __devinit mpcore_wdt_probe(struct platform_device *dev)
 		ret = -ENXIO;
 		goto err_free;
 	}
-	wdt->base = ioremap(res->start, resource_size(res));
+#ifdef CONFIG_LOCAL_TIMERS
+	wdt->base = twd_base;
+#else
+	wdt->base = ioremap(res->start, res->end - res->start + 1);
+#endif
 	if (!wdt->base) {
 		ret = -ENOMEM;
 		goto err_free;
@@ -367,7 +439,7 @@ static int __devinit mpcore_wdt_probe(struct platform_device *dev)
 		goto err_misc;
 	}
 
-	ret = request_irq(wdt->irq, mpcore_wdt_fire, IRQF_DISABLED,
+	ret = request_irq(wdt->irq, mpcore_wdt_fire, IRQF_DISABLED | IRQF_PERCPU,
 							"mpcore_wdt", wdt);
 	if (ret) {
 		dev_printk(KERN_ERR, wdt->dev,
@@ -384,7 +456,9 @@ static int __devinit mpcore_wdt_probe(struct platform_device *dev)
 err_irq:
 	misc_deregister(&mpcore_wdt_miscdev);
 err_misc:
+#ifndef CONFIG_LOCAL_TIMERS
 	iounmap(wdt->base);
+#endif
 err_free:
 	kfree(wdt);
 err_out:
@@ -402,7 +476,9 @@ static int __devexit mpcore_wdt_remove(struct platform_device *dev)
 	mpcore_wdt_dev = NULL;
 
 	free_irq(wdt->irq, wdt);
+#ifndef CONFIG_LOCAL_TIMERS
 	iounmap(wdt->base);
+#endif
 	kfree(wdt);
 	return 0;
 }
diff --git a/fs/fcntl.c b/fs/fcntl.c
index 22764c7..0ffe61f 100644
--- a/fs/fcntl.c
+++ b/fs/fcntl.c
@@ -142,6 +142,7 @@ SYSCALL_DEFINE1(dup, unsigned int, fildes)
 	}
 	return ret;
 }
+EXPORT_SYMBOL(sys_dup);
 
 #define SETFL_MASK (O_APPEND | O_NONBLOCK | O_NDELAY | O_DIRECT | O_NOATIME)
 
diff --git a/fs/ioctl.c b/fs/ioctl.c
index 1d9b9fc..0c73b04 100644
--- a/fs/ioctl.c
+++ b/fs/ioctl.c
@@ -18,6 +18,11 @@
 
 #include <asm/ioctls.h>
 
+#ifdef CONFIG_TRANSCEDE_MLOG
+#include <mach/mlog.h>
+#include <mach/tcb.h>
+#endif
+
 /* So that the fiemap access checks can't overflow on 32 bit machines. */
 #define FIEMAP_MAX_EXTENTS	(UINT_MAX / sizeof(struct fiemap_extent))
 
@@ -603,6 +608,10 @@ int do_vfs_ioctl(struct file *filp, unsigned int fd, unsigned int cmd,
 
 SYSCALL_DEFINE3(ioctl, unsigned int, fd, unsigned int, cmd, unsigned long, arg)
 {
+#ifdef CONFIG_TRANSCEDE_MLOG
+	unsigned long t = get_tick();
+	unsigned int mlogVars[10], mlogVarsCnt = 0;
+#endif
 	struct file *filp;
 	int error = -EBADF;
 	int fput_needed;
@@ -619,5 +628,19 @@ SYSCALL_DEFINE3(ioctl, unsigned int, fd, unsigned int, cmd, unsigned long, arg)
  out_fput:
 	fput_light(filp, fput_needed);
  out:
+
+#ifdef CONFIG_TRANSCEDE_MLOG
+	if (MLogIsEnabled()) {
+		unsigned long tt = get_tick();
+        mlogVars[mlogVarsCnt++] = MLOG_VAR_IOCTL;
+        mlogVars[mlogVarsCnt++] = current->pid;
+        mlogVars[mlogVarsCnt++] = error;
+        mlogVars[mlogVarsCnt++] = cmd;
+        mlogVars[mlogVarsCnt++] = arg;
+        MLogAddVariables(mlogVarsCnt, mlogVars, tt);
+        MLogTask(TASK_ID_IOCTL, 1, t, tt);
+	}
+#endif
+
 	return error;
 }
diff --git a/fs/select.c b/fs/select.c
index d33418f..c8adf40 100644
--- a/fs/select.c
+++ b/fs/select.c
@@ -29,6 +29,10 @@
 
 #include <asm/uaccess.h>
 
+#ifdef CONFIG_TRANSCEDE_MLOG
+#include <mach/mlog.h>
+#include <mach/tcb.h>
+#endif
 
 /*
  * Estimate expected accuracy in ns from a timeval.
@@ -595,6 +599,10 @@ out_nofds:
 SYSCALL_DEFINE5(select, int, n, fd_set __user *, inp, fd_set __user *, outp,
 		fd_set __user *, exp, struct timeval __user *, tvp)
 {
+#ifdef CONFIG_TRANSCEDE_MLOG
+	unsigned long t = get_tick();
+	unsigned int mlogVars[10], mlogVarsCnt = 0;
+#endif
 	struct timespec end_time, *to = NULL;
 	struct timeval tv;
 	int ret;
@@ -613,6 +621,19 @@ SYSCALL_DEFINE5(select, int, n, fd_set __user *, inp, fd_set __user *, outp,
 	ret = core_sys_select(n, inp, outp, exp, to);
 	ret = poll_select_copy_remaining(&end_time, tvp, 1, ret);
 
+#ifdef CONFIG_TRANSCEDE_MLOG
+	if (MLogIsEnabled()) {
+		unsigned long tt = get_tick();
+		mlogVars[mlogVarsCnt++] = MLOG_VAR_SELECT_0;
+		mlogVars[mlogVarsCnt++] = current->pid;
+		mlogVars[mlogVarsCnt++] = ret;
+		mlogVars[mlogVarsCnt++] = tv.tv_sec;
+		mlogVars[mlogVarsCnt++] = tv.tv_usec;
+		MLogAddVariables(mlogVarsCnt, mlogVars, tt);
+		MLogTask(TASK_ID_SELECT, 1, t, tt);
+	}
+#endif
+
 	return ret;
 }
 
@@ -621,6 +642,10 @@ static long do_pselect(int n, fd_set __user *inp, fd_set __user *outp,
 		       fd_set __user *exp, struct timespec __user *tsp,
 		       const sigset_t __user *sigmask, size_t sigsetsize)
 {
+#ifdef CONFIG_TRANSCEDE_MLOG
+	unsigned long t = get_tick();
+	unsigned int mlogVars[10], mlogVarsCnt = 0;
+#endif
 	sigset_t ksigmask, sigsaved;
 	struct timespec ts, end_time, *to = NULL;
 	int ret;
@@ -662,6 +687,19 @@ static long do_pselect(int n, fd_set __user *inp, fd_set __user *outp,
 	} else if (sigmask)
 		sigprocmask(SIG_SETMASK, &sigsaved, NULL);
 
+#ifdef CONFIG_TRANSCEDE_MLOG
+	if (MLogIsEnabled()) {
+		unsigned long tt = get_tick();
+		mlogVars[mlogVarsCnt++] = MLOG_VAR_SELECT_1;
+		mlogVars[mlogVarsCnt++] = current->pid;
+		mlogVars[mlogVarsCnt++] = ret;
+		mlogVars[mlogVarsCnt++] = ts.tv_sec;
+		mlogVars[mlogVarsCnt++] = ts.tv_nsec;
+		MLogAddVariables(mlogVarsCnt, mlogVars, tt);
+		MLogTask(TASK_ID_SELECT, 1, t, tt);
+	}
+#endif
+
 	return ret;
 }
 
diff --git a/include/asm-generic/sections.h b/include/asm-generic/sections.h
index c1a1216..08a8483 100644
--- a/include/asm-generic/sections.h
+++ b/include/asm-generic/sections.h
@@ -14,6 +14,9 @@ extern char __kprobes_text_start[], __kprobes_text_end[];
 extern char __entry_text_start[], __entry_text_end[];
 extern char __initdata_begin[], __initdata_end[];
 extern char __start_rodata[], __end_rodata[];
+#ifdef CONFIG_ARCH_TRANSCEDE
+extern char __initramfs_begin[], __initramfs_end[];
+#endif
 
 /* Start and end of .ctors section - used for constructor calls. */
 extern char __ctors_start[], __ctors_end[];
diff --git a/include/crypto/aead.h b/include/crypto/aead.h
index 0edf949..26a9f88 100644
--- a/include/crypto/aead.h
+++ b/include/crypto/aead.h
@@ -102,4 +102,11 @@ static inline void aead_givcrypt_set_giv(struct aead_givcrypt_request *req,
 	req->seq = seq;
 }
 
+static inline void aead_givcrypt_set_ip(struct aead_givcrypt_request *req,
+					struct scatterlist *ip,
+					unsigned int iplen)
+{
+        aead_request_set_ip(&req->areq, ip, iplen);
+}
+
 #endif	/* _CRYPTO_AEAD_H */
diff --git a/include/linux/crypto.h b/include/linux/crypto.h
index a6a7a1c..e66f46d 100644
--- a/include/linux/crypto.h
+++ b/include/linux/crypto.h
@@ -124,6 +124,7 @@ struct crypto_async_request {
 	struct crypto_tfm *tfm;
 
 	u32 flags;
+	u8 offloaded;
 };
 
 struct ablkcipher_request {
@@ -136,6 +137,7 @@ struct ablkcipher_request {
 	struct scatterlist *src;
 	struct scatterlist *dst;
 
+
 	void *__ctx[] CRYPTO_MINALIGN_ATTR;
 };
 
@@ -155,12 +157,13 @@ struct aead_request {
 
 	unsigned int assoclen;
 	unsigned int cryptlen;
-
+	unsigned int iplen;
 	u8 *iv;
 
 	struct scatterlist *assoc;
 	struct scatterlist *src;
 	struct scatterlist *dst;
+	struct scatterlist *ip;
 
 	void *__ctx[] CRYPTO_MINALIGN_ATTR;
 };
@@ -298,6 +301,7 @@ struct crypto_alg {
 	void (*cra_destroy)(struct crypto_alg *alg);
 	
 	struct module *cra_module;
+	int offloaded;
 };
 
 /*
@@ -551,6 +555,14 @@ static inline unsigned int crypto_tfm_ctx_alignment(void)
 	return __alignof__(tfm->__crt_ctx);
 }
 
+static inline void aead_request_set_ip(struct aead_request *req,
+					struct scatterlist *ip,
+					unsigned int iplen)
+{
+	req->ip = ip;
+	req->iplen = iplen;
+}
+
 /*
  * API wrappers.
  */
diff --git a/include/linux/gfp.h b/include/linux/gfp.h
index cb40892..c689d3f 100644
--- a/include/linux/gfp.h
+++ b/include/linux/gfp.h
@@ -36,7 +36,7 @@ struct vm_area_struct;
 #endif
 #define ___GFP_NO_KSWAPD	0x400000u
 #define ___GFP_OTHER_NODE	0x800000u
-
+#define __GFP_SD_ALLOC		0x80000000u	// to try to allocate the memory by using Storage Driver (SD)
 /*
  * GFP bitmasks..
  *
diff --git a/include/linux/init_task.h b/include/linux/init_task.h
index 1745dc2..07e4a70 100644
--- a/include/linux/init_task.h
+++ b/include/linux/init_task.h
@@ -154,10 +154,12 @@ extern struct task_group root_task_group;
  *  INIT_TASK is used to set up the first task table, touch at
  * your own risk!. Base=0, limit=0x1fffff (=2MB)
  */
+#ifdef CONFIG_AMP_STACK
 #define INIT_TASK(tsk)	\
 {									\
 	.state		= 0,						\
 	.stack		= &init_thread_info,				\
+	.amp_stack	= &init_thread_info, \
 	.usage		= ATOMIC_INIT(2),				\
 	.flags		= PF_KTHREAD,					\
 	.prio		= MAX_PRIO-20,					\
@@ -221,7 +223,75 @@ extern struct task_group root_task_group;
 	INIT_TASK_RCU_PREEMPT(tsk)					\
 	INIT_CPUSET_SEQ							\
 }
-
+#else   /* !CONFIG_AMP_STACK */
+#define INIT_TASK(tsk)	\
+{									\
+	.state		= 0,						\
+	.stack		= &init_thread_info,				\
+	.usage		= ATOMIC_INIT(2),				\
+	.flags		= PF_KTHREAD,					\
+	.prio		= MAX_PRIO-20,					\
+	.static_prio	= MAX_PRIO-20,					\
+	.normal_prio	= MAX_PRIO-20,					\
+	.policy		= SCHED_NORMAL,					\
+	.cpus_allowed	= CPU_MASK_ALL,					\
+	.mm		= NULL,						\
+	.active_mm	= &init_mm,					\
+	.se		= {						\
+		.group_node 	= LIST_HEAD_INIT(tsk.se.group_node),	\
+	},								\
+	.rt		= {						\
+		.run_list	= LIST_HEAD_INIT(tsk.rt.run_list),	\
+		.time_slice	= HZ, 					\
+		.nr_cpus_allowed = NR_CPUS,				\
+	},								\
+	.tasks		= LIST_HEAD_INIT(tsk.tasks),			\
+	INIT_PUSHABLE_TASKS(tsk)					\
+	INIT_CGROUP_SCHED(tsk)						\
+	.ptraced	= LIST_HEAD_INIT(tsk.ptraced),			\
+	.ptrace_entry	= LIST_HEAD_INIT(tsk.ptrace_entry),		\
+	.real_parent	= &tsk,						\
+	.parent		= &tsk,						\
+	.children	= LIST_HEAD_INIT(tsk.children),			\
+	.sibling	= LIST_HEAD_INIT(tsk.sibling),			\
+	.group_leader	= &tsk,						\
+	RCU_INIT_POINTER(.real_cred, &init_cred),			\
+	RCU_INIT_POINTER(.cred, &init_cred),				\
+	.comm		= INIT_TASK_COMM,				\
+	.thread		= INIT_THREAD,					\
+	.fs		= &init_fs,					\
+	.files		= &init_files,					\
+	.signal		= &init_signals,				\
+	.sighand	= &init_sighand,				\
+	.nsproxy	= &init_nsproxy,				\
+	.pending	= {						\
+		.list = LIST_HEAD_INIT(tsk.pending.list),		\
+		.signal = {{0}}},					\
+	.blocked	= {{0}},					\
+	.alloc_lock	= __SPIN_LOCK_UNLOCKED(tsk.alloc_lock),		\
+	.journal_info	= NULL,						\
+	.cpu_timers	= INIT_CPU_TIMERS(tsk.cpu_timers),		\
+	.fs_excl	= ATOMIC_INIT(0),				\
+	.pi_lock	= __RAW_SPIN_LOCK_UNLOCKED(tsk.pi_lock),	\
+	.timer_slack_ns = 50000, /* 50 usec default slack */		\
+	INIT_TIMER_LIST							\
+	.pids = {							\
+		[PIDTYPE_PID]  = INIT_PID_LINK(PIDTYPE_PID),		\
+		[PIDTYPE_PGID] = INIT_PID_LINK(PIDTYPE_PGID),		\
+		[PIDTYPE_SID]  = INIT_PID_LINK(PIDTYPE_SID),		\
+	},								\
+	.thread_group	= LIST_HEAD_INIT(tsk.thread_group),		\
+	.dirties = INIT_PROP_LOCAL_SINGLE(dirties),			\
+	INIT_IDS							\
+	INIT_PERF_EVENTS(tsk)						\
+	INIT_TRACE_IRQFLAGS						\
+	INIT_LOCKDEP							\
+	INIT_FTRACE_GRAPH						\
+	INIT_TRACE_RECURSION						\
+	INIT_TASK_RCU_PREEMPT(tsk)					\
+	INIT_CPUSET_SEQ							\
+}
+#endif  /* CONFIG_AMP_STACK */
 
 #define INIT_CPU_TIMERS(cpu_timers)					\
 {									\
diff --git a/include/linux/kthread.h b/include/linux/kthread.h
index 1e923e5..7cc5de8 100644
--- a/include/linux/kthread.h
+++ b/include/linux/kthread.h
@@ -13,6 +13,16 @@ struct task_struct *kthread_create_on_node(int (*threadfn)(void *data),
 #define kthread_create(threadfn, data, namefmt, arg...) \
 	kthread_create_on_node(threadfn, data, -1, namefmt, ##arg)
 
+#ifdef CONFIG_AMP_STACK
+struct task_struct *kthread_create_amp_on_node(int cpu, int (*threadfn)(void *data),
+                                               void *data,
+                                               int node,
+                                               const char namefmt[], ...)
+	__attribute__((format(printf, 5, 6)));
+
+#define _kthread_amp_create(cpu, threadfn, data, namefmt, arg...)	  \
+	kthread_create_amp_on_node(cpu, threadfn, data, -1, namefmt, ##arg)
+#endif	/* CONFIG_AMP_STACK */
 
 /**
  * kthread_run - create and wake a thread.
diff --git a/include/linux/mindspeed/ifdhandler.h b/include/linux/mindspeed/ifdhandler.h
new file mode 100644
index 0000000..f614a60
--- /dev/null
+++ b/include/linux/mindspeed/ifdhandler.h
@@ -0,0 +1,138 @@
+/*****************************************************************
+/
+/ File   :   ifdhandler.h
+/ Author :   David Corcoran <corcoran@linuxnet.com>
+/ Date   :   June 15, 2000
+/ Purpose:   This provides reader specific low-level calls.
+/            See http://www.linuxnet.com for more information.
+/ License:   See file LICENSE
+/
+******************************************************************/
+
+#ifndef _ifd_handler_h_
+#define _ifd_handler_h_
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#include "pcscdefines.h"
+
+  /* List of data structures available to ifdhandler */
+
+  typedef struct _DEVICE_CAPABILITIES {
+
+    LPSTR Vendor_Name;          /*!< Tag 0x0100        */
+    LPSTR IFD_Type;		/*!< Tag 0x0101        */
+    DWORD IFD_Version;		/*!< Tag 0x0102        */
+    LPSTR IFD_Serial;		/*!< Tag 0x0103        */
+    DWORD IFD_Channel_ID;  	/*!< Tag 0x0110        */
+
+    DWORD Asynch_Supported;	/*!< Tag 0x0120        */
+    DWORD Default_Clock;	/*!< Tag 0x0121        */
+    DWORD Max_Clock;		/*!< Tag 0x0122        */
+    DWORD Default_Data_Rate;	/*!< Tag 0x0123        */
+    DWORD Max_Data_Rate;	/*!< Tag 0x0124        */
+    DWORD Max_IFSD;		/*!< Tag 0x0125        */
+    DWORD Synch_Supported;	/*!< Tag 0x0126        */
+    DWORD Power_Mgmt;		/*!< Tag 0x0131        */
+    DWORD Card_Auth_Devices;	/*!< Tag 0x0140        */
+    DWORD User_Auth_Device;	/*!< Tag 0x0142        */
+    DWORD Mechanics_Supported;	/*!< Tag 0x0150        */
+    DWORD Vendor_Features;	/*!< Tag 0x0180 - 0x01F0   User Defined. */
+
+  } DEVICE_CAPABILITIES, *PDEVICE_CAPABILITIES;
+
+  typedef struct _ICC_STATE {
+
+    UCHAR ICC_Presence;		/*!< Tag 0x0300        */
+    UCHAR ICC_Interface_Status;	/*!< Tag 0x0301        */
+    UCHAR ATR[MAX_ATR_SIZE];	/*!< Tag 0x0303        */
+    UCHAR ICC_Type;		/*!< Tag 0x0304        */
+
+  } ICC_STATE, *PICC_STATE;
+
+  typedef struct _PROTOCOL_OPTIONS {
+
+    DWORD Protocol_Type;	/*!< Tag 0x0201        */
+    DWORD Current_Clock;	/*!< Tag 0x0202        */
+    DWORD Current_F;		/*!< Tag 0x0203        */
+    DWORD Current_D;		/*!< Tag 0x0204        */
+    DWORD Current_N;		/*!< Tag 0x0205        */
+    DWORD Current_W;		/*!< Tag 0x0206        */
+    DWORD Current_IFSC;		/*!< Tag 0x0207        */
+    DWORD Current_IFSD;		/*!< Tag 0x0208        */
+    DWORD Current_BWT;		/*!< Tag 0x0209        */
+    DWORD Current_CWT;		/*!< Tag 0x020A        */
+    DWORD Current_EBC;		/*!< Tag 0x020B        */
+  } PROTOCOL_OPTIONS, *PPROTOCOL_OPTIONS;
+
+  typedef struct _SCARD_IO_HEADER {
+    DWORD Protocol;             /*!< Protocol number */
+    DWORD Length;               /*!< Length field (unused) */
+  } SCARD_IO_HEADER, *PSCARD_IO_HEADER;
+
+  /* End of structure list */
+
+
+
+  /* The list of tags should be alot more but
+     this is all I use in the meantime        */
+
+#define TAG_IFD_ATR			0x0303
+
+  /* End of tag list                          */
+
+
+
+  /* List of defines available to ifdhandler */
+
+#define IFD_POWER_UP			500
+#define IFD_POWER_DOWN			501
+#define IFD_RESET			502
+
+#define IFD_NEGOTIATE_PTS1		1
+#define IFD_NEGOTIATE_PTS2		2
+#define IFD_NEGOTIATE_PTS3              4
+
+#define	IFD_SUCCESS			0
+#define IFD_ERROR_TAG			600
+#define IFD_ERROR_SET_FAILURE		601
+#define IFD_ERROR_VALUE_READ_ONLY	602
+#define IFD_ERROR_PTS_FAILURE		605
+#define IFD_ERROR_NOT_SUPPORTED		606
+#define IFD_PROTOCOL_NOT_SUPPORTED	607
+#define IFD_ERROR_POWER_ACTION		608
+#define IFD_ERROR_SWALLOW		609
+#define IFD_ERROR_EJECT			610
+#define IFD_ERROR_CONFISCATE		611
+#define IFD_COMMUNICATION_ERROR		612
+#define IFD_RESPONSE_TIMEOUT		613
+#define IFD_NOT_SUPPORTED		614
+#define IFD_ICC_PRESENT			615
+#define IFD_ICC_NOT_PRESENT		616
+
+  /* List of Defined Functions Available to IFD_Handler */
+
+  RESPONSECODE IFDHCreateChannel ( DWORD, DWORD );
+  RESPONSECODE IFDHCloseChannel ( DWORD );
+  RESPONSECODE IFDHGetCapabilities ( DWORD, DWORD, PDWORD,
+				     PUCHAR );
+  RESPONSECODE IFDHSetCapabilities ( DWORD, DWORD, DWORD, PUCHAR );
+  RESPONSECODE IFDHSetProtocolParameters ( DWORD, DWORD, UCHAR,
+					   UCHAR, UCHAR, UCHAR );
+  RESPONSECODE IFDHPowerICC ( DWORD, DWORD, PUCHAR, PDWORD );
+  RESPONSECODE IFDHTransmitToICC ( DWORD, SCARD_IO_HEADER, PUCHAR,
+				   DWORD, PUCHAR, PDWORD,
+				   PSCARD_IO_HEADER );
+  RESPONSECODE IFDHControl ( DWORD, PUCHAR, DWORD,
+			     PUCHAR, PDWORD );
+  RESPONSECODE IFDHICCPresence( DWORD );
+
+  int IFDHGetFileHandle( DWORD lun );
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* _ifd_hander_h_ */
diff --git a/include/linux/mindspeed/pcscdefines.h b/include/linux/mindspeed/pcscdefines.h
new file mode 100644
index 0000000..44af294
--- /dev/null
+++ b/include/linux/mindspeed/pcscdefines.h
@@ -0,0 +1,36 @@
+/*****************************************************************
+/
+/ File   :   pcscdefines.h
+/ Author :   David Corcoran <corcoran@linuxnet.com>
+/ Date   :   June 15, 2000
+/ Purpose:   This provides PC/SC shared defines.
+/            See http://www.linuxnet.com for more information.
+/ License:   See file LICENSE
+/
+******************************************************************/
+
+#ifndef _pcscdefines_h_
+#define _pcscdefines_h_
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/* Defines a list of pseudo types. */
+
+  typedef unsigned long      DWORD;
+  typedef unsigned long*     PDWORD;
+  typedef unsigned char      UCHAR;
+  typedef unsigned char*     PUCHAR;
+  typedef char*              LPSTR;
+  typedef long               RESPONSECODE;
+  typedef void               VOID;
+
+  #define MAX_RESPONSE_SIZE  264
+  #define MAX_ATR_SIZE       33
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* _pcscdefines_h_ */
diff --git a/include/linux/mindspeed/transcede_usim.h b/include/linux/mindspeed/transcede_usim.h
new file mode 100644
index 0000000..28ad250
--- /dev/null
+++ b/include/linux/mindspeed/transcede_usim.h
@@ -0,0 +1,147 @@
+/*
+ * Copyright(c) 2007-2014 Intel Corporation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify 
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but 
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU 
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software 
+ * Foundation, Inc., 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+ * The full GNU General Public License is included in this distribution 
+ * in the file called LICENSE.GPL.
+ *
+ * Contact Information:
+ * Intel Corporation
+ */
+/*!
+ * \file transcede_usim.h
+ * \brief Definition of T2K ioctl types and associated data.
+ *
+ * This file defines the ioctl() calls used for Transcede configuration and any
+ * associated shared structures that are used to pass data from userspace into
+ * kernelspace.
+ */
+
+#ifndef __TRANSCEDE_USIM_H__
+#define __TRANSCEDE_USIM_H__
+
+#ifdef __KERNEL__
+#include <linux/types.h>
+#include <asm/ioctl.h>
+#include <linux/mindspeed/pcscdefines.h>
+#include <linux/mindspeed/ifdhandler.h>
+#else /* __KERNEL__ */
+#include <sys/types.h>
+#include <sys/ioctl.h>
+#include <linux/mindspeed/pcscdefines.h>
+#include <linux/mindspeed/ifdhandler.h>
+#endif /* __KERNEL__ */
+
+/*!
+ * \brief Structure to pass get /set capability requests between userspace and
+ * the kernel.
+ */
+struct transcede_usim_ioc_capabilities
+{
+    struct transcede_usim_ioc_capabilities_header
+    {
+        DWORD tag;
+        DWORD length;
+    } header;
+    struct transcede_usim_ioc_capabilities_body
+    {
+        UCHAR value[MAX_RESPONSE_SIZE];
+    } body;
+};
+
+/*!
+ * \brief Structure to pass set protocol parameters requests between userspace
+ * and the kernel.
+ */
+struct transcede_usim_ioc_set_protocol_params
+{
+    DWORD protocol;
+    UCHAR flags;
+    UCHAR PTS1;
+    UCHAR PTS2;
+    UCHAR PTS3;
+};
+
+/*!
+ * \brief Structure to pass power up requests between userspace and the
+ * kernel.
+ */
+struct transcede_usim_ioc_power_icc
+{
+    DWORD action;
+    DWORD atr_length;
+    UCHAR atr[MAX_ATR_SIZE];
+};
+
+/*!
+ * \brief Structure to pass control requests between userspace and the kernel.
+ */
+struct transcede_usim_ioc_control
+{
+    DWORD length;
+    UCHAR buffer[MAX_RESPONSE_SIZE];
+};
+
+/*!
+ * \brief Structure to pass transaction requests between userspace and the kernel.
+ */
+struct transcede_usim_ioc_transmit_to_icc
+{
+     DWORD protocol;
+     DWORD length;
+     UCHAR buffer[MAX_RESPONSE_SIZE];
+};
+
+/*! The magic number for picoIf ioctl() messages. */
+#define TRANSCEDE_USIM_IOC_MAGIC       'p'
+
+/*! Get capabilities request. */
+#define TRANSCEDE_USIM_IOC_GET_CAPABILITIES \
+                                _IOWR( TRANSCEDE_USIM_IOC_MAGIC, 0, \
+                                struct transcede_usim_ioc_capabilities )
+
+/*! Set capabilities request. */
+#define TRANSCEDE_USIM_IOC_SET_CAPABILITIES \
+                                _IOWR( TRANSCEDE_USIM_IOC_MAGIC, 1, \
+                                struct transcede_usim_ioc_capabilities )
+
+/*! Set protocol params request. */
+#define TRANSCEDE_USIM_IOC_SET_PROTOCOL_PARAMS \
+                                _IOW( TRANSCEDE_USIM_IOC_MAGIC, 2, \
+                                struct transcede_usim_ioc_set_protocol_params )
+
+/*! Transaction request. */
+#define TRANSCEDE_USIM_IOC_TRANSMIT_TO_ICC \
+                                _IOWR( TRANSCEDE_USIM_IOC_MAGIC, 3, \
+                                struct transcede_usim_ioc_transmit_to_icc )
+
+/*! Power up / down smart card request. */
+#define TRANSCEDE_USIM_IOC_POWER_ICC \
+                                _IOWR( TRANSCEDE_USIM_IOC_MAGIC, 4, \
+                                struct transcede_usim_ioc_power_icc  )
+
+/*! Control request. */
+#define TRANSCEDE_USIM_IOC_CONTROL \
+                                _IOWR( TRANSCEDE_USIM_IOC_MAGIC, 5, \
+                                struct transcede_usim_ioc_control )
+
+/*! ICC Presence request. */
+#define TRANSCEDE_USIM_IOC_ICC_PRESENCE \
+                                _IOR( TRANSCEDE_USIM_IOC_MAGIC, 6, \
+                                DWORD )
+
+/*! The number of Transcede ioctl()s. */
+#define TRANSCEDE_USIM_IOC_NUM  ( 7 )
+
+#endif /* !__TRANSCEDE_USIM_H__ */
diff --git a/include/linux/miscdevice.h b/include/linux/miscdevice.h
index 18fd130..a469480 100644
--- a/include/linux/miscdevice.h
+++ b/include/linux/miscdevice.h
@@ -18,6 +18,7 @@
 #define APOLLO_MOUSE_MINOR	7
 #define PC110PAD_MINOR		9
 /*#define ADB_MOUSE_MINOR	10	FIXME OBSOLETE */
+#define CRYPTODEV_MINOR		70	/* /dev/crypto */
 #define WATCHDOG_MINOR		130	/* Watchdog timer     */
 #define TEMP_MINOR		131	/* Temperature Sensor */
 #define RTC_MINOR		135
diff --git a/include/linux/net.h b/include/linux/net.h
index b299230..f71ee03 100644
--- a/include/linux/net.h
+++ b/include/linux/net.h
@@ -97,6 +97,10 @@ enum sock_type {
 	SOCK_SEQPACKET	= 5,
 	SOCK_DCCP	= 6,
 	SOCK_PACKET	= 10,
+
+	// this is special flag used to inform the kernel.
+	// to create ZBC socket (applicable for UDP sockets)
+	SOCK_MAPPED	= (1<<30),
 };
 
 #define SOCK_MAX (SOCK_PACKET + 1)
diff --git a/include/linux/random.h b/include/linux/random.h
index ac621ce..0edd027 100644
--- a/include/linux/random.h
+++ b/include/linux/random.h
@@ -53,6 +53,10 @@ extern void add_input_randomness(unsigned int type, unsigned int code,
 				 unsigned int value);
 extern void add_interrupt_randomness(int irq, int irq_flags);
 
+extern void random_input_words(__u32 *buf, size_t wordcount, int ent_count);
+extern int random_input_wait(void);
+#define HAS_RANDOM_INPUT_WAIT 1
+
 extern void get_random_bytes(void *buf, int nbytes);
 extern void get_random_bytes_arch(void *buf, int nbytes);
 void generate_random_uuid(unsigned char uuid_out[16]);
diff --git a/include/linux/sched.h b/include/linux/sched.h
index a179dd0..fa18740 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1610,6 +1610,16 @@ struct task_struct {
 #ifdef CONFIG_DEBUG_PREEMPT
 	unsigned long preempt_disable_ip;
 #endif
+
+#if defined CONFIG_TRANSCEDE_MLOG
+	u32 last_start_time;
+#endif
+
+#ifdef CONFIG_AMP_STACK
+	void *amp_stack;
+	unsigned long amp_cpu;
+#endif	/* CONFIG_AMP_STACK */
+
 };
 
 #ifdef CONFIG_PREEMPT_RT_FULL
@@ -2304,8 +2314,13 @@ extern void mm_release(struct task_struct *, struct mm_struct *);
 /* Allocate a new mm structure and copy contents from tsk->mm */
 extern struct mm_struct *dup_mm(struct task_struct *tsk);
 
+#ifdef CONFIG_AMP_STACK
+extern int copy_thread(unsigned long, unsigned long, unsigned long,
+                       struct task_struct *, struct pt_regs *, unsigned long);
+#else  /* !CONFIG_AMP_STACK */
 extern int copy_thread(unsigned long, unsigned long, unsigned long,
-			struct task_struct *, struct pt_regs *);
+                       struct task_struct *, struct pt_regs *);
+#endif	/* CONFIG_AMP_STACK */
 extern void flush_thread(void);
 extern void exit_thread(void);
 
@@ -2324,7 +2339,11 @@ extern int disallow_signal(int);
 extern int do_execve(const char *,
 		     const char __user * const __user *,
 		     const char __user * const __user *, struct pt_regs *);
+#ifdef CONFIG_AMP_STACK
+extern long do_fork(unsigned long, unsigned long, struct pt_regs *, unsigned long, int __user *, int __user *, unsigned long);
+#else  /* !CONFIG_AMP_STACK */
 extern long do_fork(unsigned long, unsigned long, struct pt_regs *, unsigned long, int __user *, int __user *);
+#endif	/* CONFIG_AMP_STACK */
 struct task_struct *fork_idle(int);
 
 extern void set_task_comm(struct task_struct *tsk, char *from);
@@ -2478,7 +2497,11 @@ static inline void setup_thread_stack(struct task_struct *p, struct task_struct
 
 static inline unsigned long *end_of_stack(struct task_struct *p)
 {
+#ifdef CONFIG_AMP_STACK
+	return (unsigned long *)((unsigned long)(p->amp_stack) + sizeof(struct thread_info));
+#else  /* !CONFIG_AMP_STACK */
 	return (unsigned long *)(task_thread_info(p) + 1);
+#endif	/* CONFIG_AMP_STACK */
 }
 
 #endif
diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h
index 7c0b32e..985369e 100644
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@ -29,6 +29,7 @@
 #include <linux/rcupdate.h>
 #include <linux/dmaengine.h>
 #include <linux/hrtimer.h>
+#include <mach/syslib.h>
 
 /* Don't change this without changing skb_csum_unnecessary! */
 #define CHECKSUM_NONE 0
@@ -36,6 +37,11 @@
 #define CHECKSUM_COMPLETE 2
 #define CHECKSUM_PARTIAL 3
 
+#define SKB_FLAGS_DEF_INIT      (0)
+#define SKB_FLAGS_DATA_MAPPED   (1 << 0)   // mapped buffer (for TX)
+#define SKB_FLAGS_RX_PACKET     (1 << 1)   // rx buffer (used by recycler)
+#define SKB_FLAGS_L1_CACHED     (1 << 2)   // this skb is of L1-only cached pool
+
 #define SKB_DATA_ALIGN(X)	(((X) + (SMP_CACHE_BYTES - 1)) & \
 				 ~(SMP_CACHE_BYTES - 1))
 #define SKB_WITH_OVERHEAD(X)	\
@@ -204,6 +210,9 @@ struct skb_shared_info {
 	struct sk_buff	*frag_list;
 	struct skb_shared_hwtstamps hwtstamps;
 
+        /* the number of fragments (used for mapped sockets)*/
+	unsigned short  map_fr_num;
+
 	/*
 	 * Warning : all fields before dataref are cleared in __alloc_skb()
 	 */
@@ -267,7 +276,7 @@ typedef unsigned char *sk_buff_data_t;
 #define NET_SKBUFF_NF_DEFRAG_NEEDED 1
 #endif
 
-/** 
+/**
  *	struct sk_buff - socket buffer
  *	@next: Next buffer in list
  *	@prev: Previous buffer in list
@@ -296,7 +305,7 @@ typedef unsigned char *sk_buff_data_t;
  *	@priority: Packet queueing priority
  *	@users: User count - see {datagram,tcp}.c
  *	@protocol: Packet protocol from driver
- *	@truesize: Buffer size 
+ *	@truesize: Buffer size
  *	@head: Head of buffer
  *	@data: Data head pointer
  *	@tail: Tail pointer
@@ -415,6 +424,11 @@ struct sk_buff {
 
 	__u16			vlan_tci;
 
+#if defined(CONFIG_IPSEC_DMA_MAP_HACK_TX) || defined(CONFIG_IPSEC_DMA_MAP_HACK_RX)
+	/* Beginning of IPSec frame is DMA unmapped, the rest - already DMA mapped */
+        uint8_t			*dma_mapped_data; /* pointer to DMA mapped region */
+#endif
+
 	sk_buff_data_t		transport_header;
 	sk_buff_data_t		network_header;
 	sk_buff_data_t		mac_header;
@@ -425,6 +439,7 @@ struct sk_buff {
 				*data;
 	unsigned int		truesize;
 	atomic_t		users;
+	unsigned int        opt_flags;      // The bitmask of the SKB options, see: SKB_FLAGS_xxx
 };
 
 #ifdef __KERNEL__
@@ -450,7 +465,7 @@ struct sk_buff {
  */
 static inline struct dst_entry *skb_dst(const struct sk_buff *skb)
 {
-	/* If refdst was not refcounted, check we still are in a 
+	/* If refdst was not refcounted, check we still are in a
 	 * rcu_read_lock section
 	 */
 	WARN_ON((skb->_skb_refdst & SKB_DST_NOREF) &&
@@ -493,6 +508,14 @@ extern void consume_skb(struct sk_buff *skb);
 extern void	       __kfree_skb(struct sk_buff *skb);
 extern struct sk_buff *__alloc_skb(unsigned int size,
 				   gfp_t priority, int fclone, int node);
+
+extern struct sk_buff * alloc_skb_mapped(  unsigned int hdr_size,  // The headers(all layers) size in bytes
+                                        unsigned int data_size, // The data(payload) size in bytes
+                                        u8 * pMappedMem,        // The mapped in kernel space memory where packet payload is already located
+                                        gfp_t gfp_mask,         // The flags of memory allocation
+			                            int fclone,
+			                            int node);
+
 static inline struct sk_buff *alloc_skb(unsigned int size,
 					gfp_t priority)
 {
@@ -1557,6 +1580,8 @@ static inline struct sk_buff *__dev_alloc_skb(unsigned int length,
 
 extern struct sk_buff *dev_alloc_skb(unsigned int length);
 
+extern struct sk_buff *sd_dev_alloc_skb(unsigned int length);
+
 extern struct sk_buff *__netdev_alloc_skb(struct net_device *dev,
 		unsigned int length, gfp_t gfp_mask);
 
@@ -1691,7 +1716,7 @@ static inline int skb_cow_head(struct sk_buff *skb, unsigned int headroom)
  *	is untouched. Otherwise it is extended. Returns zero on
  *	success. The skb is freed on error.
  */
- 
+
 static inline int skb_padto(struct sk_buff *skb, unsigned int len)
 {
 	unsigned int size = skb->len;
@@ -2286,5 +2311,60 @@ static inline void skb_checksum_none_assert(struct sk_buff *skb)
 }
 
 bool skb_partial_csum_set(struct sk_buff *skb, u16 start, u16 off);
+
+extern int skb_is_zbc(struct sk_buff *skb);
+extern int skb_drop_zbc_frags(struct sk_buff *skb);
+extern int skb_zbc_correct_frag_size(struct sk_buff *skb, int index, int ds);
+
+
+typedef struct sk_buff *PSK_BUFF;
+
+#define RX_RECYCLER_GRANULE       128    /* 128 -MAX, must be power of 2. */
+
+typedef struct tagRXdesc
+{
+        U32 addr;          // Physical pointer to data to receive into, least significant bit is used bit
+        U32 status;        // Control word
+} GEMRxDMADesc, *PGEMRxDMADesc;
+
+typedef struct tagRX_LIST_CONTROL
+{
+	PGEMRxDMADesc pDmaCurr;
+	PGEMRxDMADesc pDmaStart;
+	PSK_BUFF      *pFDescCurr;
+	PSK_BUFF      *pFDescStart;
+	U32 Count;
+	U32 Reserved[3];  // padd to cacheline boundary
+} RX_LIST_CONTROL, *PRX_LIST_CONTROL;
+
+typedef struct tagRECYCLER_CACHE
+{
+	U32 *pCache;      // cache pointer
+	U32 Count;        // number of skb currently placed to cache
+} RECYCLER_CACHE, *PRECYCLER_CACHE;
+
+typedef struct tagTX_RECYCLER_CONTROL
+{
+	PFASTQUEUE skb_pool;        // per-cpu pool
+	RECYCLER_CACHE WCache;      // W-Cache buffer (pointer to skb buffer where we release to)
+	RECYCLER_CACHE RCache;      // R-Cache (cache that can be allocated)
+} TX_RECYCLER_CONTROL, *PTX_RECYCLER_CONTROL;
+
+typedef struct tagSKB_RECYCLER
+{
+	// Rx related
+	RX_LIST_CONTROL RxList[2];  // make sure this is page cacheline boundary aligned
+	PFASTQUEUE pq;      		 // should be SMP safe, otherwise need two queues
+	U32        nRedoze;
+	int        (*refill)(void *, struct tagSKB_RECYCLER *);
+	arch_spinlock_t lock;
+
+	// Tx related
+//	TX_RECYCLER_CONTROL TxCtrl[2];  // per cpu TX control
+} TSKB_RECYCLER, *PSKB_RECYCLER;
+
+PSKB_RECYCLER skb_recycler_init(U32 *pListOfIccRxBuffers);
+void skb_rx_recycler_doze(PSKB_RECYCLER pSkbRxRecycler);
+
 #endif	/* __KERNEL__ */
 #endif	/* _LINUX_SKBUFF_H */
diff --git a/include/linux/socket.h b/include/linux/socket.h
index 635c213..f0ca558 100644
--- a/include/linux/socket.h
+++ b/include/linux/socket.h
@@ -40,7 +40,7 @@ typedef unsigned short	sa_family_t;
 /*
  *	1003.1g requires sa_family_t and that sa_data is char.
  */
- 
+
 struct sockaddr {
 	sa_family_t	sa_family;	/* address family, AF_xxx	*/
 	char		sa_data[14];	/* 14 bytes of protocol address	*/
@@ -58,7 +58,7 @@ struct linger {
  *	system, not 4.3. Thus msg_accrights(len) are now missing. They
  *	belong in an obscure libc emulation or the bin.
  */
- 
+
 struct msghdr {
 	void	*	msg_name;	/* Socket name			*/
 	int		msg_namelen;	/* Length of name		*/
@@ -122,7 +122,7 @@ struct cmsghdr {
  *	inside range, given by msg->msg_controllen before using
  *	ancillary object DATA.				--ANK (980731)
  */
- 
+
 static inline struct cmsghdr * __cmsg_nxthdr(void *__ctl, __kernel_size_t __size,
 					       struct cmsghdr *__cmsg)
 {
@@ -239,10 +239,10 @@ struct ucred {
 /* Maximum queue length specifiable by listen.  */
 #define SOMAXCONN	128
 
-/* Flags we can use with send/ and recv. 
+/* Flags we can use with send/ and recv.
    Added those for 1003.1g not all are supported yet
  */
- 
+
 #define MSG_OOB		1
 #define MSG_PEEK	2
 #define MSG_DONTROUTE	4
@@ -317,10 +317,11 @@ extern void cred_to_ucred(struct pid *pid, const struct cred *cred, struct ucred
 extern int memcpy_fromiovec(unsigned char *kdata, struct iovec *iov, int len);
 extern int memcpy_fromiovecend(unsigned char *kdata, const struct iovec *iov,
 			       int offset, int len);
-extern int csum_partial_copy_fromiovecend(unsigned char *kdata, 
-					  struct iovec *iov, 
-					  int offset, 
+extern int csum_partial_copy_fromiovecend(unsigned char *kdata,
+					  struct iovec *iov,
+					  int offset,
 					  unsigned int len, __wsum *csump);
+extern int csum_partial_fromiovecend(struct iovec *iov, int offset, unsigned int len, __wsum *csump);
 
 extern int verify_iovec(struct msghdr *m, struct iovec *iov, struct sockaddr *address, int mode);
 extern int memcpy_toiovec(struct iovec *v, unsigned char *kdata, int len);
diff --git a/include/linux/spi/cdce62005.h b/include/linux/spi/cdce62005.h
new file mode 100644
index 0000000..7293983
--- /dev/null
+++ b/include/linux/spi/cdce62005.h
@@ -0,0 +1,137 @@
+/*
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#ifndef __LINUX_DEJITTER_CDCE62005_H
+#define __LINUX_DEJITTER_CDCE62005_H
+
+#include <linux/platform_device.h>
+#include <linux/mutex.h>
+#include <linux/interrupt.h>
+
+#define CDCE62005_DRIVER_NAME		"cdce62005"
+
+#define CDCE62005_DELAY_USEC		10  			// usec
+#define CDCE62005_CLOCK_FREQ		500000  	// 0.5MHz
+#define CDCE62005_BITWIDTH			8
+
+#if 1
+static const u32 DejitterDefaultConfig[8] =
+{
+     0x0EB80032 , // 0: LVDS 307.2 MHz, Output Div by 2,  PRI input divide by 1
+     0x0EB80032 , // 1: LVDS 307.2 MHz, Output Div by 2,  SEC input divide by 1
+     0x0EB84031 , // 2: LVDS 153.6 MHz, Output Div by 4,  PRI/SEC input divide by 6(with /2) -> 12.8 (same as AUX input)
+     0x0EB0C031 , // 3: LVDS  38.4 MHz, Output Div by 16, PRI/SEC input divide by 6(with /2) -> 12.8 (same as AUX input)
+     0x0EB84031 , // 4: LVDS 153.6 MHz, Div by 4
+     0x010040B0 , // 5: Input div by 2, AUX only, EEPROM clk settings, LVDS input buffer, Medium lock window, PLL lock on first detect
+     0x034BF0B2 , // 6: Feedback div by 96, Prescaler:3, VCO low range, Bypass divider bypass, 2mA charge pump, AUX enabled, source OUT 3, internal loop filter
+     0x0BD0037F , // 7: Loop filter C2:473.5pF, R2:20kOhm, C1:0pF, C3:0pF, R3:10kOhm; Long Delay 1&2
+};
+#else
+static const u32 DejitterDefaultConfig[8] =
+{
+    { 0x0EB80032 }, // 0: LVDS 307.2 MHz, Output Div by 2,  PRI input divide by 1
+    { 0x0EB80032 }, // 1: LVDS 307.2 MHz, Output Div by 2,  SEC input divide by 1
+    { 0x0EB84031 }, // 2: LVDS 153.6 MHz, Output Div by 4,  PRI/SEC input divide by 6(with /2) -> 12.8 (same as AUX input)
+    { 0x0EB0C031 }, // 3: LVDS  38.4 MHz, Output Div by 16, PRI/SEC input divide by 6(with /2) -> 12.8 (same as AUX input)
+    { 0x0EB84031 }, // 4: LVDS 153.6 MHz, Div by 4
+    { 0x010040B0 }, // 5: Input div by 2, AUX only, EEPROM clk settings, LVDS input buffer, Medium lock window, PLL lock on first detect
+    { 0x01010100 }, // 6: Feedback div by 96, Prescaler:3, VCO low range, Bypass divider bypass, 2mA charge pump, AUX enabled, source OUT 3, internal loop filter
+    { 0x0BD0037F }, // 7: Loop filter C2:473.5pF, R2:20kOhm, C1:0pF, C3:0pF, R3:10kOhm; Long Delay 1&2
+};
+#endif
+
+struct cdce62005;
+
+void cdce62005_lock(struct cdce62005 *cdce62005);
+void cdce62005_unlock(struct cdce62005 *cdce62005);
+
+int cdce62005_reg_read(struct cdce62005 *cdce62005, unsigned int offset, u32 *val);
+int cdce62005_reg_write(struct cdce62005 *cdce62005, unsigned int offset, u32 val);
+int cdce62005_reg_rmw(struct cdce62005 *cdce62005, unsigned int offset,
+		u32 mask, u32 val);
+
+int cdce62005_irq_request(struct cdce62005 *cdce62005, int irq,
+		irq_handler_t handler, const char *name, void *dev);
+int cdce62005_irq_request_nounmask(struct cdce62005 *cdce62005, int irq,
+		irq_handler_t handler, const char *name, void *dev);
+int cdce62005_irq_free(struct cdce62005 *cdce62005, int irq, void *dev);
+int cdce62005_ackirq(struct cdce62005 *cdce62005, int irq);
+
+int cdce62005_mask(struct cdce62005 *cdce62005, int irq);
+int cdce62005_unmask(struct cdce62005 *cdce62005, int irq);
+
+/* to be cleaned up */
+struct regulator_init_data;
+
+struct cdce62005_regulator_init_data {
+	int id;
+	struct regulator_init_data *init_data;
+};
+
+struct cdce62005_regulator_platform_data {
+	int num_regulators;
+	struct cdce62005_regulator_init_data *regulators;
+};
+
+struct cdce62005_platform_data {
+	unsigned int	gpio_ss_in;	/* slave sel > "=0 : spi-ip toggle SPI_LE automatically"; "n : GPIO_n toggle SPI_LE by CPU manually  "*/
+	unsigned int 	flags;
+};
+
+/**********************************/
+
+//struct cdce62005_device {
+struct cdce62005 {
+	struct spi_device *spidev;
+	struct mutex lock;
+	int irq;
+	int flags;
+	struct bin_attribute	bin;
+
+	struct spi_transfer		xfer[2];
+	struct spi_transfer		*last_xfer;
+
+	struct spi_message	msg;
+	struct spi_message	*last_msg;
+};
+
+#define CDCE62005_REG0000		0x0
+#define CDCE62005_REG0001		0x1
+#define CDCE62005_REG0002		0x2
+#define CDCE62005_REG0003		0x3
+#define CDCE62005_REG0004		0x4
+#define CDCE62005_REG0005		0x5
+#define CDCE62005_REG0006		0x6
+#define CDCE62005_REG0007		0x7
+#define CDCE62005_REG0008		0x8
+
+#define CDCE62005_MAXREGS 		0x8
+
+/* max of CDCE62005 reading size */
+#define CDCE62005_REGISTER_BLOCKSIZE	36		/* bytes of CDCE62005 Register Block */
+
+static inline int cdce62005_set_bits(struct cdce62005 *cdce62005, unsigned int offset,
+		u32 mask, u32 val)
+{
+	int ret;
+	cdce62005_lock(cdce62005);
+	ret = cdce62005_reg_rmw(cdce62005, offset, mask, val);
+	cdce62005_unlock(cdce62005);
+
+	return ret;
+}
+
+#endif /* __LINUX_DEJITTER_CDCE62005_H */
diff --git a/include/net/inet_sock.h b/include/net/inet_sock.h
index 14dd9c7..d806228 100644
--- a/include/net/inet_sock.h
+++ b/include/net/inet_sock.h
@@ -173,6 +173,62 @@ struct inet_sock {
 	struct inet_cork_full	cork;
 };
 
+#define INET_SAD_TTL		(1 << 0)
+#define INET_SAD_TOS		(1 << 1)
+
+struct inet_sock_ancillary_data {
+    __u16	mask;		// The mask of found and stored values, see: INET_SAD_xxx
+    __s16	ttl;
+    __s16	ttl_prev;
+    __u8	tos;
+    __u8	tos_prev;
+};
+
+static inline void inet_sad_init(struct inet_sock_ancillary_data * pdata)
+{
+    pdata->mask = 0;
+}
+
+static inline void inet_sad_apply_cfg(struct inet_sock * pinet, struct inet_sock_ancillary_data * pdata)
+{
+    if (pdata->mask & INET_SAD_TTL)
+    {
+	pdata->ttl_prev = pinet->uc_ttl;
+	pinet->uc_ttl = pdata->ttl;
+    }
+
+    if (pdata->mask & INET_SAD_TOS)
+    {
+	pdata->tos_prev = pinet->tos;
+	pinet->tos = pdata->tos;
+    }
+}
+
+static inline void inet_sad_restore_cfg(struct inet_sock * pinet, struct inet_sock_ancillary_data * pdata)
+{
+    if (pdata->mask & INET_SAD_TTL)
+    {
+	pinet->uc_ttl = pdata->ttl_prev;
+    }
+
+    if (pdata->mask & INET_SAD_TOS)
+    {
+	pinet->tos = pdata->tos_prev;
+    }
+}
+
+static inline void inet_sad_set_ttl(struct inet_sock_ancillary_data * pdata, __u8 val)
+{
+    pdata->mask |= INET_SAD_TTL;
+    pdata->ttl = (__s16)val;
+}
+
+static inline void inet_sad_set_tos(struct inet_sock_ancillary_data * pdata, __u8 val)
+{
+    pdata->mask |= INET_SAD_TOS;
+    pdata->tos = val;
+}
+
 #define IPCORK_OPT	1	/* ip-options has been held in ipcork.opt */
 #define IPCORK_ALLFRAG	2	/* always fragment (for ipv6 for now) */
 
diff --git a/include/net/ip.h b/include/net/ip.h
index 66dd491..1e5b074 100644
--- a/include/net/ip.h
+++ b/include/net/ip.h
@@ -114,6 +114,8 @@ extern int		ip_append_data(struct sock *sk, struct flowi4 *fl4,
 				struct rtable **rt,
 				unsigned int flags);
 extern int		ip_generic_getfrag(void *from, char *to, int offset, int len, int odd, struct sk_buff *skb);
+extern int      ip_generic_frag_csum(void *from, char *to, int offset, int len, int odd, struct sk_buff *skb);
+
 extern ssize_t		ip_append_page(struct sock *sk, struct flowi4 *fl4, struct page *page,
 				int offset, size_t size, int flags);
 extern struct sk_buff  *__ip_make_skb(struct sock *sk,
@@ -138,13 +140,13 @@ static inline struct sk_buff *ip_finish_skb(struct sock *sk, struct flowi4 *fl4)
 }
 
 /* datagram.c */
-extern int		ip4_datagram_connect(struct sock *sk, 
+extern int		ip4_datagram_connect(struct sock *sk,
 					     struct sockaddr *uaddr, int addr_len);
 
 /*
  *	Map a multicast IP onto multicast MAC for type Token Ring.
  *      This conforms to RFC1469 Option 2 Multicasting i.e.
- *      using a functional address to transmit / receive 
+ *      using a functional address to transmit / receive
  *      multicast packets.
  */
 
@@ -159,13 +161,13 @@ static inline void ip_tr_mc_map(__be32 addr, char *buf)
 }
 
 struct ip_reply_arg {
-	struct kvec iov[1];   
+	struct kvec iov[1];
 	int	    flags;
 	__wsum 	    csum;
 	int	    csumoffset; /* u16 offset of csum in iov[0].iov_base */
-				/* -1 if not needed */ 
+				/* -1 if not needed */
 	int	    bound_dev_if;
-}; 
+};
 
 #define IP_REPLY_ARG_NOSRCCHECK 1
 
@@ -411,13 +413,13 @@ int ip_frag_nqueues(struct net *net);
 /*
  *	Functions provided by ip_forward.c
  */
- 
+
 extern int ip_forward(struct sk_buff *skb);
- 
+
 /*
  *	Functions provided by ip_options.c
  */
- 
+
 extern void ip_options_build(struct sk_buff *skb, struct ip_options *opt,
 			     __be32 daddr, struct rtable *rt, int is_frag);
 extern int ip_options_echo(struct ip_options *dopt, struct sk_buff *skb);
@@ -440,6 +442,8 @@ extern int	ip_queue_rcv_skb(struct sock *sk, struct sk_buff *skb);
 extern void	ip_cmsg_recv(struct msghdr *msg, struct sk_buff *skb);
 extern int	ip_cmsg_send(struct net *net,
 			     struct msghdr *msg, struct ipcm_cookie *ipc);
+extern int	ip_cmsg_send_ex(struct net *net,
+			     struct msghdr *msg, struct ipcm_cookie *ipc, struct inet_sock_ancillary_data* psad);
 extern int	ip_setsockopt(struct sock *sk, int level, int optname, char __user *optval, unsigned int optlen);
 extern int	ip_getsockopt(struct sock *sk, int level, int optname, char __user *optval, int __user *optlen);
 extern int	compat_ip_setsockopt(struct sock *sk, int level,
@@ -449,7 +453,7 @@ extern int	compat_ip_getsockopt(struct sock *sk, int level,
 extern int	ip_ra_control(struct sock *sk, unsigned char on, void (*destructor)(struct sock *));
 
 extern int 	ip_recv_error(struct sock *sk, struct msghdr *msg, int len);
-extern void	ip_icmp_error(struct sock *sk, struct sk_buff *skb, int err, 
+extern void	ip_icmp_error(struct sock *sk, struct sk_buff *skb, int err,
 			      __be16 port, u32 info, u8 *payload);
 extern void	ip_local_error(struct sock *sk, int err, __be32 daddr, __be16 dport,
 			       u32 info);
diff --git a/include/net/sock.h b/include/net/sock.h
index b2deeab..181b5a1 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -345,10 +345,21 @@ struct sock {
 	void			(*sk_write_space)(struct sock *sk);
 	void			(*sk_error_report)(struct sock *sk);
   	int			(*sk_backlog_rcv)(struct sock *sk,
-						  struct sk_buff *skb);  
+						  struct sk_buff *skb);
 	void                    (*sk_destruct)(struct sock *sk);
+
+	u32         sk_mapped;      // 0 - by default, 1 - for UDP sockets where user data is already in the kernel space and copy operation is no
+	u8*         sk_mapped_ptr;  // The pointer to the application space payload accessed from the kernel memory
 };
 
+static inline int sk_is_mapped(struct sock * sk)
+{
+    if (sk == NULL)
+	return 0;
+
+    return (sk->sk_mapped != 0) && (sk->sk_protocol == IPPROTO_UDP);
+}
+
 /*
  * Hashed lists helper routines
  */
@@ -726,10 +737,10 @@ struct raw_hashinfo;
  * transport -> network interface is defined by struct inet_proto
  */
 struct proto {
-	void			(*close)(struct sock *sk, 
+	void			(*close)(struct sock *sk,
 					long timeout);
 	int			(*connect)(struct sock *sk,
-				        struct sockaddr *uaddr, 
+				        struct sockaddr *uaddr,
 					int addr_len);
 	int			(*disconnect)(struct sock *sk, int flags);
 
@@ -740,12 +751,12 @@ struct proto {
 	int			(*init)(struct sock *sk);
 	void			(*destroy)(struct sock *sk);
 	void			(*shutdown)(struct sock *sk, int how);
-	int			(*setsockopt)(struct sock *sk, int level, 
+	int			(*setsockopt)(struct sock *sk, int level,
 					int optname, char __user *optval,
 					unsigned int optlen);
-	int			(*getsockopt)(struct sock *sk, int level, 
-					int optname, char __user *optval, 
-					int __user *option);  	 
+	int			(*getsockopt)(struct sock *sk, int level,
+					int optname, char __user *optval,
+					int __user *option);
 #ifdef CONFIG_COMPAT
 	int			(*compat_setsockopt)(struct sock *sk,
 					int level,
@@ -762,14 +773,14 @@ struct proto {
 					   struct msghdr *msg, size_t len);
 	int			(*recvmsg)(struct kiocb *iocb, struct sock *sk,
 					   struct msghdr *msg,
-					size_t len, int noblock, int flags, 
+					size_t len, int noblock, int flags,
 					int *addr_len);
 	int			(*sendpage)(struct sock *sk, struct page *page,
 					int offset, size_t size, int flags);
-	int			(*bind)(struct sock *sk, 
+	int			(*bind)(struct sock *sk,
 					struct sockaddr *uaddr, int addr_len);
 
-	int			(*backlog_rcv) (struct sock *sk, 
+	int			(*backlog_rcv) (struct sock *sk,
 						struct sk_buff *skb);
 
 	/* Keeping track of sk's, looking them up, and port selection methods. */
@@ -1097,7 +1108,7 @@ extern int			sock_setsockopt(struct socket *sock, int level,
 						unsigned int optlen);
 
 extern int			sock_getsockopt(struct socket *sock, int level,
-						int op, char __user *optval, 
+						int op, char __user *optval,
 						int __user *optlen);
 extern struct sk_buff 		*sock_alloc_send_skb(struct sock *sk,
 						     unsigned long size,
@@ -1125,7 +1136,7 @@ static inline void sock_update_classid(struct sock *sk)
  * Functions to fill in entries in struct proto_ops when a protocol
  * does not implement a particular function.
  */
-extern int                      sock_no_bind(struct socket *, 
+extern int                      sock_no_bind(struct socket *,
 					     struct sockaddr *, int);
 extern int                      sock_no_connect(struct socket *,
 						struct sockaddr *, int, int);
@@ -1154,7 +1165,7 @@ extern int			sock_no_mmap(struct file *file,
 					     struct vm_area_struct *vma);
 extern ssize_t			sock_no_sendpage(struct socket *sock,
 						struct page *page,
-						int offset, size_t size, 
+						int offset, size_t size,
 						int flags);
 
 /*
@@ -1177,7 +1188,7 @@ extern void sk_common_release(struct sock *sk);
 /*
  *	Default socket callbacks and setup code
  */
- 
+
 /* Initialise core socket variables */
 extern void sock_init_data(struct socket *sock, struct sock *sk);
 
@@ -1611,7 +1622,7 @@ extern int sock_queue_err_skb(struct sock *sk, struct sk_buff *skb);
 /*
  *	Recover an error report and clear atomically
  */
- 
+
 static inline int sock_error(struct sock *sk)
 {
 	int err;
@@ -1627,7 +1638,7 @@ static inline unsigned long sock_wspace(struct sock *sk)
 
 	if (!(sk->sk_shutdown & SEND_SHUTDOWN)) {
 		amt = sk->sk_sndbuf - atomic_read(&sk->sk_wmem_alloc);
-		if (amt < 0) 
+		if (amt < 0)
 			amt = 0;
 	}
 	return amt;
@@ -1671,7 +1682,7 @@ static inline struct page *sk_stream_alloc_page(struct sock *sk)
 /*
  *	Default write policy as shown to user space via poll/select/SIGIO
  */
-static inline int sock_writeable(const struct sock *sk) 
+static inline int sock_writeable(const struct sock *sk)
 {
 	return atomic_read(&sk->sk_wmem_alloc) < (sk->sk_sndbuf >> 1);
 }
@@ -1830,8 +1841,8 @@ extern void sock_enable_timestamp(struct sock *sk, int flag);
 extern int sock_get_timestamp(struct sock *, struct timeval __user *);
 extern int sock_get_timestampns(struct sock *, struct timespec __user *);
 
-/* 
- *	Enable debug/info messages 
+/*
+ *	Enable debug/info messages
  */
 extern int net_msg_warn;
 #define NETDEBUG(fmt, args...) \
diff --git a/kernel/fork.c b/kernel/fork.c
index 086388f..e3e3c1e 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -79,6 +79,10 @@
 
 #include <trace/events/sched.h>
 
+#ifdef CONFIG_AMP_STACK
+#include <mach/mmu_protect.h>
+#endif	/* CONFIG_AMP_STACK */
+
 /*
  * Protected counters by write_lock_irq(&tasklist_lock)
  */
@@ -262,10 +266,17 @@ int __attribute__((weak)) arch_dup_task_struct(struct task_struct *dst,
 	return 0;
 }
 
+#ifdef CONFIG_AMP_STACK
+static struct task_struct *dup_task_struct(struct task_struct *orig, unsigned long cpu)
+#else  /* !CONFIG_AMP_STACK */
 static struct task_struct *dup_task_struct(struct task_struct *orig)
+#endif	/* CONFIG_AMP_STACK */
 {
 	struct task_struct *tsk;
 	struct thread_info *ti;
+#ifdef CONFIG_AMP_STACK
+	struct thread_info *ti_amp = NULL;
+#endif	/* CONFIG_AMP_STACK */
 	unsigned long *stackend;
 	int node = tsk_fork_get_node(orig);
 	int err;
@@ -282,17 +293,55 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 		return NULL;
 	}
 
- 	err = arch_dup_task_struct(tsk, orig);
+#ifdef CONFIG_AMP_STACK
+	if (cpu != MMU_CPU_SMP) {
+		ti_amp = amp_stack_alloc();
+
+		if (!ti_amp) {
+			free_task_struct(tsk);
+			return NULL;
+		}
+	}
+#endif	/* CONFIG_AMP_STACK */
+
+	err = arch_dup_task_struct(tsk, orig);
 	if (err)
 		goto out;
 
 	tsk->stack = ti;
 
+#ifdef CONFIG_AMP_STACK
+	if (cpu != MMU_CPU_SMP) {
+		MMU_DEBUG("task: 0x%08lx", (unsigned long)tsk);
+		MMU_DEBUG("stack: 0x%08lx", (unsigned long)ti);
+		MMU_DEBUG("stack AMP: 0x%08lx", (unsigned long)ti_amp);
+		tsk->amp_stack = ti_amp;
+	} else {
+		tsk->amp_stack = ti;
+	}
+#endif	/* CONFIG_AMP_STACK */
+
 	err = prop_local_init_single(&tsk->dirties);
 	if (err)
 		goto out;
 
 	setup_thread_stack(tsk, orig);
+
+#ifdef CONFIG_AMP_STACK
+	ti->stack_smp = ti;
+
+	if (cpu != MMU_CPU_SMP) {
+		struct smp_value smp;
+
+		smp.dst = (unsigned long *)(&ti_amp->stack_smp);
+		smp.src = (unsigned long *)(&ti);
+		smp.size = sizeof(ti);
+
+		smp_call_function_single(0, smp_set_value, &smp, 1);
+		smp_call_function_single(1, smp_set_value, &smp, 1);
+	}
+#endif	/* CONFIG_AMP_STACK */
+
 	clear_user_return_notifier(tsk);
 	clear_tsk_need_resched(tsk);
 	stackend = end_of_stack(tsk);
@@ -1081,12 +1130,17 @@ static void posix_cpu_timers_init(struct task_struct *tsk)
  * flags). The actual kick-off is left to the caller.
  */
 static struct task_struct *copy_process(unsigned long clone_flags,
-					unsigned long stack_start,
-					struct pt_regs *regs,
-					unsigned long stack_size,
-					int __user *child_tidptr,
-					struct pid *pid,
-					int trace)
+                                        unsigned long stack_start,
+                                        struct pt_regs *regs,
+                                        unsigned long stack_size,
+                                        int __user *child_tidptr,
+                                        struct pid *pid,
+#ifdef CONFIG_AMP_STACK
+                                        int trace,
+                                        unsigned long amp_cpu)
+#else  /* CONFIG_AMP_STACK */
+	                                    int trace)
+#endif	/* CONFIG_AMP_STACK */
 {
 	int retval;
 	struct task_struct *p;
@@ -1125,10 +1179,22 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		goto fork_out;
 
 	retval = -ENOMEM;
+#ifdef CONFIG_AMP_STACK
+	p = dup_task_struct(current, amp_cpu);
+#else  /* !CONFIG_AMP_STACK */
 	p = dup_task_struct(current);
+#endif	/* CONFIG_AMP_STACK */
 	if (!p)
 		goto fork_out;
 
+#ifdef CONFIG_AMP_STACK
+	if (amp_cpu != MMU_CPU_SMP) {
+		MMU_DEBUG("bind to CPU%lu", amp_cpu);
+		do_set_cpus_allowed(p, cpumask_of(amp_cpu));
+		p->flags |= PF_THREAD_BOUND;
+	}
+#endif	/* CONFIG_AMP_STACK */
+
 	ftrace_graph_init_task(p);
 
 	rt_mutex_init_task(p);
@@ -1272,7 +1338,11 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		goto bad_fork_cleanup_mm;
 	if ((retval = copy_io(clone_flags, p)))
 		goto bad_fork_cleanup_namespaces;
+#ifdef CONFIG_AMP_STACK
+	retval = copy_thread(clone_flags, stack_start, stack_size, p, regs, amp_cpu);
+#else  /* !CONFIG_AMP_STACK */
 	retval = copy_thread(clone_flags, stack_start, stack_size, p, regs);
+#endif	/* CONFIG_AMP_STACK */
 	if (retval)
 		goto bad_fork_cleanup_io;
 
@@ -1478,8 +1548,13 @@ struct task_struct * __cpuinit fork_idle(int cpu)
 	struct task_struct *task;
 	struct pt_regs regs;
 
+#ifdef CONFIG_AMP_STACK
+	task = copy_process(CLONE_VM, 0, idle_regs(&regs), 0, NULL,
+	                    &init_struct_pid, 0, MMU_CPU_SMP);
+#else  /* !CONFIG_AMP_STACK */
 	task = copy_process(CLONE_VM, 0, idle_regs(&regs), 0, NULL,
-			    &init_struct_pid, 0);
+	                    &init_struct_pid, 0);
+#endif  /* CONFIG_AMP_STACK */
 	if (!IS_ERR(task)) {
 		init_idle_pids(task->pids);
 		init_idle(task, cpu);
@@ -1495,11 +1570,16 @@ struct task_struct * __cpuinit fork_idle(int cpu)
  * it and waits for it to finish using the VM if required.
  */
 long do_fork(unsigned long clone_flags,
-	      unsigned long stack_start,
-	      struct pt_regs *regs,
-	      unsigned long stack_size,
-	      int __user *parent_tidptr,
-	      int __user *child_tidptr)
+             unsigned long stack_start,
+             struct pt_regs *regs,
+             unsigned long stack_size,
+             int __user *parent_tidptr,
+#ifdef CONFIG_AMP_STACK
+             int __user *child_tidptr,
+             unsigned long amp_cpu)
+#else  /* !CONFIG_AMP_STACK */
+	         int __user *child_tidptr)
+#endif  /* CONFIG_AMP_STACK */
 {
 	struct task_struct *p;
 	int trace = 0;
@@ -1526,8 +1606,14 @@ long do_fork(unsigned long clone_flags,
 	if (likely(user_mode(regs)))
 		trace = tracehook_prepare_clone(clone_flags);
 
+#ifdef CONFIG_AMP_STACK
 	p = copy_process(clone_flags, stack_start, regs, stack_size,
-			 child_tidptr, NULL, trace);
+	                 child_tidptr, NULL, trace, amp_cpu);
+#else  /* !CONFIG_AMP_STACK */
+	p = copy_process(clone_flags, stack_start, regs, stack_size,
+	                 child_tidptr, NULL, trace);
+#endif  /* CONFIG_AMP_STACK */
+
 	/*
 	 * Do this prior waking up the new thread - the thread pointer
 	 * might get invalid after that point, if the thread exits quickly.
diff --git a/kernel/irq/handle.c b/kernel/irq/handle.c
index a768885..af0ce21 100644
--- a/kernel/irq/handle.c
+++ b/kernel/irq/handle.c
@@ -20,6 +20,11 @@
 
 #include "internals.h"
 
+#if defined(CONFIG_TRANSCEDE_MLOG)
+#include <mach/mlog.h>
+#include <mach/tcb.h>
+#endif
+
 /**
  * handle_bad_irq - handle spurious and unhandled irqs
  * @irq:       the interrupt number
@@ -121,17 +126,31 @@ handle_irq_event_percpu(struct irq_desc *desc, struct irqaction *action)
 
 	do {
 		irqreturn_t res;
+#if defined(CONFIG_TRANSCEDE_MLOG)
+		unsigned long tt, t = get_tick();
+		unsigned int mlogVars[10], mlogVarsCnt = 0;
+#endif
 
 		trace_irq_handler_entry(irq, action);
 		res = action->handler(irq, action->dev_id);
 		trace_irq_handler_exit(irq, action, res);
 
+#if defined(CONFIG_TRANSCEDE_MLOG)
+		tt = get_tick();
+#endif
+
 		if (WARN_ONCE(!irqs_disabled(),"irq %u handler %pF enabled interrupts\n",
 			      irq, action->handler))
 			local_irq_disable();
 
 		switch (res) {
 		case IRQ_WAKE_THREAD:
+#if defined(CONFIG_TRANSCEDE_MLOG)
+			mlogVars[mlogVarsCnt++] = MLOG_VAR_IRQ_0;
+			mlogVars[mlogVarsCnt++] = action->thread_fn;
+			mlogVars[mlogVarsCnt++] = action->thread->pid;
+#endif
+
 			/*
 			 * Catch drivers which return WAKE_THREAD but
 			 * did not set up a thread function
@@ -145,6 +164,10 @@ handle_irq_event_percpu(struct irq_desc *desc, struct irqaction *action)
 
 			/* Fall through to add to randomness */
 		case IRQ_HANDLED:
+#if defined(CONFIG_TRANSCEDE_MLOG)
+			mlogVars[mlogVarsCnt++] = MLOG_VAR_IRQ_1;
+#endif
+
 			flags |= action->flags;
 			break;
 
@@ -152,6 +175,14 @@ handle_irq_event_percpu(struct irq_desc *desc, struct irqaction *action)
 			break;
 		}
 
+#if defined(CONFIG_TRANSCEDE_MLOG)
+		if (MLogIsEnabled()) {
+			mlogVars[mlogVarsCnt++] = irq;
+			MLogAddVariables(mlogVarsCnt, mlogVars, tt);
+			MLogTask(TASK_ID_IRQ, RESOURCE_LARM, t, tt);
+		}
+#endif
+
 		retval |= res;
 		action = action->next;
 	} while (action);
diff --git a/kernel/kthread.c b/kernel/kthread.c
index 4ba7ccc..53e39e0 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -18,6 +18,10 @@
 #include <linux/freezer.h>
 #include <trace/events/sched.h>
 
+#ifdef CONFIG_AMP_STACK
+#include <mach/mmu_protect.h>
+#endif	/* CONFIG_AMP_STACK */
+
 static DEFINE_SPINLOCK(kthread_create_lock);
 static LIST_HEAD(kthread_create_list);
 struct task_struct *kthreadd_task;
@@ -29,6 +33,10 @@ struct kthread_create_info
 	void *data;
 	int node;
 
+#ifdef CONFIG_AMP_STACK
+	unsigned long amp_cpu;
+#endif	/* CONFIG_AMP_STACK */
+
 	/* Result passed back to kthread_create() from kthreadd. */
 	struct task_struct *result;
 	struct completion done;
@@ -116,8 +124,18 @@ static void create_kthread(struct kthread_create_info *create)
 #ifdef CONFIG_NUMA
 	current->pref_node_fork = create->node;
 #endif
+
+#ifdef CONFIG_AMP_STACK
 	/* We want our own signal handler (we take no signals by default). */
+	if (create->amp_cpu != MMU_CPU_SMP) {
+		pid = kernel_amp_thread(kthread, create, CLONE_FS | CLONE_FILES | SIGCHLD, create->amp_cpu);
+	} else {
+		pid = kernel_thread(kthread, create, CLONE_FS | CLONE_FILES | SIGCHLD);
+	}
+#else	/* !CONFIG_AMP_STACK */
 	pid = kernel_thread(kthread, create, CLONE_FS | CLONE_FILES | SIGCHLD);
+#endif	/* CONFIG_AMP_STACK */
+
 	if (pid < 0) {
 		create->result = ERR_PTR(pid);
 		complete(&create->done);
@@ -157,6 +175,9 @@ struct task_struct *kthread_create_on_node(int (*threadfn)(void *data),
 	create.threadfn = threadfn;
 	create.data = data;
 	create.node = node;
+#ifdef CONFIG_AMP_STACK
+	create.amp_cpu = MMU_CPU_SMP;
+#endif	/* CONFIG_AMP_STACK */
 	init_completion(&create.done);
 
 	spin_lock(&kthread_create_lock);
@@ -185,6 +206,48 @@ struct task_struct *kthread_create_on_node(int (*threadfn)(void *data),
 }
 EXPORT_SYMBOL(kthread_create_on_node);
 
+#ifdef CONFIG_AMP_STACK
+struct task_struct *kthread_create_amp_on_node(int cpu, int (*threadfn)(void *data),
+                                               void *data,
+                                               int node,
+                                               const char namefmt[],
+                                               ...)
+{
+	struct kthread_create_info create;
+
+	create.threadfn = threadfn;
+	create.data = data;
+	create.node = node;
+	create.amp_cpu = cpu;
+	init_completion(&create.done);
+
+	spin_lock(&kthread_create_lock);
+	list_add_tail(&create.list, &kthread_create_list);
+	spin_unlock(&kthread_create_lock);
+
+	wake_up_process(kthreadd_task);
+	wait_for_completion(&create.done);
+
+	if (!IS_ERR(create.result)) {
+		static const struct sched_param param = { .sched_priority = 0 };
+		va_list args;
+
+		va_start(args, namefmt);
+		vsnprintf(create.result->comm, sizeof(create.result->comm),
+			  namefmt, args);
+		va_end(args);
+		/*
+		 * root may have changed our (kthreadd's) priority or CPU mask.
+		 * The kernel thread should not inherit these properties.
+		 */
+		sched_setscheduler_nocheck(create.result, SCHED_NORMAL, &param);
+		set_cpus_allowed_ptr(create.result, cpumask_of(cpu));
+	}
+	return create.result;
+}
+EXPORT_SYMBOL(kthread_create_amp_on_node);
+#endif	/* CONFIG_AMP_STACK */
+
 /**
  * kthread_bind - bind a just-created kthread to a cpu.
  * @p: thread created by kthread_create().
diff --git a/kernel/pid.c b/kernel/pid.c
index 57a8346..5c93fca 100644
--- a/kernel/pid.c
+++ b/kernel/pid.c
@@ -427,6 +427,7 @@ struct task_struct *find_task_by_vpid(pid_t vnr)
 {
 	return find_task_by_pid_ns(vnr, current->nsproxy->pid_ns);
 }
+EXPORT_SYMBOL(find_task_by_vpid);
 
 struct pid *get_task_pid(struct task_struct *task, enum pid_type type)
 {
diff --git a/kernel/rtmutex.c b/kernel/rtmutex.c
index d58db99..8684a4d 100644
--- a/kernel/rtmutex.c
+++ b/kernel/rtmutex.c
@@ -21,6 +21,10 @@
 #include <linux/sched.h>
 #include <linux/timer.h>
 
+#ifdef CONFIG_AMP_STACK
+#include <mach/mmu_protect.h>
+#endif	/* CONFIG_AMP_STACK */
+
 #include "rtmutex_common.h"
 
 /*
@@ -706,6 +710,93 @@ static int adaptive_wait(struct rt_mutex *lock,
  * We store the current state under p->pi_lock in p->saved_state and
  * the try_to_wake_up() code handles this accordingly.
  */
+#ifdef CONFIG_AMP_STACK
+static void  noinline __sched rt_spin_lock_slowlock(struct rt_mutex *lock)
+{
+	struct task_struct *lock_owner, *self = current;
+	struct rt_mutex_waiter *top_waiter;
+	struct rt_mutex_waiter waiter_smp;
+	struct rt_mutex_waiter *waiter = &waiter_smp;
+	int ret = 0;
+
+	if (!is_mmu_shared((unsigned long)waiter)) {
+		waiter = (struct rt_mutex_waiter *)&(current_thread_info()->waiter_spinlock);
+	}
+
+	rt_mutex_init_waiter(waiter, true);
+
+	raw_spin_lock(&lock->wait_lock);
+	init_lists(lock);
+
+	if (__try_to_take_rt_mutex(lock, self, NULL, STEAL_LATERAL)) {
+		raw_spin_unlock(&lock->wait_lock);
+		return;
+	}
+
+	BUG_ON(rt_mutex_owner(lock) == self);
+
+	/*
+	 * We save whatever state the task is in and we'll restore it
+	 * after acquiring the lock taking real wakeups into account
+	 * as well. We are serialized via pi_lock against wakeups. See
+	 * try_to_wake_up().
+	 */
+	pi_lock(&self->pi_lock);
+	self->saved_state = self->state;
+	__set_current_state(TASK_UNINTERRUPTIBLE);
+	pi_unlock(&self->pi_lock);
+
+	ret = task_blocks_on_rt_mutex(lock, waiter, self, 0);
+	BUG_ON(ret);
+
+	for (;;) {
+		/* Try to acquire the lock again. */
+		if (__try_to_take_rt_mutex(lock, self, waiter, STEAL_LATERAL))
+			break;
+
+		top_waiter = rt_mutex_top_waiter(lock);
+		lock_owner = rt_mutex_owner(lock);
+
+		raw_spin_unlock(&lock->wait_lock);
+
+		debug_rt_mutex_print_deadlock(waiter);
+
+		if (top_waiter != waiter || adaptive_wait(lock, lock_owner))
+			schedule_rt_mutex(lock);
+
+		raw_spin_lock(&lock->wait_lock);
+
+		pi_lock(&self->pi_lock);
+		__set_current_state(TASK_UNINTERRUPTIBLE);
+		pi_unlock(&self->pi_lock);
+	}
+
+	/*
+	 * Restore the task state to current->saved_state. We set it
+	 * to the original state above and the try_to_wake_up() code
+	 * has possibly updated it when a real (non-rtmutex) wakeup
+	 * happened while we were blocked. Clear saved_state so
+	 * try_to_wakeup() does not get confused.
+	 */
+	pi_lock(&self->pi_lock);
+	__set_current_state(self->saved_state);
+	self->saved_state = TASK_RUNNING;
+	pi_unlock(&self->pi_lock);
+
+	/*
+	 * try_to_take_rt_mutex() sets the waiter bit
+	 * unconditionally. We might have to fix that up:
+	 */
+	fixup_rt_mutex_waiters(lock);
+
+	BUG_ON(rt_mutex_has_waiters(lock) && waiter == rt_mutex_top_waiter(lock));
+	BUG_ON(!plist_node_empty(&waiter->list_entry));
+
+	raw_spin_unlock(&lock->wait_lock);
+
+	debug_rt_mutex_free_waiter(waiter);
+}
+#else  /* !CONFIG_AMP_STACK */
 static void  noinline __sched rt_spin_lock_slowlock(struct rt_mutex *lock)
 {
 	struct task_struct *lock_owner, *self = current;
@@ -785,6 +876,7 @@ static void  noinline __sched rt_spin_lock_slowlock(struct rt_mutex *lock)
 
 	debug_rt_mutex_free_waiter(&waiter);
 }
+#endif	/* CONFIG_AMP_STACK */
 
 /*
  * Slow path to release a rt_mutex spin_lock style
@@ -981,6 +1073,7 @@ __rt_mutex_slowlock(struct rt_mutex *lock, int state,
 /*
  * Slow path lock function:
  */
+#ifndef CONFIG_AMP_STACK
 static int __sched
 rt_mutex_slowlock(struct rt_mutex *lock, int state,
 		  struct hrtimer_sleeper *timeout,
@@ -1036,6 +1129,69 @@ rt_mutex_slowlock(struct rt_mutex *lock, int state,
 	return ret;
 }
 
+#else  /* CONFIG_AMP_STACK */
+
+static int __sched
+rt_mutex_slowlock(struct rt_mutex *lock, int state,
+		  struct hrtimer_sleeper *timeout,
+		  int detect_deadlock)
+{
+	struct rt_mutex_waiter waiter_smp;
+	struct rt_mutex_waiter *waiter = &waiter_smp;
+	int ret = 0;
+
+	if (!is_mmu_shared((unsigned long)waiter)) {
+		waiter = (struct rt_mutex_waiter *)&(current_thread_info()->waiter_mutex);
+	}
+
+	rt_mutex_init_waiter(waiter, false);
+
+	raw_spin_lock(&lock->wait_lock);
+	init_lists(lock);
+
+	/* Try to acquire the lock again: */
+	if (try_to_take_rt_mutex(lock, current, NULL)) {
+		raw_spin_unlock(&lock->wait_lock);
+		return 0;
+	}
+
+	set_current_state(state);
+
+	/* Setup the timer, when timeout != NULL */
+	if (unlikely(timeout)) {
+		hrtimer_start_expires(&timeout->timer, HRTIMER_MODE_ABS);
+		if (!hrtimer_active(&timeout->timer))
+			timeout->task = NULL;
+	}
+
+	ret = task_blocks_on_rt_mutex(lock, waiter, current, detect_deadlock);
+
+	if (likely(!ret))
+		ret = __rt_mutex_slowlock(lock, state, timeout, waiter);
+
+	set_current_state(TASK_RUNNING);
+
+	if (unlikely(ret))
+		remove_waiter(lock, waiter);
+
+	/*
+	 * try_to_take_rt_mutex() sets the waiter bit
+	 * unconditionally. We might have to fix that up.
+	 */
+	fixup_rt_mutex_waiters(lock);
+
+	raw_spin_unlock(&lock->wait_lock);
+
+	/* Remove pending timer: */
+	if (unlikely(timeout))
+		hrtimer_cancel(&timeout->timer);
+
+	debug_rt_mutex_free_waiter(waiter);
+
+	return ret;
+}
+#endif	/* CONFIG_AMP_STACK */
+
 /*
  * Slow path try-lock function:
  */
diff --git a/kernel/sched.c b/kernel/sched.c
index 2cf4c4b..6f0c6b6 100644
--- a/kernel/sched.c
+++ b/kernel/sched.c
@@ -84,6 +84,21 @@
 #define CREATE_TRACE_POINTS
 #include <trace/events/sched.h>
 
+#ifdef CONFIG_TRANSCEDE_MIPS_MONITOR
+#include <mach/hardware.h>
+#include <mach/mips_monitor.h>
+#endif 
+
+#if defined(CONFIG_TRANSCEDE_MLOG_KERNEL_SCHEDULER)
+#include <mach/hardware.h>
+#include <mach/mlog.h>
+#include <mach/tcb.h>
+#endif
+
+#if defined(CONFIG_TRANSCEDE_MLOG_SEPARATE_IDLE)
+u32	mlog_idle_start[NR_CPUS] = {0};
+#endif
+
 /*
  * Convert user-nice values [ -20 ... 0 ... 19 ]
  * to static priority [ MAX_RT_PRIO..MAX_PRIO-1 ],
@@ -4408,6 +4423,39 @@ need_resched:
 		rq->curr = next;
 		++*switch_count;
 
+#ifdef CONFIG_TRANSCEDE_MIPS_MONITOR
+		if (prev) {
+			cpu = smp_processor_id();
+			if (mips_monitor_idle_start(cpu) && prev->pid == 0) {
+				mips_monitor_idle_update(mips_monitor_idle_start(cpu), *((volatile unsigned int*)TIMER0_CURR_COUNT), cpu);
+				mips_monitor_idle_start_update(cpu, 0);
+			}        
+		}
+#endif 
+
+#if defined(CONFIG_TRANSCEDE_MLOG_KERNEL_SCHEDULER)
+{
+		if (next) {
+			next->last_start_time = *((volatile unsigned int*)TIMER0_CURR_COUNT);
+		}
+
+		if (prev) {
+#ifdef CONFIG_TRANSCEDE_MLOG_SEPARATE_IDLE
+			cpu = smp_processor_id();
+			if (mlog_idle_start[cpu] && prev->pid == 0) {
+				MLogTask(-1 /*idle task id*/, 1, mlog_idle_start[cpu], *((volatile unsigned int*)TIMER0_CURR_COUNT));
+				MLogTask(prev->pid, 1, prev->last_start_time, mlog_idle_start[cpu]);
+				mlog_idle_start[cpu] = 0;
+			} else {
+#endif
+
+				MLogTask(prev->pid, 1, prev->last_start_time, *((volatile unsigned int*)TIMER0_CURR_COUNT));
+#ifdef CONFIG_TRANSCEDE_MLOG_SEPARATE_IDLE
+			}
+#endif
+		}
+}
+#endif
 		context_switch(rq, prev, next); /* unlocks the rq */
 		/*
 		 * The context switch have flipped the stack from under us
@@ -4625,7 +4673,7 @@ static void __wake_up_common(wait_queue_head_t *q, unsigned int mode,
 		unsigned flags = curr->flags;
 
 		if (curr->func(curr, mode, wake_flags, key) &&
-				(flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)
+		    (flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)
 			break;
 	}
 }
diff --git a/kernel/smp.c b/kernel/smp.c
index fb67dfa..8145dbd 100644
--- a/kernel/smp.c
+++ b/kernel/smp.c
@@ -310,6 +310,7 @@ int smp_call_function_single(int cpu, smp_call_func_t func, void *info,
 	 */
 	this_cpu = get_cpu();
 
+#ifndef CONFIG_PREEMPT_RT_FULL
 	/*
 	 * Can deadlock when called with interrupts disabled.
 	 * We allow cpu's that are not yet online though, as no one else can
@@ -318,6 +319,7 @@ int smp_call_function_single(int cpu, smp_call_func_t func, void *info,
 	 */
 	WARN_ON_ONCE(cpu_online(this_cpu) && irqs_disabled()
 		     && !oops_in_progress);
+#endif
 
 	if (cpu == this_cpu) {
 		local_irq_save(flags);
@@ -449,6 +451,7 @@ void smp_call_function_many(const struct cpumask *mask,
 	unsigned long flags;
 	int refs, cpu, next_cpu, this_cpu = smp_processor_id();
 
+#ifndef CONFIG_PREEMPT_RT_FULL
 	/*
 	 * Can deadlock when called with interrupts disabled.
 	 * We allow cpu's that are not yet online though, as no one else can
@@ -457,6 +460,7 @@ void smp_call_function_many(const struct cpumask *mask,
 	 */
 	WARN_ON_ONCE(cpu_online(this_cpu) && irqs_disabled()
 		     && !oops_in_progress && !early_boot_irqs_disabled);
+#endif
 
 	/* Try to fastpath.  So, what's a CPU they want? Ignoring this one. */
 	cpu = cpumask_first_and(mask, cpu_online_mask);
diff --git a/kernel/wait.c b/kernel/wait.c
index f45ea8d..cc43d6c 100644
--- a/kernel/wait.c
+++ b/kernel/wait.c
@@ -68,13 +68,17 @@ void
 prepare_to_wait(wait_queue_head_t *q, wait_queue_t *wait, int state)
 {
 	unsigned long flags;
+	unsigned long imask;
 
 	wait->flags &= ~WQ_FLAG_EXCLUSIVE;
+
+	local_irq_save(imask);
 	spin_lock_irqsave(&q->lock, flags);
 	if (list_empty(&wait->task_list))
 		__add_wait_queue(q, wait);
 	set_current_state(state);
 	spin_unlock_irqrestore(&q->lock, flags);
+	local_irq_restore(imask);
 }
 EXPORT_SYMBOL(prepare_to_wait);
 
diff --git a/net/core/filter.c b/net/core/filter.c
index 36f975f..5fd2d78 100644
--- a/net/core/filter.c
+++ b/net/core/filter.c
@@ -40,6 +40,10 @@
 #include <linux/reciprocal_div.h>
 #include <linux/ratelimit.h>
 
+#ifdef K
+#undef K
+#endif
+
 /* No hurry in this branch */
 static void *__load_pointer(const struct sk_buff *skb, int k, unsigned int size)
 {
diff --git a/net/core/iovec.c b/net/core/iovec.c
index c40f27e..5b15da7 100644
--- a/net/core/iovec.c
+++ b/net/core/iovec.c
@@ -262,3 +262,57 @@ out_fault:
 	goto out;
 }
 EXPORT_SYMBOL(csum_partial_copy_fromiovecend);
+
+
+/*
+ *	And now for the all-in-one: copy and checksum from a user iovec
+ *	directly to a datagram
+ *	Calls to csum_partial but the last must be in 32 bit chunks
+ *
+ *	ip_build_xmit must ensure that when fragmenting only the last
+ *	call to this function will be unaligned also.
+ */
+int csum_partial_fromiovecend(struct iovec *iov, int offset, unsigned int len, __wsum *csump)
+{
+    __wsum csum = *csump;
+    int err = 0;
+    u8 __user *base;
+    int copy = 0;
+    int odd_flag = 0;
+    u8 tmp=0;
+
+    /* Skip over the finished iovecs */
+    while (offset >= iov->iov_len) {
+	offset -= iov->iov_len;
+	iov++;
+    }
+
+    while (len > 0)
+    {
+        copy = min_t(unsigned int, len, (iov->iov_len - odd_flag) - offset);
+        base = (u8*)iov->iov_base + offset + odd_flag;
+        offset = 0;
+        odd_flag = 0;
+
+        if ((copy&1) && len > copy)
+        {
+    	    tmp = base[copy];
+    	    base[copy] = ((u8*)iov[1].iov_base)[0];
+    	    odd_flag = 1;
+        }
+
+	csum = csum_partial(base, copy+odd_flag, csum);
+
+	if (odd_flag != 0)
+	{
+	    base[copy] = tmp;
+	}
+
+        len -= (copy+odd_flag);
+        iov++;
+    }
+
+    *csump = csum;
+    return err;
+}
+EXPORT_SYMBOL(csum_partial_fromiovecend);
diff --git a/net/core/neighbour.c b/net/core/neighbour.c
index eb8857a..dc8d04a 100644
--- a/net/core/neighbour.c
+++ b/net/core/neighbour.c
@@ -871,6 +871,8 @@ static void neigh_invalidate(struct neighbour *neigh)
 
 static void neigh_timer_handler(unsigned long arg)
 {
+	extern void* (*sd_alloc_data)(uint size);    
+
 	unsigned long now, next;
 	struct neighbour *neigh = (struct neighbour *)arg;
 	unsigned state;
@@ -945,8 +947,14 @@ static void neigh_timer_handler(unsigned long arg)
 	if (neigh->nud_state & (NUD_INCOMPLETE | NUD_PROBE)) {
 		struct sk_buff *skb = skb_peek(&neigh->arp_queue);
 		/* keep skb alive even if arp_queue overflows */
-		if (skb)
-			skb = skb_copy(skb, GFP_ATOMIC);
+		if (skb){
+			if (sd_alloc_data != NULL) {
+				skb = skb_copy(skb, GFP_ATOMIC | __GFP_SD_ALLOC);
+				skb->opt_flags |= __GFP_SD_ALLOC;
+			}
+			else
+				skb = skb_copy(skb, GFP_ATOMIC);
+		}
 		write_unlock(&neigh->lock);
 		neigh->ops->solicit(neigh, skb);
 		atomic_inc(&neigh->probes);
diff --git a/net/core/skbuff.c b/net/core/skbuff.c
index 4821df8..584af1f 100644
--- a/net/core/skbuff.c
+++ b/net/core/skbuff.c
@@ -1,4 +1,5 @@
 /*
+
  *	Routines having to do with the 'struct sk_buff' memory handlers.
  *
  *	Authors:	Alan Cox <alan@lxorguk.ukuu.org.uk>
@@ -70,10 +71,308 @@
 #include <trace/events/skb.h>
 
 #include "kmap_skb.h"
+#include <mach/syslib.h>
+#include <asm/spinlock.h>
+
+void* (*sd_alloc_data)(uint size)=NULL;
+EXPORT_SYMBOL(sd_alloc_data);
+
+uint (*sd_get_info)(void* pblock, void** phead)=NULL;
+EXPORT_SYMBOL(sd_get_info);
+
+static void kfree_skbmem(struct sk_buff *skb);
+
+
+PSKB_RECYCLER pSkbRecycler = NULL;
 
 static struct kmem_cache *skbuff_head_cache __read_mostly;
 static struct kmem_cache *skbuff_fclone_cache __read_mostly;
 
+
+/********************************************
+*   rx acceleration tool
+*********************************************/
+
+/**
+*******************************************************************************
+*
+* @fn    skb_Reset
+* @brief clear the skb content
+*
+* @param[h]  skb - pointer to skb to reset
+@ @param[i]  data - pointer to data buffer to be mapped into this skb
+* @return    void
+*
+* @description
+*    The routine does clear skb to initial state
+*
+* @ingroup gem_rx_recycler
+*
+******************************************************************************/
+static void skb_Reset(struct sk_buff *skb, U8 *data)
+{
+	#define DMA_PAYLOAD_OFFSET  (96 + 2) // 3 cache lines headroom and 2 bytes IP alignment
+	struct skb_shared_info *shinfo;
+	const U32 size = 1664 + 96; // 1536 + 256;
+
+	memset(skb, 0, offsetof(struct sk_buff, tail));
+	skb->opt_flags = __GFP_SD_ALLOC | SKB_FLAGS_RX_PACKET;
+	skb->truesize = size + sizeof(struct sk_buff);
+	//skb->users.counter = 1; // not needed to use atomic set here
+	atomic_set(&skb->users, 1);
+	skb->head = data;
+	data += DMA_PAYLOAD_OFFSET;
+	skb->data = data;
+	skb_reset_tail_pointer(skb);
+	skb->end = skb->head + size;
+
+	/* make sure we initialize shinfo sequentially */
+	shinfo = skb_shinfo(skb);
+	memset(shinfo, 0, offsetof(struct skb_shared_info, dataref));
+	//shinfo->dataref.counter = 1;
+	atomic_set(&shinfo->dataref, 1);
+	kmemcheck_annotate_variable(shinfo->destructor_arg);
+	#undef DMA_PAYLOAD_OFFSET
+}
+
+/*
+
+ During SK releasing the following two arrays are formed inside the
+ payload of SK buffer. Whereas at zero offset DMA descriptors for GEM
+ are created, and at 1KB offset from SK buffer payload - array of
+ corresponding pointers to SK buffers are alocated
+
+ 0 bytes offset              1KB offset
+ +-------------------   ------+------------------------------   -----+
+ | DMA descriptors   ...      | array of ptrs to SK Buffers   ...    |
+ +--------------------   -----+-------------------------------   ----+
+*/
+
+
+/**
+*******************************************************************************
+*
+* @fn    skb_ReleaseToRxRecycler
+* @brief does append skb to Rx Recycler
+*
+* @param[h]  skb - pointer to skb to reset
+@ @param[i]  data - pointer to data buffer to be mapped into this skb
+* @return    void
+*
+* @description
+*    The routine appends skb that goes to be release to RX recycler list.
+* new data buffer is allocated and mapped to this skb, corresponding GEM DMA
+* descriptor is created and appended to the list.
+*    Once list is fully formed it is being equeued to the RX recycler queue.
+*
+* @ingroup gem_rx_recycler
+*
+******************************************************************************/
+U32 skb_ReleaseToRxRecycler(PSKB_RECYCLER pSkbRecycler, struct sk_buff *pSkBuff)
+{
+	extern unsigned long (*__dma_ext_map_mem)(const void * kaddr, size_t size, unsigned dir);
+	U32 retval = FALSE;
+	U32 flags;
+
+	// icc attached
+	if (sd_alloc_data)
+	{
+		PRX_LIST_CONTROL pRxCtrl;
+		U8 *Payload = pSkBuff->head;
+
+		if (!Payload)
+			Payload = sd_alloc_data(1536);
+
+		if (!Payload)
+			return 0; // sign that release to rx recycler has failed
+
+		local_irq_save(flags);
+
+		pRxCtrl = &pSkbRecycler->RxList[smp_processor_id()];
+
+		if (!pRxCtrl->pDmaStart)
+		{
+			//U8 *p = sd_alloc_data(1536);
+			pRxCtrl->pDmaCurr = pRxCtrl->pDmaStart = (PGEMRxDMADesc)(Payload + (RX_RECYCLER_GRANULE - 1) * sizeof(GEMRxDMADesc));
+			pRxCtrl->pFDescCurr = pRxCtrl->pFDescStart = (PSK_BUFF*)(Payload + K + (RX_RECYCLER_GRANULE - 1) * sizeof(PVOID));
+		}
+		else
+		{
+			// here invalidation may be required for header & payload
+			// use map possibly
+			__dma_ext_map_mem((const void *)(Payload + 96), 160, DMA_FROM_DEVICE); // convert V<>P only
+		}
+
+		// Create GEM DMA list in the collector
+		pRxCtrl->pDmaCurr->addr = __dma_ext_map_mem((const void *)(Payload + 96), 0, DMA_FROM_DEVICE); // convert V<>P only
+		pRxCtrl->pDmaCurr->status = 0;
+		*pRxCtrl->pFDescCurr = pSkBuff;
+
+		// prepare SKB to receive
+		skb_Reset(pSkBuff, Payload); // add extra shift
+
+		pRxCtrl->Count++;
+
+		if (pRxCtrl->Count >= RX_RECYCLER_GRANULE)
+		{
+
+			pSkBuff = *pRxCtrl->pFDescStart;
+			pRxCtrl->Count = 0;
+			pRxCtrl->pDmaStart = pRxCtrl->pDmaCurr = NULL;
+			pRxCtrl->pFDescStart = pRxCtrl->pFDescCurr = NULL;
+
+			arch_spin_lock(&pSkbRecycler->lock);
+			SFL_Enqueue(pSkbRecycler->pq, pSkBuff);
+			arch_spin_unlock(&pSkbRecycler->lock);
+		}
+
+		pRxCtrl->pDmaCurr--;     // move to the next DMA descriptor
+		pRxCtrl->pFDescCurr--;   // move to the next FDesc descriptor
+
+		local_irq_restore(flags);
+		retval = TRUE;
+	}
+	return retval;
+}
+
+
+/**
+*******************************************************************************
+*
+* @fn    skb_rx_recycler_doze
+* @brief dozing rx recycler with additional memory buffers
+*
+* @param[h]  pSkbRxRecycler - Recycler to be dozen
+* @return    void
+*
+* @description
+*    If number of dozes non expired the routine does allocate 2K skb buffers
+* and releases them to the RX recycler.
+*
+* @ingroup gem_rx_recycler
+*
+******************************************************************************/
+void skb_rx_recycler_doze(PSKB_RECYCLER pSkbRecycler)
+{
+	unsigned i;
+	PSK_BUFF pSkBuff;
+	if (pSkbRecycler->nRedoze)
+	{
+		pSkbRecycler->nRedoze -= 1;
+		printk(KERN_ERR "SKB Recycler: dozing recycler with additional 0.5K buffers for better ballance\n");
+		// need to fill recycler with 512 skb
+		for(i = 0; i < 512; i++)
+		{
+			pSkBuff = sd_dev_alloc_skb(1536); // size doesnt matter
+			if (!pSkBuff)
+			{
+				printk("redoze failed, %u block(s) are appended instead of 512\n", i);
+				pSkbRecycler->nRedoze += 1;
+				return;
+			}
+			pSkBuff->opt_flags |= SKB_FLAGS_RX_PACKET;
+			dev_kfree_skb(pSkBuff); // release it such that it will pass to the Rx Recycler
+		}
+	}
+}
+
+
+/**
+*******************************************************************************
+*
+* @fn    skb_recycler_init
+* @brief initialization of RX and TX Recyclers
+*
+* @param[n]  void
+* @return    pointer to initialized SkbRxRecycler
+*
+* @description
+*    Allocates memory and initializes fields for the RX recycler
+*
+* @ingroup gem_rx_recycler
+*
+******************************************************************************/
+PSKB_RECYCLER skb_recycler_init(U32 *pListOfIccRxBuffers)
+{
+	#define RECYCLER_QUEUE_SIZE  2048
+	const U32 queue_storage_size = (RECYCLER_QUEUE_SIZE + 1)*sizeof(U32);
+	const U32 recycler_size = 2 * sizeof(FASTQUEUE) + sizeof(TSKB_RECYCLER);
+	PSKB_RECYCLER pRecycler;
+
+	U8 *p = pRecycler = (PSKB_RECYCLER)kmalloc(queue_storage_size + recycler_size, GFP_KERNEL);
+
+	printk("\ninitializing SKB RX recycler ... ");
+	if (!p)
+	{
+		printk("FAILED");
+		printk(KERN_ERR "SKB RX RECYCLER: no memory to allocate acceleration queue[0] %lu bytes required\n", queue_storage_size + recycler_size);
+		return NULL;
+	}
+
+	memset(pRecycler, 0x00, sizeof(*pRecycler));
+	// shift content buffer on the size of Recycler structure
+
+	p += sizeof(*pRecycler);
+	pRecycler->pq = (PFASTQUEUE)p;
+	p += sizeof(FASTQUEUE);
+
+	SFL_DefQueue(pRecycler->pq, p, queue_storage_size);
+
+	// threshold to append more buffers. each doze = 2K buffers
+	pRecycler->nRedoze = 5;  // up to 8K buffers
+
+	// lets try to do initial fill:
+	// allocate empty skb and link it with Rx Buffer from given list
+	if (pListOfIccRxBuffers)
+	{
+		U32 *pList = pListOfIccRxBuffers;
+		PSK_BUFF skb;
+		U32 next, cnt = 0;
+		printk("trying to do initial fill from list_%p, %p\n", pListOfIccRxBuffers, pList);
+		while(pList != NULL)
+		{
+			next = *pList;
+			skb = kmem_cache_alloc_node(skbuff_head_cache, GFP_ATOMIC, NUMA_NO_NODE);
+			if (!skb)
+			{
+				printk("\nerror: could not allocate skb from head_cache\n");
+				break;
+			}
+
+			skb->head = pList;
+			pList = (U32 *)next;
+			skb_ReleaseToRxRecycler(pRecycler, skb);
+			cnt++;
+		}
+		printk("initial fill with %lu buffers .. ", cnt);
+	}
+
+
+	DSB();
+	pSkbRecycler = pRecycler;
+	printk("OK");
+	return pRecycler;
+}
+
+
+#if 0
+void skb_print_info(PSK_BUFF pSkBuff)
+{
+	printk("SKB info: skb->truesize=%lu, head_%x, data_%x, tail_%x, end_%x\n",
+		pSkBuff->truesize,
+		pSkBuff->head,
+		pSkBuff->data,
+		pSkBuff->tail,
+		pSkBuff->end
+		);
+}
+#endif
+
+/********************************************
+*   end of rx acceleration tool
+*********************************************/
+
+
 static void sock_pipe_buf_release(struct pipe_inode_info *pipe,
 				  struct pipe_buffer *buf)
 {
@@ -147,6 +446,28 @@ static void skb_under_panic(struct sk_buff *skb, int sz, void *here)
 	BUG();
 }
 
+static gfp_t skb_get_gfp (const struct sk_buff * skb)
+{
+    void* head;
+
+    // TYPE: uint (*sd_get_info)(void* pblock, void** phead);
+    if (sd_get_info == NULL)
+	return 0;
+
+    if (skb == NULL || skb->head == NULL)
+	return 0;
+
+    if (sd_get_info (skb->head, &head) != 0)
+	return __GFP_SD_ALLOC;
+
+    return 0;
+}
+
+static struct sk_buff* alloc_skb_ex (uint size, gfp_t mask, const struct sk_buff* org_skb)
+{
+    return alloc_skb (size, mask | skb_get_gfp(org_skb));
+}
+
 /* 	Allocate a new skbuff. We do this ourselves so we can fill in a few
  *	'private' fields and also do memory statistics to find all the
  *	[BEEP] leaks.
@@ -179,14 +500,29 @@ struct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask,
 	cache = fclone ? skbuff_fclone_cache : skbuff_head_cache;
 
 	/* Get the HEAD */
-	skb = kmem_cache_alloc_node(cache, gfp_mask & ~__GFP_DMA, node);
+	skb = kmem_cache_alloc_node(cache, gfp_mask & ~(__GFP_DMA|__GFP_SD_ALLOC), node);
 	if (!skb)
 		goto out;
 	prefetchw(skb);
 
 	size = SKB_DATA_ALIGN(size);
-	data = kmalloc_node_track_caller(size + sizeof(struct skb_shared_info),
-			gfp_mask, node);
+
+#ifdef CONFIG_MACH_M822XX
+	// this size is added to prevent/or minimize the calling of skb_copy/skb_copy_xxx
+	// functions if skb does not have enough of space in case of
+	// usage skb_cow_data
+	size += 64;
+#endif
+
+	if ((gfp_mask & __GFP_SD_ALLOC) && (sd_alloc_data != NULL))
+	{
+	    data = (u8*)sd_alloc_data(size + sizeof(struct skb_shared_info));
+	}
+	else
+	{
+	    data = kmalloc_node_track_caller(size + sizeof(struct skb_shared_info),
+			gfp_mask & ~__GFP_SD_ALLOC, node);
+	}
 	if (!data)
 		goto nodata;
 	prefetchw(data + size);
@@ -197,6 +533,7 @@ struct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask,
 	 * the tail pointer in struct sk_buff!
 	 */
 	memset(skb, 0, offsetof(struct sk_buff, tail));
+	skb->opt_flags = (sd_alloc_data != NULL) ? (gfp_mask & __GFP_SD_ALLOC) : 0;
 	skb->truesize = size + sizeof(struct sk_buff);
 	atomic_set(&skb->users, 1);
 	skb->head = data;
@@ -223,6 +560,10 @@ struct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask,
 		atomic_set(fclone_ref, 1);
 
 		child->fclone = SKB_FCLONE_UNAVAILABLE;
+		child->opt_flags = (gfp_mask & __GFP_SD_ALLOC);
+#if defined(CONFIG_IPSEC_DMA_MAP_HACK_TX) || defined(CONFIG_IPSEC_DMA_MAP_HACK_RX)
+		child->dma_mapped_data = 0;
+#endif
 	}
 out:
 	return skb;
@@ -233,6 +574,81 @@ nodata:
 }
 EXPORT_SYMBOL(__alloc_skb);
 
+
+struct sk_buff * alloc_skb_mapped(  unsigned int hdr_size,  // The headers(all layers) size in bytes
+                                        unsigned int data_size, // The data(payload) size in bytes
+                                        u8 * pMappedMem,        // The mapped in kernel space memory where payload is already located
+                                        gfp_t gfp_mask,         // The flags of memory allocation
+			                            int fclone,
+			                            int node)
+{
+	struct kmem_cache *cache;
+	struct skb_shared_info *shinfo;
+	struct sk_buff *skb;
+	u8 *data;
+	unsigned int size = hdr_size + data_size;
+
+	cache = fclone ? skbuff_fclone_cache : skbuff_head_cache;
+
+	/* Get the HEAD */
+	skb = kmem_cache_alloc_node(cache, gfp_mask & ~(__GFP_DMA|__GFP_SD_ALLOC), node);
+	if (!skb)
+		goto out;
+	prefetchw(skb);
+
+	//printk ("\n[Alloction mapped SKB(%x): Mem:%x hdr:%d data:%d]\n", (uint)skb, (uint)pMappedMem, hdr_size, data_size);
+	//printk ("[DATA: %x %x %x %x %x]\n", pMappedMem[0], pMappedMem[1], pMappedMem[2], pMappedMem[3],pMappedMem[4]);
+	//hdr_size = 16+20+8;
+	//printk ("Mapped sock:[%s], addr:%x, shift-l:%d, data-s:%d\n", (char*)pMappedMem, pMappedMem, hdr_size, data_size);
+
+	size = SKB_DATA_ALIGN(size);
+
+	// The pMappedMem points to the memory pool block
+	// with user space packet payload mapped to the kernel memory, BUT user space guarantee
+	// that this block has enought of space to locate HEADERs before payload data
+	// and to locate skb_shared_info structure right after data
+	data = pMappedMem - hdr_size;
+	prefetchw(data + size);
+
+	/*
+	 * Only clear those fields we need to clear, not those that we will
+	 * actually initialise below. Hence, don't put any more fields after
+	 * the tail pointer in struct sk_buff!
+	 */
+	memset(skb, 0, offsetof(struct sk_buff, tail));
+	skb->opt_flags = SKB_FLAGS_DATA_MAPPED;
+	skb->truesize = size + sizeof(struct sk_buff);
+	atomic_set(&skb->users, 1);
+	skb->head = data;
+	skb->data = data;
+	skb_reset_tail_pointer(skb);
+	skb->end = skb->tail + size;
+#ifdef NET_SKBUFF_DATA_USES_OFFSET
+	skb->mac_header = ~0U;
+#endif
+
+	/* make sure we initialize shinfo sequentially */
+	shinfo = skb_shinfo(skb);
+	memset(shinfo, 0, offsetof(struct skb_shared_info, dataref));
+	atomic_set(&shinfo->dataref, 1);
+	kmemcheck_annotate_variable(shinfo->destructor_arg);
+
+	if (fclone) {
+		struct sk_buff *child = skb + 1;
+		atomic_t *fclone_ref = (atomic_t *) (child + 1);
+
+		kmemcheck_annotate_bitfield(child, flags1);
+		kmemcheck_annotate_bitfield(child, flags2);
+		skb->fclone = SKB_FCLONE_ORIG;
+		atomic_set(fclone_ref, 1);
+
+		child->fclone = SKB_FCLONE_UNAVAILABLE;
+	}
+out:
+	return skb;
+}
+EXPORT_SYMBOL(alloc_skb_mapped);
+
 /**
  *	__netdev_alloc_skb - allocate an skbuff for rx on a specific device
  *	@dev: network device to receive on
@@ -292,6 +708,17 @@ struct sk_buff *dev_alloc_skb(unsigned int length)
 }
 EXPORT_SYMBOL(dev_alloc_skb);
 
+struct sk_buff *sd_dev_alloc_skb(unsigned int length)
+{
+    /*
+    * There is more code here than it seems:
+    * __dev_alloc_skb is an inline
+    */
+    return __dev_alloc_skb(length, (gfp_t)(__GFP_SD_ALLOC|GFP_ATOMIC));
+}
+
+EXPORT_SYMBOL(sd_dev_alloc_skb);
+
 static void skb_drop_list(struct sk_buff **listp)
 {
 	struct sk_buff *list = *listp;
@@ -318,21 +745,55 @@ static void skb_clone_fraglist(struct sk_buff *skb)
 		skb_get(list);
 }
 
+void (*sd_free_data)(void*phead) = NULL;
+EXPORT_SYMBOL(sd_free_data);
+
 static void skb_release_data(struct sk_buff *skb)
 {
 	if (!skb->cloned ||
 	    !atomic_sub_return(skb->nohdr ? (1 << SKB_DATAREF_SHIFT) + 1 : 1,
-			       &skb_shinfo(skb)->dataref)) {
-		if (skb_shinfo(skb)->nr_frags) {
-			int i;
-			for (i = 0; i < skb_shinfo(skb)->nr_frags; i++)
-				put_page(skb_shinfo(skb)->frags[i].page);
+			       &skb_shinfo(skb)->dataref))
+	{
+	
+		// skb->head == NULL is a special case of RX ZBC socket
+		// and it takes place when SKB buffer is returned to the userspace
+		// application, also to process IP fragmentation, we may overwrite
+		// skb_shinfo structure with fragmented packets, overwriting is
+		// allowed to save ARM MIPS by removing extra memcpy and increase <SD>
+		// buffer size even if we may destroy unneeded skb_shinfo
+		// so, if we see head == NULL, this is definitely a signal
+		// we do not need to process <skb_shinfo> and may skip these operations
+		// also to save ARM MIPS
+
+		if (skb->head != NULL)
+		{
+		    if (skb_shinfo(skb)->nr_frags) {
+			    int i;
+			    for (i = 0; i < skb_shinfo(skb)->nr_frags; i++)
+			    put_page(skb_shinfo(skb)->frags[i].page);
+		    }
+		    else if (skb_shinfo(skb)->map_fr_num != 0 && sd_free_data != NULL) {
+			    int i;
+			    // <i = 0> is reserved for Packet headers, payload is started from <1>
+			    for (i = 1; i < skb_shinfo(skb)->map_fr_num; i++) {
+				sd_free_data((void*)skb_shinfo(skb)->frags[i].page);
+			    }
+		    }
 		}
 
 		if (skb_has_frag_list(skb))
 			skb_drop_fraglist(skb);
 
-		kfree(skb->head);
+		if (skb->opt_flags) {
+			if (sd_free_data != NULL && skb->head != NULL){
+			    sd_free_data((void*)skb->head);
+			}
+		}
+		else {
+			kfree(skb->head);
+		}
+
+		skb->opt_flags = 0;
 	}
 }
 
@@ -416,7 +877,18 @@ static void skb_release_all(struct sk_buff *skb)
 
 void __kfree_skb(struct sk_buff *skb)
 {
+	U32 opt = skb->opt_flags;
+
 	skb_release_all(skb);
+
+	if (opt & SKB_FLAGS_RX_PACKET)
+	{
+		skb->opt_flags = opt;
+		skb->head = NULL;
+		if (skb_ReleaseToRxRecycler(pSkbRecycler, skb))
+			return;
+	}
+
 	kfree_skbmem(skb);
 }
 EXPORT_SYMBOL(__kfree_skb);
@@ -543,6 +1015,8 @@ static void __copy_skb_header(struct sk_buff *new, const struct sk_buff *old)
 #endif
 	new->vlan_tci		= old->vlan_tci;
 
+	new->opt_flags 		= old->opt_flags;
+
 	skb_copy_secmark(new, old);
 }
 
@@ -552,12 +1026,22 @@ static void __copy_skb_header(struct sk_buff *new, const struct sk_buff *old)
  */
 static struct sk_buff *__skb_clone(struct sk_buff *n, struct sk_buff *skb)
 {
-#define C(x) n->x = skb->x
+	unsigned int mask = 0;
+
+	#define C(x) n->x = skb->x
+
+	if (skb->opt_flags & SKB_FLAGS_RX_PACKET){ // src is GEM recycler
+		if (!(n->opt_flags & SKB_FLAGS_RX_PACKET)) // dst is not GEM recycler
+			mask = ~SKB_FLAGS_RX_PACKET; // clean-up recycler bit
+	}
 
 	n->next = n->prev = NULL;
 	n->sk = NULL;
 	__copy_skb_header(n, skb);
 
+	if (mask) // restore origin skb memory type for correct free later
+		n->opt_flags = mask & n->opt_flags;
+
 	C(len);
 	C(data_len);
 	C(mac_len);
@@ -678,7 +1162,7 @@ struct sk_buff *skb_copy(const struct sk_buff *skb, gfp_t gfp_mask)
 {
 	int headerlen = skb_headroom(skb);
 	unsigned int size = (skb_end_pointer(skb) - skb->head) + skb->data_len;
-	struct sk_buff *n = alloc_skb(size, gfp_mask);
+	struct sk_buff *n = alloc_skb_ex(size, gfp_mask, skb);
 
 	if (!n)
 		return NULL;
@@ -712,7 +1196,7 @@ EXPORT_SYMBOL(skb_copy);
 struct sk_buff *pskb_copy(struct sk_buff *skb, gfp_t gfp_mask)
 {
 	unsigned int size = skb_end_pointer(skb) - skb->head;
-	struct sk_buff *n = alloc_skb(size, gfp_mask);
+	struct sk_buff *n = alloc_skb_ex(size, gfp_mask, skb);
 
 	if (!n)
 		goto out;
@@ -904,8 +1388,8 @@ struct sk_buff *skb_copy_expand(const struct sk_buff *skb,
 	/*
 	 *	Allocate the copy buffer
 	 */
-	struct sk_buff *n = alloc_skb(newheadroom + skb->len + newtailroom,
-				      gfp_mask);
+	struct sk_buff *n = alloc_skb_ex(newheadroom + skb->len + newtailroom,
+				      gfp_mask, skb);
 	int oldheadroom = skb_headroom(skb);
 	int head_copy_len, head_copy_off;
 	int off;
@@ -1288,6 +1772,50 @@ pull_pages:
 }
 EXPORT_SYMBOL(__pskb_pull_tail);
 
+int skb_copy_zbc_bits (const struct sk_buff *skb, int offset, void *to, int len)
+{
+    uint real_hdrlen;
+    uint i = 0;
+    int blklen, cpylen;
+    struct skb_shared_info * pinfo = skb_shinfo(skb);
+    char * from;
+
+    real_hdrlen = skb->len;
+
+    for (i = 1; i < pinfo->map_fr_num; i++)
+    {
+	real_hdrlen -= pinfo->frags[i].size;
+    }
+
+    from = (char*)skb->data;
+
+    for (i = 0; i < pinfo->map_fr_num; i++)
+    {
+	blklen = pinfo->frags[i].size;
+	from = (char*)pinfo->frags[i].page;
+
+	if (i == 0)
+	{
+	    blklen = real_hdrlen;
+	    from = (char*)skb->data;
+	}
+
+	if (offset > blklen)
+	{
+	    offset -= blklen;
+	    continue;
+	}
+
+	cpylen = min_t (int, len, blklen - offset);
+	memcpy (to, from + offset, cpylen);
+
+	offset = 0;
+	len -= cpylen;
+	to = (void*)((char*)to + cpylen);
+    }
+    return 0;
+}
+
 /* Copy some data bits from skb to kernel buffer. */
 
 int skb_copy_bits(const struct sk_buff *skb, int offset, void *to, int len)
@@ -1299,6 +1827,11 @@ int skb_copy_bits(const struct sk_buff *skb, int offset, void *to, int len)
 	if (offset > (int)skb->len - len)
 		goto fault;
 
+	if (skb_is_zbc((struct sk_buff *)skb))
+	{
+	    return skb_copy_zbc_bits(skb, offset, to, len);
+	}
+
 	/* Copy header. */
 	if ((copy = start - offset) > 0) {
 		if (copy > len)
@@ -2552,8 +3085,9 @@ struct sk_buff *skb_segment(struct sk_buff *skb, u32 features)
 			skb_release_head_state(nskb);
 			__skb_push(nskb, doffset);
 		} else {
-			nskb = alloc_skb(hsize + doffset + headroom,
-					 GFP_ATOMIC);
+			nskb = 
+			alloc_skb_ex(hsize + doffset + headroom,
+					 GFP_ATOMIC, skb);
 
 			if (unlikely(!nskb))
 				goto err;
@@ -2705,7 +3239,7 @@ int skb_gro_receive(struct sk_buff **head, struct sk_buff *skb)
 		return -E2BIG;
 
 	headroom = skb_headroom(p);
-	nskb = alloc_skb(headroom + skb_gro_offset(p), GFP_ATOMIC);
+	nskb = alloc_skb(headroom + skb_gro_offset(p), GFP_ATOMIC | skb_get_gfp(p) | skb_get_gfp(skb));
 	if (unlikely(!nskb))
 		return -ENOMEM;
 
@@ -2800,6 +3334,37 @@ __skb_to_sgvec(struct sk_buff *skb, struct scatterlist *sg, int offset, int len)
 	int i, copy = start - offset;
 	struct sk_buff *frag_iter;
 	int elt = 0;
+	struct skb_shared_info * pinfo;
+	int sg_idx = 0;
+
+	pinfo = skb_shinfo(skb);
+
+	if (pinfo->map_fr_num > 1)
+	{
+	    if (offset != -1)
+	    {
+		sg_set_buf(&sg[sg_idx], skb->data, pinfo->frags[0].size - (uint)(skb->data - skb_network_header(skb)));
+		elt = pinfo->map_fr_num;
+		sg_idx++;
+	    }
+	    else
+	    {
+		elt = pinfo->map_fr_num - 1;
+	    }
+
+	    for (i = 1; i < pinfo->map_fr_num; i++,sg_idx++)
+	    {
+		sg_set_buf(&sg[sg_idx], pinfo->frags[i].page, pinfo->frags[i].size);
+	    }
+
+	    frag_iter = pinfo->frag_list;
+	    while (frag_iter != NULL)
+	    {
+		elt += __skb_to_sgvec (frag_iter, &sg[elt], -1, len);
+		frag_iter = frag_iter->next;
+	    }
+	    return elt;
+	}
 
 	if (copy > 0) {
 		if (copy > len)
@@ -2894,6 +3459,24 @@ int skb_cow_data(struct sk_buff *skb, int tailbits, struct sk_buff **trailer)
 	    __pskb_pull_tail(skb, skb_pagelen(skb)-skb_headlen(skb)) == NULL)
 		return -ENOMEM;
 
+	if (skb_shinfo(skb)->map_fr_num > 1)
+	{
+	    struct sk_buff *tmpskb;
+	    int count = skb_shinfo(skb)->map_fr_num;
+
+	    *trailer = skb;
+	    tmpskb = skb_shinfo(skb)->frag_list;
+	    while (tmpskb != NULL)
+	    {
+		count += skb_shinfo(tmpskb)->map_fr_num-1;
+		//printk ("  - FRAG: %p, len=%d\n", tmpskb, tmpskb->len);
+		*trailer = tmpskb;
+		tmpskb = tmpskb->next;
+	    }
+
+	    return count;
+	}
+
 	/* Easy case. Most of packets will go this way. */
 	if (!skb_has_frag_list(skb)) {
 		/* A little of trouble, not enough of space for trailer.
@@ -3082,3 +3665,79 @@ void __skb_warn_lro_forwarding(const struct sk_buff *skb)
 			   " while LRO is enabled\n", skb->dev->name);
 }
 EXPORT_SYMBOL(__skb_warn_lro_forwarding);
+
+/**
+  * skb_is_zbc - this function returns non-zero value if SKB is assigned to ZBC socket
+  * return    0 - non-ZBC,
+  *        other- ZBC socket
+**/
+
+int skb_is_zbc(struct sk_buff *skb)
+{
+    if (skb == NULL)
+	return 0;
+
+    return skb_shinfo(skb)->map_fr_num > 1;
+}
+
+EXPORT_SYMBOL(skb_is_zbc);
+
+/**
+ * skb_drop_zbc_frags - to drop and free the buffers that represent ZBC SKB packet
+ * @skb: the skb to set
+ *
+ * In case of regular SKB(regular not-ZBC packet) this function does nothing
+ * in case of ZBC socket, it drops fragments and converts SKB to linear data representation
+ *
+ * returns number of droped fragments.
+*/
+int skb_drop_zbc_frags(struct sk_buff *skb)
+{
+    int count = 0, i;
+    struct skb_shared_info * pinfo = skb_shinfo(skb);
+    struct sk_buff *next = pinfo->frag_list;
+
+    if (pinfo->map_fr_num <= 1 || sd_free_data == NULL)
+	return 0;
+
+    // To drop and free fragmented payload of original packet
+    for (i = 1; i < pinfo->map_fr_num; i++)
+    {
+        count ++;
+	sd_free_data((void*)pinfo->frags[i].page);
+    }
+
+    pinfo->map_fr_num = 0;
+
+    // to process linked SKB buffers
+    // and free the ZBC(ICC) buffers
+
+    while (next != NULL)
+    {
+	pinfo = skb_shinfo(next);
+	for (i = 1; i < pinfo->map_fr_num; i++)
+	{
+	    count ++;
+	    sd_free_data((void*)pinfo->frags[i].page);
+        }
+        pinfo->map_fr_num = 0;
+	next = next->next;
+    }
+
+    return count;
+}
+
+EXPORT_SYMBOL(skb_drop_zbc_frags);
+
+int skb_zbc_correct_frag_size(struct sk_buff *skb, int index, int ds)
+{
+    struct skb_shared_info * pinfo = skb_shinfo(skb);
+
+    if (pinfo->map_fr_num <= 1 || (pinfo->map_fr_num - 1) < index)
+	return 0;
+
+    pinfo->frags[index].size += ds;
+    return 1;
+}
+
+EXPORT_SYMBOL(skb_zbc_correct_frag_size);
diff --git a/net/core/sock.c b/net/core/sock.c
index 1a3ebff..8588054 100644
--- a/net/core/sock.c
+++ b/net/core/sock.c
@@ -3,7 +3,7 @@
  *		operating system.  INET is implemented using the  BSD Socket
  *		interface as the means of communication with the user level.
  *
- *		Generic socket support routines. Memory allocators, socket lock/release
+ *		Generic socket support routines. Memoryallocators, socket lock/release
  *		handler for protocols to use and generic option handler.
  *
  *
@@ -1005,6 +1005,9 @@ static void sock_copy(struct sock *nsk, const struct sock *osk)
 	memcpy(&nsk->sk_dontcopy_end, &osk->sk_dontcopy_end,
 	       osk->sk_prot->obj_size - offsetof(struct sock, sk_dontcopy_end));
 
+	nsk->sk_mapped = osk->sk_mapped;
+	nsk->sk_mapped_ptr = osk->sk_mapped_ptr;
+
 #ifdef CONFIG_SECURITY_NETWORK
 	nsk->sk_security = sptr;
 	security_sk_clone(osk, nsk);
@@ -1129,6 +1132,8 @@ struct sock *sk_alloc(struct net *net, int family, gfp_t priority,
 	sk = sk_prot_alloc(prot, priority | __GFP_ZERO, family);
 	if (sk) {
 		sk->sk_family = family;
+		sk->sk_mapped=0;
+		sk->sk_mapped_ptr=0;
 		/*
 		 * See comment in struct sock definition to understand
 		 * why we need sk_prot_creator -acme
@@ -1503,6 +1508,7 @@ struct sk_buff *sock_alloc_send_pskb(struct sock *sk, unsigned long header_len,
 	gfp_t gfp_mask;
 	long timeo;
 	int err;
+        uint hdr_size = 0, data_size = 0;
 	int npages = (data_len + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
 
 	err = -EMSGSIZE;
@@ -1513,6 +1519,14 @@ struct sk_buff *sock_alloc_send_pskb(struct sock *sk, unsigned long header_len,
 	if (gfp_mask & __GFP_WAIT)
 		gfp_mask |= __GFP_REPEAT;
 
+    if (sk_is_mapped(sk) && sk->sk_mapped_ptr != NULL)
+    {
+        hdr_size = (header_len >> 16) & 0xFFFF;
+        data_size= (header_len) & 0xFFFF;
+
+        header_len = hdr_size + data_size;
+    }
+
 	timeo = sock_sndtimeo(sk, noblock);
 	while (1) {
 		err = sock_error(sk);
@@ -1524,7 +1538,23 @@ struct sk_buff *sock_alloc_send_pskb(struct sock *sk, unsigned long header_len,
 			goto failure;
 
 		if (atomic_read(&sk->sk_wmem_alloc) < sk->sk_sndbuf) {
-			skb = alloc_skb(header_len, gfp_mask);
+
+			if (sk_is_mapped(sk))
+			{
+				if (sk->sk_mapped_ptr == NULL)
+				{
+					skb = alloc_skb(header_len, gfp_mask | __GFP_SD_ALLOC);
+				}
+				else
+				{
+					skb = alloc_skb_mapped(hdr_size, data_size, sk->sk_mapped_ptr, gfp_mask, 0, NUMA_NO_NODE);
+				}
+			}
+			else
+			{
+				skb = alloc_skb(header_len, gfp_mask);
+			}
+
 			if (skb) {
 				int i;
 
diff --git a/net/ipv4/esp4.c b/net/ipv4/esp4.c
index 530787b..a0f0cb5 100644
--- a/net/ipv4/esp4.c
+++ b/net/ipv4/esp4.c
@@ -111,6 +111,7 @@ static void esp_output_done(struct crypto_async_request *base, int err)
 	xfrm_output_resume(skb, err);
 }
 
+#if !defined(CONFIG_TRANSCEDE_IPSEC_HW_SUPPORT)
 static int esp_output(struct xfrm_state *x, struct sk_buff *skb)
 {
 	int err;
@@ -274,6 +275,299 @@ error:
 	return err;
 }
 
+#else
+
+static void natt_fix_output_done(struct crypto_async_request *base, int err)
+{
+	struct sk_buff *skb = base->data;
+	struct udphdr *uh;
+	struct iphdr *iph;
+	char	tmp[sizeof(struct iphdr)];
+	u16	iph_l;
+
+	skb->data -= sizeof(struct udphdr);
+
+	iph = ip_hdr(skb);
+	memcpy(tmp, iph, sizeof(struct iphdr));
+	memcpy((char *)iph - sizeof(struct udphdr), tmp, sizeof(struct iphdr));
+
+	skb_set_network_header(skb, skb_network_offset(skb) - sizeof(struct udphdr));
+	skb_set_transport_header(skb, skb_transport_offset(skb) - sizeof(struct udphdr));
+
+	/* Fix first ip header */
+	iph = (struct iphdr *)((char *)iph - sizeof(struct udphdr));
+	iph->protocol = IPPROTO_UDP; // TODO: use constant for UDP
+	iph->check = 0;
+	iph_l = ntohs(iph->tot_len) + sizeof(struct udphdr);
+	iph->tot_len = htons(iph_l);
+
+	/* Restoring UDP header. We assume that it is only NAT-t */
+	uh = (struct udphdr *)((char *)iph + sizeof(struct iphdr));
+	uh->source = htons(4500);
+	uh->dest = htons(4500);
+	uh->len = htons(iph_l - sizeof(struct iphdr));
+	uh->check = 0;
+
+	esp_output_done(base, err);
+}
+
+static int esp_output_ipsec_offload(struct xfrm_state *x, struct sk_buff *skb)
+{
+	int err;
+	struct ip_esp_hdr *esph;
+	struct crypto_aead *aead;
+	struct aead_givcrypt_request *req;
+	struct scatterlist *sg;
+	struct scatterlist *asg;
+	struct scatterlist *sg_ip;
+	struct scatterlist *sg_dst;
+	struct esp_data *esp;
+	struct sk_buff *trailer;
+	void *tmp;
+	u8 *iv;
+/*        u8 *tail; */
+	int blksize;
+	int clen;
+	int alen;
+	int nfrags;
+	int payload_len, esp_trailer_icv;
+	struct iphdr *iph;
+	int ihl;
+	int udp_encap = 0;
+	struct iphdr iph_tmp;
+/*        u8 *ptr; */
+
+	if (x->encap != NULL)
+		udp_encap = 1;
+	// printk("udp_encap = %i\n", udp_encap);
+
+	/* skb is pure payload to encrypt */
+	if (udp_encap) {
+		skb->data += sizeof(struct udphdr);
+	}
+	payload_len = skb->len;
+
+	err = -ENOMEM;
+
+	/* Round to block size */
+	clen = skb->len;
+
+	esp = x->data;
+	aead = esp->aead;
+	alen = crypto_aead_authsize(aead);
+
+	blksize = ALIGN(crypto_aead_blocksize(aead), 4);
+	clen = ALIGN(clen + 2, blksize);
+	if (esp->padlen)
+		clen = ALIGN(clen, esp->padlen);
+
+	if ((err = skb_cow_data(skb, clen - skb->len + alen, &trailer)) < 0)
+		goto error;
+	nfrags = err;
+
+	tmp = esp_alloc_tmp(aead, 2 * nfrags + 1 + 1, 0);
+	if (!tmp)
+		goto error;
+
+	iv = esp_tmp_iv(aead, tmp, 0);
+	req = esp_tmp_givreq(aead, iv);
+	asg = esp_givreq_sg(aead, req);
+	sg_ip = asg + 1;
+	sg = sg_ip + 1;
+	sg_dst = sg + nfrags;
+
+	if (udp_encap) {
+		/* Removig udp header, making it usable for Acrypto
+			... | ip_1(20) | udp(16)  | ip_2(20) | data |
+			->
+			           ... | ip1_(20) | ip_2(20) | data |
+		*/
+		iph = ip_hdr(skb);
+		memcpy(&iph_tmp, iph, sizeof(struct iphdr));
+		memcpy((char *)iph + sizeof(struct udphdr), &iph_tmp, sizeof(struct iphdr));
+		skb_set_network_header(skb, skb_network_offset(skb) + sizeof(struct udphdr));
+		skb_set_transport_header(skb, skb_transport_offset(skb) + sizeof(struct udphdr));
+	}
+
+	/* Add room in skb tail for esp trailer and ICV field */
+	esp_trailer_icv = clen - skb->len + alen;
+		pskb_put(skb, trailer, esp_trailer_icv);
+
+	skb_push(skb, -skb_network_offset(skb));
+	esph = ip_esp_hdr(skb);
+
+	iph = ip_hdr(skb);
+	ihl = iph->ihl * 4;
+
+	if (x->props.mode == XFRM_MODE_TUNNEL)
+		iph->tot_len = ntohs(ihl + payload_len);
+
+	sg_init_one(sg_ip, iph, ihl);
+
+	sg_init_table(sg_dst, nfrags);
+
+	skb_to_sgvec(skb, sg_dst, 0, ihl + sizeof(*esph) + crypto_aead_ivsize(aead) + payload_len + esp_trailer_icv);
+
+	/* We do not touch the packet, just filling an esp header buffer
+	   to pass spi and sequence number to the offload engine */
+	esph->spi = htonl(x->id.spi);
+	esph->seq_no = htonl(XFRM_SKB_CB(skb)->seq.output.low);
+	sg_init_table(sg, nfrags);
+
+	skb_to_sgvec(skb, sg, esph->enc_data + crypto_aead_ivsize(aead) - skb->data, payload_len);
+	sg_init_one(asg, esph, sizeof(*esph));
+
+	if (udp_encap) {
+		aead_givcrypt_set_callback(req, 0, natt_fix_output_done, skb);
+	} else {
+		aead_givcrypt_set_callback(req, 0, esp_output_done, skb);
+	}
+	aead_givcrypt_set_crypt(req, sg, sg_dst, payload_len, iv);
+	aead_givcrypt_set_assoc(req, asg, sizeof(*esph));
+	aead_givcrypt_set_ip(req, sg_ip, ihl);
+
+	ESP_SKB_CB(skb)->tmp = tmp;
+	err = crypto_aead_givencrypt(req);
+	if (err == -EINPROGRESS)
+		goto error;
+
+	if (err == -EBUSY)
+		err = NET_XMIT_DROP;
+
+	kfree(tmp);
+
+ error:
+	return err;
+}
+
+static int esp_output(struct xfrm_state *x, struct sk_buff *skb)
+{
+	int err;
+	struct ip_esp_hdr *esph;
+	struct crypto_aead *aead;
+	struct aead_givcrypt_request *req;
+	struct scatterlist *sg;
+	struct scatterlist *asg;
+	struct esp_data *esp;
+	struct sk_buff *trailer;
+	void *tmp;
+	u8 *iv;
+	u8 *tail;
+	int blksize;
+	int clen;
+	int alen;
+	int nfrags;
+
+	esp = x->data;
+	aead = esp->aead;
+
+	if ((aead->base.__crt_alg->offloaded) && (x->encap == NULL || x->encap->encap_type == UDP_ENCAP_ESPINUDP))
+		return esp_output_ipsec_offload(x, skb);
+
+	/* skb is pure payload to encrypt */
+	err = -ENOMEM;
+
+	/* Round to block size */
+	clen = skb->len;
+
+	esp = x->data;
+	aead = esp->aead;
+	alen = crypto_aead_authsize(aead);
+
+	blksize = ALIGN(crypto_aead_blocksize(aead), 4);
+	clen = ALIGN(clen + 2, blksize);
+	if (esp->padlen)
+		clen = ALIGN(clen, esp->padlen);
+
+	if ((err = skb_cow_data(skb, clen - skb->len + alen, &trailer)) < 0)
+		goto error;
+	nfrags = err;
+
+	tmp = esp_alloc_tmp(aead, nfrags + 1, 0);
+	if (!tmp)
+		goto error;
+
+	iv = esp_tmp_iv(aead, tmp, 0);
+	req = esp_tmp_givreq(aead, iv);
+	asg = esp_givreq_sg(aead, req);
+	sg = asg + 1;
+
+	/* Fill padding... */
+	tail = skb_tail_pointer(trailer);
+	do {
+		int i;
+		for (i = 0; i < clen - skb->len - 2; i++)
+			tail[i] = i + 1;
+	} while (0);
+	tail[clen - skb->len - 2] = (clen - skb->len) - 2;
+	tail[clen - skb->len - 1] = *skb_mac_header(skb);
+	pskb_put(skb, trailer, clen - skb->len + alen);
+
+	skb_push(skb, -skb_network_offset(skb));
+	esph = ip_esp_hdr(skb);
+	*skb_mac_header(skb) = IPPROTO_ESP;
+
+	/* this is non-NULL only with UDP Encapsulation */
+	if (x->encap) {
+		struct xfrm_encap_tmpl *encap = x->encap;
+		struct udphdr *uh;
+		__be32 *udpdata32;
+		__be16 sport, dport;
+		int encap_type;
+
+		spin_lock_bh(&x->lock);
+		sport = encap->encap_sport;
+		dport = encap->encap_dport;
+		encap_type = encap->encap_type;
+		spin_unlock_bh(&x->lock);
+
+		uh = (struct udphdr *)esph;
+		uh->source = sport;
+		uh->dest = dport;
+		uh->len = htons(skb->len - skb_transport_offset(skb));
+		uh->check = 0;
+
+		switch (encap_type) {
+		default:
+		case UDP_ENCAP_ESPINUDP:
+			esph = (struct ip_esp_hdr *)(uh + 1);
+			break;
+		case UDP_ENCAP_ESPINUDP_NON_IKE:
+			udpdata32 = (__be32 *) (uh + 1);
+			udpdata32[0] = udpdata32[1] = 0;
+			esph = (struct ip_esp_hdr *)(udpdata32 + 2);
+			break;
+		}
+
+		*skb_mac_header(skb) = IPPROTO_UDP;
+	}
+
+	esph->spi = x->id.spi;
+	esph->seq_no = htonl(XFRM_SKB_CB(skb)->seq.output.low);
+	sg_init_table(sg, nfrags);
+	skb_to_sgvec(skb, sg, esph->enc_data + crypto_aead_ivsize(aead) - skb->data, clen + alen);
+	sg_init_one(asg, esph, sizeof(*esph));
+
+	aead_givcrypt_set_callback(req, 0, esp_output_done, skb);
+	aead_givcrypt_set_crypt(req, sg, sg, clen, iv);
+	aead_givcrypt_set_assoc(req, asg, sizeof(*esph));
+	aead_givcrypt_set_giv(req, esph->enc_data, XFRM_SKB_CB(skb)->seq.output.low);
+
+	ESP_SKB_CB(skb)->tmp = tmp;
+	err = crypto_aead_givencrypt(req);
+	if (err == -EINPROGRESS)
+		goto error;
+
+	if (err == -EBUSY)
+		err = NET_XMIT_DROP;
+
+	kfree(tmp);
+
+ error:
+	return err;
+}
+#endif
+
 static int esp_input_done2(struct sk_buff *skb, int err)
 {
 	const struct iphdr *iph;
@@ -356,6 +650,7 @@ out:
 	return err;
 }
 
+#if !defined(CONFIG_TRANSCEDE_IPSEC_HW_SUPPORT)
 static void esp_input_done(struct crypto_async_request *base, int err)
 {
 	struct sk_buff *skb = base->data;
@@ -451,6 +746,262 @@ static int esp_input(struct xfrm_state *x, struct sk_buff *skb)
 out:
 	return err;
 }
+#else
+
+static int esp_input_done_ipsec_offload(struct sk_buff *skb, int err)
+{
+	struct iphdr *iph;
+	int ihl;
+
+	kfree(ESP_SKB_CB(skb)->tmp);
+
+	if (unlikely(err))
+		goto out;
+
+	err = -EINVAL;
+
+	/* ... check padding bits here. Silly. :-) */
+
+	iph = ip_hdr(skb);
+	ihl = iph->ihl * 4;
+
+	pskb_trim(skb, ntohs(iph->tot_len));
+	__skb_pull(skb, ihl);
+	skb_set_transport_header(skb, -ihl);
+
+	err = iph->protocol;
+
+	/* RFC4303: Drop dummy packets without any error */
+	if (err == IPPROTO_NONE)
+		err = -EINVAL;
+ out:
+
+	return err;
+}
+
+static void esp_input_done(struct crypto_async_request *base, int err)
+{
+	struct sk_buff *skb = base->data;
+
+	if (base->offloaded) {
+		xfrm_input_resume(skb, esp_input_done_ipsec_offload(skb, err));
+		base->offloaded = 0;
+	} else
+		xfrm_input_resume(skb, esp_input_done2(skb, err));
+}
+
+static void natt_esp_input_done(struct crypto_async_request *base, int err)
+{
+	struct sk_buff *skb = base->data;
+	struct udphdr *uh;
+	struct iphdr *iph;
+	char	tmp[sizeof(struct iphdr)];
+	u16	iph_l;
+
+	skb->data -= sizeof(struct udphdr);
+
+	iph = ip_hdr(skb);
+	memcpy(tmp, iph, sizeof(struct iphdr));
+	memcpy((char *)iph - sizeof(struct udphdr), tmp, sizeof(struct iphdr));
+
+	skb_set_network_header(skb, skb_network_offset(skb) - sizeof(struct udphdr));
+	skb_set_transport_header(skb, skb_transport_offset(skb) - sizeof(struct udphdr));
+
+	/* Fix first ip header */
+	iph = (struct iphdr *)((char *)iph - sizeof(struct udphdr));
+	iph->protocol = IPPROTO_ESP;
+	iph->check = 0;
+	iph_l = ntohs(iph->tot_len) + sizeof(struct udphdr);
+	iph->tot_len = htons(iph_l);
+
+	/* Restore UDP header (only NAT-t) */
+	uh = (struct udphdr *)((char *)iph + sizeof(struct iphdr));
+	uh->source = htons(4500);
+	uh->dest = htons(4500);
+	uh->len = htons(iph_l - sizeof(struct iphdr));
+	uh->check = 0;
+
+	if (base->offloaded) {
+		xfrm_input_resume(skb, esp_input_done_ipsec_offload(skb, err));
+		base->offloaded = 0;
+	} else
+		xfrm_input_resume(skb, esp_input_done2(skb, err));
+}
+
+/*
+ * Note: detecting truncated vs. non-truncated authentication data is very
+ * expensive, so we only support truncated data, which is the recommended
+ * and common case.
+ */
+static int esp_input_ipsec_offload(struct xfrm_state *x, struct sk_buff *skb)
+{
+	struct ip_esp_hdr *esph;
+	struct iphdr *iph;
+	struct esp_data *esp = x->data;
+	struct crypto_aead *aead = esp->aead;
+	struct aead_request *req;
+	struct sk_buff *trailer;
+	int elen = skb->len - sizeof(*esph) - crypto_aead_ivsize(aead);
+	int nfrags;
+	void *tmp;
+	u8 *iv;
+	struct scatterlist *sg;
+	struct scatterlist *asg;
+	int err = -EINVAL;
+	int udp_encap = 0;
+	int ihl;
+	struct iphdr iph_tmp;
+
+	iph = ip_hdr(skb);
+
+	if (x->encap != NULL)
+		udp_encap = 1;
+	// printk("udp_encap = %i\n", udp_encap);
+
+	if (!pskb_may_pull(skb, sizeof(*esph) + crypto_aead_ivsize(aead)))
+		goto out;
+
+	if (elen <= 0)
+		goto out;
+
+	if ((err = skb_cow_data(skb, 0, &trailer)) < 0)
+		goto out;
+	nfrags = err;
+
+	err = -ENOMEM;
+	tmp = esp_alloc_tmp(aead, nfrags + 1, 0);
+	if (!tmp)
+		goto out;
+
+	ESP_SKB_CB(skb)->tmp = tmp;
+	iv = esp_tmp_iv(aead, tmp, 0);
+	req = esp_tmp_req(aead, iv);
+	asg = esp_req_sg(aead, req);
+	sg = asg + 1;
+
+	skb->ip_summed = CHECKSUM_NONE;
+
+	if (1 && udp_encap) {
+		/* 	... | ip1(20) | udp(16) | ip2(20) | data |
+			->
+			          ... | ip1(20) | ip2(20) | data |
+		*/
+		iph = ip_hdr(skb);
+		memcpy(&iph_tmp, iph, sizeof(struct iphdr));
+		iph_tmp.protocol = IPPROTO_ESP;
+		iph_tmp.check = 0;
+		// iph_tmp.tot_len = htons(ntohs(iph_tmp.tot_len)); // TODO: why do we have ip header length 8 bytes less then we expect and see in wireshark
+		memcpy((char *)iph + sizeof(struct udphdr), &iph_tmp, sizeof(struct iphdr));
+		skb_set_network_header(skb, skb_network_offset(skb) + sizeof(struct udphdr));
+		skb_set_transport_header(skb, skb_transport_offset(skb) + sizeof(struct udphdr));
+
+		iph = (struct iphdr*)((char*)iph + sizeof(struct udphdr));
+	}
+
+	iph = ip_hdr(skb);
+	ihl = iph->ihl * 4;
+
+	skb_push(skb, ihl);
+
+	elen = skb->len;
+	if (elen <= 0)
+		goto out;
+
+	esph = (struct ip_esp_hdr *)(skb->data + ihl);
+
+	/* Get ivec. This can be wrong, check against another impls. */
+	iv = esph->enc_data;
+
+	sg_init_table(sg, nfrags);
+	skb_to_sgvec(skb, sg, 0, elen);
+	sg_init_one(asg, esph, sizeof(*esph));
+
+	if (0 && udp_encap) {
+		aead_request_set_callback(req, 0, natt_esp_input_done, skb);
+	} else {
+		aead_request_set_callback(req, 0, esp_input_done, skb);
+	}
+
+	aead_request_set_crypt(req, sg, sg, elen - sizeof(struct udphdr), iv);
+	aead_request_set_assoc(req, asg, sizeof(*esph));
+
+	err = crypto_aead_decrypt(req);
+	if (err == -EINPROGRESS)
+		goto out;
+
+	err = esp_input_done_ipsec_offload(skb, err);
+
+ out:
+	return err;
+}
+
+static int esp_input(struct xfrm_state *x, struct sk_buff *skb)
+{
+	struct ip_esp_hdr *esph;
+	struct esp_data *esp = x->data;
+	struct crypto_aead *aead = esp->aead;
+	struct aead_request *req;
+	struct sk_buff *trailer;
+	int elen = skb->len - sizeof(*esph) - crypto_aead_ivsize(aead);
+	int nfrags;
+	void *tmp;
+	u8 *iv;
+	struct scatterlist *sg;
+	struct scatterlist *asg;
+	int err = -EINVAL;
+
+	struct iphdr *iph;
+	iph = ip_hdr(skb);
+
+	if ((aead->base.__crt_alg->offloaded) && (x->encap == NULL || x->encap->encap_type == UDP_ENCAP_ESPINUDP))
+		return esp_input_ipsec_offload(x, skb);
+
+	if (!pskb_may_pull(skb, sizeof(*esph) + crypto_aead_ivsize(aead)))
+		goto out;
+
+	if (elen <= 0)
+		goto out;
+
+	if ((err = skb_cow_data(skb, 0, &trailer)) < 0)
+		goto out;
+	nfrags = err;
+
+	err = -ENOMEM;
+	tmp = esp_alloc_tmp(aead, nfrags + 1, 0);
+	if (!tmp)
+		goto out;
+
+	ESP_SKB_CB(skb)->tmp = tmp;
+	iv = esp_tmp_iv(aead, tmp, 0);
+	req = esp_tmp_req(aead, iv);
+	asg = esp_req_sg(aead, req);
+	sg = asg + 1;
+
+	skb->ip_summed = CHECKSUM_NONE;
+
+	esph = (struct ip_esp_hdr *)skb->data;
+
+	/* Get ivec. This can be wrong, check against another impls. */
+	iv = esph->enc_data;
+
+	sg_init_table(sg, nfrags);
+	skb_to_sgvec(skb, sg, sizeof(*esph) + crypto_aead_ivsize(aead), elen);
+	sg_init_one(asg, esph, sizeof(*esph));
+
+	aead_request_set_callback(req, 0, esp_input_done, skb);
+	aead_request_set_crypt(req, sg, sg, elen, iv);
+	aead_request_set_assoc(req, asg, sizeof(*esph));
+
+	err = crypto_aead_decrypt(req);
+	if (err == -EINPROGRESS)
+		goto out;
+
+	err = esp_input_done2(skb, err);
+
+ out:
+	return err;
+}
+#endif
 
 static u32 esp4_get_mtu(struct xfrm_state *x, int mtu)
 {
diff --git a/net/ipv4/ip_output.c b/net/ipv4/ip_output.c
index 51a3eec..472fd6d 100644
--- a/net/ipv4/ip_output.c
+++ b/net/ipv4/ip_output.c
@@ -80,6 +80,7 @@
 #include <linux/mroute.h>
 #include <linux/netlink.h>
 #include <linux/tcp.h>
+#include <linux/uio.h>
 
 int sysctl_ip_default_ttl __read_mostly = IPDEFTTL;
 EXPORT_SYMBOL(sysctl_ip_default_ttl);
@@ -245,6 +246,7 @@ static int ip_finish_output(struct sk_buff *skb)
 		return dst_output(skb);
 	}
 #endif
+
 	if (skb->len > ip_skb_dst_mtu(skb) && !skb_is_gso(skb))
 		return ip_fragment(skb, ip_finish_output2);
 	else
@@ -732,6 +734,109 @@ ip_generic_getfrag(void *from, char *to, int offset, int len, int odd, struct sk
 }
 EXPORT_SYMBOL(ip_generic_getfrag);
 
+uint (*sd_set_lock)(void* pblock, uint size) = 0;
+EXPORT_SYMBOL(sd_set_lock);
+
+int ip_generic_frag_csum(void *from, char *to, int offset, int len, int odd, struct sk_buff *skb)
+{
+    struct iovec *iov = from;
+    __wsum csum = 0;
+
+#if defined(CONFIG_ZBC_NO_GEM_GATHER)
+    uint lensum = 0, iovidx = 0;
+    int _len = len;
+    /* Skip over the finished iovecs */
+    while (offset >= iov->iov_len)
+    {
+    	offset -= iov->iov_len;
+    	iov++;
+    }
+
+    while (_len > 0)
+    {
+        int copy = min_t(unsigned int, _len, iov[iovidx].iov_len - offset);
+        memcpy (to, iov[iovidx].iov_base + offset, copy);
+
+        // Here we need to lock the block or part of block
+        // to prevent block free from the application layer
+        if (sd_set_lock != NULL)
+        {
+            sd_set_lock(iov[iovidx].iov_base + offset, copy);
+        }
+
+        _len -= copy;
+	to += copy;
+        iovidx++;
+    }
+
+#else
+    struct skb_shared_info * pinfo;
+    uint iovidx, fragidx;
+    int _len, _offset;
+
+    // Here we need to skip the <offset> bytes and start processing of
+    // IOVEC buffers from the specified data offset
+
+    pinfo = skb_shinfo(skb);
+    iovidx = 0;
+
+    pinfo->frags[0].page = (struct page*)skb->data;
+    pinfo->frags[0].size = skb->len;
+    pinfo->frags[0].page_offset = 0;
+
+    fragidx= 1; // 0 buffer is reserved for <Eth HEADERS>
+
+    /* Skip over the finished iovecs */
+    while (offset >= iov->iov_len)
+    {
+    	offset -= iov->iov_len;
+    	iov++;
+    }
+    _len = len;
+    _offset = offset;
+    while (_len > 0)
+    {
+        int copy = min_t(unsigned int, _len, iov[iovidx].iov_len - _offset);
+	pinfo->map_fr_num = (fragidx+1); // convert index to number 
+	pinfo->frags[fragidx].page = (struct page*)((u8*)iov[iovidx].iov_base + _offset);
+	pinfo->frags[fragidx].size = copy;
+        pinfo->frags[fragidx].page_offset = 0;
+	pinfo->frags[0].size -= copy;
+
+        // Here we need to lock the block or part of block
+        // to prevent block free from the application layer
+        if (sd_set_lock != NULL)
+        {
+            sd_set_lock((u8*)iov[iovidx].iov_base + _offset, copy);
+        }
+
+	_len -= copy;
+	fragidx++;
+        iovidx ++;
+        _offset =0;
+    }
+#endif
+
+    #if 0
+    printk ("\n\n{CSUM-FLAG ONLY: to:%p, {%p,%d}{%p,%d}{%p,%d}, len:%d", to,
+	iov[0].iov_base, iov[0].iov_len,
+	iov[1].iov_base, iov[1].iov_len,
+	iov[2].iov_base, iov[2].iov_len,
+	len
+    );
+    #endif
+
+    if (skb->ip_summed != CHECKSUM_PARTIAL)
+    {
+	if (csum_partial_fromiovecend(iov, offset, len, &csum) < 0)
+	    return -EFAULT;
+	skb->csum = csum_block_add(skb->csum, csum, odd);
+    }
+
+    return 0;
+}
+EXPORT_SYMBOL(ip_generic_frag_csum);
+
 static inline __wsum
 csum_page(struct page *page, int offset, int copy)
 {
@@ -828,7 +933,6 @@ static int __ip_append_data(struct sock *sk,
 			       mtu-exthdrlen);
 		return -EMSGSIZE;
 	}
-
 	/*
 	 * transhdrlen > 0 means that this is the first fragment and we wish
 	 * it won't be fragmented in the future.
@@ -906,9 +1010,19 @@ alloc_new_skb:
 				alloclen += rt->dst.trailer_len;
 
 			if (transhdrlen) {
-				skb = sock_alloc_send_skb(sk,
-						alloclen + hh_len + 15,
+				if (sk_is_mapped(sk))
+				{
+                                        //uint ud_len;
+					// ud_len - the length of user data
+					//ud_len = datalen - transhdrlen - fraggap;
+					sk->sk_mapped_ptr = NULL;
+					skb = sock_alloc_send_skb(sk, alloclen + hh_len + 15,
 						(flags & MSG_DONTWAIT), &err);
+				} else {
+						skb = sock_alloc_send_skb(sk,
+								alloclen + hh_len + 15,
+								(flags & MSG_DONTWAIT), &err);
+				}
 			} else {
 				skb = NULL;
 				if (atomic_read(&sk->sk_wmem_alloc) <=
@@ -954,6 +1068,7 @@ alloc_new_skb:
 			}
 
 			copy = datalen - transhdrlen - fraggap;
+
 			if (copy > 0 && getfrag(from, data + transhdrlen, offset, copy, fraggap, skb) < 0) {
 				err = -EFAULT;
 				kfree_skb(skb);
@@ -972,7 +1087,6 @@ alloc_new_skb:
 			__skb_queue_tail(queue, skb);
 			continue;
 		}
-
 		if (copy > length)
 			copy = length;
 
@@ -1384,6 +1498,7 @@ int ip_send_skb(struct sk_buff *skb)
 
 	return err;
 }
+EXPORT_SYMBOL(ip_send_skb);
 
 int ip_push_pending_frames(struct sock *sk, struct flowi4 *fl4)
 {
diff --git a/net/ipv4/ip_sockglue.c b/net/ipv4/ip_sockglue.c
index ab0c9ef..dd43303 100644
--- a/net/ipv4/ip_sockglue.c
+++ b/net/ipv4/ip_sockglue.c
@@ -227,6 +227,56 @@ int ip_cmsg_send(struct net *net, struct msghdr *msg, struct ipcm_cookie *ipc)
 	return 0;
 }
 
+int ip_cmsg_send_ex(struct net *net, struct msghdr *msg, struct ipcm_cookie *ipc, struct inet_sock_ancillary_data* psad)
+{
+	int err;
+	struct cmsghdr *cmsg;
+
+	for (cmsg = CMSG_FIRSTHDR(msg); cmsg; cmsg = CMSG_NXTHDR(msg, cmsg)) {
+		if (!CMSG_OK(msg, cmsg))
+		{
+			return -EINVAL;
+		}
+		if (cmsg->cmsg_level != SOL_IP)
+			continue;
+		switch (cmsg->cmsg_type) {
+		case IP_RETOPTS:
+			err = cmsg->cmsg_len - CMSG_ALIGN(sizeof(struct cmsghdr));
+			err = ip_options_get(net, &ipc->opt, CMSG_DATA(cmsg),
+					     err < 40 ? err : 40);
+			if (err)
+			{
+				return err;
+			}
+			
+			break;
+		case IP_PKTINFO:
+		{
+			struct in_pktinfo *info;
+			if (cmsg->cmsg_len != CMSG_LEN(sizeof(struct in_pktinfo)))
+				return -EINVAL;
+			info = (struct in_pktinfo *)CMSG_DATA(cmsg);
+			ipc->oif = info->ipi_ifindex;
+			ipc->addr = info->ipi_spec_dst.s_addr;
+			break;
+		}
+		case IP_TTL:
+		{
+			inet_sad_set_ttl(psad, *(__u8*)CMSG_DATA(cmsg));
+			break;
+		}
+		case IP_TOS:
+		{
+			inet_sad_set_tos(psad, *(__u8*)CMSG_DATA(cmsg));
+			break;
+		}
+		default:
+			printk ("SOCK ANCILLARY DATA UNKN OPT(%d), data: %d\r\n", cmsg->cmsg_type, (uint)*(char*)CMSG_DATA(cmsg));
+			return -EINVAL;
+		}
+	}
+	return 0;
+}
 
 /* Special input handler for packets caught by router alert option.
    They are selected only by protocol field, and then processed likely
diff --git a/net/ipv4/udp.c b/net/ipv4/udp.c
index 198f75b..ec8cbac 100644
--- a/net/ipv4/udp.c
+++ b/net/ipv4/udp.c
@@ -805,6 +805,7 @@ int udp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
 	int (*getfrag)(void *, char *, int, int, int, struct sk_buff *);
 	struct sk_buff *skb;
 	struct ip_options_data opt_copy;
+	struct inet_sock_ancillary_data sad;
 
 	if (len > 0xFFFF)
 		return -EMSGSIZE;
@@ -812,7 +813,6 @@ int udp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
 	/*
 	 *	Check the flags.
 	 */
-
 	if (msg->msg_flags & MSG_OOB) /* Mirror BSD error message compatibility */
 		return -EOPNOTSUPP;
 
@@ -821,6 +821,13 @@ int udp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
 
 	getfrag = is_udplite ? udplite_getfrag : ip_generic_getfrag;
 
+	if (sk_is_mapped(sk))
+	{
+	    getfrag = ip_generic_frag_csum;
+	}
+
+	inet_sad_init(&sad);
+
 	fl4 = &inet->cork.fl.u.ip4;
 	if (up->pending) {
 		/*
@@ -872,12 +879,13 @@ int udp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
 	if (err)
 		return err;
 	if (msg->msg_controllen) {
-		err = ip_cmsg_send(sock_net(sk), msg, &ipc);
+		err = ip_cmsg_send_ex(sock_net(sk), msg, &ipc, &sad);
 		if (err)
 			return err;
 		if (ipc.opt)
 			free = 1;
 		connected = 0;
+		inet_sad_apply_cfg(inet, &sad);
 	}
 	if (!ipc.opt) {
 		struct ip_options_rcu *inet_opt;
@@ -1000,6 +1008,7 @@ do_append_data:
 	release_sock(sk);
 
 out:
+	inet_sad_restore_cfg(inet, &sad);
 	ip_rt_put(rt);
 	if (free)
 		kfree(ipc.opt);
@@ -1156,7 +1165,6 @@ EXPORT_SYMBOL(udp_ioctl);
  * 	This should be easy, if there is something there we
  * 	return it, otherwise we block.
  */
-
 int udp_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
 		size_t len, int noblock, int flags, int *addr_len)
 {
@@ -1168,7 +1176,7 @@ int udp_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
 	int err;
 	int is_udplite = IS_UDPLITE(sk);
 	bool slow;
-
+	u32 non_zbc = 0;
 	/*
 	 *	Check any passed addresses
 	 */
@@ -1201,16 +1209,125 @@ try_again:
 			goto csum_copy_err;
 	}
 
-	if (skb_csum_unnecessary(skb))
-		err = skb_copy_datagram_iovec(skb, sizeof(struct udphdr),
+	if (sk_is_mapped(sk))
+	{
+	    uint i;
+	    u8* rx_addr = ((u8*)skb->data + sizeof(struct udphdr));
+	    extern void* (*sd_vtp)(void* vaddr);
+	    struct sk_buff *frag_iter;
+	    int start_base = skb_headlen(skb);
+	    u8* rx_payload = (u8*)skb->data+start_base;
+	    u32 ud_asked_len = 0;
+	    struct skb_shared_info * pInfo = skb_shinfo(skb);
+
+            // if UDP packet is fragmented and consists of many IP packets
+	    if (pInfo->frag_list != NULL)
+	    {
+                extern void* (*sd_alloc_data)(uint size);
+                extern uint (*sd_get_info)(void* pblock, void** phead);
+                void * phead=NULL;
+                u32 block_size=0;     // The size of the partition block pointed by (skb->data) in bytes
+
+                // this code calculates the real size of UDP packet
+                // according to the UDP format, the length is stored in UDP header
+                // in big-endian format, also it includes a size of UDP header
+                u32 udp_size = ((struct udphdr*)skb->data)->len;
+                udp_size = (((udp_size>>8)&0xFF) | (udp_size&0xFF)<<8);
+
+                // Storage driver should implement this function to get information
+                // about a block allocated in the memory handled by the storage driver
+                if (sd_get_info != NULL)
+                {
+                    block_size = sd_get_info (skb->data, &phead);
+
+		    // pInfo is located at the end of packet payload,
+		    // Here we guarantee that shinfo block is not going
+		    // to be touched by fragmented blocks payload
+                    if ((u32)phead + block_size > (u32)pInfo)
+                    {
+			block_size -= ((u32)phead + block_size) - (u32)pInfo;
+                    }
+
+                    // to remove any headers except UDP
+                    // block_size is equal the size of block in bytes that can be used to keep UDP header and data
+                    block_size -= ((u32)skb->data - (u32)phead);
+                }
+
+		// Here we need to check do we need to allocate new block
+                // to store complete UDP packet or to use a current block and just to add tail(s)
+		if (block_size != 0 && block_size < udp_size)
+                {
+                    #if defined(CONFIG_ZBC_SOCKET_USER_BUFFER_USAGE)
+                    // The user may provide us with a pointer to the personal big buffer
+                    // if this buffer is not NULL, let's use that buffer and do not allocate new one
+                    get_user(rx_payload, (u8**)msg->msg_iov[0].iov_base);
+
+                    // but if user buffer is NULL, the system allocates new one buffer
+                    if (rx_payload == NULL)
+                    #endif
+                    {
+                       if ((rx_payload = (u8*)sd_alloc_data(udp_size)) == NULL)
+                       {
+                          BUG();
+                       }
+                    }
+
+                    memcpy (rx_payload, rx_addr, start_base-sizeof(struct udphdr));
+		    rx_addr = rx_payload;
+		    rx_payload += start_base-sizeof(struct udphdr);
+
+                    non_zbc = 1;
+                }
+
+                // to add the tail(s) to the first(main) UDP packet
+                // to make possible to use linear buffer from user space
+                skb_walk_frags(skb, frag_iter) {
+		   memcpy (rx_payload, frag_iter->data, frag_iter->len);
+                   rx_payload += frag_iter->len;
+                }
+	    }
+
+	    if (sd_vtp != NULL)
+	    {
+	        rx_addr = (void*)sd_vtp((void*)rx_addr);
+	    }
+
+	    // here we need to process the array of IOV msg provided by the
+	    // application layer
+
+	    #if 0
+	    printk ("\nUDP-RECV: msg.num=%d len:%d,ulen:%d {%p,%d}{%p,%d}{%p,%d}\n", msg->msg_iovlen, len, ulen,
+		    msg->msg_iov[0].iov_base, msg->msg_iov[0].iov_len,
+		    msg->msg_iov[1].iov_base, msg->msg_iov[1].iov_len,
+		    msg->msg_iov[2].iov_base, msg->msg_iov[2].iov_len
+	    );
+	    #endif
+
+	    for (i=0;i<msg->msg_iovlen;i++)
+	    {
+		err = put_user(rx_addr, ((void**)msg->msg_iov[i].iov_base));
+		if (err != 0)
+		    break;
+		ud_asked_len+= msg->msg_iov[i].iov_len;
+		rx_addr += msg->msg_iov[i].iov_len;
+	    }
+	    
+	    if (ulen > ud_asked_len)
+		ulen = ud_asked_len;
+	}
+	else
+	{
+	    if (skb_csum_unnecessary(skb))
+		    err = skb_copy_datagram_iovec(skb, sizeof(struct udphdr),
 					      msg->msg_iov, len);
-	else {
-		err = skb_copy_and_csum_datagram_iovec(skb,
+	    else {
+		    err = skb_copy_and_csum_datagram_iovec(skb,
 						       sizeof(struct udphdr),
 						       msg->msg_iov);
 
-		if (err == -EINVAL)
+		    if (err == -EINVAL)
 			goto csum_copy_err;
+	    }
 	}
 
 	if (err)
@@ -1237,6 +1354,16 @@ try_again:
 		err = ulen;
 
 out_free:
+
+	// to prevent de-allocating of payload buffer
+	// that buffer will be used by the application
+	// and application is responcible to free that memory
+	// to be consistence with ZBC
+	if (sk_is_mapped(sk) && (non_zbc == 0))
+	{
+	    skb->head = NULL;
+	}
+
 	skb_free_datagram_locked(sk, skb);
 out:
 	return err;
diff --git a/net/ipv4/xfrm4_output.c b/net/ipv4/xfrm4_output.c
index 327a617..45679a2 100644
--- a/net/ipv4/xfrm4_output.c
+++ b/net/ipv4/xfrm4_output.c
@@ -99,3 +99,4 @@ int xfrm4_output(struct sk_buff *skb)
 			    x->outer_mode->afinfo->output_finish,
 			    !(IPCB(skb)->flags & IPSKB_REROUTED));
 }
+EXPORT_SYMBOL(xfrm4_output);
diff --git a/net/socket.c b/net/socket.c
index 1b0f0fc..3d2158a 100644
--- a/net/socket.c
+++ b/net/socket.c
@@ -1179,6 +1179,13 @@ int __sock_create(struct net *net, int family, int type, int protocol,
 	int err;
 	struct socket *sock;
 	const struct net_proto_family *pf;
+	uint mapped = 0;
+	if (type == (SOCK_DGRAM | (SOCK_MAPPED)) && (family == AF_INET))
+	{
+		mapped = 1;
+		type &= ~(SOCK_MAPPED);
+		printk ("KERNEL MAPPED SOCKED IS CREATED, type=%d\r\n", type);
+	}
 
 	/*
 	 *      Check protocol is in range
@@ -1268,6 +1275,10 @@ int __sock_create(struct net *net, int family, int type, int protocol,
 	err = security_socket_post_create(sock, family, type, protocol, kern);
 	if (err)
 		goto out_sock_release;
+
+	if (sock->sk != NULL)
+		sock->sk->sk_mapped = mapped;
+
 	*res = sock;
 
 	return 0;
@@ -1304,6 +1315,15 @@ SYSCALL_DEFINE3(socket, int, family, int, type, int, protocol)
 	int retval;
 	struct socket *sock;
 	int flags;
+        uint mapped = 0;
+
+        if (type == (SOCK_DGRAM | (SOCK_MAPPED)) && (family == AF_INET))
+        {
+            mapped = 1;
+            type &= ~(SOCK_MAPPED);
+
+            printk ("\n\nMAPPED SOCKED IS CREATED, type=%d\r\n", type);
+        }
 
 	/* Check the SOCK_* constants for consistency.  */
 	BUILD_BUG_ON(SOCK_CLOEXEC != O_CLOEXEC);
@@ -1327,6 +1347,8 @@ SYSCALL_DEFINE3(socket, int, family, int, type, int, protocol)
 	if (retval < 0)
 		goto out_release;
 
+        sock->sk->sk_mapped = mapped ? 1 : 0;
+
 out:
 	/* It may be already another descriptor 8) Not kernel problem. */
 	return retval;
@@ -1530,6 +1552,12 @@ SYSCALL_DEFINE4(accept4, int, fd, struct sockaddr __user *, upeer_sockaddr,
 	if (err < 0)
 		goto out_fd;
 
+	if (newsock->sk != NULL)
+	{
+	    newsock->sk->sk_mapped = 0;
+	    newsock->sk->sk_mapped_ptr=0;
+	}
+
 	if (upeer_sockaddr) {
 		if (newsock->ops->getname(newsock, (struct sockaddr *)&address,
 					  &len, 2) < 0) {
@@ -1664,6 +1692,10 @@ SYSCALL_DEFINE3(getpeername, int, fd, struct sockaddr __user *, usockaddr,
 	return err;
 }
 
+void * (*sd_ptv)(void* paddr)=NULL;
+void * (*sd_vtp)(void* vaddr)=NULL;
+EXPORT_SYMBOL(sd_ptv);
+EXPORT_SYMBOL(sd_vtp);
 /*
  *	Send a datagram to a given address. We move the address into kernel
  *	space and check the user space data area is readable before invoking
@@ -1687,6 +1719,28 @@ SYSCALL_DEFINE6(sendto, int, fd, void __user *, buff, size_t, len,
 	if (!sock)
 		goto out;
 
+        if (sk_is_mapped(sock->sk))
+        {
+        // here we know that this socket
+        // is designed to be used with already mapped
+        // to the kernel space memory, so copy operations
+        // (user->kernel) are not needed and we need just to
+        // convert physical address provided by the application to kernel space virtual address
+
+	    //printk("\nThe address:%x", buff);
+
+	    if (sd_ptv != NULL)
+		buff = sd_ptv((void*)buff);
+
+	    //printk (" converted to %x\n", buff);
+
+	    //printk("To be PTR aligned sock:%d, (val:%d), sk:%x ", fd, sock->sk->sk_mapped, (uint)sock->sk);
+	    //printk("(ops:%x  f:%x  sk:%x, type:%d/proto:%d\n",(uint)sock->ops, (uint)sock->file, (uint)sock->sk, sock->sk->sk_type, sock->sk->sk_protocol);
+	    //return -1;
+
+           //buff = (void*)buff;
+        }
+
 	iov.iov_base = buff;
 	iov.iov_len = len;
 	msg.msg_name = NULL;
@@ -1910,16 +1964,18 @@ static int __sys_sendmsg(struct socket *sock, struct msghdr __user *msg,
 		if (!iov)
 			goto out;
 	}
-
 	/* This will also move the address data into kernel space */
 	if (MSG_CMSG_COMPAT & flags) {
 		err = verify_compat_iovec(msg_sys, iov,
 					  (struct sockaddr *)&address,
 					  VERIFY_READ);
 	} else
+	{
 		err = verify_iovec(msg_sys, iov,
 				   (struct sockaddr *)&address,
 				   VERIFY_READ);
+	}
+
 	if (err < 0)
 		goto out_freeiov;
 	total_len = err;
@@ -1972,6 +2028,22 @@ static int __sys_sendmsg(struct socket *sock, struct msghdr __user *msg,
 		err = sock_sendmsg_nosec(sock, msg_sys, total_len);
 		goto out_freectl;
 	}
+
+	if (sk_is_mapped(sock->sk))
+	{
+	    //printk ("Mapped socket address convertation, iovlen=%d\n", msg_sys->msg_iovlen);
+	    if (sd_ptv != NULL)
+	    {
+		int len;
+		struct iovec * _pio_;
+
+		for (len = 0; len < msg_sys->msg_iovlen; len++)
+		{
+		    _pio_ = (msg_sys->msg_iov+len);
+		    _pio_->iov_base = sd_ptv(_pio_->iov_base);
+		}
+	    }
+	}
 	err = sock_sendmsg(sock, msg_sys, total_len);
 	/*
 	 * If this is sendmmsg() and sending to current destination address was
@@ -1983,7 +2055,6 @@ static int __sys_sendmsg(struct socket *sock, struct msghdr __user *msg,
 			memcpy(&used_address->name, msg_sys->msg_name,
 			       used_address->name_len);
 	}
-
 out_freectl:
 	if (ctl_buf != ctl)
 		sock_kfree_s(sock->sk, ctl_buf, ctl_len);
-- 
1.7.9.5

